# Lecture 7. Hypothesis Testing (Continued): Levels and P-values

There are 7 topics and 5 exercises.

## 1. Review of Parametric Hypothesis Testing

### Hypothesis testing

You have i.i.d. data $X_1, ..., X_n$  generated by a distribution $\mathbf{P}_\theta$ for some unknown parameter $\theta \in \R$. You would like to test some null hypothesis $H_0$ against an alternative hypothesis $H_1$. What is the purpose of hypothesis testing?

**Answer**: To decide with a quantified probability of error whether or not $\theta$ lies in a certain region of the parameter set.

**Solution**: The null and the alternative hypotheses describe disjoint subsets of the parameter set. A statistical test is a data dependent rule that decides whether or not to reject the statement (hypothesis) that the unknown true parameter belongs to the subset described by $H_0$ or fail to reject it. In designing a statistical test, we must quantify how likely it is that the observed sample is generated by a probability distribution $\mathbf{P}_\theta$ for $\theta$ in $H_0$.

### Rejection region

You have samples $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Ber}(p^*)$ for some true parameter $p^* \in (0,1)$. Let $(\{ 0,1\} , \{ \mathbf{P}_ p\} _{p \in (0,1)})$ denote the associated statistical model, where $\mathbf{P}_p = \mathsf{Ber}(p).$

You conduct a hypothesis test between

* A null hypothesis $H_0: p^* \in \Theta _0$ and
* An alternative hypothesis $H_1: p^* \in \Theta _1$,

where $\Theta _0, \Theta _1 \subset (0,1)$ and $\Theta_0$ and $\Theta_1$ are disjoint.

You construct a statistical test
$$
\psi :\{ 0,1\} ^ n \to \{ 0, 1\}
$$
which takes as input the sample $(X_1, ...,X_n)$. If $\psi(X_1, ...,X_n)=1$, you will reject the null $H_0$ in favor of the alternative $H_1$, and otherwise you will fail to reject the null.

Recall that the rejection region $R_\psi$ describes which outcomes $(x_1, ..., x_n)$ will result in $\psi(x_1, ..., x_n)=1$ and, hence, rejection of the null.

The rejection region is a subset of ...

a. $(\Theta_0)^n$ where $\Theta_0$ defines the null hypothesis $H_0$ in the parameter space $\Theta$.

b. $(\Theta_1)^n$ where $\Theta_1$ defines the alternative hypothesis $H_1$ in the parameter space $\Theta$.

c. $(\Theta)^n$ where $\Theta$ is the parameter space

d. $E^n$ where $E$ is the sample space of $X_i$

**Answer**: d

**Solution**: The rejection region is by definition the set of all observed outcomes for which $H_0$ will be rejected by the test $ \psi =\mathbf{1}\left((X_1,\ldots ,X_ n)\in R_\psi \right)$. It is a subset of $E^n$, where $E$ is the sample space of $X_i$.

### Type 1/2 error and power

Setup as above, let $\alpha_\psi$ and $\beta_\psi$ denote the type 1 error and type 2 error, respectively. Determine which type of mathematical object each of the following is.

1. Rejection region
2. Type 1 or type 2 error
3. Level
4. Power

a. A number

b. A set

c. A function

**Answer**: $1-b,\quad 2-c,\quad 3-a,\quad 4-a$

**Solution**: 

Recall the definition of each object in the context of the statistical model $(\{ 0,1\} , \{ \mathbf{P}_ p\} _{p \in (0,1)})$

1) The rejection region is defined to be 
$$
R_\psi := \{  \mathbf{x} \in \{ 0,1\} ^ n: \,  \psi (\mathbf{x}) = 1 \} ,
$$
So this is a **set**.

2) Type 1 and 2 errors are defined to be 
$$
\begin{aligned}
\alpha _\psi : \Theta _0 &\rightarrow [0,1]\\
p &\mapsto P_p(\psi = 1)\\
\beta _\psi : \Theta _1 &\rightarrow [0,1]\\
p &\mapsto P_p(\psi = 0)\\
\end{aligned}
$$
So this is a **function** of $p$.

3) The power $\pi_\psi$ is defined as
$$
\pi _\psi := \inf _{p \in \Theta _1} (1 - \beta _\psi (p)).
$$
This greatest lower bound of $(1 - \beta _\psi (p))$ over a set of $p$ values is a **number**.

### Type 1 vs. Type 2 Error

Recall type 1 and type 2 error

| Test \ Truth | $H_0$  | $H_1$  |
| ------------ | ------ | ------ |
| $\psi = 0$   | \      | Type 2 |
| $\psi = 1 $  | Type 1 | \      |

Setup as above, you would like to hypothesis test between two simple hypotheses:
$$
H_0:p^* \in \Theta _0=\{ 1/2\}\\
H_1:p^* \in \Theta _1=\{ 3/4\}\\
$$
That is, each of the regions defined by the null and alternative hypotheses consists of a single point in the parameter space $\Theta = [0,1].$

You constructed a statistical test $\psi$, and let $\alpha_\psi$ and $\beta_\psi$ denote the type 1 error and type 2 error, respectively, associated to this test. What do $\alpha_\psi(1/2)$  and $\beta_\psi(3/4)$ represent?

**Answer**: 

$\alpha_\psi(1/2)$: The probability that we **reject** $p^* = 1/2$ in favor of $p^* = 3/4$ even though in fact $p^* =  1/2$.

$\beta_\psi(3/4)$: The probability that we **fail to reject** $p^* = 1/2$ in favor of $p^* = 3/4$ even though in fact $p^* = 3/4$.

**Solution**:

1) If $\psi = 1$, then we would reject the null-hypothesis $p^* \in \Theta_0 = \{1/2\}$. Therefore $\alpha_\psi(1/2)=\mathbf{P}_{1/2}(\psi =1)$ is the probability of rejecting $p^* \in \Theta_0 = \{1/2\}$ in favor of $p^* \in \Theta_1 = {3/4}$ even when in fact $p^* \in \Theta_0 = \{1/2\}$.

2) If $\psi = 0$, then we would fail to reject the null-hypothesis $p^* \in \Theta_0 = \{1/2\}$ in favor of the alternative hypothesis $p^* \in \Theta_0 = \{3/4\}$. Therefore $\beta_\psi(3/4)=\mathbf{P}_{3/4}(\psi =0)$ is the probability of not rejecting $H_0: p^* \in \Theta_0 = \{1/2\}$ even when in fact $p^* \in \Theta_0 = \{3/4\}$.

### Interpreting the level

What is a correct interpretation of the (smallest) **level** of a test?

**Answer**: 

* The level of a test is an **upper bound** on the type 1 error. 
* The level of a test gives an **upper bound** on the **worst**-case probability of making an error under the null hypothesis.

**Solution**: 

Recall the definition of the level of a test $\psi$. Let 
$$
\begin{aligned}
\alpha _\psi : \Theta &\rightarrow [0,1]\\
\theta& \mapsto P_\theta(\psi = 1)
\end{aligned}
$$
denote the type 1 error. Then the level of $\psi$ is any real number $\alpha$ such that
$$
\alpha _\psi (\theta ) \leq \alpha , \quad \text {for all} \,  \,  \theta \in \Theta _0.
$$

### Test Statistics

Recall the statistical experiment in which you flip a coin $n$ times to decide the coin is fair.

You model the coin flip as $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Ber}(p)$ where $p$ is an unknown parameter, and formulate the hypothesis:
$$
H_0: p = 0.5\\
H_1:p \neq 0.5
$$
And design the test $\psi$ using the statistic $T_n$:
$$
\psi_n = \mathbf{1}(T_n > C)\\
\text{where } T_n = \sqrt{n}{\left\vert \overline{X}_n - 0.5 \right\vert \over \sqrt{0.5(1-0.5)}}
$$
where the number $C$ is the threshold. Note that absolute value in $T_n$ for this two sided test.

If it is true that $p = 1/2$, which of the following are true about $T_n$?

a. $T_n$ Is a consistent estimator of the true parameter $p = 1/2$.

b. $\lim _{n \to \infty } T_ n \xrightarrow [n \to \infty ]{(d)} |Z|$ where $Z \sim \mathcal{N}(0,1)$ is a standard Gaussian.

c. $T_n$ Involves a shift and rescaling of the sample average so that as $n \rightarrow \infty$, this random variable will converge in distribution.

d. The limiting distribution of $T_n$ can be understood using computational software or tables.

**Answer**: bcd

**Solution**:

a. The first choice is incorrect. The statistic $T_n$ does NOT converge to a real number as $n \rightarrow \infty$. By the CLT, $T_n$ converges in **distribution**, meaning that asymptotically, it is a **random variable**.

**Remark**: This example illustrates one of the main strategies involved in hypothesis testing. Namely, we want to work with a test statistic, that, asymptotically, tends to a distribution that we can easily work with. In many cases, this will involve shifting and rescaling the sample mean so that the CLT applies and we can just work with a standard Gaussian $\mathcal{N}(0,1)$.

### Design a test to have a given asymptotic level

Setup as above, recall that the test $\psi$ has asymptotic level $\alpha$ if
$$
\lim _{n \to \infty } P_{1/2}( \psi = 1) \leq \alpha .
$$
This is a graph of the standard normal distribution $\mathcal{N}(0,1)$, along with the lines $Z = \pm C$. The letters $A,B$ denote the areas of the corresponding shaded regions:
$$
\begin{aligned}
A &= \mathbf{P}(Z < -C) = \mathbf{P}(Z > C) \quad \text{by symmetry}\\
B &= \mathbf{P}(-C \leq Z \leq C)
\end{aligned}
$$
where $\mathbf{P}$ is the probability distribution of $\mathbf{N}(0,1)$.

![images_u2s5_normalTn_areas](../assets/images/images_u2s5_normalTn_areas.svg)

1. What is the smallest $C$ such that the test $\psi(T_n > C)$ has asymptotic level $\alpha$? 

2. As a function of $\alpha$, what is $C_\alpha$?

3. Let the rejection region for the test $\psi(T_n > C_\alpha)$ be 
   $$
   R_\alpha = \{(X_1, ...,X_n) \in \{0,1\}^n: \overline{X}_n < L \cup \overline{X}_n > R \}.
   $$
   What are $L$ and $R$?

**Answer**: 

$1. \quad 2A;\\2. \quad q_{\alpha/2};\\3. \quad L = 0.5-q_{\alpha /2}\frac{\sqrt{0.5(1 - 0.5)}}{\sqrt{n}}; \quad R = 0.5+q_{\alpha /2}\frac{\sqrt{0.5(1 - 0.5)}}{\sqrt{n}}$

**Solution**:

1) By CLT, if $\mathbb{E}[X] = p^* = 0.5$, then
$$
\sqrt{n}\frac{\overline{X}_ n - 0.5}{\sqrt{0.5(1 - 0.5)}} \xrightarrow [n \to \infty ]{(d)} N(0,1).
$$
Let $\mathbf{P}_{1/2} = \text {Ber}(1/2)$ for notational convenience. Then for the test statistics
$$
T_n = \left|\sqrt{n}\frac{\overline{X}_ n - 0.5}{\sqrt{0.5(1 - 0.5)}}\right|,
$$
We have (by observing the graph)
$$
\mathbf{P}_{1/2}\left(T_ n>C\right)\xrightarrow [n \to \infty ]{}A + A = 2A
$$
where $2A$ are the total area of the shaded regions under the graph of the normal distribution.

Since $H_0$ is defined by a single value $p=1/2$, the asymptotic level is equal to the asymptotic type 1 error at $p=1/2$, which is $\mathbf{P}_{1/2}(T_n > C)$. Therefore, given a desired asymptotic level $\alpha$, choosing a threshold $C_\alpha$ such that
$$
P(Z<-C_\alpha )+P(Z>C_\alpha )\, =\, A+A=2A\qquad Z\sim \mathcal{N}(0,1)
$$
will result in a test $\psi = \mathbf{1}(T_n > C_\alpha)$ that has asymptotic level $\alpha$. Furthermore, for any threshold $C < C_\alpha$ will yield a larger asymptotic type 1 error, as shown in the figure below.

![images_u2s5_normalTn_smallerC](../assets/images/images_u2s5_normalTn_smallerC.svg)

For $C < C_\alpha$, the type 1 error for $\psi = \mathbf{1}(T_n > C)$ (shaded blue) is larger than the type 1 error for $\psi = \mathbf{1}(T_n > C_\alpha)$ (shaded orange).

2) Since $\alpha =P(Z<-C_\alpha )+P(Z>C_\alpha )=2 P(Z>C_\alpha )$ by symmetry, we have $C_\alpha = q_{\alpha/2}$.

3) The rejection region of $\psi = \mathbf{1}(T_n > q_{\alpha/2})$ is defined by
$$
T_ n\, =\, \left|\sqrt{n}\frac{\overline{X}_ n - 0.5}{\sqrt{0.5(1 - 0.5)}}\right| > q_{\alpha/2}\\
\implies \overline{X}_ n\, <\,  0.5-q_{\alpha /2}\frac{\sqrt{0.5(1 - 0.5)}}{\sqrt{n}} \cup \overline{X}_ n\, >\, 0.5+q_{\alpha /2}\frac{\sqrt{0.5(1 - 0.5)}}{\sqrt{n}}.
$$

### Rejection region

1. **True or False:** The rejection region of a test $\psi$ depends on the value of the true unknown parameter $\theta^*$ explicitly, in the sense that we need to specify the value of $\theta^*$ in order to compute the rejection region. 

   False. The rejection region cannot depend on the parameter value $\theta^*$ because it is **unknown**. Instead, we use the **sample** (and **implicitly** the true unknown distribution $\mathbf{P}_{\theta^*}$ of the data) to design the test.

2. **True or False:** To define a statistical test $\psi$, it is enough to define the rejection region $R_\psi$.

   True. A test is by definition an indicator function of its rejection region.
   $$
   \psi = \mathbf{1}((X_1,..., X_n)\in R_\psi)
   $$
   Hence, yes, to define a test, all that is needed is to define its rejection region.

## 2. Worked Example: A Two-Sided Test Associated to a Bernoulli Experiment

* Let $X_1, ...X_n \stackrel{iid}{\sim} \mathsf{Ber}(p),$ for some unknown $p \in (0,1)$.

* We want to test
  $$
  H_0: p = 1/2 \text{ vs. }H_1: p \neq 1/2
  $$
  With asymptotic level $\alpha \in (0,1).$

* Let the test statistic to be $T_n  = \left\vert  \sqrt{n} {\hat{p}_n - 0.5 \over\sqrt{0.5(1-0.5)} }\right\vert$, where $\hat{p}_n = \overline{X}_n$

  We know that
  $$
  \sqrt{n} {\hat{p}_n - p \over\sqrt{p(1-p)} } \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
  $$
  We do the standardization in order to calculate the probability: $\mathbb{P}_\theta [\psi = 1]$, where $\theta = \Theta_0$.

* The probability of reject the null when $p$ is true is
  $$
  \mathbb{P}_p \left[ \sqrt{n} {\hat{p}_n - p \over\sqrt{p(1-p)} } \in \mathcal{R} \right] = \alpha(p)
  $$
  Since $p \in \Theta_0 = \{1/2\}$,
  $$
  \mathbb{P}_{1/2} \left[ \sqrt{n} {\hat{p}_n - 0.5 \over\sqrt{0.5(1-0.5)} } \in \mathcal{R} \right] = \alpha(1/2)
  $$

* We are going to reject ($\psi = 1$) if $ \left\vert  \sqrt{n} {\hat{p}_n - 0.5 \over\sqrt{0.5(1-0.5)} }\right\vert> C$. In other words,
  $$
  \mathbb{P}_{1/2}\left[ \sqrt{n} {|\hat{p}_n - 0.5| \over 0.5 } > C\right] \xrightarrow[n \rightarrow \infty]{} \alpha
  $$
  We would replace $C$ with $q_{\alpha/2}$, so
  $$
  \mathbb{P}_{1/2}\left[ \sqrt{n} {|\hat{p}_n - 0.5| \over 0.5 } > q_{\alpha/2}\right] \xrightarrow[n \rightarrow \infty]{} \alpha
  $$
  In this problem, if $H_0$ is true, then by CLT,
  $$
  \mathbb{P}[T_n > q_{\alpha/2}] \xrightarrow[n\rightarrow \infty]{} 0.05
  $$
  So the test $\psi$ with reject region
  $$
  R_\psi = \{ \sqrt{n} |\bar{X}_n - 0.5| \times 2 > q_{\alpha/2}\}
  $$
  has asymptotic level $\alpha.$

* Let $\psi_\alpha = \mathbf{1}\{T_n > q_{\alpha/2}\}$

## 3. Worked Examples: Two-sided and one-sided test

1. **Fair coin**: Recall Lecture 6 Example: Test the fairness of a coin: Our data gives $\sqrt{n}{\bar{X}_n - 0.5 \over\sqrt{0.5(1-0.5)} } \approx -0.77$.

   For $\alpha = 5\%, \, q_{\alpha/2} = 1.96$, $H_0$ is not rejected at the asymptotic level $5\%$ by the test $\psi_{5\%}$, since $0.77 < 1.96 $.

2. **News on YouTube**: 

   * $H_0: p \geq 0.33$ vs. $H_1: p < 0.33$. This is a one-sided test.

   * We reject if
     $$
     \sqrt{n} {\hat{p}_n - p \over \sqrt{p(1-p)} } < C
     $$
     Since we want to test whether $\hat{p} $ is small enough. 

     So we want
     $$
     \mathbb{P}_p \left[ \sqrt{n} {\hat{p} - p \over \sqrt{p(1-p)} } < C \right] \xrightarrow[n \rightarrow \infty]{} \alpha(p)\\
     \sup_{p \in \Theta_0} \alpha(p) = \alpha
     $$
     We can replace $C$ with $-q_\alpha$ and re-write it as 
     $$
     \mathbb{P}_p \left[ \sqrt{n} {\hat{p} - p \over \sqrt{p(1-p)} } < -q_\alpha \right] \xrightarrow[n \rightarrow \infty]{} \alpha
     $$

   * Common sense: Reject the null hypothesis if $\hat{p}_n = \overline{X}_n < \lambda$, for $\lambda$ to be chosen later.
     $$
     \sup_{p \leq 0.33} \mathbb{P}_p\left[ \overline{X}_n < \lambda \right] \xrightarrow[n \rightarrow \infty]{} \alpha\\
     
     \sup_{p \leq 0.33} \mathbb{P}_p\left[ \sqrt{n} {\overline{X}_n - p \over \sqrt{p (1 - p)}} < \sqrt{n} {\lambda - p \over \sqrt{p (1 - p)}} \right] \xrightarrow[n \rightarrow \infty]{} \alpha
     $$
     where $\sqrt{n} {\overline{X}_n - p \over \sqrt{p (1 - p)}} \sim \mathcal{N}(0,1)$ and $\sqrt{n} {\lambda - p \over \sqrt{p (1 - p)}} = C$.

   * But what value for $p \in \Theta_0 =  [0.33,1]$ should we choose?

     Let's pick $p_o$ that for any $p$ in $H_0$, the probability is asymptotically less than $\alpha$:
     $$
     \mathbb{P}_p \left[ \sqrt{n} {\hat{p} - p_o \over \sqrt{p_o(1-p_o)} } < -q_\alpha \right] = f(p,p_o)  \leq \alpha, \quad \forall p \geq 0.33
     $$
     In other words, we want to choose $p_o$ such that
     $$
     f_n(p_o) = \sup_{p} \mathbb{P}_p\left[ \sqrt{n} {\overline{X}_n - p_o \over \sqrt{p_o (1 - p_o)}} < -q_{\alpha} \right] \xrightarrow[n \rightarrow \infty]{} \alpha
     $$

   * Type 1 error is the function $p \mapsto \mathbb{P}_p[\psi = 1]$. To control the level we need to find the $p$ that maximizes it over $\Theta_0$

     $\rightarrow$ No need for computations it's clearly $p = 0.33$, which is the one that is at the boundary, **maximizes type 1 error**.

     $H_0$ is not reject at the asymptotic level $5\%$ by the test $\psi_{5\%}$, since with $q_\alpha = 1.645$, $\sqrt{n}{\hat{p} -  0.33\over\sqrt{0.33(1-0.33)} } =  -1.50 > -1.645$.

> #### Exercise 39
>
> If the test $\psi = \mathbf{1}(T_n > q_{\alpha/2})$ is designed to have asymptotic level $5\%$ and the null hypothesis is rejected, what if the asymptotic level is $10\%$?
>
> **Answer**: the null hypothesis is also rejected.
>
> **Solution**: A test with a smaller (asymptotic) level is more “stringent" than a test of the same form with a greater (asymptotic) level.

> #### Exercise 40 
>
> Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Ber}(p^*)$ for some true parameter $p^* \in (0,1)$, and let $(\{ 0,1\} , \{ P_ p\} _{p \in (0,1)})$ denote the associated statistical model where $P_ p = \text {Ber}(p)$.
>
> Suppose the null hypothesis is $H_0: p^* \leq 1/2$ and the alternative hypothesis is $H_1: p^* > 1/2$. Let $\psi$ continue to denote the statistical test we will use. (Recall that a test takes value either $0$ or $1$.) Usually it is of the form $\mathbf{1}(T_n>C)$ where $C$ is a threshold to be specified and $T_n$ is known as a test statistic. 
>
> Consider the following graph of this hypothesis testing set-up.
>
> ![images_u2s4_hypotest_graph](../assets/images/images_u2s4_hypotest_graph.svg)
>
> * Continuous curve on the left: type 1 error, $\alpha_\psi$, graphed as a function of $\theta$.
> * Continuous curve on the right: type 2 error, $\beta_\psi$, graphed as a function of $\theta$.
> * Horizontal axis: the parameter space $\Theta = (0,1)$.
>
> 1. Which letter indicates $\Theta_0$ the region defined by the null hypothesis?
>
> 2. Which letter indicates $\Theta_1$ the region defined by the alternative hypothesis?
>
> 3. Let $p \in (0,1)$ denote the point where the power is attained, i.e., the point where
>    $$
>    \pi _\psi = \inf _{\Theta _1} (1 - \beta _\psi (p)).
>    $$
>    Which letter indicates the ordered pair $(p, \pi_\psi)$?
>
> 4. Which of the following are levels of $\psi$?
>
>    a. $5\%$
>
>    b. $10\%$
>
>    c. $20\%$
>
> **Answer**:
>
> $1. \quad B; \\2. \quad C; \\3. \quad A.\\4. \quad c.$
>
> **Solution**:
>
> 1) We are given that $\Theta_0:p\leq 1/2$, then the interval $(0,1/2]$ defines $\Theta_0$.
>
> 2) We are given that $\Theta_1:p > 1/2$, then the interval $(1/2,1]$ defines $\Theta_1$.
>
> 3) The continuous curve on the right, which graphs $\beta_\psi$, attains its maximum at $p = 1/2$, and this maximum is given by $\beta_\psi(1/2) = 0.8$. Therefore,
> $$
> \pi _\psi = \inf _{p \in (0,1)} (1 - \beta _\psi (p)) =1 - 0.8 = 0.2,
> $$
> 4) The level of $\psi$ is given by any real $\alpha \in \R$ such that
> $$
> \alpha _{\psi }(p) \leq \alpha , \quad \text {for all} \,  \,  p \in \Theta _0 = (0,1/2]
> $$
> That is, the type 1 error is uniformly bounded above by $\alpha$. According to the graph, the continuous curve on the left curve stays below $0.2$.

## 4. Behavior of Type 1 and Type 2 Errors for One-Sided Tests

**Setup**:

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } X\sim \mathbf{P}_{\mu }$ where $\mu \in \R$ is the true unknown mean of $X$, and the variance $\sigma^2$ of $X$ is fixed. The associated statistical model is $\left( E, \{\mathbf{P}_\mu\}_{\mu \in \R} \right)$ where $\mathbb{E}$ is the sample space of $X$.

We conduct a one-sided hypothesis test with the following hypotheses:
$$
H_0: \mu \leq \mu _0\qquad \Longleftrightarrow \Theta _0\, =\, (-\infty , \mu _0]\\
H_1: \mu > \mu _0\qquad \Longleftrightarrow \Theta _1\, =\, (\mu _0,+\infty )
$$
Note the boundary between $\Theta_0$ and $\Theta_1$. You use the statistical test:
$$
\psi_n = \mathbf{1}(T_n > q_\alpha)\\
\text{where }T_n = \sqrt{n}\frac{\overline{X}_ n - \mu _0}{\sigma }.
$$
The shaded region corresponds the type 1 error $\alpha_{\psi_n}(\mu_0)$ for large $n$.

![lec7-4-type1error](../assets/images/lec7-4-type1error.png)

At $\mu = \mu_0$ and when $n$ is large, $T_n \sim \mathcal{N}(0,1)$ by the CLT. Therefore, when $n$ is large, the type 1 error $\mathbf{P}_{\mu _0}\left(T_ n>q_{\alpha }\right)$ is geometrically approximately the area of the "right tail" of standard normal distribution defined by the line $T_n = q_\alpha$.

Alternatively, since
$$
T_ n\, =\,  \sqrt{n}\frac{\overline{X}_ n - \mu _0}{\sigma } \, >\, q_{\alpha } \iff   \overline{X}_ n>\, \mu _0+q_{\alpha }\frac{\sigma }{\sqrt{n}},
$$
we have
$$
\mathbf{P}_{\mu _0}\left(T_ n>q_{\alpha }\right) = \mathbf{P}_{\mu _0}\left(\overline{X}_ n>\mu _0+q_{\alpha }\frac{\sigma }{\sqrt{n}}\right),
$$
which is the area of the "right tail" of the distribution of $\overline{X}_n$ to the right of $\overline{X}_ n=\mu _0+q_{\alpha }\frac{\sigma }{\sqrt{n}}$. By CLT, for $n$ large, the distribution of $\overline{X}_n$ is approximately Gaussian, with mean $\mathbb{E}[X]$ and variance ${\sigma^2 \over n }$.

The graph of the distribution of $\overline{X}_n$ for $\mu < \mu_0$ is as follows. It is a single shift without rescaling, since the variance of $X$ is fixed at $\sigma$.

![images_u2s5_errortrend_Xnbar_shiftleft](../assets/images/images_u2s5_errortrend_Xnbar_shiftleft.svg)

As $\mu$ decreases from $\mu_0$ (i.e. moving away from the boundary of $\Theta_0$ and $\Theta_1$), the type 1 error $\alpha_{\psi_n}(\mu)$ decreases. The threshold is
$$
\tau _{n,\alpha }=\mu _0+q_{\alpha }\frac{\sigma }{\sqrt{n}}
$$
Since the type 1 error $\alpha _{\psi _ n}(\mu )=\mathbf{P}_{\mu }(\overline{X}_ n>\tau )$ is the area of the tail to the right of $\tau$, we see that the type 1 error continues to decrease as $\mu$ (and the distribution of $\overline{X}_n$) moves to the left.

![images_u2s5_errortrend_Xnbar_shiftleft_witherror](../assets/images/images_u2s5_errortrend_Xnbar_shiftleft_witherror.svg)

**Remark**: The type 2 error $\beta _{\psi _ n}(\mu )=1-\mathbf{P}_{\mu }(\overline{X}_ n>\tau )$ decreases as $\mu$ increases from $\mu_0$.

## 5. The p-value of a Statistical Test

* Definition of p-value:

  The **(asymptotic) p-value** of a test $\psi_\alpha$ is the smallest (asymptotic) level $\alpha$ at which $\psi_\alpha$ rejects $H_0$. It is random, it depends on the sample.

* Golden rule

  P-value $\leq \alpha \iff H_0$ is rejected by $\psi_\alpha$, at the (asymptotic) level $\alpha$.

  The smaller the p-value, the more confidently one can reject $H_0$.

## 6. Worked Example: Find the P-value

**Setup:**

We have sample $X_1, \ldots , X_{n} \stackrel{iid}{\sim } \text {Ber}(p^*)$ and associated statistical model $(\{ 0,1\} , \{  \text {Ber}(p) \} _{p \in (0,1)})$. The null and alternative hypotheses are 
$$
H_0: p^* = 1/2\\H_1: p^* \neq 1/2
$$
Let
$$
T_ n = \sqrt{n} \left|  \frac{\left( \overline{X}_ n - 0.5\right)}{\sqrt{0.5(1 - 0.5)}}  \right|
$$
Denote the test statistic and let
$$
\psi = \mathbf{1}\left( T_ n \geq q_{\eta /2} \right).
$$
Denote the test where $q_\eta$ is the $1 - \eta$ quantile of a standard Gaussian.

**Questions:**

1. In one run of the experiment, you obtain the data set consisting of $80$ Heads, and evaluated test statistics $T_n$ at this data set to be $T_n = 2.82842$.

   What is the asymptotic p-value for this data set?

2. In another run of the experiment, you obtain the data set consisting of $106$ Heads, and evaluated test statistics $T_n$ at this data set to be $T_n = 0.8485$.

   What is the asymptotic p-value for this second data set?

3. Now let's generalize our findings above. In this two-sided test, as the test statistic $T_n$ increases, the p-value ...

   a. increases

   b. decreases

**Answer**:

$1.\quad 0.0047; \\2. \quad 0.3962;\\3. \quad b.$

**Solution**:

1) In Lec 6 Exercise 35, we observed that $T_n = |-2.82842|$. For notational convenience, let $P_{1/2} = \mathsf{Ber}(1/2)$. Recall that the asymptotic level is given by
$$
\lim _{n \to \infty } P_{1/2}(T_ n \geq q_{\eta /2}) = P(|Z| > q_{\eta /2}) = \eta
$$
where $Z \sim \mathcal{N}(0,1)$. Hence, we need to find the smallest level $\eta$ such that $\psi$ rejects, i.e., such that
$$
T_ n \geq |-2.82842|.
$$
Hence, we should set $q_{\eta /2} = 2.82842$ and solve for $\eta$. Using computational tools or a table of the standard Gaussian, we find that
$$
\eta = 2 P(Z \geq 2.82842) \approx 2(0.002339)=0.00467.
$$
2) We observed $T_ n = 0.8485$. Following the same procedure as above, we set $T_ n = 0.8485$, and using computational tools or a table of the standard Gaussian, we find that
$$
\eta = 2 P(Z \geq 0.8485) \approx 0.3961596
$$
3) As the test statistic increases, the p-value will decreases. Note that $T_n$ measures (up to some rescaling) the deviation from the true mean under $H_0: p^* = 0.5$. As this value grows, our observation moves further into the tails of the distribution $\mathcal{N}(0,1)$. Since the asymptotic p-value for this problem is given by $1-\Phi(T_n)$ where $\Phi$ is the CDF of $\mathcal{N}(0,1)$, this implies that the asymptotic p-value decreases as $T_n$ increases.

**Remark 1**: As a rule of thumb, a smaller p-value implies that one can more confidently reject the null hypothesis. Hence, in this scenario, we can more confidently reject the null for experiment $I$ than the null from experiment $II$. You can think of a p-value as a measure of '**how surprised**' you are to observe the given data set under the assumption that the null hypothesis holds. In particular, the smaller the p-value is, the more surprised you should be.

**Remark 2**: A very large value of $T_n$ indicates a rare event under the null hypothesis, so we should be ‘more surprised' at the data if we observe a very large value of $T_n$ as opposed to a small one. The fact that the p-value decreases as $T_n$ increases is consistent with that intuition, since our heuristic is to be more surprised at very small p-values than large ones under $H_0$.

**Remark 3:** Here is an explanation of heuristic of p-value. As the p-value gets smaller, this means we can set the level of a test smaller and smaller and will still reject the null hypothesis based on the data. Since a smaller type 1 error tolerates rarer events under the null, this means that a small p-value lends evidence that the observation was a rare event under $H_0$. Therefore, a smaller p-value suggests more evidence against $H_0$.

## 7. Worked Example: the P-value of a One-Sided Test

Students are asked to count the number of chocolate chips in $15$ cookies for a class activity. They found that the cookies on **average** had $16.5$ chocolate chips with a **standard deviation** of $5.2$ chocolate chips. The packaging for these cookies claims that there are at least $20$ chocolate chips per cookie.

One student thinks this number is unreasonably high since the average they found is significantly lower. Another student claims the difference might be due to chance.

As a statistician, you decide to approach this question with the tools of hypothesis testing. You make the following modeling assumptions on the cookies:

* $X_1, ..., X_n$ are i.i.d. Gaussian random variables,
* $\sqrt{\mathsf{Var}(X_1)} = 5.2$, and
* $\mathbb{E}[X_1] = \mu$ Is an unknown parameter.

You defined the hypotheses as follows
$$
H_0: \mu \geq 20, \quad H_1: \mu < 20.
$$
and specify the test
$$
\psi _ n := \mathbf{1}\left( \sqrt{n}\frac{\overline{X}_ n - 20}{5.2} < -q_{\eta } \right),
$$
where $q_\eta$ is the $1 -\eta $ quantile of a standard Gaussian. (Note that if $Z \sim \mathcal{N}(0,1)$), then $P(Z < - q_\eta ) = P(Z > q_\eta ) = \eta$. Also, since this is a **one-sided test**, we will not use an absolute value to define our test statistic.)

If $\mu =20$ and $X_1, ..., X_n \stackrel{iid}{\sim} \mathcal{N}(\mu, \sigma^2)$, the given test statistic is a standard Gaussian:
$$
\sqrt{n}\left(\frac{\overline{X}_ n - 20}{5.2}\right) \sim N(0,1).
$$
The above holds for *any* value of $n$, not just asymptotically.

For this test and the observed sample mean $\overline{X}_n = 16.5$, the associated p-value is calculated as follows.

**Answer**: $0.00466$

**Solution**:

For notational convenience, let $\mathbf{P}_\mu$ denote the distribution $\mathcal{N}(\mu, 5.2^2)$. Recall that the level $\alpha$ is a bound on the type 1 error. i.e., $\alpha$ is a level of $\psi$ if 
$$
\alpha _\psi (\mu ) = \mathbf{P}_\mu (T_ n <- q_{\eta }) \leq \alpha \quad \text {for all} \,  \,  \mu \geq 20,
$$
where,
$$
T_n = \sqrt{n}\frac{\overline{X}_ n - 20}{5.2}.
$$
Observe that is $X_1,...,X_n \sim P_\mu$ and $\mu > 20$, then
$$
\begin{aligned}
T_n &= \sqrt{n}\frac{\overline{X}_ n - \mu +\left(\mu -20\right)}{5.2}\\
& \sim Z+ \frac{\sqrt{n}}{5.2}(\mu -20).
\end{aligned}
$$
In particular, the distribution of $T_n$ is normal with mean shifted to the right of $\mathcal{N}(0,1)$. Comparing the tails visually (as in previous problems) shows the inequality
$$
\mathbf{P}_\mu (T_ n < -q_{\eta }) < \mathbf{P}_{20}(T_ n < - q_\eta ) = \eta .
$$
Therefore, $\mu = 20$ is the "worse-case" possibility under the null, and $\psi$ is a test of level $\eta$. To compute the p-value, we just need to find the smallest possible $\eta$ such that $\psi$ rejects $H_0$. Hence, we set
$$
-q_\eta = \sqrt{15}\left( \frac{16.5 - 20}{5.2} \right) \approx -2.6068
$$
and compute
$$
P\left(Z < \sqrt{15}\left( \frac{16.5 - 20}{5.2} \right)\right) = P\left(Z > -\sqrt{15}\left( \frac{16.5 - 20}{5.2} \right)\right) \approx 0.0047
$$
where $Z \sim \mathcal{N}(0,1)$. This gives a p-value of $\approx 0.0047$ or roughly $0.5\%$.

**Remark**: A p-value less than $1\%$ indicates that observing a sample mean smaller than $16.5$ is a less than $1\%$ chance event if $\mu = 20$ (which is the worst-case scenario under $H_0$). This indicates a fairly rare event, so it seems reasonable, given our modeling assumptions, to doubt the second student's claim that the low number of chocolate chips was due to chance.

> #### Exercise 41
>
> Suppose we have a test statistic $T_n$ such that $T_ n \sim |Z|$ where $Z \sim \mathcal{N}(0,1)$. In particular, for this problem we know the distribution of $T_n$ for any fixed $n$ and not just asymptotically. You design the test
> $$
> \psi _ n = \mathbf{1}(T_ n \geq q_{\eta /2})
> $$
> where $q_\eta$ is the $1-\eta$ quantile of a standard Gaussian (i.e. if $Z \sim \mathcal{N}(0,1)$, then $P(Z > q_\eta) = \eta$). If $\psi  =1$, we will reject $H_0$, and if $\psi = 0$, we will fail to reject $H_0$.
>
> With this setup, you observe a data set and compute $T_n$. Consider the following figure.
>
> ![images_u2s5_visualizing_pvalue](../assets/images/images_u2s5_visualizing_pvalue.svg)
>
> 1. On which side, **to the left** or **to the right**, of $T_n$ should the value $q_{\eta/2}$ be such that $\psi_n$ rejects on our data set?
>
> 2. What is the largest value of $q_{\eta/2}$ such that $\psi_n$ rejects on our data set?
>
> 3. What is the smallest value of $\eta$ so that $\psi_n$ rejects on our data set?
>
> 4. Now you observe a new data set and compute a new value of the test statistic, which we denote by $T_n'$ . Suppose that $T_ n' < T_ n$, *i.e.*, the test statistic has a smaller value than from before.
>
>    Will the new p-value be **larger** or **smaller** than the p-value from the previous data set considered in this problem?
>
> **Answer**: 
>
> $1.\quad \text{To the left};\\ 2. \quad B;\\ 3. \quad \eta =2\times (\text {the area under the curve to the right of B});\\ 4. \quad \text{Larger}.$
>
> **Solution**:
>
> 1) If $q_{\eta/2}$ is to the left of $T_n$ (i.e., $q_{\eta /2} < T_ n$), then we see that $\psi =\mathbf{1}(T_ n \geq q_{\eta /2}) = 1$. Hence, we would reject in this situation.
>
> 2) We know that $\psi$ rejects if $q_{\eta /2}$ is to the left of $T_n$. Hence, we should make $q_{\eta /2}$ as large as possible so that we still reject.
>
> 3) Note that $\eta/2$ is the area under the curve to the right of $q_{\eta /2}$. 
>
> 4) If $T_ n' < T_ n$, then we know that new p-value is the area under the curve to the right of $T_ n'$ and to the left of $-T_ n' $. Referring to the graphic in this problem, we see that this means the p-value for $T_ n'$ will be larger than the p-value for $T_n$.

> #### Exercise 42
>
> Consider a statistical experiment with iid $X_1, \ldots , X_ n \sim N(\mu , 1)$, where $\mu$ is an unknown parameter. We will hypothesis test on the parameter $\mu$ by setting $H_0: \mu = 0$ and $H_1: \mu \neq 0$. Our test is designed to be 
> $$
> \psi _ n = \mathbf{1}\left( |\sqrt{n} \overline{X}_ n |\geq q_{\eta /2} \right)
> $$
> where $q_\eta$ denotes the $1-\eta$ quantile of a standard Gaussian.
>
> What is the p-value?
>
> **Answer**:  $2(1 - \Phi (|\sqrt{n} \overline{X}_ n |)$.
>
> **Solution**:
>
> Let $T_ n = \sqrt{n} |\overline{X}_ n|$, the p-value is obtained by setting $q_{\eta /2} = T_ n$ and solving for $\eta$. Since $\eta = P(Z \geq q_{\eta })$ for $Z \sim N(0,1)$, we have that the p-value is given by
> $$
> \eta = 2 P( Z > q_{\eta /2}) = 2 P( Z > T_ n) = 2( 1 - \Phi (T_ n)).
> $$

> #### Exercise 43
>
> Which of the following are true statements regarding p-values? 
>
> a. The p-value represents a **tipping point** in the sense that for any level smaller than the p-value, our test would fail to reject the null hypothesis based on the data.
>
> b. The smaller the p-value, the more confidently one can reject the null hypothesis. 
>
> c. The p-value is computed based on the sample that we observe. 
>
> d. One way that scientists and companies have, in some instances, artificially lowered p-values is by specifying the null and alternative hypotheses *after* observing the data. 
>
> **Answer**: abcd
>
> **Solution**:
>
> a) The first choice elaborates on the definition of the (asymptotic) p-value, which is the smallest (asymptotic) level at which a test $\psi$ will reject the null hypothesis. Hence, for any asymptotic level below the p-value, our test will **fail to reject** on our observed sample.
>
> d) It is very important in practice that one specifies the null and alternative hypotheses *before* conducting the experiment. Otherwise, it is possible to ‘tweak' the hypotheses. This can artificially result in a lower p-value, which would favor the scientist or company's desired conclusion.

