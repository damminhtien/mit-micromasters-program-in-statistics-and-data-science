Problem set for Lec5.

# 1. Confidence Intervals for Curved Gaussian Family

Let $X_1, ..., X_n$ be i.i.d. random variables with distribution $\mathcal{N}(\theta, \theta),$ for some unknown parameter $\theta > 0$.

1. Expectation and variance of $\overline{X}_n$.

2. Find an interval $\mathcal{I}_{\theta } = [ A_\theta , B_\theta ]$ (that depends on $\theta$) centered about $\overline{X}_n$ such that
   $$
   \mathbf{P}(\mathcal{I}_{\theta} \ni \theta) = 0.9 \quad \text{ for all n (i.e., not for large } n).
   $$
   (Use the estimate $q_{0.05} \approx 1.6448$ for best results.)

3. Find a C.I. $ \mathcal{I}_{\text {plug-in}} = [ A_{\text {plug-in}}, B_{\text {plug-in}}]$ with **asymptotic** C.I. $90\%$ by plugging in $\overline{X}_n$ for all occurrences of $\theta$ in $\mathcal{I}_\theta$.

   (Use the estimate $q_{0.05} \approx 1.6448$ for best results.)

4. Find a C.I. $\mathcal{I}_{\text {solve}} = [ A_{\text {solve}}, B_{\text {solve}}]$ for $\theta$ with **non-asymptotic** level $90\%$ solving the bounds in $\mathcal{I}_\theta$ for $\theta$.

> 1) 
>
> Since the sample average $\overline{X}_n$ follows a normal distribution for any integer $n \geq 1$,
>
> The expectation and variance are
> $$
> \mathbb E[\overline{X}_ n] = \theta , \quad \textsf{Var}(\overline{X}_ n) = \frac{1}{n^2} \sum _{i=1}^{n} \textsf{Var}(X_ i) = \frac{\theta }{n}.
> $$
> Hence,
> $$
> \sqrt{\frac{n}{\theta }} (\overline{X}_ n - \theta ) \sim \mathcal{N}(0,1).
> $$
> 2) 
>
> We look up the quantile value for a symmetric $90\%$ C.I. for a Gaussian random variable $Z \sim \mathcal{N}(0,1)$
> $$
> \mathbf{P}(\vert Z\vert \leq 1.6448) \approx 0.9
> $$
> we obtain
> $$
> \mathbf{P}\left(\left|\sqrt{\frac{n}{\theta }} (\overline{X}_ n - \theta ) \right| \leq 1.6448\right) = 0.9,
> $$
> Hence, 
> $$
> \mathcal{I}_1 = \left[ \overline{X}_ n - \frac{1.6448 \sqrt{\theta }}{\sqrt{n}}, \overline{X}_ n + \frac{1.6448 \sqrt{\theta }}{\sqrt{n}} \right].
> $$
> 3) 
>
> By LLN we have
> $$
> \overline{X}_ n \xrightarrow [n \to \infty ]{\mathbf{P}} \theta .
> $$
> Then CLT and Slutsky together imply that
> $$
> \sqrt{\frac{n}{\bar{X_ n}}} (\overline{X}_ n - \theta ) \xrightarrow [n \to \infty ]{d} \mathcal{N}(0,1).
> $$
> Hence, we obtain an asymptotic C.I. with level $90\%$ by replacing $\theta$ by $\overline{X}_n$ in the expression for $\mathcal{I}_1$ and we can set
> $$
> \mathcal{I}_2 = \left[ \overline{X}_ n - \frac{1.6448 \sqrt{\overline{X}_ n}}{\sqrt{n}}, \overline{X}_ n + \frac{1.6448 \sqrt{\overline{X}_ n}}{\sqrt{n}} \right].
> $$
> 4) From Q2 we have
> $$
> \mathbf{P}\left( \left| \sqrt{\frac{n}{\theta }} (\overline{X}_ n - \theta ) \right| \leq 1.65 \right) = 90\% .
> $$
> with $t = 1.6448$, the constraint on $\theta$ is equivalent to 
> $$
> \begin{aligned}
> &\left| \sqrt{\frac{n}{\theta }} (\overline{X}_ n - \theta ) \right| \leq t\\
> \iff & \frac{n}{\theta }(\overline{X}_ n - \theta )^2 \leq t^2\\
> \iff & \theta ^2 - 2 \theta \overline{X}_ n + \overline{X}_ n^2 \leq {t^2 \theta \over n}\\
> \iff & \theta ^2 - \left(2 \overline{X}_ n + \frac{t^2}{n}\right) \theta + \overline{X}_ n^2 \leq 0\\
> \iff & \theta \in \left[ \overline{X}_ n + \frac{t^2}{2n} - \sqrt{\Delta }, \, \overline{X}_ n + \frac{t^2}{2n} + \sqrt{\Delta } \right], \quad \text {where }\, \Delta = \frac{t^4}{4n^2} + \frac{t^2 \overline{X}_ n}{n}
> \end{aligned}
> $$
> By the quadratic formula. Substituting $t = 1.65$ gives
> $$
> \mathcal{I}_{\text {solve}} = \left[ \overline{X}_ n + \frac{1.6448^2}{2n} - \sqrt{\frac{1.6448^4}{4n^2} + \frac{1.6448^2 \overline{X}_ n}{n}}, \overline{X}_ n + \frac{1.6448^2}{2n} + \sqrt{\frac{1.6448^4}{4n^2} + \frac{1.6448^2 \overline{X}_ n}{n}} \right].
> $$

# 2. Delta method and asymptotic variances

#### Review of arguments: the Law of Large Numbers, the Central Limit Theorem, and the Delta Method.

Let $X_1, X_2, ...$ , be random variables. The **(weak) Law of Large Numbers** says that under suitable assumptions, with
$$
\overline{X}_ n = \frac{1}{n} \sum _{i=1}^{n} X_ i,
$$
We have
$$
\overline{X}_ n \xrightarrow {\mathbf{P}} \mathbb E[X_1].
$$
The **Central Limit Theorem** states that under some assumptions, there is a $V$ such that
$$
\sqrt{n} (\overline{X}_ n - \mathbb E[X_1]) \xrightarrow {\mathrm{(D)}} \mathcal{N}(0,V).
$$
The **Delta Method** gives us a way to control the asymptotic variance of a transformation of a random variable. Let $\theta \in \R$ be a parameter and $Z_n \in \R$ be a sequence of random variables that satisfies.
$$
\sqrt{n}(Z_ n - \theta ) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0,V)
$$
for some $V>0$.

Given a function $g \,  \colon \Omega \subseteq \mathbb {R}\to \mathbb {R}$,
$$
\sqrt{n}(g(Z_ n) - g(\theta )) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0,W).
$$
for some $X > 0$.

1. What are the assumptions we need for the **weak Law of Large Numbers**?

* $\mathbb E[|X_ i|] < \infty$ for all $i$
* $X_1, X_2, ...$ independent
* $X_1, X_2, ...$ identically distributed

2. What are the assumptions we need for the **Central Limit Theorem**?

* $\mathbb E[|X_ i|] < \infty$ for all $i$
* $\textsf{Var}(X_ i) < \infty$ for all $i$

* $X_1, X_2, ...$ independent
* $X_1, X_2, ...$ identically distributed

3. Pick the following assumptions and conditions that apply to the **Delta Method** as stated in class:

* $g$ is continuously differentiable at $\theta$

* $W = g'(\theta)^2 V$

#### Problem statement

Compute the **asymptotic variance** of some estimators. Recall that the asymptotic variance of an estimator $\hat{\theta}$ for a parameter $θ$ is defined as $V(\hat{θ})$, if
$$
\sqrt{n}(\widehat{\theta } - \theta ) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, V(\widehat{\theta })).
$$

1. Argue that the proposed estimators $\widehat{\lambda }$ and $\widetilde\lambda$ below are both consistent and asymptotically normal. Then, give their asymptotic variances $V(\widehat{\lambda })$ and $V(\tilde{\lambda})$, and decide if one of them is always bigger than the other.

   Let $X_1,\ldots ,X_ n\stackrel{i.i.d.}{\sim }\textsf{Poiss}(\lambda )$, for some $\lambda > 0$. Let $\hat{\lambda} = \overline{X}_n$ and $\tilde{\lambda} = -\ln(\overline{Y}_n)$, where $Y_i = \mathbf{1}\{X_i = 0\}, i = 1,...,n$.

   > **Answer**: 
   >
   > $V(\widehat{\lambda}) = \lambda\\ V(\tilde{\lambda}) = \exp(\lambda) - 1$
   >
   > **Solution**: 
   >
   > For $\widehat{\lambda}$, By the LLN,
   > $$
   > \overline{X}_ n \xrightarrow [n \to \infty ]{\mathbf{P}} \mathbb E[X_1] = \lambda .
   > $$
   > By the CLT,
   > $$
   > \sqrt{n} (\overline{X}_ n - \lambda ) \sim \mathcal{N}(0,\textsf{Var}(X_1)) = \mathcal{N}(0,\lambda ),
   > $$
   > Hence,
   > $$
   > V(\widehat\lambda ) = \lambda .
   > $$
   > For $\tilde{\lambda}$, first observe that by the LLN,
   > $$
   > \overline{Y}_ n \xrightarrow [n \to \infty ]{\mathbf{P}} \mathbb E[Y_1] = \mathbf{P}(X_1 = 0) = \exp (-\lambda ),
   > $$
   > so with $g(t) = -\log (t)$,
   > $$
   > \tilde\lambda = g(\overline{Y}_ n) \xrightarrow [n \to \infty ]{\mathbf{P}} g(\exp (-\lambda )) = \lambda .
   > $$
   > The CLT yields
   > $$
   > \sqrt{n} (\overline{Y}_ n - \mathbb E[Y_1]) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, \textsf{Var}(Y_1)) = \mathcal{N}(0, \exp (-\lambda ) (1 - \exp (-\lambda )),
   > $$
   > where we used the formula $\textsf{Var}(Z) = p(1-p) $ if $Z \sim \textsf{Be}(p)$. In order to apply the Delta Method for the above $g(t)$, we compute
   > $$
   > g'(t) = -\frac{1}{t}, \quad g'(\exp (-\lambda )) = -\exp (\lambda ),
   > $$
   > which results in
   > $$
   > \sqrt{n}(\tilde\lambda - \lambda ) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, \exp (\lambda ) - 1).
   > $$
   > Moreover, by the series expansion for the exponential,
   > $$
   > \exp (\lambda ) - 1 = \sum _{k = 1}^{\infty } \frac{\lambda ^ k}{k!} > \lambda , \quad \text {for all } \lambda > 0,
   > $$
   > so $V(\widehat{\lambda}) < V(\tilde{\lambda})$ for all $\lambda$.

2. As above, argue that both proposed estimators $\widehat{\lambda}$ and $\tilde{\lambda}$ are consistent and asymptotically normal. Then, give their asymptotic variances $V(\widehat{\lambda})$ and $V(\tilde{\lambda})$, and decide if one of them is always bigger than the other.

   Let $X_1,\ldots ,X_ n\stackrel{i.i.d.}{\sim }\textsf{Exp}(\lambda )$, for some $\lambda > 0$. Let $\widehat\lambda = \frac{1}{\overline{X}_ n}$ and $\tilde\lambda = -\ln (\overline{Y}_ n)$, where $Y_ i=\mathbf{1}\{ X_ i>1\} , i=1,\ldots ,n$.

   > **Answer**: 
   >
   > $V(\widehat{\lambda}) = \lambda^2\\ V(\tilde{\lambda}) = \exp(\lambda) - 1$
   >
   > **Solution**: 
   >
   > For $\widehat{\lambda}$, by the LLN,
   > $$
   > \overline{X}_ n \xrightarrow [n \to \infty ]{\mathbf{P}} \mathbb E[X_1] = \frac{1}{\lambda }.
   > $$
   > With $g(t) = 1/t$, we have that
   > $$
   > \widehat\lambda \xrightarrow [n \to \infty ]{\mathbf{P}} \frac{1}{\mathbb E[X_1]} = \lambda .
   > $$
   > By the CLT,
   > $$
   > \sqrt{n} (\overline{X}_ n - \frac{1}{\lambda }) \sim \mathcal{N}(0,\textsf{Var}(X_1)) = \mathcal{N}\left(0,\frac{1}{\lambda ^2}\right).
   > $$
   > The fact that
   > $$
   > g'(t) = -\frac{1}{t^2}
   > $$
   > together with the Delta Method then yields
   > $$
   > V(\widehat\lambda ) = \lambda ^2.
   > $$
   > For $\tilde{λ}$, first observe that it is the average of Bernoulli variables, and by the LLN,
   > $$
   > \overline{Y}_ n \xrightarrow [n \to \infty ]{\mathbf{P}} \mathbb E[Y_1] = \mathbf{P}(X_1 > 1) = \exp (-\lambda ),
   > $$
   > so with $\tilde g(t) = -\log (t)$
   > $$
   > \tilde\lambda = \tilde g(\overline{Y}_ n) \xrightarrow [n \to \infty ]{\mathbf{P}} g(\exp (-\lambda )) = \lambda .
   > $$
   > The CLT yields
   > $$
   > \sqrt{n} (\overline{Y}_ n - \mathbb E[Y_1]) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, \textsf{Var}(Y_1)) = \mathcal{N}(0, \exp (-\lambda ) (1 - \exp (-\lambda )).
   > $$
   > In order to apply the Delta Method for the above $\tilde g(t)$, we compute
   > $$
   > \tilde g'(t) = -\frac{1}{t}, \quad \tilde g'(\exp (-\lambda )) = -\exp (\lambda ),
   > $$
   > which results in 
   > $$
   > \sqrt{n}(\tilde\lambda - \lambda ) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, \exp (\lambda ) - 1).
   > $$
   > In order to compare these two asymptotic variances, first observe that similar to previous part,
   > $$
   > \exp (\lambda ) - 1 \geq \lambda , \quad \text {for all } \lambda > 0,
   > $$
   > and since $ \lambda ^2 < \lambda$ for $\lambda \in (0, 1)$, we have
   > $$
   > \exp (\lambda ) - 1 \geq \lambda ^2, \quad \text {for } \lambda \in (0,1).
   > $$
   > Moreover,
   > $$
   > \exp (1) - 1 = e -1 > 1 = 1^2,
   > $$
   > and
   > $$
   > \frac{d}{d \lambda } (\exp (\lambda ) - 1) = \exp (\lambda ), \quad \frac{d}{d \lambda } \lambda ^2 = 2 \lambda ,
   > $$
   > so that
   > $$
   > \frac{d}{d \lambda } (\exp (\lambda ) - 1) = \exp (\lambda ) \geq 1 + \lambda + \frac{\lambda ^2}{2} > 2 \lambda = \frac{d}{d \lambda } \lambda ^2, \quad \text {for all } \lambda > 0,
   > $$
   > which can be checked by the quadratic formula. This means that for $\lambda \geq 1$,
   > $$
   > \exp (\lambda ) - 1 = e + \int _{1}^{\lambda } \exp (t) \, dt -1 > 1 + \int _{1}^{\lambda } 2 t \, dt = \lambda ^2.
   > $$
   > so $V(\widehat{\lambda}) < V(\tilde{\lambda})$ for all $\lambda$.

3. As above, argue that both proposed estimators $\widehat{p},\, \widetilde{p},$ are consistent and asymptotically normal. Then, give their asymptotic variances $V(\widehat{\lambda})$ and $V(\tilde{\lambda})$, and decide if one of them is always bigger than the other.

   Let $ X_1,\ldots ,X_ n\stackrel{i.i.d.}{\sim }\textsf{Geom}(p) $, for some $p \in (0,1)$. That means that
   $$
   \mathbf{P}(X_1=k)=p(1-p)^{k-1}, \quad \text {for } k = 1, 2, \dots .
   $$
   Let
   $$
   \hat{p} = {1\over \overline{X_n}}
   $$
   and $\tilde{p}$ be the number of ones in the sample divided by $n$.

   > **Answer**: 
   >
   > $V(\widehat{p}) = p^2 (1-p)\\ V(\tilde{p}) = p(1-p)$
   >
   > **Solution**: 
   >
   > By the LLN,
   > $$
   > \overline{X}_ n \xrightarrow [n \to \infty ]{\mathbf{P}} \mathbb E[X_1] = \frac{1}{p}.
   > $$
   > Setting
   > $$
   > g(t) = \frac{1}{t},
   > $$
   > we obtain consistency of $ \widehat{p} = g(\overline{X}_ n)$, i.e.,
   > $$
   > \widehat{p} = g(\overline{X}_ n) \xrightarrow [n \to \infty ]{\mathbf{P}} g(\mathbb E[X_1]) = p.
   > $$
   > By the CLT,
   > $$
   > \sqrt{n} (\overline{X}_ n - \frac{1}{p}) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0,\textsf{Var}(X_1)) = \mathcal{N}\left(0,\frac{1-p}{p^2}\right),
   > $$
   > and hence by the Delta Method, together with
   > $$
   > g'\left( \frac{1}{p} \right)^2 = \left( -p^2 \right)^2 = p^4,
   > $$
   > we end up with
   > $$
   > \sqrt{n} (\widehat{p} - p) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, p^2(1-p)),
   > $$
   > So
   > $$
   > V(\widehat{p}) = p^2(1-p).
   > $$
   > For $\tilde{p}$, note that we can write it as
   > $$
   > \tilde p = \overline{Y}_ n, \quad \text {where } Y_ i = \mathbf{1} \{ X_ i = 1\} ,
   > $$
   > so it is again an average over Bernoulli variables. The LLN gives
   > $$
   > \overline{Y}_ n \xrightarrow [n \to \infty ]{\mathbf{P}} \mathbb E[Y_1] = \mathbf{P}(X_1 = 1) = p,
   > $$
   > while the CLT yields
   > $$
   > \sqrt{n} (\overline{Y}_ n - p) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, \textsf{Var}(Y_1)) = \mathcal{N}(0, p(1-p)).
   > $$
   > Since $p^2 < p$ for $p \in (0,1)$,
   > $$
   > V(\widehat{p}) < V(\tilde p).
   > $$

# 3. Application of Delta Method on Gamma Variables

The Gamma distribution $\text {Gamma}(\alpha ,\beta )$ with parameter $\alpha > 0$, and $\beta > 0$ is defined by the density
$$
f_{\alpha ,\beta }(x)=\frac{\beta ^\alpha }{\Gamma (\alpha ) } x^{\alpha -1} e^{-\beta x}, \quad \text {for all} x\geq 0.
$$
The $\Gamma$ function is defined by
$$
\Gamma (s)=\int _{0}^\infty x^{s-1} e^{-x} dx.
$$
As usual, the constant $\frac{\beta ^\alpha }{\Gamma (\alpha ) }$ is a normalization constant that gives $\int _{0}^{\infty } f_{\alpha ,\beta }(x)dx=1.$

In this problem, let $X_1,\ldots ,X_ n$ be i.i.d. Gamma variables with
$$
\beta =\frac{1}{\alpha }\quad \text {for some } \alpha >0.
$$
That is, $X_1,\ldots ,X_ n\sim \text {Gamma}\left(\alpha ,\frac{1}{\alpha }\right)$ random variables for some $\alpha > 0$. The pdf for $X_i$ is therefore
$$
f_\alpha (x)=\frac{1}{\Gamma (\alpha ) \alpha ^\alpha } x^{\alpha -1} e^{-x/\alpha }, \quad \text {for all } x\geq 0.
$$

1. What is the limit, in probability, of the sample average $\overline{X}_n$ of the sample in terms of $\alpha$? $\overline{X}_ n\xrightarrow [n \to \infty ]{\mathbf{P}}\quad ? $
2. Use the result from the previous problem to give a consistent estimator $\hat{\alpha}$ of $\alpha$ in terms of $\overline{X}_n$.
3. For the Delta method to apply, at what value of $x$ does $g$ need to be continuously differentiable? 
4. Find C.I. for $\alpha$ with asymptotic level $90\%$ using both the "solving" and the "plug-in" methods. Use $n=25$ and $\overline{X}_n = 4.5$. Use $q_{0.05} \approx 1.6448$.

> 1) $\overline{X}_ n\xrightarrow [n \to \infty ]{\mathbf{P}} \alpha^2$
>
> By LLN,
> $$
> \overline{X}_ n\xrightarrow [n \to \infty ]{\mathbf{P}}\mathbb E[X_ i].
> $$
> In general, for $\text{Gamma}(\alpha, \beta)$，
> $$
> \mathbb{E}[X] = {\alpha \over \beta}, \quad \mathsf{Var}(X) = {\alpha \over \beta^2}
> $$
> Hence, for $X_ i\sim \text {Gamma}\left(\alpha ,\frac{1}{\alpha }\right)$, we have
> $$
> \mathbb{E}[X_i] = \frac{\alpha }{1/\alpha }\, =\, \alpha ^2.
> $$
> 2) 
>
> By the continuous mapping theorem, 
> $$
> \hat{\alpha }=\sqrt{\overline{X}_ n}\xrightarrow [\mathbf{P}]{n\to \infty } \sqrt{\alpha ^2}=\alpha, \quad \text{since }\alpha > 0
> $$
> Hence,
> $$
> \hat{\alpha} = \sqrt{\overline{X}_n}
> $$
> 3) 
>
> The Delta method gives
> $$
> \sqrt{n}\left(\hat{\alpha }-\alpha \right)=\sqrt{n}\left(\sqrt{\overline{X}_ n}-\alpha \right)\xrightarrow [d.]{n\to \infty } \mathcal{N}\left(0,\left(g'(\mathbb E[X_ i])\right)^2 \textsf{Var}(X_ i)\right)=\mathcal{N}\left(0,\left(g'(\alpha ^2)\right)^2 \textsf{Var}(X)\right)\qquad  \text{where }\, g(x)=\sqrt{x}
> $$
> If $g$ is continuously differentiable at $\alpha^2$. 
>
> Since $g'(x) = {1\over 2 \sqrt{x}}$ exists and is continuous for all $x > 0$, $g'$ is continuously differentiable at any $\alpha^2$ value. Hence, the Delta method does apply.
>
> To compute the asymptotic variance $\left(g'(\alpha ^2)\right)^2 \textsf{Var}(X_ i)$, we need $g'(\alpha ^2)$ and $\textsf{Var}(X_ i)$, which are as follows
> $$
> g'(\alpha ^2) = {1\over 2 \sqrt{\alpha^2}} = {1\over 2 \alpha}\\
> \textsf{Var}(X) = {\alpha \over \beta^2} = \alpha^3 \quad \text{since }\beta = 1 / \alpha
> $$
> Combined together, the asymptotic variance is
> $$
> \left(g'(\alpha ^2)\right)^2 \textsf{Var}(X_ i) = {1\over 4 \alpha^2}(\alpha^3) = {\alpha \over 4}
> $$
> 4) 
>
> From previous questions we know that
> $$
> \begin{aligned}
> \sqrt{n}\left(\hat{\alpha }-\alpha \right) &\xrightarrow[d.]{n\rightarrow \infty} \mathcal{N}\left(0,\tau ^2\right) \qquad \text {where }\, \tau ^2=\frac{\alpha }{4}\\
> \implies \frac{\sqrt{n}}{\tau }\left(\hat{\alpha }-\alpha \right) &\xrightarrow[d.]{n\rightarrow \infty} \quad \mathcal{N}(0,1) \quad \text{where }\tau^2 = {\alpha \over 4}
> \end{aligned}
> $$
> For large $n$, approximately
> $$
> \mathbf{P}\left(\hat{\alpha }-q_{0.05}\frac{\tau }{\sqrt{n}}<\alpha <\hat{\alpha }+q_{0.05}\frac{\tau }{\sqrt{n}}\right)= 0.9.
> $$
> Plugging in the asymptotic variance $\tau =\sqrt{\alpha }/2$ gives
> $$
> \mathbf{P}\left(\hat{\alpha }-q_{0.05}\frac{\sqrt{\alpha }}{2\sqrt{n}}<\alpha <\hat{\alpha }+q_{0.05}\frac{\sqrt{\alpha }}{2\sqrt{n}}\right)= 0.9.
> $$
> We now go through the three methods of solving for the C.I.:
>
> * Conservative bound: Since $\sqrt{α}$ is not bounded, the conservative bound method does not give a C.I.
>
> * Solving for $α$: 
>   $$
>   \begin{aligned}
>   &\vert \hat{\alpha} - \alpha \vert < q_{0.05} {\tau \over \sqrt{n}} = q_{0.05}{\sqrt{\alpha} \over 2 \sqrt{n}}\\
>   \iff& (\hat{\alpha} - \alpha)^2 < q_{0.05}^2 {\alpha \over 4n}\\
>   \iff& \alpha ^2-\left(2\hat{\alpha }+\frac{q_{0.05}^2}{4n}\right) \alpha +\hat{\alpha }^2 = 0
>   \end{aligned}
>   $$
>   where $\hat{\alpha}^2 = \overline{X}_n = 4.5$, and $q_{0.05} = 1.6448$. Using the quadratic formula or software, we get the C.I.
>   $$
>   \mathcal{I}_{\text {solve}}=[1.89,2.37]
>   $$
>
> * Plug-in: Since $\hat{\alpha }^2=\overline{X}_ n=4.5$, the plug-in C.I. is 
>   $$
>   \begin{aligned}
>   \mathcal{I}_{\text {plug-in}} &= \left[\hat{\alpha }-q_{0.05}\frac{\sqrt{\hat{\alpha }}}{2\sqrt{n}}, \hat{\alpha }+q_{0.05}\frac{\sqrt{\hat{\alpha }}}{2\sqrt{n}}\right]\\
>   &=[1.88, 2.36]
>   \end{aligned}
>   $$
>
> 

# Lecture 6. Introduction to Hypothesis Testing, and Type 1 and Type 2 Errors

There are 11 topics and 6 exercises.

## 1. Intro to Hypothesis Testing

* Let $X$ (resp. $Y$) denote the boarding time of a random JetBlue (resp. United) flight. 

* We assume that $X \sim \mathcal{N}(\mu_1, \sigma^2_1)$ and $Y \sim \mathcal{N}(\mu_2, \sigma^2_2)$

* Let $n$ and $m$ denote the JetBlue and United sample sizes respectively. 
* We have $X_1, ..., X_n$ independent copies of $X$ and $Y_1, ..., Y_m$ independent copies of $Y$.
* We further assume that the two samples are independent.

We want to answer the question: "Is $\mu_1 = \mu_2$? " or "Is $\mu_1 > \mu_2$? "

By making **modeling assumptions**, we have reduced the number of ways the hypothesis $\mu_1 = \mu_2$ may be rejected. We do not allow that $\mu_1 < \mu_2$.

We have two samples: this is a **two-sample test**.

## 2. Statistical Model of a Two Sample Experiment

An associated statistical model is $\left(E,\{ P_\theta \} _{\theta \in \Theta }\right)$ where

* $E$ is the smallest sample space of the pair $(X,Y)$, 
* $P_\theta$ is the family of joint distribution of $(X,Y)$ with parameter $\theta$. Because $X $ and $Y$ are independent, their joint distribution is the product of their respective distributions.

In the first scenario, the observed outcome of a statistical experiment consists of two samples:
$$
X_1,X_2,\ldots X_ n\stackrel{\text {i.i.d.}}{\sim }X\sim \textsf{Ber}(p_1)\\
Y_1,Y_2,\ldots Y_ m\stackrel{\text {i.i.d.}}{\sim }Y\sim \textsf{Ber}(p_2).
$$
where in addition, $X,Y$ and the two samples $X_1, ..., X_n$ and $Y_1, ..., Y_m$ are independent.

Identify the sample space $E$ and the parameter space $\Theta$: (Use $]x,y[$ to denote an open interval.)

* Sample space $E$: $\{0,1\}\times \{0,1\} = \{(0,0), (0,1),(1,0), (1,1)\}$.

  Since $X\sim \textsf{Ber}(p_1)$ and $X\sim \textsf{Ber}(p_2)$, the pair $(X,Y)$ takes value in the sample space $E=\{ 0,1\} \times \{ 0,1\} =\{ (0,0),(0,1),(1,0),(1,1)\}$.

* Parameter space $\Theta$: $]0,1[\;  \times \; ]0,1[\;  \subset \mathbb {R}^2$.

  Since $X,Y$ are independent, the joint distribution of $(X,Y)$ is the product $\textsf{Ber}(p_1)\times \textsf{Ber}(p_2)$. Hence, the family $\{ P_\theta \} _{\theta \in \Theta }$ of joint distributions is parametrized by $\theta = (p_1, p_2)$ and the parameter space is 
  $$
  \Theta =\{ (p_1,p_2): p_1\in ]0,1[, p_2\in ]0,1[\} = ]0,1[\times ]0,1[\subset \mathbb {R}^2.
  $$

---

In the second scenario, we aim to test whether boarding times by the Window-Middle-Aisle boarding method is shorter than boarding times by the rear-to-front method. We collect a sample of boarding times of each method and model these boarding times as the following two sets of normal variables:
$$
X_1,X_2,\ldots X_ n \text { are } i.i.d. \text { copies of } X\sim \mathcal{N}(\mu _1,\sigma _1^2)\qquad \text {boarding times of rear-to-front}\\
Y_1,Y_2,\ldots Y_ m \text { are } i.i.d. \text { copies of } Y\sim \mathcal{N}(\mu _2,\sigma _2^2)\qquad \text {boarding times of window-middle-aisle}
$$
where $X$ and $Y$ are also independent.

For simplicity, **assume the two standard deviations $\sigma^2_1$ and $\sigma^2_2$ are some known, fixed quantities $\sigma^*_1$ and $\sigma^*_2$** .

Identify the sample space $E$ and the parameter space $\Theta$: (Use $]x,y[$ to denote an open interval.)

* Parameter space $\Theta$: $(\mu_1, \mu_2)$

  Since $X$ and $Y$ are independent, the joint distribution of $(X,Y)$ is the product $\mathcal{N}\left(\mu _1,(\sigma _1)^2\right) \times \mathcal{N}\left(\mu _2,(\sigma _2)^2\right)$. Since $\sigma_1$ and $\sigma_2$ are fixed and known, the only parameter determining the joint distribution is $\mu_1$ and $\mu_2$. Hence, a choice of the parameter $\theta$ is the 2D vector $(\mu_1 \quad \mu_2)$.

* Sample space $E$: 

  Since the family of joint distribution would be parametrized by $(\mu_1 \quad \mu_2)$, we have the parameter space 
  $$
  \Theta =\{ (\mu _1,\mu _2): \mu _1\in \mathbb {R}, \mu _2\in \mathbb {R}\} =\mathbb {R}^2.
  $$
  Because $\mu_1$ and $\mu_2$ model average boarding times, we can further restrict to 
  $$
  \Theta =\{ (\mu _1,\mu _2): \mu _1\in [0,\infty ), \mu _2\in [0,\infty )\} =[0,\infty )\times [0,\infty )\} .
  $$

## 3. Heuristics for Two Sample Tests

**Simple heuristic:** 

If $\bar{X}_n > \bar{Y}_m$, then $\mu_1 > \mu_2$.

This could go wrong if I randomly pick only full flights in my sample $X_1, ..., X_n$ and empty flights in my sample $Y_1,...,Y_m$.

**Better heuristic:**

If $\bar{X}_n - \text{Buffer}_n > \bar{Y}_m + \text{Buffer}_m$, then $\mu_1 > \mu_2$.

To make this intuition more precise, we need to take the **size of the random fluctuations** of $\bar{X}_n$ and $\bar{Y}_m$ into account (including variance, sample size, confidence, fluctuations of $\bar{X}_n$ and $\bar{Y}_m$ about their respective means $\mu_{\text{drug}}$ and $\mu_{\text{control}}$, those are what "Buffer" depends on).

## 4. Heuristics for One Sample Tests

Waiting time in the ER:

* Is it true that the new hospital has a longer waiting time than 30min, which is the average waiting time in the Emergency Room (ER).
* We collect only one sample $X_1,...,X_n$ (waiting time in minutes for $n$ random patients) with unknown expected value $\mathbb{E}[X_1] = \mu$.
* We want to know if $\mu > 30$

Heuristic:

If $\bar{X}_n + \text{Buffer}_n < 30$, then conclude that $\mu \leq 30$. 

If $\bar{X}_n - \text{Buffer}_n > 30$, then conclude that $\mu \geq 30$.

> #### Exercise 33
>
> Which of the following are **true statements** regarding **hypothesis testing** as exemplified above and **parameter estimation** as discussed in previous lectures?
>
> a. In the above hypothesis testing set-up and in the models in the previous lectures on parameter estimation, we make the assumption that our data is iid from some unknown distribution.
>
> b. When carrying out parameter estimation, we are interested in coming up with an estimator $\hat{\mu}$ that we want to be close to the true parameter $\mu$.
>
> c. When performing hypothesis testing (as above), we are **not** necessarily interested in finding an estimator for $\mu$. Rather, our goal is to decide whether or not the true parameter lies in a certain region.
>
> d. When performing hypothesis testing, our main goal is to come up with a good approximation of the true parameter.
>
> **Answer**: abc
>
> **Solution**: 
>
> a. In the parameter estimation unit, for all statistical models we assumed that our sample consisted of iid random variables.
>
> b. The main goal of parameter estimation is to come up with some approximation for the unknown true parameter using the sample $X_1, ..., X_n$.
>
> cd. The goal of hypothesis testing is to decide if the true parameter has some particular property (e.g. whether it lies in a particular region or not).

## 5. Two Sample vs. One Sample Tests

A **one-sample test** is a hypothesis test where an unknown parameter $\mu$ is to be compared to a known reference value.

A **two-sample test** is a hypothesis test where two unknown parameters are compared to each other.

## 6. Examples

#### 1. Does at most a third of Americans get at least some news from YouTube?

According to a survey conducted in 2017 on 4,971 randomly sampled Americans, $32\%$ report to get at least some of their news on Youtube. Can we conclude that at most a third of all Americans get at least some of their news on Youtube?

* $n =  4,971, X_1, ..., X_n \stackrel{iid}{\sim}\mathsf{Ber}(p):$

* $\bar{X}_n = 0.32$

* If it was true that $p = .33$. 

* The mean is  $\mathbf{E}[\bar{X}_n] = 0.33$, the variance is $ \mathsf{Var}(\bar{X}_n) = {0.33 (1- 0.33)\over 4,971 }$

* By CLT,
  $$
  \sqrt{n} {\bar{X}_n - 0.33 \over \sqrt{0.33 (1- 0.33)}} \approx \mathcal{N}(0,1)
  $$

* $\sqrt{n} {\bar{X}_n - 0.33 \over \sqrt{0.33 (1- 0.33)}} \approx -1.50$

* Remark: 
  * Once the $\bar{X}_n$ is normalized, the fluctuations are from standard Gaussian. That is, the $\bar{X}_n$ not being equal to $0.33$ might be due to the fluctuations of the standard Gaussian.
  * What statistics is about is to take a problem that might be on its own scale, map it into a scale which is in terms of percentage of a Gaussian, then we are willing to tolerate $99\%$ of probability, or $1\%$ of error, which means a confidence level of $99\%$. Then we need to map it back onto the original scale.

> #### Exercise 34 Review CLT
>
> Recall the CLT states that if 
>
> * $X_1 ,..., X_n$ are i.i.d.;
> * $\mathbb {E}[X_1] = \mu < \infty ,$ and $\text {Var}(X_1) = \sigma ^2 < \infty$
>
> Then a shift and a rescaling of the sample mean $\overline{X}_ n = \displaystyle \frac{1}{n} \sum _{i = 1}^ n X_ i$ converges to a standard Gaussian $\mathcal{N}(0,1)$ in distribution as $n \rightarrow \infty$:
> $$
> \sqrt{n}\left( \frac{\overline{X}_ n - \mu }{\sigma } \right) \xrightarrow [n \to \infty ]{(d)} \mathcal{N}(0,1).
> $$
> Suppose $\mu = 0$ and $\sigma^2 = 1$. Given this assumption, which of the following limits is strictly between 0 and 1?
>
> a. $\lim _{n \to \infty } P(\overline{X}_ n \in (-1, 1))$
>
> b. $\lim _{n \to \infty } P\left(\overline{X}_ n \in \left(-\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}} \right)\right)$
>
> c. $\lim _{n \to \infty } P\left(\overline{X}_ n \in \left(-\frac{1}{n}, \frac{1}{n} \right)\right)$
>
> **Answer**: b
>
> **Solution**:
>
> Let $Z \sim \mathcal{N}(0,1)$ and let $a_n, b_n$ denote sequences depending on $n$. By the CLT,
> $$
> \begin{aligned}
> \lim _{n \to \infty } P(\overline{X}_ n \in (a_ n,b_ n)) &= \lim _{n \to \infty } P(\sqrt{n}\,  \overline{X}_ n \in (\sqrt{n} a_ n, \sqrt{n} b_ n))\\
> &= P(Z \in (\lim _{n \to \infty } \sqrt{n} a_ n, \lim _{n \to \infty } \sqrt{n} b_ n))
> \end{aligned}
> $$
>
> * For option 'a', $\lim _{n \to \infty } P(\overline{X}_ n \in (-1, 1))  = 1$. 
>
>   By setting $a_n = -1$ and $b_n = 1$, we see that
>   $$
>   \lim _{n \to \infty } \sqrt{n} a_ n = - \infty , \quad \lim _{n \to \infty } \sqrt{n} b_ n = \infty .
>   $$
>   Hence, by the above calculation,
>   $$
>   \lim _{n \to \infty } P(\overline{X}_ n \in (a_ n,b_ n)) = P(Z \in (-\infty , \infty )) = 1.
>   $$
>
> * $\lim _{n \to \infty } P\left(\overline{X}_ n \in \left(-\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}} \right)\right)$ lies strictly between $0$ and $1$, as we will show below. Setting $a_n = -{1\over \sqrt{n}}$ and $b_n = {1\over \sqrt{n}}$, we see that
>   $$
>   \sqrt{n}a_n = -1, \quad \sqrt{n} b_n = 1.
>   $$
>   Hence, by the above calculation,
>   $$
>   \lim _{n \to \infty } P(\overline{X}_ n \in (a_ n,b_ n)) = P(Z \in (-1,1))
>   $$
>   Since Gaussian variables have a positive probability of being inside $(-1,1)$ and also a positive probability of being outside $(-1,1)$, we can also conclude without doing any computation that $0 < \mathbf{P}(Z \in (-1,1)) < 1$.
>
>   **Remark**: Alternatively we can compute, using computational tools or a table that
>   $$
>   P(Z \in (-1,1)) = \int _{-1}^1 \frac{1}{\sqrt{2 \pi }} e^{-x^2/2} \,  dx \approx 0.6827.
>   $$
>
> * $\lim _{n \to \infty } P\left(\overline{X}_ n \in \left(-\frac{1}{n}, \frac{1}{n} \right)\right)=0$.
>
>   Setting $a_n = -{1\over n },$ and $b_n = {1\over n}$, we see that 
>   $$
>   \lim _{n \to \infty } \sqrt{n} a_ n = \lim _{n \to \infty } -\frac{1}{\sqrt{n}} = 0, \quad \lim _{n \to \infty } \sqrt{n} b_ n = \lim _{n \to \infty } \frac{1}{\sqrt{n}} = 0
>   $$
>   Hence, by the above calculation,
>   $$
>   \lim _{n \to \infty } P(\overline{X}_ n \in (a_ n,b_ n)) = P(Z \in (0, 0)) = 0.
>   $$
>
> **Remark:** This exercise emphasizes the heuristic interpretation of the CLT which states that the sample mean $\bar{X}_n$ lives inside an interval of radius  "$Constant \times {1\over \sqrt{n}}$" around its expectation. This heuristic will be useful for designing hypothesis tests.

#### 2. Testing fairness of a coin

A coin is tossed $30$ times and Heads are obtained $13$ times. Can we conclude that the coin is significantly unfair?

* $n =30, X_1, ..., X_n \stackrel{iid}{\sim} \mathsf{Ber}(p);$

* $\bar{X}_n = 13/30 \approx 0.43$

* If it was true that $p = 0.5$. By CLT,
  $$
  \sqrt{n}{\bar{X}_n - 0.5 \over\sqrt{0.5(1-0.5)} } \approx \mathcal{N}(0,1)
  $$

* Our data gives $\sqrt{n}{\bar{X}_n - 0.5 \over\sqrt{0.5(1-0.5)} } \approx -0.77$

* The number $-0.77$ is a plausible realization of a random variable $Z \sim \mathcal{N}(0.1).$

* Conclusion: **It is not unlikely that the coin is fair**.

> #### Exercise 35
>
> We use the statistical set-up from the previous problem. Consider a statistical experiment where you flip the coin $200$ times. In one run of this experiment, you observe $80$ **heads**. We will use this data and the estimator $\sqrt{n}\frac{\left( \overline{X}_ n - 0.5\right)}{\sqrt{0.5(1 - 0.5)}}$ to provide an answer to the hypothesis testing question of interest: "Does $p = 0.5$ or does $p \neq 0.5$?".
>
> 1. Let $D_1$ denote the value of the realization of the statistic $\sqrt{n}\frac{\left( \overline{X}_ n - 0.5\right)}{\sqrt{0.5(1 - 0.5)}}$ on the given data set. (Here $n=200$, the number of flips.) What is $D_1$?
>
> 2. Let $Z \sim \mathcal{N}(0,1)$. What is $\mathbf{P}(Z < D_1)$?
>
> 3. Since $n=200$ is fairly large, we may assume that if $p=0.5$ that $\sqrt{n}\frac{\left( \overline{X}_ n - 0.5\right)}{\sqrt{0.5(1 - 0.5)}} \sim \mathcal{N}(0,1).$
>
>    Suppose that $p=0.5$ and you ran the experiment above (consisting of $200$ coin flips) a total of $1000$ times (i.e. a total $200 \times 1000$ coin flips). What is the expected number of experiments such that the estimator $\sqrt{n}\frac{\left( \overline{X}_ n - 0.5\right)}{\sqrt{0.5(1 - 0.5)}}$ is small than the value $D_1$ attained in the first experiment?
>
> **Answer**:
>
> $1) -2.82842$ ; 2) $0.00234$ ; 3) $2$
>
> **Solution**: 
>
> 1) First, 
>$$
> D_1 = \sqrt{200}\left( \frac{\frac{80}{200} - 0.5 }{\sqrt{0.25}} \right) \approx -2.82842.
> $$
> 2) Using a table or computational software, we can also compute that if $Z \sim \mathcal{N}(0,1)$,
> $$
> P(Z < D_1) = \int _{-\infty }^{-2.82842} \frac{1}{\sqrt{2 \pi }} e^{-x^2/2} \,  dx \approx .00234
> $$
> ![lec6-ex35-gaussian](../assets/images/lec6-ex35-gaussian.svg)
> 
> 3) Hence, for a single experiment, if $p=0.5$, then there is (approximately) a $0.23\%$ chance of seeing an observation smaller than $D_1 \approx -2.82842$. Thus if we run $1000$ experiments, we would expect to see
>$$
> 1000*(.00234) \approx 2.33907
> $$
> Experiments where $\sqrt{n}\frac{\left( \overline{X}_ n - 0.5\right)}{\sqrt{0.5(1 - 0.5)}}$ is smaller than $D_1 \approx -2.82842$.
> 
> **Remark 1**:
>
> We can conclude that it is unlikely that $p=0.5$. Indeed, if $p=0.5$, observing the value $D_1 \approx -2.82842$ would be a very rare event, intuitively speaking. 
>
> **Remark 2**:
>
> In general, we will transform our data into a given statistic whose distribution we know well that does **not** depend on the true parameter (e.g., as in this problem, the standard Gaussian). Such a distribution is known as **pivotal**. Then we can reduce our hypothesis testing question to a problem of deciding whether or not a given observation is likely (or not) for this pivotal distribution.

## 7. Statistical Formulation of Hypothesis Testing

Statistical formulation:

* Consider a sample $X_1, ..., X_n$ of i.i.d. random variables and a statistical model $(E, (\mathbb{P}_\theta)_{\theta \in \Theta})$.
* Let $\Theta_0$ and $\Theta_1$ be disjoint subsets of $\Theta$.
* Conside the two hypotheses: $\begin{cases}H_0: &\theta \in \Theta_0 \\ H_1: & \theta \in \Theta_1 \end{cases}$
* $H_0$ is the *null hypothesis*, $H_1$ is the *alternative hypothesis*.
* If we believe that the true $\theta$ is either in $\Theta_0$ or in $\Theta_1$, we may want to *test $H_0$ against $H_1$*.
* We want to decide whether to *reject* $H_0$ (look for evidence against $H_0$ in the data.)

If $\Theta_1$ lies on only one side of $\Theta_0$, this is called a **one sided test**. If $\Theta_1$ lies on both sides of $\Theta_0$, this is called a **two sided test**.

**Remark**: 

Suppose the question is "whether this hospital has longer waiting time than average?" We state the hypothesis as $H_0: \mu \leq 30; H_1 : \mu > 30$, where $H_1$ is what we are looking for evidence in the data to show. Intuitively, if it is "innocnet" at the end, we did not find enough evidence to prove that it is "guilty". And the burden is on the trial to bring evidence. But if you walk away free, it does not mean that it is "innocnet". It is just that we were not able to bring enough evidence.

Regardless of the data, our conclusion will never be to *accept* the null. On observing the data, we will either **reject** the null in favor of the alternative OR we will **fail to reject** the null. In the latter case, we are not claiming that the null is true, rather we are stating that the data does not provide us with enough evidence to refute the null hypothesis.

## 8. Statistical Tests

#### Asymmetry in the hypothesis

* $H_0$ and $H_1$ do not play a symmetric role: the data is only used to try to disprove $H_0$
* In particular lack of evidence, does not mean that $H_0$ is true (" innocent until proven guilty")
* A *(statistical) test* is a **statistic** $\psi \in \{0,1\}$, which does not depend explicitly on the value of true unknown parameter, such that:
  * If $\psi=0$, $H_0$ is not rejected.
  * If $\psi = 1$, $H_1$ is rejected.
* Coin example: $H_0: p = 1/2$ vs. $H_1: p \neq 1/2$.
* $\psi = \mathbf{1} \{ \sqrt{n} {|\bar{X}_n - 0.5|\over \sqrt{0.5(1-0.5)} } > C \}$, for some $C > 0$.

## 9. Type 1/2 Error and Power of a Statistical Test

* Rejection region of a test $\psi$:
  $$
  R_{\psi} = \{ x \in E^n : \psi(x)=1\}.
  $$
  Rejection region of test $\psi_n$:
  $$
  R_{\psi _ n} := \{ (x_1, \ldots , x_ n) \in E^ n: \,  \psi _ n(x_1, \ldots , x_ n) =1 \}
  $$
  where $E$ is the sample space of the i.i.d. variables $X_i$, which is $\R_{\geq0}$ in this example since $X_i$ are uniform random variables.

* Type 1 error of a test $\psi$ (rejecting $H_0$ when it is actually true):
  $$
  \begin{aligned}
  \alpha_{\psi}: \Theta_0 &\rightarrow \R\\
  \theta &\mapsto \mathbb{P_\theta}[\psi = 1].
  \end{aligned}
  $$
  Type 1 error of test $\psi_n$ (rejecting $H_0$ when it is actually true):
  $$
  \begin{aligned}
  \alpha_{\psi_n}: \Theta_0 &\rightarrow \R\\
  \theta &\mapsto \mathbb{P_\theta}[\psi_n = 1].
  \end{aligned}
  $$
  Where $\mathbf{P}_\theta(\psi_n=1)$ is the probability of the event $\psi_n=1$ under the probability distribution $\mathbf{P}_\theta$ when $\theta \in \Theta_0$, i.e. the probability of rejecting $H_0$ when $H_0$ is true. 

* Type 2 error of a test $\psi$ (not rejecting $H_0$ although $H_1$ is actually true):
  $$
  \begin{aligned}
  \beta_\psi:\Theta_1 &\rightarrow \R\\
  \theta &\mapsto \mathbb{P_\theta}[\psi = 0].
  \end{aligned}
  $$
  Type 2 error of test $\psi_n$ (not rejecting $H_0$ although $H_1$ is actually true):
  $$
  \begin{aligned}
  \beta_{\psi_n}:\Theta_1 &\rightarrow \R\\
  \theta &\mapsto \mathbb{P_\theta}[\psi_n = 0].
  \end{aligned}
  $$
  Where $\mathbf{P}_\theta(\psi_n=0)$ is the probability of the event $\psi_n=0$ under the probability distribution $\mathbf{P}_\theta$ when $\theta \in \Theta_1$, i.e. the probability of not rejecting $H_0$ when $H_1$ is true. 

* Power of a test $\psi$:
  $$
  \pi_\psi = \inf_{\theta \in \Theta_1} (1-\beta_\psi(\theta))
  $$

> #### Exercise 36 Type 1 error
>
> Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } N(\mu , 1)$ where $\mu$ is an unknown parameter. You are interested in answering the question of interest: "Does $\mu = 0$?".
>
> To do so, you construct
>
> * the **null hypothesis** $H_0: \mu = 0$;
> * the **alternative hypothesis** $H_1: \mu \neq 0$.
>
> Motivated by the CLT, you decide to use a test of the form
> $$
> \psi _ C = \mathbf{1}(\sqrt{n} \,  | \overline{X}_ n | > C ).
> $$
> Recall that the **type 1 error** (also known as **type 1 error rate** ) of a test $\psi$ is the **function**:
> $$
> \begin{aligned}
> \alpha _{\psi }: \Theta _0 &\rightarrow [0,1]\\
> \theta &\mapsto \mathbf{P}_\theta(\psi = 1)
> \end{aligned}
> $$
> If you choose the threshold $C = q_{0.05}$, what is the type 1 error $\alpha_\psi$?
>
> (In this case, since $H_0$ only consists of one point, the function $\alpha_\psi$ is defined only at one point, and we loosely use the terminology "type 1 error" to mean the value of $\alpha_\psi$ at that point.)
>
> **Solution**:
>
> If we assume the null hypothesis $H_0: \mu = 0$, and since the variance is known to be $1$, the CLT gives
> $$
> \sqrt{n}\,  \overline{X}_ n \sim \mathcal{N}(0,1)\qquad \text {for large } \, n.
> $$
> The probability of a type 1 error is
> $$
> \alpha _\psi (0)\, =\, \mathbf{P}_{0}(\psi _ C = 1)
> $$
> as depicted in the figure below
>
> ![lec6-ex36-type1error](../assets/images/lec6-ex36-type1error.svg)
>
> If $H_0$ is true, i.e. $\mu = 0$, then $\sqrt{n}\overline{X}_n$ is asymptotically normal. Hence, the total area of the two shaded regions is $\mathbf{P}_{0}(\psi _ C = 1)\, =\, \mathbf{P}_0\left(\sqrt{n} \,  | \overline{X}_ n | > q_{0.05} \right)$, the probability that $H_0$ is rejected even through it is true.

> #### Exercise 37 A Non-Asymptotic Test for the Support of a Uniform Variable
>
> Let $X_1, ...,X_n \stackrel{iid}{\sim}\mathsf{Unif}(0, \theta)$ for an unknown parameter $\theta$. Let $\left(\mathbb {R}_{\geq 0}, \{ \text {Unif}[0, \theta ]\} _{\theta > 0}\right)$ denote the associated statistical model. (Here, $\R_{\geq 0}$ denotes the nonnegative real numbers. )
>
> You want to answer the question of interest: "Is $\theta\leq 1/2$?". To do so you formulate a hypothesis test with
> $$
> H_0: \theta \leq 1/2\\H_1: \theta > 1/2
> $$
> And also design the test
> $$
> \psi _ n = \mathbf{1}(\displaystyle \max _{1 \leq i \leq n} X_ i > 1/2)
> $$
> If $\psi_n = 1$ we will reject the null hypothesis. Note the dependence of $\psi_n$ on the sample size.
>
> 1. We use $\Theta_0$ to denote the region of $\Theta$ defined by the null hypothesis. In this example, $\Theta_0$ can be written as an interval $(A,B]$. What are the numbers $A$ and $B$? Similarly, we use $\Theta_1$ to denote the region of $\Theta$ defined by the alternative hypothesis. $\Theta_1$ can be written as an interval $(C,\infty)$. What is the number $C$?
>
> 2. Consider the complement $C_n$ of the rejection region: this is all the points in $(\R_\geq 0)^n$ that do not lie in $R_{\psi_n}$. Note that the dimension of $C_n$ is determined by the sample size $n$. What is the length of $C_1, C_2, C_3$?
>
> 3. Recall that the type 1 error (or error rate) of the test $\psi_n$ is the function
>    $$
>    \begin{aligned}
>    \alpha _{\psi }: \Theta _0 &\rightarrow [0,1]\\
>    \theta &\mapsto \mathbf{P}_\theta(\psi = 1)
>    \end{aligned}
>    $$
>    Where $\mathbf{P}_{\theta} = \mathsf{Unif}[0,\theta],$ and $\mathbf{P}_\theta (\psi_n = 1)$ is the probability of the event $\{\psi_n=1\}$ under the probability distribution $\mathbf{P}_\theta$ when $\theta \in \Theta_0$. i.e. the probability of rejecting $H_0$ when $H_0$ is true.
>
>    What is $\alpha_{\psi_n}(\theta)$?
>
> **Answer**: 
>
> $1.\quad A = 0, B = 1/2, C=1/2.\\2.\quad C_1 = 1/2, C_2 = 1/4, C_3 = 1/8.\\3.\quad 0.$
>
> **Solution**: 
>
> 1) The parameter space is $\Theta = \{\theta: \theta > 0\}$. Since the null hypothesis is $H_0:\theta \leq 1/2$, then $\Theta_0 = (0,1/2]$. Similarly, $\Theta_1 = (1/2, \infty)$.
>
> 2) The complement $C_n$ of the rejection region is the set of all $(x_1, \ldots , x_ n) \in \mathbb {R}_{\geq 0}^ n$ such that $\max _{1 \leq i \leq n} x_ i \leq 1/2$. (Equivalently, it is the set of all $(X_1, ..., X_n)$) such that $\psi _ n = \mathbf{1}(\displaystyle \max _{1 \leq i \leq n} x_ i > 1/2) = 0$. The region defined by the constraint $x_i \leq 1/2$ for all $1 \leq i \leq n$ is the set $[0,1/2]^n$.
>
> In one dimension, this is the interval $[0,1/2]$ which has length $1/2$. In two dimensions, this is the square $[0,1/2]\times [0,1/2]$, which has area $(1/2)^2 = 1/4$. Finally in three dimensions, $C_3$ is a cube $[0,1/2] \times [0,1/2] \times [0,1/2]$, which has volume $(1/2)^3 = 1/8$.
>
> 3)  By definition,
>$$
> \alpha _{\psi _ n}(\theta ) = P_\theta (\max _{1 \leq i \leq n} X_ i > 1/2)
>$$
> Where $P_\theta = \text {Unif}[0, \theta ]$ and we restrict $\theta \in \Theta_0 = \{\theta: \theta \leq 1/2\}$. Observe that if $\theta \leq 1/2$, then there is a $0\%$ chance of generating an observation which is larger than $1/2$. Hence, the type 1 error $\alpha_{\psi_n}(\theta)$ is 0 for all $\theta \in \Theta_0$.
>
> **Remark:** In general, the type 1 error will be a function of $\theta$ , but in this special case it is constant.

## 10. Level of statistical test

"Level" is a very important notion. When building a test, we say “build a test at level $\alpha$.” 

* A test $\psi$ has level $\alpha$ if
  $$
  \alpha_{\psi}(\theta) \leq \alpha, \quad \forall\theta \in \Theta_0.
  $$

* A test $\psi$ has asymptotic level $\alpha$ if
  $$
  \alpha_\psi(\theta) \leq \alpha, \quad \forall \theta \in \Theta_0.
  $$
  Where $\alpha _{\psi }=\mathbf{P}_\theta (\psi =1)$ is the type 1 error. We will use the word "level" to mean the "smallest" such level, i.e. the least upper bound of the type 1 error, defined as follows
  $$
  \alpha = \text {sup}_{\theta \in \Theta _0} \alpha _{\psi }(\theta )
  $$
  Here, $\text {sup}_{\theta \in \Theta _0} $ stands for the supremum over all values of $\theta$ within $\Theta_0$. If $\Theta_0$ is a closed (*resp*. closed half-interval), and if $\alpha_\psi(\theta)$ is continuous (*resp*. continuous and decreasing as it approaches infinity), then its supremum equals the maximum.

* In general, a test has the form
  $$
  \psi = \mathbf{1} \{ T_n > c\},
  $$
  For some statistic $T_n$ and threshold $c\in \R$.

* $T_n$ is called the test statistic. The rejection region is $\R_\psi= \{T_n>c\}$.

> #### Exercise 38 Type 2 error
>
> Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Unif}[0, \theta ]$ for an unknown parameter $\theta$ and we designed the statistical test
> $$
> \psi _ n = \mathbf{1}(\displaystyle \max _{1 \leq i \leq n} X_ i > 1/2)
> $$
> To decide between the null and alternative hypotheses
> $$
> H_0: \theta \leq 1/2\\H_1: \theta > 1/2
> $$
>
> 1. Evaluate $\displaystyle \mathbf{P}_{\theta }(\psi _ n = 0)=\mathbf{P}_{\theta } \left(\max _{1 \leq i \leq n} X_ i \leq 1/2\right)$ at $\theta = 1/2$, the boundary between $\Theta_0$ and $\Theta_1$. In other words, what is $\mathbf{P}_{\theta =1/2} \left(\max _{1 \leq i \leq n} X_ i \leq 1/2\right)$?
>
> 2. In this example, $\Theta_1 = (1/2, \infty)$, and $\mathbf{P}_\theta = \mathsf{Unif}[0,\theta]$. What is $\beta_{\psi_n}(\theta), \lim\limits_{\theta \rightarrow 1/2} \beta_{\psi_n}(\theta),$ and $\lim\limits_{\theta \rightarrow \infty} \beta_{\psi_n}(\theta)$?
>
> 3. What is the power $\pi_{\psi_n}$?
>
> 4. Do the following
>
>    * Place a vertical line at the boundary of $\Theta_0$ and $\Theta_1$.
>    * Sketch the graph of $\mathbf{P}_\theta(\psi_n=1)$ as a function of $\theta$ in black.
>    * Sketch the graph of the type 1 error $\alpha _{\psi _ n}(\theta )$ in yellow.
>    * Sketch the graph of the type 2 error $\beta _{\psi _ n}(\theta )$ in blue.
>
> 5. What is the smallest level $\alpha$ of the test $\psi_n$ ?
>
> 6. How should the threshold of the test be changed to increase the smallest level $\alpha$? In other words, consider tests of the form
>    $$
>    \psi _{n,{{C}} } = \mathbf{1}(\displaystyle \max _{1 \leq i \leq n} X_ i >{{C}}  )
>    $$
>    where $C$ is the threshold. In the original test above, $C=1/2$. What should the value of $C$ be so that the level of $\psi_{n,C}$ is greater than the level of the $\psi_{n,1/2}$?
>
> 7. Determine the smallest threshold $C$ such that the test $\psi_{n,C}$ has level $\alpha$.
>
> **Answer**: 
>
> $1.\quad 1.\\2. \quad \left({1/2\over \theta}\right)^2,\quad 1,\quad 0. \\ 3. \quad 0.\\ 4.$
>
> ![lec6-ex38](../assets/images/lec6-ex38.png)
>
> $5. \quad 0.\\6. \quad C < 1/2.\\7.\quad C=\frac{1}{2}\sqrt[n]{1-\alpha }$
>
> **Solution**:
>
> 1. 
>
> $$
> \begin{aligned}
>  \beta _{\psi _ n}(1/2) &= \mathbf{P}_{1/2}(\displaystyle \max _{1 \leq i \leq n} X_ i < 1/2)\\
>  &= \mathbf{P}_{1/2}(X_1 < 1/2) \ldots \mathbf{P}_{1/2}(X_ n < 1/2)\\
>  &=1 \times 1 ... \times 1 = 1 
> \end{aligned}
> $$
> Where we applied independence of the $X_i's$ in the second line.
>
> 2) For any $\theta \in \Theta_1 = [1/2, \infty),$
> $$
> \begin{aligned}
> \beta _{\psi _ n}(\theta ) \, =\,  \mathbf{P}_{\theta }(\psi _ n=0) & = \mathbf{P}_{\theta }\left(\displaystyle \max _{1 \leq i \leq n} X_ i < 1/2\right)\,\\
> &=\mathbf{P}_{\theta }(X_1 < 1/2) \ldots \mathbf{P}_{\theta }(X_ n < 1/2) = \left(\frac{1/2}{\theta }\right)^ n.
> \end{aligned}
> $$
> As $\theta \rightarrow 1/2,$
> $$
> \beta _{\psi _ n}(\theta )\to \left(\frac{1/2}{1/2}\right)^ n=1.
> $$
> As $\theta \rightarrow \infty$,
> $$
> \beta _{\psi _ n}(\theta )= \left(\frac{1/2}{\theta }\right)^ n\to 0.
> $$
> **Remark**: This test is rather extreme example in that it minimizes type-1 error while maximizing the type-2 error. In general, we want to design tests so that the type-1 and type-2 error are both controlled. These types of trade-offs are crucial to consider in the context of hypothesis testing.
>
> 3)  We computed above that $\beta _{\psi _ n}(1/2) = P_{0.5}[\psi _ n = 0] = 1$. Thus $1- \beta _{\psi _ n}(1/2) = 0$.
>
> Moreover,
> $$
> \pi _{\psi _ n} = \inf _{\theta \in [1/2, \infty )} (1 - P_\theta (\psi _ n = 0)) = \inf _{\theta \in [1/2, \infty )} P_\theta (\psi _ n = 1) \geq 0.
> $$
> Thus, $\pi_{\psi_n} = 0.$
>
> **Remark**: The power of a test is the **largest lower bound** on the probability that if $H_1$ is true, that indeed $H_0$ is rejected in favor of $H_1$. In this example, as $\theta \in \Theta_1$ approaches the boundary $1/2$, the probability of rejecting $H_0$ decreases and approaches $0$. As $\theta \in \Theta_1$ approaches infinity, $P_\theta (\psi _ n = 1) $ increases.
>
> 5)  Since the type 1 error $\alpha _{\psi _ n}(\theta )$ is constantly zero over $\Theta_0$, the smallest level of this test $\psi$ is $\alpha = 0$.
>
> 6) To increase the smallest level $\alpha$ from $0$, note that $\mathbf{P}_\theta \left(\max\limits_{1 \leq i \leq n} X_ i >C\right)=0$ if and only if $\theta \leq C$. This means the constant zero region of graph of $\mathbf{P}_\theta \left(\psi _ C\right)=0$ shifts to the right as $C$ increases from $1/2$, and to the left as $C$ decreases from $1/2$. Since the maximum of type 1 error occurs at the boundary $\theta = 1/2$, this means $C< 1/2$ is required for the level to be positive.
>
> **Remark:** The reason behind increasing the level in this example is to increase the power of the test from $0$. In general, one of the first requirements of a test is to have a small-enough level so that the probability of concluding a false positive, (i.e. rejecting the null while the null is true) is controlled.
>
> 7) Following similar computation as in a previous problem where $C = 1/2$, we have $\mathbf{P}_{\theta }(\psi _{n,C}=1)\, =\, 1-\left(\frac{C}{\theta }\right)^ n$.  
>
> Note that
> $$
> P(\max_i X_i > C) = 1 - P(\max_i X_i < C) = 1 - (P_\theta(X_i < C))^n
> $$
> And the level $\alpha$ is
> $$
> \alpha _{\psi }=\mathbf{P}_\theta (\psi =1)
> $$
> The smallest level is 
> $$
> \begin{aligned}
> \alpha &= \max _{\theta \in \Theta _0} p_{\theta }(\psi _{n,C}=1)\\
> &= p_{1/2}(\psi _{n,C}=1)\, =\, 1-\left(\frac{C}{1/2}\right)^ n
> \end{aligned}
> $$
> A test with threshold $C=\frac{1}{2}\sqrt[n]{1-\alpha }$ or smaller will have level $\alpha$.
>
> **Remark**: Notice the threshold $C$ depends on $n,\alpha$, as well as the value of $\theta$ at the boundary of $\Theta_0$ and $\Theta_1$.

## 11. One-sided vs two-sided tests

We can refine the terminology when $\theta \in \Theta \subset \R$ and $H_0$ is of the form
$$
H_0:\theta = \theta_0 \iff \Theta_0 = \{\theta_0\}
$$

* If $H_1: \theta \neq \theta_0$: two-sided test
* If $H_1:\theta > \theta_0$ or $H_1:\theta <\theta_0$: one-sided test

One or two sided tests will have different rejection regions.



Problem set for Lec11-12.

# 1. Asymptotic Variance of MLE for Curved Gaussian

Let $X_1, ..., X_n$​​ be $n$​ i.i.d. random variables with distribution $\mathcal{N}(\theta, \theta)$​ for some unknown $\theta > 0$.

In the last homework, you have computed the MLE $\hat{\theta}$​ for $\theta$​ in terms of the sample averages of the linear and quadratic means, i.e. $\overline{X}_n$​ and $\overline{X^2_n}$​, and applied the CLT and delta method to find its asymptotic variance. 

In this problem, you will compute the asymptotic variance of $\hat{\theta}$ Via the Fisher Information.

1. Denoting the log likelihood for one sample by $\ell(\theta,x)$​, compute the second derivative $\,   \frac{d^2}{d \theta ^2} \ell (\theta , x)  \,$​.
2. Then, compute the Fisher information $\mathcal{I}(\theta)$​.
3. Finally, what does this tell us about the asymptotic variance of $\hat{\theta}$​?

> **Solution:**
>
> Let $\ell(\theta, x)$​ denote the log likelihood for one sample. Recall its first derivative
> $$
> \begin{aligned}
> \ell (\theta ,x) &= -\frac{1}{2} \left( \log (2) + \log (\pi ) + \log (\theta ) \right) - \left( \frac{1}{2 \theta } X^2 - X + \frac{1}{2} \theta \right)\\
> \Longrightarrow \frac{d}{d \theta } \ell (\theta ,x) &= - \frac{1}{2 \theta } + \frac{1}{2 \theta ^2} X^2 - \frac{1}{2}.
> \end{aligned}
> $$
> Differentiating one more time yields
> $$
> \frac{d^2}{d \theta ^2} \ell (\theta , x) = \frac{1}{2 \theta ^2} - \frac{x^2}{\theta ^3}.
> $$
> Since
> $$
> I(\theta ) =-\mathbb E\left[ \frac{d^2}{d \theta ^2} \ell (\theta , X) \right] = \frac{2 \theta + 1}{2 \theta ^2}.
> $$
> By the theorem about the asymptotic variance of the MLE from class, we finally have
> $$
> V(\hat\theta ) = I(\theta )^{-1} = \frac{2 \theta ^2}{(2 \theta + 1)}
> $$

# 2. Maximum Likelihood Estimators and Fisher Information

For each of the following distribution, compute the MLE based on $n$​ i.i.d. observations $X_1,...,X_n$​ and the Fisher information, if defined.

1. Let $X_ i \sim \textsf{Ber}(p),\ p \in (0, 1)$​, what is the MLE $\hat{p}$​, Fisher information $\mathcal{I}(p)$​, and the asymptotic variance $V(\hat{p})$​ of the MLE $\hat{p}$​.
2. Let $X_ i \sim \textsf{Poiss}(\lambda ), \lambda > 0,$​​ what is the MLE $\hat{\lambda}$​​, Fisher information $\mathcal{I}(\lambda)$​​, and the asymptotic variance $V(\hat{\lambda})$​​ of the MLE $\hat{\lambda}$​.
3. Let $X_ i \sim \textsf{Exp}(\lambda ),\ \lambda > 0,$ what is the MLE $\hat{\lambda}$, Fisher information $\mathcal{I}(\lambda)$, and the asymptotic variance $V(\hat{\lambda})$ of the MLE $\hat{\lambda}$​.
4. Let $X_ i \sim \mathcal{N}(\mu , \sigma ^2), \ \mu \in \mathbb {R}, \,  \sigma ^2 > 0,$​​​​ what is the MLE $\hat{\mu}$​​​, and $\widehat{\sigma^2}$​​​, Fisher information $\mathcal{I}(\lambda)$​​​, and the asymptotic variance $V(\widehat{\sigma^2})$​​​ of the MLE $\widehat{\sigma^2}$​​​​.
5. $X_i$ follows a shifted exponential distribution with parameters $a \in \R$ and $\lambda > 0$. What is the MLE $\hat{a}$ and $\hat{\lambda}$, Fisher information $\mathcal{I}(\lambda)$, and the asymptotic variance $V(\hat{\lambda})$ of the MLE $\hat{\lambda}$.

> **Solution:**
>
> 1. The likelihood for one sample can be written as
>    $$
>    L_1(X_1,p) = p^{X_1} (1-p)^{1-X_1}.
>    $$
>    That means that the log likelihood for one sample is
>    $$
>    \ell _1(X_1, p) = X_1 \ln (p) + (1-X_1) \ln (1-p),
>    $$
>    and for $n$​​ samples this yields
>    $$
>    \ell _ n(X_1, \dots , X_ n, p) = \ln (p) \sum _{i=1}^{n} X_ i + \ln (1-p) \left(n - \sum _{i=1}^{n} X_ i \right).
>    $$
>    Differentiating with respect to $p$ yields
>    $$
>    \frac{\partial }{\partial p} \ell _ n(p) = \frac{1}{p} \sum _{i=1}^{n} X_ i - \frac{1}{1-p} \left(n - \sum _{i=1}^{n} X_ i\right).
>    $$
>    Setting this to zero to find the MLE $\hat{p}$​ then gives
>    $$
>    \begin{aligned}
>    0&=(1-\hat p) \sum _{i=1}^{n} X_ i - \hat p \left( n - \sum _{i=1}^{n} X_ i \right)\\
>    \iff \hat{p} &= \frac{1}{n} \sum _{i=1}^{n} X_ i = \overline{X}_ n.
>    \end{aligned}
>    $$
>    That this is indeed the global maximum can be verified by checking the concavity of the log likelihood, which in turn can be seen from the negativity of the second derivative.
>
>    The second derivative for one sample is
>    $$
>    \frac{\partial ^2}{\partial p^2} \ell _1(p) = -\frac{X_1}{p^2}-\frac{1-X_ i}{(1 - p)^2} < 0,
>    $$
>    and by $\mathbb{E}[X_i]=p$, we obtain
>    $$
>    I(p) = -\mathbb E\left[ \frac{\partial ^2}{\partial p^2} \ell _1(p) \right] = \frac{1}{p} + \frac{1}{1-p} = \frac{1}{p(1-p)}.
>    $$
>    Take the inverse of this to obtain the (asymptotic) variance: $V(\hat{p}) = p(1-p)$​.
>
> 2. Each $X_i$ has distribution
>    $$
>    \mathbf{P}_\lambda (X = k) = e^{-\lambda } \frac{\lambda ^ k}{k!}, \quad k \in \mathbb {N}.
>    $$
>    The likelihood for one sample can be written as
>    $$
>    L_1(X_1,\lambda ) = e^{-\lambda } \frac{\lambda ^{X_1}}{X_1!}.
>    $$
>    That means that the log likelihood for one sample is
>    $$
>    \ell _1(X_1, \lambda ) = -\lambda + X_1 \ln (\lambda ) - \ln (X_1!),
>    $$
>    and for $n$​ samples this yields
>    $$
>    \ell _ n(X_1, \dots , X_ n, p) = -n \lambda + n \overline{X}_ n \ln (\lambda ) - \sum _{i=1}^{n} \ln (X_ i!).
>    $$
>    Differentiating with respect to $\lambda$ yields
>    $$
>    \frac{\partial }{\partial \lambda } \ell _ n(\lambda ) = -n + n \overline{X}_ n \frac{1}{\lambda }
>    $$
>    Setting this to zero to find the MLE $\hat{\lambda}$​​ then gives
>    $$
>    \hat\lambda = \overline{X}_ n.
>    $$
>    That this is indeed the global maximum can be verified by checking the concavity of the log likelihood, which in turn can be seen from the negativity of the second derivative.
>
>    The second derivative for one sample is
>    $$
>    \frac{\partial ^2}{\partial \lambda ^2} \ell _1(\lambda ) = - \frac{X_ i}{\lambda ^2} < 0,
>    $$
>    and by $\,   \mathbb E[X_ i] = \lambda  \,$​, we obtain
>    $$
>    I(\lambda ) = -\mathbb E\left[ \frac{\partial ^2}{\partial \lambda ^2} \ell _1(\lambda ) \right] = \frac{\lambda }{\lambda ^2} = \frac{1}{\lambda }.
>    $$
>    Take the inverse of this to obtain the (asymptotic) variance: $V(\hat{p}) = \lambda$​​.
>
> 3. Each $X_1$ has density
>    $$
>    f_{\lambda }(x)= \lambda e^{-\lambda x}, \quad x > 0.
>    $$
>    The likelihood for one sample can be written as
>    $$
>    L_1(X_1,\lambda ) = \lambda e^{-\lambda X_1}.
>    $$
>    The log likelihood for one sample is
>    $$
>    \ell _1(X_1, \lambda ) = \ln (\lambda ) - \lambda X_1,
>    $$
>    and for $n$ samples this yields
>    $$
>    \ell _ n(X_1, \dots , X_ n, p) = n \ln (\lambda ) - n \lambda \overline{X}_ n.
>    $$
>    Differentiating with respect to $\lambda$ yields
>    $$
>    \frac{\partial }{\partial \lambda } \ell _ n(\lambda ) = \frac{n}{\lambda } - n \overline{X}_ n.
>    $$
>    Setting this to zero to find the MLE $\hat{\lambda}$​ then gives
>    $$
>    \hat\lambda = \frac{1}{\overline{X}_ n}.
>    $$
>    That this is indeed the global maximum can be verified by checking the concavity of the log likelihood, which in turn can be seen from the negativity of the second derivative.
>
>    The second derivative for one sample is
>    $$
>    \frac{\partial ^2}{\partial \lambda ^2} \ell _1(\lambda ) = -\frac{1}{\lambda ^2}.
>    $$
>    and hence
>    $$
>    I(\lambda ) = -\mathbb E\left[ \frac{\partial ^2}{\partial \lambda ^2} \ell _1(\lambda ) \right] = \frac{1}{\lambda ^2}.
>    $$
>    Take the inverse of this to obtain the (asymptotic) variance: $V(\widehat{\sigma^2}) = \lambda^2$​.
>
> 4. Each $X_1$ Has density
>    $$
>    f_{\mu , \sigma ^2}(x)= \frac{1}{\sqrt{2 \pi \sigma ^2}} \exp \left( -\frac{(x - \mu )^2}{2 \sigma ^2} \right).
>    $$
>    The likelihood for one sample can be written as
>    $$
>    L_1(X_1,\mu ,\sigma ^2) = \frac{1}{\sqrt{2 \pi \sigma ^2}} \exp \left( -\frac{(X_1-\mu )^2}{2 \sigma ^2} \right).
>    $$
>    The log likelihood for one sample is
>    $$
>    \ell _1(X_1, \mu , \sigma ^2) = -\frac{1}{2} \left( \ln (2) + \ln (\pi ) + \ln (\sigma ^2) \right) - \frac{(X_1 - \mu )^2}{2 \sigma ^2},
>    $$
>    and for $n$ samples this yields
>    $$
>    \ell _ n(X_1, \dots , X_ n, \mu , \sigma ^2) = -\frac{n}{2} \left( \ln (2 \pi ) + \ln (\sigma ^2) \right) - \sum _{i=1}^{n} \frac{(X_ i - \mu )^2}{2 \sigma ^2}.
>    $$
>    Differentiating with respect to $\mu$ yields
>    $$
>    \frac{\partial }{\partial \mu } \ell _ n(\mu , \sigma ^2) = \sum _{i=1}^{n} \frac{X_ i - \mu }{2 \sigma ^2}.
>    $$
>    Setting this to zero to find the MLE $\hat{\mu}$​ then gives
>    $$
>     \hat\mu = \overline{X}_ n.
>    $$
>    Similarly, differentiating the log likelihood with respect to $\sigma^2$​​, we find
>    $$
>    \frac{\partial }{\partial \sigma ^2} \ell _ n(\mu , \sigma ^2) = -\frac{n}{2 \sigma ^2} + \sum _{i=1}^{n} \frac{(X_ i - \mu )^2}{2 \sigma ^4}.
>    $$
>    Setting this to zero gives
>    $$
>    \widehat{\sigma ^2} = \frac{1}{n} \sum _{i=1}^{n} (X_ i - \hat\mu )^2 = \frac{1}{n} \sum _{i=1}^{n} (X_ i - \overline{X}_ n)^2 = \overline{X_ n^2} - \overline{X}_ n^2.
>    $$
>    That this is indeed the global maximum can be verified by checking the concavity of the log likelihood, which in turn can be seen from the negative definiteness of the Hessian.
>
>    We compute the second derivatives:
>    $$
>    \begin{aligned}
>    \frac{\partial ^2}{\partial \mu ^2} \ell _1(\mu , \sigma ^2) &= -\frac{1}{\sigma ^2},\\
>    \frac{\partial ^2}{\partial \mu \partial (\sigma ^2)} &= - \frac{1}{(\sigma ^2)^2} (X_1 - \mu ),\\
>    \frac{\partial ^2}{\partial (\sigma ^2)^2} &= \frac{1}{2 (\sigma ^2)^2} - \frac{1}{(\sigma ^2)^3} (X_1 - \mu )^2,
>    \end{aligned}
>    $$
>    from where negative definiteness can be checked by testing $\,   \operatorname {tr}H \ell _1 < 0  \,$​​ and $\,   \mathrm{det}H \ell _1 > 0  \,$​. Moreover,
>    $$
>    \begin{aligned}
>    I(\mu , \sigma ^2) &= - \begin{pmatrix}  \mathbb E\left[ \frac{1}{\sigma ^2} \right] &  \frac{\mathbb E[X_1 - \mu ]}{(\sigma ^2)^2}\\ \frac{\mathbb E[X_1 - \mu ]}{(\sigma ^2)^2} &  \mathbb E\left[ \frac{1}{2 (\sigma ^2)^2} - \frac{(X_1 - \mu )^2}{(\sigma ^2)^3} \right] \end{pmatrix}\\
>    &= \begin{pmatrix}  \frac{1}{\sigma ^2} &  0\\ 0 &  \frac{1}{2 \sigma ^4} \end{pmatrix}. 
>    \end{aligned}
>    $$
>    Take the inverse of this to obtain the (asymptotic) variance: $V(\widehat{\sigma^2}) = 2\sigma^4$​​.​
>
> 5. Each $X_i$ has density
>    $$
>    f_{a, \lambda }(x) = \lambda e^{-\lambda (x-a)} \mathbf{1}\{ x \geq a\} , \quad x \in \mathbb {R}.
>    $$
>    The likelihood for one sample can be written as
>    $$
>    L_1(X_1,a,\lambda ) = \lambda e^{-\lambda (X_ i - a)} \mathbf{1}\{ X_ i \geq a\} .
>    $$
>    The likelihood for $n$​​ samples is
>    $$
>    L_ n(X_1, \dots , X_ n, a, \lambda ) = \lambda ^ n \exp \left(-\lambda \sum _{i=1}^{n} (X_ i - a)\right) \mathbf{1}\{  \min X_ i \geq a\} .
>    $$
>    Consider two cases, we note that if $a > \min{X_i}$, then $\mathbb{1}\{\min X_i \geq a\}=0$ and hence 
>
>    $$
>    \,   L_ n(X_1, \dots , X_ n, a, \lambda ) = 0  \,
>    $$
>
>    On the other hand, if $a \leq \min X_i$​, then $\mathbb{1}\{\min X_i \geq a\}$, and
>    $$
>    L_ n(X_1, \dots , X_ n,a, \lambda ) = e^{n \lambda a} \lambda ^ n \exp \left( -\lambda \sum _{i=1}^{n} X_ i \right),
>    $$
>    which is an increasing function of $a$. Hence, for any fixed value of $\lambda$, the MLE for $a$ is
>    $$
>    \hat a = \min _{1 \leq i \leq n} X_ i.
>    $$
>     For this value of $a$, we can now optimize the log likelihood
>    $$
>    \ln L_ n(X_1, \dots , X_ n, \hat a, \lambda ) = n \ln (\lambda ) - \lambda (n \overline{X}_ n - n \hat a), \\
>    \frac{\partial }{\partial \lambda } \ln L_ n(\hat a, \lambda ) = \frac{n}{\lambda } - n (\overline{X}_ n - \hat a).
>    $$
>    
>    Setting the first derivative to zero then yields
>    $$
>    \hat\lambda = \frac{1}{\overline{X}_ n - \min X_ i}.
>    $$
>    For this example, the Fisher information is not well-defined since the support of the distribution depends on the parameter $a$.

# 3. Method of Moments Estimators

For each of the following distributions, give the method of moments estimator in terms of the sample average $\overline{X_n}$​​ and $\overline{X^2_n}$​, assuming we have access to $n$​ i.i.d. observations $X_1,..., X_n$​. In other words, express the parameters as functions of $\mathbb{E}[X_1]$​ and $\mathbb{E}[X_1^2]$​ and then apply these functions to $\overline{X_n}$ and $\overline{X_n^2}$​.​

1. Let $X_ i \sim \textsf{Ber}(p), \ p \in (0, 1)$​​​​, what is the method of moments estimator $\hat{p} \ $?
2. Let $X_ i \sim \textsf{Poiss}(\lambda ), \ \lambda > 0$​​​​ what is the method of moments estimator $\hat{\lambda} \ $?​​​​
3. Let $X_ i \sim \textsf{Exp}(\lambda ), \ \lambda > 0,$​​ what is the method of moments estimator $\hat{\lambda} \ $?​
4. Let $X_ i \sim \mathcal{N}(\mu , \sigma ^2), \ \mu \in \mathbb {R}, \,  \sigma ^2 > 0$​​​, what is the method of moments estimator $\hat{\mu}$​​ and $\hat{\sigma} \ $​​​?
5. Let $X_i$​​ Follows a shifted exponential distribution with parameters $a \in \R$​​ And $\lambda > 0$​​. what is the method of moments estimator $\hat{a}$​​ and $\hat{\lambda} \ $?

> **Solution:**
>
> 1. For Bernoulli variables, we have
>    $$
>    \mathbb E_ p[X_1] = p,
>    $$
>    hence,
>    $$
>    \hat p = \overline{X}_ n.
>    $$
>
> 2. For a Poisson random variable, we have
>    $$
>    \mathbb E_{\lambda }[X_1] = \lambda ,
>    $$
>    hence,
>    $$
>    \hat\lambda = \overline{X}_ n.
>    $$
>
> 3. For an Exponential random variable, we have
>    $$
>    \mathbb E_\lambda [X_1] = \frac{1}{\lambda },
>    $$
>    so
>    $$
>    \lambda = \frac{1}{\mathbb E_\lambda [X_1]}.
>    $$
>    Hence, the method of moments estimator is
>    $$
>    \hat\lambda = \frac{1}{\overline{X}_ n}.
>    $$
>
> 4. For a Gaussian distribution, we have
>    $$
>    \begin{aligned}
>    \mathbb E_{\mu , \sigma ^2}[X_1] &= \mu \\
>    \mathbb E_{\mu , \sigma ^2}[X_1^2] &= \textsf{Var}_{\mu , \sigma ^2}(X_1) + \mathbb E[X_1]^2 = \mu ^2 + \sigma ^2,
>    \end{aligned}
>    $$
>    which we can invert to obtain
>    $$
>    \begin{aligned}
>    \mu &= \mathbb E_{\mu , \sigma ^2}[X_1]\\
>    \sigma^2 &= \mathbb E_{\mu , \sigma ^2}[X_1^2] - \mathbb E_{\mu , \sigma ^2}[X_1]^2.
>    \end{aligned}
>    $$
>    Plugging in the estimator $\overline{X_n}$​ and $\overline{X^2_n}$​ then yields
>    $$
>    \begin{aligned}
>    \hat{\mu} &= \overline{X_n}\\
>    \widehat{\sigma^2} &= \overline{X_n^2} - \overline{X_n}^2.
>    \end{aligned}
>    $$
>
> 5. Since $X_1$ Is a shifted exponential random variable
>    $$
>    \mathbb E_{a, \lambda }[X_1] = \mathbb E_{0, \lambda }[a + X_1] = a + \frac{1}{\lambda },
>    $$
>    and
>    $$
>    \begin{aligned}
>     \mathbb E_{a, \lambda }[X_1^2] &= \textsf{Var}_{0, \lambda }(X_1) + \mathbb E_{a, \lambda }[X_1]^2\\
>     &= \frac{1}{\lambda ^2} + \left( \frac{1}{\lambda } + a \right)^2.
>    \end{aligned}
>    $$
>    That means
>    $$
>    a = \mathbb E_{a, \lambda }[X_1] - \frac{1}{\lambda },
>    $$
>    and plugging this into the equation for the second order moment, we obtain
>    $$
>    \begin{aligned}
>    \frac{1}{\lambda ^2} + \left( \frac{1}{\lambda } + \mathbb E_{a, \lambda }[X_1] - \frac{1}{\lambda } \right)^2  &=  \mathbb E_{a, \lambda }[X_1^2]\\
>    \iff {1\over \lambda} &= \left( \mathbb E_{a, \lambda }[X_1^2] - \mathbb E_{a, \lambda }[X_1]^2 \right)^{1/2}\\
>    \iff {\lambda} &=  \left( \mathbb E_{a, \lambda }[X_1^2] - \mathbb E_{a, \lambda }[X_1]^2 \right)^{-1/2},
>    \end{aligned}
>    $$
>    which plugged back into the first equation yields
>    $$
>    a = \mathbb E_{a, \lambda }[X_1] - \left( \mathbb E_{a, \lambda }[X_1^2] - \mathbb E_{a, \lambda }[X_1]^2 \right)^{1/2}.
>    $$
>    Hence, the method of moment estimators are
>    $$
>    \begin{aligned}
>    \hat\lambda =&\left( \overline{X_ n^2} - (\overline{X}_ n)^2 \right)^{-1/2}\\
>    \hat a =& \overline{X}_ n - \left( \overline{X_ n^2} - (\overline{X}_ n)^2 \right)^{1/2}. 
>    \end{aligned}
>    $$

# 4. Maximum Likelihood Estimation, Tests, and Confidence Intervals

Let $\, X_1, \dots , X_ n\stackrel{iid}{\sim } X\,$​ be distributed i.i.d. with probability density function
$$
f_\theta (x)=(x/\theta ^2)\exp (-x^2/2\theta ^2)\, \mathbf{1}(x\ge 0), \theta >0.
$$

1. Let $l(\theta )=\ln L(X_1,\ldots ,X_ n,\theta )$ denote the log likelihood. Find the critical point of $l(\theta)$. (The critical point is unique because KL divergence is definite.). Find the second derivative $l^{\prime \prime }=\frac{d^2l}{d\theta ^2}\,$ of $l(\theta)$. Determine whether the critical point is a global maximum or a global minimum, or neither of $l(\theta)$ in the domain $\theta >0$? What can you conclude about the maximum likelihood estimator $\hat{\theta}$ for $\theta$?

2. What is the Fisher information $I(\theta)$​​​ of the random variables $X_i$​?

3. Use the theorem for the MLE to write down the asymptotic distribution of the MLE $\hat{\theta}$​. Specifically, give an asymptotic $95\%$​ C.I. $\, \mathcal{I}_{\text {plug-in}}\,$​ for $\theta$ using the plug-in method. 

4. Use the results from the previous parts to give a test with asymptotic level $\alpha$ for testing
   $$
   H_0: \theta = 1 \quad \mbox{ v.s. } \quad H_1: \theta \neq 1 .
   $$
   Suppose $n=100$​​ and the data gives $\overline{X}_ n=1.5$​​ and $\overline{X_ n^2}=4.0$​. Find the $p$-value associated to this data for this hypothesis test.

> **Solution:**
>
> 1. Given $f_\theta (x)=(x/\theta ^2)\exp (-x^2/2\theta ^2)\mathbf{1}(x\ge 0), \theta >0.$​
>
>    The log-likelihood is
>    $$
>    l_ n(\theta ) \, =\, \ln \prod _ i^ n f_\theta (x_ i)=\sum _{i=1}^ n \ln x_ i -2n\ln \theta -\frac{1}{2\theta ^2}\sum _{i=1}^{n} (x_ i)^2
>    $$
>    Now, find the critical point of $\, \ln L(x_1,\ldots ,x_ n, \theta )\,$​ (there is a unique one because the KL divergence is definite):
>    $$
>    \begin{aligned}
>    \frac{dl_ n}{d\theta } &=-\frac{2n}{\theta }+\frac{\sum _{i=1}^{n} (x_ i)^2}{\theta ^3}\, =\, 0\\
>    \iff \theta &= \sqrt{\frac{\sum _{i=1}^{n} (x_ i)^2}{2n}}.
>    \end{aligned}
>    $$
>    Check that the critical point is indeed a maximum of $l_n(\theta)$:
>    $$
>    \begin{aligned}
>    \frac{d^2l_ n}{d\theta ^2} &= \frac{2n}{\theta ^2}-3\frac{\sum _{i=1}^{n} (x_ i)^2}{\theta ^4}\, =\, \frac{1}{\theta ^2}\left(2n-3\frac{\sum _{i=1}^{n} (x_ i)^2}{\theta ^2}\right)\\
>    \left.\frac{d^2l_ n}{d\theta ^2}\right|_{\theta =\sqrt{\frac{\sum _{i=1}^{n} (x_ i)^2}{2n}}} &=\frac{2n}{\sum _{i=1}^{n} (x_ i)^2}\left(2n-6n\right)\\
>    &=-{8n^2}{\sum _{i=1}^{n} (x_ i)^2}<0.
>    
>    \end{aligned}
>    $$
>    This means that the critical point we found is a local maximum.
>
>    Finally, check that the critical point is a global maximum. Since $l'_n(\theta)$​​ is defined for all $\theta > 0$​, and there is only one critical point, it follows that this critical point is a global maximum in $\theta > 0$​. (The function $l_n(\theta)$​ is strictly increasing to the left of the critical point and strictly decreasing to the right of the critical point.) Hence, the MLE of $\theta$ is
>    $$
>    \widehat{\theta } = \sqrt{\frac{\overline{X_ n^2}}{2}}.
>    $$
>
> 2. Setting $n=1$​​ in the expression for $l''_n(\theta)$​ computed above, we get
>    $$
>    l_1^{\prime \prime }(\theta ) = \frac{2}{\theta ^2}-\frac{3x^2}{\theta ^4}.
>    $$
>    This gives Fisher information $I(\theta)$:
>    $$
>    I(\theta )\, =\, -\mathbb E(l_1^{\prime \prime }(\theta )) = -\frac{2}{\theta ^2}+\frac{3}{\theta ^4}\mathbb E[x^2].
>    $$
>    It remains to compute the second moment $\, \mathbb E[x^2]$:
>    $$
>    \begin{aligned}
>    \, \mathbb E[x^2] &= \int _0^\infty \frac{x^3}{\theta ^2} e^{-\frac{x^2}{2\theta ^2}}\, dx\\
>    &=\int _0^\infty (x^2)\left( \frac{x}{\theta ^2} e^{-\frac{x^2}{2\theta ^2}}\right)\, dx\\
>    &= \left.Cx^2e^{-\frac{x^2}{2\theta ^2}}\right|_0^\infty + \int _0^\infty (2x)\left(e^{-\frac{x^2}{2\theta ^2}}\right)\, dx\qquad (\text {Integration by part})\\
>    &= 0+2\theta ^2\left[-e^{-\frac{x^2}{2\theta ^2}} \right]_0^\infty\\
>    &= 2\theta ^2.
>    \end{aligned}
>    $$
>    Plugging this back into the expression for the Fisher information, we got
>    $$
>    I(\theta )\, =\, \frac{4}{\theta ^2}.
>    $$
>
> 3. Since the asymptotic variance is given by $I^{-1}(\theta )=\frac{\theta ^2}{4}$​, a plug-in C.I. at confidence level  $95\%$ is
>    $$
>    \begin{aligned}
>    \mathcal{I} &= \left[\hat{\theta }-\frac{q_{0.025}}{\sqrt{n I}},\,  \hat{\theta }+\frac{q_{0.025}}{\sqrt{n I}}\right]\\
>    &= \left[\hat{\theta }-\frac{q_{0.025}}{\sqrt{n}}\frac{\hat{\theta }}{2},\,  \hat{\theta }+\frac{q_{0.025}}{\sqrt{n}}\frac{\hat{\theta }}{2}\right]
>    \end{aligned}
>    $$
>
> 4. The desired test is
>    $$
>    \Psi =\mathbf{1}\left(|T_ n|>q_{\alpha /2}\right)\quad \text {where } T_ n
>    $$
>    where $\theta_0=1$ is the value under the null hypothesis, and with $n=100, \overline{X}_n=1.5$ And $\overline{X_ n^2}=4.0, \hat{\theta }=\sqrt{\frac{\overline{X_ n^2}}{2}}\, =\,  \sqrt{2},\,$and $I(\theta _0)=\frac{4}{\theta _0^2}=4$. This gives the associated $p$-value of
>    $$
>    \begin{aligned}
>    2\left(1-\Phi (T_ n)\right) &= 2\left(1-\Phi \left(\sqrt{n I(\theta _0)}\left(\hat{\theta }-1\right)\right)\right)\\
>    &= 2\left(1-\Phi \left(\sqrt{(100)(4)}\left(\sqrt{2}-1\right)\right)\right)
>    \end{aligned}
>    $$
>    Hence, for any test with level larger than this expression, the test will reject the null hypothesis.

# 5. Censored data

In a given population, $n$ individuals are sampled randomly, with replacement, and each sampled individual is asked whether his/her salary is greater than some fixed threshold $z$. Assume that the salary of a randomly chosen individual has the exponential distribution with unknown parameter $\lambda$. Asking whether the salary overcomes a given threshold rather than directly asking for the salary increases the number people that are willing to answer and decreases the number of mistakes in the collected answers.

Denote by $\,  X_1,\ldots ,X_ n \,$​​​ the binary responses of the $n$​​ sampled individuals, so that $\,   X_ i \in \{ 0, 1\}   \,$​. We call the $X_i$ **censored data**.

1. What kind of distribution do the $X_i$​​​'s follow? Give the parameter of this distribution in terms of $\lambda$​ and $z$:
2. Let $\overline{X}_n$ be the proportion of sampled individuals whose response was $1$ (Corresponding to Yes). Convince yourself that $\overline{X}_n$​ is asymptotically normal. What is its asymptotic variance?
3. Find a function $f$​​ such that $\,  f(\overline{X}_ n) \,$​​ is a consistent estimator of $\lambda$​.
4. Convince yourself that $\,  f(\overline{X}_ n) \,$​ Is asymptotically normal and compute its asymptotic variance.
5. What equation must $z$​ satisfy in order to minimize the asymptotic variance computed in (4)? Write the equation in the form $g_\lambda(z)=z$​.​
6. Let $Y_1, ..., Y_n$ be the salaries of the $n$ sampled people. If one could actually observe $Y_1, ..., Y_n$, what would be the Fisher information of $Y, I_Y(\lambda)$, depending on $\lambda$​?
7. In the model where only the $X_i$​'S are observed (with fixed threshold $z$​), what is the Fisher information $I_X(\lambda)$​​​?
8. Compare $\,  I_ Y(\lambda ) \,$​​ and $\,  I_ X(\lambda ) \,$​. How to interpret this in this model?

> **Solution:**
>
> 1. If $Y_1, ..., Y_n$ denote the salaries of the sampled individuals, then
>    $$
>    Y_ i \sim \textsf{Exp}(\lambda ), \quad 1 \leq i \leq n, 
>    $$
>    and
>    $$
>    X_ i = \mathbf{1}\{ Y_ i \geq z\} , \quad 1 \leq i \leq n.
>    $$
>    Hence, $X_i$ follows a Bernoulli distribution with parameter
>    $$
>    \mu (\lambda ) = p(\lambda ) = \mathbb E[X_1] = \mathbf{P}_\lambda (Y_ i \geq z) = e^{-\lambda z}.
>    $$
>
> 2. $\overline{X}_n$​ is just the sample average and hence asymptotically normal by the CLT.
>
>    As a Bernoulli variable, the variance of $X_i$ is
>    $$
>    \textsf{Var}(X_ i) = p(\lambda ) (1-p(\lambda )) = e^{-\lambda z} (1 - e^{-\lambda z}).
>    $$
>    Hence, we have
>    $$
>    \sqrt{n} (\overline{X}_ n - e^{-\lambda z}) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, e^{-\lambda z} (1 - e^{-\lambda z})).
>    $$
>
> 3. By the LLN, we have
>    $$
>    \overline{X}_ n \xrightarrow [n \to \infty ]{\mathrm{\mathbf{P}}} \mathbb E[X_1] = e^{-\lambda z}.
>    $$
>    Hence, we can solve for $\lambda$​ with a continuous function,
>    $$
>    \lambda = -\frac{1}{z} \ln (\mathbb E[X_1]),
>    $$
>    and obtain a consistent estimator by setting
>    $$
>    f(\overline{X}_ n) = -\frac{1}{z} \ln (\overline{X}_ n).
>    $$
>
> 4. Since we have
>    $$
>    \sqrt{n} (\overline{X}_ n - e^{-\lambda z}) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, e^{-\lambda z} (1 - e^{-\lambda z}))
>    $$
>    by Delta method, we obtain
>    $$
>    \sqrt{n}(f(\overline{X}_ n) - f(e^{-\lambda z})) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, (f'(e^{-\lambda z}))^2 e^{- \lambda z}(1 - e^{-\lambda z})),
>    $$
>    with
>    $$
>    f(u) = -\frac{1}{z} \ln (u).
>    $$
>    Computing the first derivative yields
>    $$
>    f'(u) = -\frac{1}{zu}, \quad \text {so } f'(e^{-\lambda z}) = -\frac{1}{z e^{-\lambda z}}.
>    $$
>    Plugging this into the above Delta Method formula gives
>    $$
>    \sqrt{n}(f(\overline{X}_ n) - \lambda )) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, e^{2 \lambda z} e^{- \lambda z}(1 - e^{-\lambda z}) \frac{1}{z^2}),
>    $$
>    so the asymptotic variance is
>    $$
>    V(f(\overline{X}_ n)) = \frac{e^{\lambda z} - 1}{z^2}.
>    $$
>
> 5. Differentiating the asymptotic variance yields
>    $$
>    V'(z) = \frac{2 + e^{\lambda z}(-2 + \lambda z)}{z^{3}}.
>    $$
>    We solve for stationarity by setting $\,   V'(z) = 0  \,$​, which is equivalent to
>    $$
>    \begin{aligned}
>    0 &=2 + e^{\lambda z}(-2 + \lambda z)\\
>    z&=\frac{2}{\lambda } (1 - e^{-\lambda z}).
>    \end{aligned}
>    $$
>    Since
>    $$
>    \lim _{z \rightarrow 0} \frac{e^{\lambda z} - 1}{z^2} = \infty,\\
>     \lim _{z \rightarrow \infty } \frac{e^{\lambda z} - 1}{z^2} = \infty,
>    $$
>    one of the solutions to
>    $$
>    z = \frac{2}{\lambda } (1 - e^{-\lambda z})
>    $$
>    will have to be the global minimizer of $V$. Hence, we can set $g_\lambda(z)$ to be the right hand side above. Note that we could also rearrange this equation and then apply a logarithm
>    $$
>    \begin{aligned}
>    \frac{2-\lambda z}{2} &=e^{-\lambda z}\\
>    \frac{\ln \left(\frac{2}{2-\lambda z}\right)}{\lambda }  &= z
>    \end{aligned}
>    $$
>    Hence, a different answer for $g_\lambda (z)$​ is the left hand side above.
>
>    In the following, we show that there is only one solution apart from $z=0$.
>
>    Let
>    $$
>    h(z) = z - \frac{2}{\lambda } (1 - e^{-\lambda z}).
>    $$
>    Then $h(0) = 0$ and 
>    $$
>    h'(z) = 1 - 2 e^{-\lambda z}.
>    $$
>    The function $h'$ has a unique zero at
>    $$
>    z^\ast = -\frac{1}{\lambda } \ln \left( \frac{1}{2} \right).
>    $$
>    Hence, $h$​​ is first monotonically decreasing from $0$​​ and then strictly monotonically increasing. That means there can only be a unique crossing point with $0$​ apart from $z=0$​.​
>
> 6. The likelihood for one sample can be written as
>    $$
>    L_1(Y_1,\lambda ) = \lambda e^{-\lambda Y_1}.
>    $$
>    That means that the log likelihood for one sample is
>    $$
>    \ell _1(Y_1, \lambda ) = \ln (\lambda ) - \lambda Y_1.
>    $$
>    The second derivative is then given by
>    $$
>    \frac{\partial ^2}{\partial \lambda ^2} \ell _1(\lambda ) = -\frac{1}{\lambda ^2}.
>    $$
>    and hence
>    $$
>    I(\lambda ) = -\mathbb E\left[ \frac{\partial ^2}{\partial \lambda ^2} \ell _1(\lambda ) \right] = \frac{1}{\lambda ^2}.
>    $$
>
> 7. The likelihood for one sample can be written as
>    $$
>    L_1(X_1,\lambda ) = e^{-\lambda z X_1}(1 - e^{-\lambda z})^{1 - X_1}
>    $$
>    That means that the log likelihood for one sample is
>    $$
>    \ell _1(X_1, \lambda ) = -\lambda z X_1 + (1-X_1) \ln (1 - e^{-\lambda z})
>    $$
>    Its first derivative is
>    $$
>    \frac{\partial }{\partial \lambda } \ell _1(X_1, \lambda ) = -z X_1 + \frac{z e^{-\lambda z}(1-X_1)}{1 - e^{-\lambda z}}.
>    $$
>    The second derivative is then given by
>    $$
>    \frac{\partial ^2}{\partial \lambda ^2} \ell _1(X_1, \lambda ) = -\frac{z^2(1-X_1)e^{\lambda z}}{(e^{\lambda z}-1)^2},
>    $$
>    and hence,
>    $$
>    \begin{aligned}
>    I(\lambda) &= -\mathbb E\left[ \frac{\partial ^2}{\partial \lambda ^2} \ell _1(\lambda ) \right]\\
>    &= \displaystyle  \frac{(1 - e^{-\lambda z})z^2}{(e^{\lambda z} - 1)(1 - e^{-\lambda z})}\\
>    &= \frac{z^2}{(e^{\lambda z} - 1)}.
>    \end{aligned}
>    $$
>
> 8. $\,   I_ Y(\lambda ) \geq I_ X(\lambda )  \,$​ for all $\lambda$. 
>
>    In order to show this, we need to show 
>    $$
>    e^{u} - 1 - u^2 \geq 0, \quad \text {for all } u > 0,
>    $$
>    By setting $u = \lambda z$.
>
>    We know
>    $$
>    \exp (u) - 1 \geq u, \quad \text {for all } u > 0,
>    $$
>    and since $u^2 > u$​ for $u \in (0,1)$, we have
>    $$
>    \exp (u) - 1 \geq u^2, \quad \text {for } u \in (0,1).
>    $$
>    **Interpretation:**
>
>    The actual data always provides a better estimate.
>
>    This means that in terms of asymptotic statistical performance, the actualy observations beat the censored data, which is what we expected. On the other hand, if the actual data is not available (or at a much lower sample size), it might still be better to use the $X_i$.

# 6. Maximum Likelihood Estimation for a Multivariate Standard Normal

Let $\,  \mathbf{X}_1,\ldots ,\mathbf{X}_ n \stackrel{i.i.d.}{\sim } \mathcal{N}(\mathbf{\mu },\mathbf{1})  \,$​​​, where $\mu \in \R^d$​​ and $\mathbf{1}$​ is the $d \times d$​ identity matrix. (The $X_i$​ are random vectors.)

Recall the PDF defining the distribution $\mathcal{N}(\mu, \mathbf{1})$ is
$$
f(\mathbf{x}) =\frac{1}{(2\pi )^{d/2}} \exp \left(-\frac{1}{2}(\mathbf{x}-\mu )^ T\mathbf{1} (\mathbf{x}-\mu )\right)
$$

1. What is the likelihood function $\,  L(\mathbf{X}_1, \dots , \mathbf{X}_ n, \mathbf{\mu }) \,$​​ for $\mu$?
2. Compute the maximum likelihood estimator $\,  \hat\mu _{MLE} \,$​ for $\mu$​.
3. What is the distribution of $\,  \hat{\mu }_{MLE} \,$​?
4. What is the asymptotic variance of $\,  \mathbf{A}\hat{\mu }_{MLE} \,$​​? (here, A is a fixed $m \times d$​ matrix)
5. What is the asymptotic variance of $\,  \left\|  \hat{\mu }_{MLE} \right\| ^2 \,$​?

> **Solution:**
>
> 1. The likelihood function is
>    $$
>    \begin{aligned}
>    L(\mathbf{X}_1, \dots , \mathbf{X}_ n, \mathbf{\mu }) &= \prod _{i=1}^ n \frac{1}{(2\pi )^{d/2}} \exp (-\frac{1}{2}(\mathbf{x}_ i-\mu )^ T\mathbf{1}(\mathbf{x}_ i-\mu )\\
>    &= \prod _{i=1}^ n \frac{1}{(2\pi )^{d/2}} \exp (-\frac{1}{2}\| \mathbf{x}_ i-\mu \| _2^2)\\
>    &=(2\pi )^{-nd/2} \exp (-\frac{1}{2}\sum _{i=1}^ n\| \mathbf{x}_ i-\mu \| _2^2)
>    \end{aligned}
>    $$
>
> 2. The log likelihood is
>    $$
>    \ell (\mu ) = \frac{nd}{2}\ln 2\pi - \frac{1}{2}\sum _{i=1}^ n\| \mathbf{x}_ i-\mu \| _2^2
>    $$
>    The gradient of the log likelihood function is
>    $$
>    \nabla \ell (\mu ) = \sum _{i=1}^ n(\mathbf{x}_ i - \mu )
>    $$
>    Setting the gradient to zero
>    $$
>    \begin{aligned}
>    \sum _{i=1}^ n(\mathbf{x}_ i - \mu ) &= 0 \\
>    \sum _{i=1}^ n\mathbf{x}_ i- n\mu &= 0 \\
>    \mu &= \frac{1}{n}\sum _{i=1}^ n\mathbf{x}_ i\\
>    \hat\mu _{MLE} &= \bar{X}_n
>    \end{aligned}
>    $$
>
> 3. When the distribution of the population is normal, then the distribution of the sample mean is also normal. For a normal population distribution with mean $\mu$​ and variance $\sigma^2$, the distribution of the sample mean is normal, with mean $\mu$ and variance ${\sigma^2 \over n}$​. So in this multivariate case,
>    $$
>    \hat{\mu }_{MLE} \sim \mathcal{N}(\mu , \frac{1}{n}\mathbf{1})
>    $$
>
> 4. $\,  \mathbf{A}\hat{\mu }_{MLE} \in \mathbb {R}^ m \,$​​, so its variance is actually a $m \times m$​ covariance matrix
>    $$
>    \begin{aligned}
>    \textsf{Cov}(\mathbf{A}\hat{\mu }_{MLE}) &= \mathbf{A}\textsf{Cov}(\hat{\mu }_{MLE})\mathbf{A}^ T\\
>    &=  \mathbf{A}\frac{1}{n}\mathbf{1} \mathbf{A}^ T\\
>    &= \frac{1}{n}\mathbf{A}\mathbf{A}^ T
>    \end{aligned}
>    $$
>    Therefore, the asymptotic covariance is the covariance of $\sqrt{n}\left(\mathbf{A}\hat{\mu }_{MLE}\right)$​​, which is equal to $\mathbf{A}\mathbf{A}^ T$​.
>
> 5. Define the function $\,  g(\mathbf{X}) = \mathbf{X}^ T \mathbf{X} \,$​​ (i.e. $g(\mathbf{X})$​ is the squared norm of a vector $\mathbf{X}$).
>    $$
>    \begin{aligned}
>    g(\mathbf{X}) &= \mathbf{X}^ T \mathbf{X}\\
>    \nabla g(\mathbf{X}) &= 2\mathbf{X}
>    \end{aligned}
>    $$
>    We know from (3) that 
>    $$
>    \hat{\mu }_{MLE} \sim \mathcal{N}(\mu , \frac{1}{n}\mathbf{1})
>    $$
>    So
>    $$
>    \sqrt{n}(\hat{\mu }_{MLE} - \mu ) \sim \mathcal{N}(0, \mathbf{1})
>    $$
>    Note that this is stronger than saying that convergence in distribution.
>
>    By multivariate delta method,
>    $$
>    \sqrt{n}(g(\hat{\mu }_{MLE}) - g(\mu )) \xrightarrow [n\rightarrow \infty ]{(d)}\mathcal{N}(0, \nabla g(\mu )^ T\mathbf{1}\nabla g(\mu ))=\mathcal{N}(0, (2\mu )^ T (2\mu ))=
>    \mathcal{N}(0, 4\left\|  \mu  \right\| ^2)
>    $$
>    Therefore, the asymptotic variance is $\,  4\left\|  \mu  \right\| ^2 \,$​.



# 2. Probability Redux

There are 8 topics and 14 exercises.

## 1. Two important probability tools

Averages of random variables: Laws of Large Numbers and Central Limit Theorem

Let $X, X_1, X_2, ..., X_n$ be i.i.d. random variables, with $\mu = \mathbb{E}[X]$ and $\sigma^2 = \rm{Var}[X]$.

* Laws (weak and strong) of large numbers (LLN):
  $$
  \overline X_ n:=\frac{1}{n}\sum _{i=1}^ n X_ i \xrightarrow [n\to \infty ]{\mathbf{P},\text{ a.s.}} \mu
  $$
  where the convergence is in probability (as denoted by $\mathbf{P}$ on the convergence arrow) and almost surely (as denoted by a.s. on the arrow) for the weak and strong laws respectively.

* Central limit theorem (CLT):
  $$
  \begin{aligned}
  \sqrt{n}\, \frac{\overline X_ n-\mu }{\sigma }&\xrightarrow [n\to \infty ]{(d)}\mathcal{N}(0,1)\quad  \text{or equivalently }\\
  \sqrt{n}\, \left(\overline X_ n-\mu \right)&\xrightarrow [n\to \infty ]{(d)}\mathcal{N}(0,{\sigma ^2})
  \end{aligned}
  $$
  where the convergence is in distribution, as denoted by ($d$) on top of the convergence arrow.

  or equivalently
  $$
  \frac{S_ n-n\mu }{\sqrt{n}\sigma }\xrightarrow [n\to \infty ]{(d)}\mathcal{N}(0,1)
  $$
  where $S_ n=\sum _{i=1}^{n} X_ i$ is the sum (not the average) of $X_i$.

> #### Exercise 1 
>
> Let $X_1, X_2,...,X_n$ be i.i.d. **standard normal random variables**. For a finite $n$, what is the distribution, mean, and variance?
> $$
> \overline{X}_ n=\frac{X_1+X_2+\cdots +X_ n}{n}
> $$
>
> **Solution**:
> 
> The sum of i.i.d. Gaussian random variables is also Gaussian, so $X_1+\cdots +X_ n \sim N(0,n)$. Multiplying by $1/n$, we get $\overline{X}_ n \sim N(0,1/n)$, as scaling a random variable with a constant $c$ scales its variance by $c^2$.
> 
> Therefore, $\overline{X}_ n$ is a Gaussian random variable with mean $0$ and variance $1/n$.

> #### Exercise 2
>
> Let $X_1, X_2,...,X_n$ be a sequence of i.i.d. random variables with $\mathbb{E}[X] = \mu$, and $\rm{Var}(X) = \sigma^2$. Assuming that $n$ is very large, according to the CLT, what is the best approximate characterization of the distribution of $\overline{X}_ n$?
>
> **Solution**:
>
> Given CLT $\sqrt{n}\, \frac{\bar X_ n-\mu }{\sigma } \xrightarrow [n\to \infty ]{(d)}\mathcal{N}(0,1)$, we can use **approximate normality** and get
>
>  $$
> \overline{X}_ n \approx N(\mu ,\sigma ^2/n)
>  $$

In practice, for $X_i \sim Ber(p_i), p_i = p \quad \forall_i$: 

* The LLN's tell us that $\bar{R}_n \xrightarrow[n \rightarrow \infty]{\mathbf{P},~a.s.}  p$.

* When the size $n$ of the experiment becomes large, $\bar{R}_n$ is a **consistent** estimator of $p$.

* The CLT refines this by quantifying *how good* this estimate is: for $n$ large enough the distribution of $\hat{p}$ is almost:
  $$
  \mathbf{P}(|\bar{R}_n - p| \geq \epsilon) \simeq \mathbf{P}(|\mathcal{N}(0, \frac{p(1-p)}{n})| > \epsilon), \quad n \geq 30
  $$

  $$
  \mathbf{P}(|\bar{R}_n - p| \geq 0.084) \simeq 5\%
  $$

  This means that 5% of the time the interval is bad. 

## 2. Hoeffding's Inequality

Used when $n$ is not large enough to apply CLT.

Let $n$ be a positive integer and $X_1,X_2,\ldots ,X_ n\stackrel{iid}{\sim }X$ such that $\mu = \mathbb{E}[X]$ and $\mathbf{P}(X \notin [a,b])=0$ ($a<b$ are given numbers). Then,
$$
\mathbf{P}\left(\left|\overline{X}_ n-\mathbb E[X]\right|\geq \epsilon \right) \leq 2 \exp \left(-\frac{2n\epsilon ^2}{(b-a)^2}\right)\qquad \forall \epsilon >0.
$$
Unlike for the central limit theorem, here the **sample size** $n$ **does not need to be large**.

In practice, for $X_i \sim \rm{Ber}(p_i), p_i = p \quad \forall_i$: 

* Hoeffding's inequality tells us that
  $$
  \mathbf{P}(|\bar{R}_n - p| \geq 0.084) \leq 2 \exp\{\frac{-2.124(0.084)^2}{(1-0)^2}\} \leq 0.35
  $$
  This means that 35% of the time the interval is bad. So Hoeffding's inequality is more conservative.

* CLT is mostly used since people want to make more precise statements.

## 3. Probability review: Markov and Chebyshev inequalities

**Markov inequality**: 

For a random variable $X≥0$ with mean $μ>0$, and any number $t>0$:
$$
\mathbf{P}(X\geq t)\leq \frac{\mu }{t}.
$$
Note that the Markov inequality is restricted to **non-negative** random variables.

**Chebyshev inequality**: 

For a random variable $X$ with (finite) mean $μ$ and variance $σ^2$, and for any number $t>0$,
$$
\mathbf{P}\left(\left|X-\mu \right|\geq t\right)\leq \frac{\sigma ^2}{t^2}.
$$

> #### Exercise 3
>
> Let $X_1,X_2,\ldots ,X_ n\stackrel{iid}{\sim }\textsf{Unif}(0,b)$ be $n$ i.i.d. uniform random variables on the interval $[0,b]$ for some positive $b$. Suppose $n$ is small (i.e. $n < 30$) so that the CLT is not justified.
>
> Find an upper bound on the probability that the sample mean is “far away" from the expectation (the true mean) of $X$. More specifically, find the respective upper bounds given by the Chebyshev and Hoeffding inequalities on the following probability:
> $$
> \mathbf{P}\left(\left|\overline{X}_ n-\mathbb E[X]\right|\geq c\frac{\sigma }{\sqrt{n}} \right)\qquad \text {where }\, \sigma ^2=\textsf{Var}{X_ i}
> $$
> for $c = 2$ and $c = 6$.
>
> **Answer**: 
>
> Using Chebyshev inequality: $\mathbf{P}\left(\left|\overline{X}_ n-\mathbb E[X]\right|\geq {{2}}  \frac{\sigma }{\sqrt{n}} \right)\leq 1/4$
>
> Using Hoeffding inequality: $\mathbf{P}\left(\left|\overline{X}_ n-\mathbb E[X]\right|\geq {{2}}  \frac{\sigma }{\sqrt{n}} \right)\leq 2 \times e^{-2/3}$ 
>
> **Solution**: 
>
> Chebyshev inequality gives 
> $$
> \mathbf{P}\left(\left|\overline{X}_ n-\mathbb E[X]\right|\geq t \right) \leq \frac{\sigma^2/n}{t^2}
> $$
> The variance is 
> $$
> \rm{Var}(\bar{X}_n) = \frac{\sigma^2}{n}
> $$
> Substitute $t = c\frac{\sigma^2}{\sqrt{n}}$, we get
> $$
> \mathbf{P}\left(\left|\overline{X}_ n-\mathbb E[X]\right|\geq c\frac{\sigma }{\sqrt{n}} \right)\leq \frac{1}{c^2}.
> $$
> Hoeffding inequality gives
> $$
> \mathbf{P}\left(\left|\overline{X}_ n-\mathbb E[X]\right|\geq c \frac{\sigma }{\sqrt{n}} \right)  \leq 2 \exp \left(-2c^2 \frac{\sigma ^2}{b^2}\right)
> $$
> The variance for $X_i \sim \rm{Unif}(0,b)$ is 
> $$
> \sigma^2 = \frac{b^2}{12}
> $$
> Substitute $\epsilon =c\frac{\sigma }{\sqrt{n}}$ in Hoeffding's inequality
> $$
> \mathbf{P}\left(\left|\overline{X}_ n-\mathbb E[X]\right|\geq c \frac{\sigma }{\sqrt{n}} \right)  \leq 2 \exp \left(-2c^2 \frac{\sigma ^2}{b^2}\right) \leq 2 \exp \left(-2c^2 \frac{1}{12}\right)=2 \exp \left(-\frac{c^2}{6}\right)
> $$
> Plug in $c = 2$ and $c = 6$ to get the following numerical upper bounds
>
> |           | c=2                | c=6                   |
> | --------- | ------------------ | --------------------- |
> | Chebyshev | $1/4 = 0.25$       | $1/36 = 0.0278$       |
> | Hoeffding | $2e^{-4/6} =1.027$ | $2e^{-36/6} =0.00496$ |
>
> **Remark**:
>
> When $c$ is small, Chebyshev may give a better bound. But as $c$ increases, the bound given by Hoeffding decays exponentially in $c^2$ while the bound given by Chebysheve decays only by $1 \over c^ 2$.

## 4. Gaussian distribution

The **Gaussian distribution**:

* $X \sim \mathcal{N}(\mu, \sigma^2)$
* $\mathbb{E}[X] = \mu$
* $\rm{var}(X) = \sigma^2 > 0$

The **Gaussian density function (PDF)**:

$$
f_{\mu,\sigma^2}(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp (-\frac{(x-\mu)^2}{2 \sigma^2})
$$

* Tails decay very fast (like $\exp(- \frac{x^2}{2 \sigma^2})$): almost in finite interval

* No closed form for their **cumulative distribution function (CDF)**. (often use computers)
  $$
  \mathbf{P}(X \leq x) = F_{\mu,\sigma^2}(x) = \frac{1}{\sigma \sqrt{2 \pi}} \int^x_{-\infty} \exp(-\frac{(t-\mu)^2}{2\sigma^2}) \rm{dt}
  $$

> #### Exercise 4
>
> Let $X\sim \mathcal{N}\left(\mu ,\sigma ^2\right)$, i.e. the PDF of $X$ is 
> $$
> f_ X(x)=\frac{1}{\sigma \sqrt{2\pi }} \exp \left(-\frac{(x-\mu )^2}{2 \sigma ^2}\right).
> $$
> Let $Y = 2X$. Write down the PDF of the random variable $Y$. 
>
> **Answer**: 
>
> $f_ Y(y)=\frac{1}{2\sigma \sqrt{2\pi }} \exp \left(-\frac{(y-2\mu )^2}{2 \left(4\sigma ^2\right)}\right).$
>
> **Solution**: 
>
> Since $Y=2X \sim \mathcal{N}\left(2\mu , 4\sigma ^2\right)$, we have
> $$
> f_ Y(y)=\frac{1}{2\sigma \sqrt{2\pi }} \exp \left(-\frac{(y-2\mu )^2}{2 \left(4\sigma ^2\right)}\right).
> $$
> Alternative method:
>
> In general, for any continuous random variables $X$ and any continuous monotonous (i.e. always increasing or always decreasing) function $g$, such that $Y=g(X)$, the pdf of $Y$ is given by:
> $$
> f_Y(y)=\frac{f_ X(x)}{|g'(x)|}\qquad \text {where } x=g^{-1}(y).
> $$
> In this problem, $X\sim \mathcal{N}\left(\mu ,\sigma ^2\right), Y = g(X) = 2X$, and $g'(x) = 2$. Therefore, 
> $$
> \begin{aligned}
> f_ Y(y) &=\frac{f_ X\left(\frac{y}{2}\right)}{\left|g'\left(\frac{y}{2}\right)\right|}\\
> &=\frac{1}{g'(y/2) \sigma \sqrt{2\pi }} \exp \left(-\frac{(y/2-\mu )^2}{2 \sigma ^2}\right)\\
> &=\frac{1}{2 \sigma \sqrt{2\pi }} \exp \left(-\frac{\left((y-2\mu )/2\right)^2}{2\sigma ^2}\right)\\
> &=\frac{1}{2 \sigma \sqrt{2\pi }} \exp \left(-\frac{\left((y-2\mu )\right)^2}{2(4)\sigma ^2}\right)
> \end{aligned}
> $$
> and we recover the same answer as above.

> #### Exercise 5
>
> Let $X∼N(5,1)$ and $f_X(x)$ be its pdf. Sketch the cumulative distribution function (cdf) $Φ(x)=∫^x_{−∞}f_X(t)dt$ of $X$ by doing the following:
>
> ![gaussian-cdf](../assets/images/gaussian-cdf.png)

## 5. Properties of the Gaussian distribution

Gaussian family is invariant under affine transformation

* $X \sim \mathcal{N}(\mu, \sigma^2)$, then for any $a, b \in \R$,
  $$
  a \cdot X + b \sim \mathcal{N}(a\mu+b, a^2 \sigma^2)
  $$

* **Standardization** (a.k.a. Normalization / Z-score): If $X \sim \mathcal{N}(\mu,\sigma^2)$, then
  $$
  Z = \frac{X - \mu}{\sigma} \sim \mathcal{N}(0,1)
  $$
  Useful to compute probabilities from CDF of $Z \sim \mathcal{N}(0,1)$:
  $$
  \mathbf{P}(u \leq X \leq v) = \mathbf{P}(\frac{u - \mu}{\sigma} \leq Z \leq \frac{v-\mu}{\sigma})
  $$

* **Symmetry**: If $X \sim \mathcal{N}(0, \sigma^2)$ then $-X \sim \mathcal{N}(0, \sigma^2): $ If $x > 0$
  $$
  \mathbf{P}(|X| > x) = \mathbf{P}(X > x) + \mathbf{P}(-X > x) = 2 \mathbf{P}(X > x)
  $$
  Note that here $x>0$.

> #### Exercise 6
>
> Let $X_1, X_2, \ldots ,X_ n\,$ be i.i.d. random variables with mean $\mu$ and variance $\sigma^2$. Denote the sample mean by $\overline{X}_ n= \frac{\sum _{i=1}^ n X_ i}{n}$.
>
> Assume that $n$ is large enough that the CLT holds. Find a random variable $Z$ with approximate distribution $\mathcal{N}(0,1)$.
>
> **Answer**: 
>
> $Z \sim \mathcal{N}(0,1)$ for $Z =\sqrt{n}\frac{\overline{X}_ n-\mu }{\sigma }.$
>
> **Solution**: 
>
> Compute the mean and variance of $\overline{X}_n$
> $$
> \mathbb E\left[\overline{X}_ n\right] =\mathbb E[X_ i]=\mu\\
>  \textsf{Var}\left(\overline{X}_ n\right) =\frac{\sum _{i=1}^ n \textsf{Var}[X_ i]}{n^2}\, =\, \frac{n\sigma ^2}{n^2}\, =\, \frac{\sigma ^2}{n}.
> $$
> Thus, standardize by defining
> $$
> Z = \frac{\overline{X}_ n-\mathbb E\left[\overline{X}_ n\right]}{\sqrt{\textsf{Var}\left(\overline{X}_ n\right)}}=\frac{\overline{X}_ n-\mu }{\sqrt{\sigma ^2/n}}=\sqrt{n}\frac{\overline{X}_ n-\mu }{\sigma }.
> $$
> By the CLT, $Z \sim \mathcal{N}(0,1)$.

> #### Exercise 7
>
> Let $X\sim \mathcal{N}(2,2),$ i.e. $X$ is a Gaussian variable with mean $\mu = 2$ and variance $\sigma^2 = 2$. Let $x > 0$.
>
> Write $\mathbf{P}(X \geq -x)$ in terms of the CDF $\Phi$ of the standard Gaussian variable with a positive argument $\Phi (g(x))$, where $g(x)$ is a function of $x$ which takes only **positive** values for $x>0$.
>
> **Answer**: $\mathbf{P}\left(X\geq -x\right)=\Phi \left(\frac{{{x+2}} }{\sqrt{2}}\right).$
>
> Standardizing $X \sim \mathcal{N}(2,2)$, we have $\frac{X-2}{\sqrt{2}}\sim \mathcal{N}(0,1)$. 
>
> **Solution**: 
> $$
> \begin{aligned}
> \mathbf{P}\left(X\geq -x\right) &=\mathbf{P}\left(\frac{X-2}{\sqrt{2}}\geq \frac{-x-2}{\sqrt{2}}\right)\\
> &=\mathbf{P}\left(\frac{X-2}{\sqrt{2}}{\color{blue}{\leq }}  \frac{{{x+2}} }{\sqrt{2}}\right) \quad \text{ by symmetry}\\
> &=\Phi \left(\frac{{{x+2}} }{\sqrt{2}}\right).
> \end{aligned}
> $$
> The expression $\Phi \left(\frac{{{-x-2}} }{\sqrt{2}}\right)$ gives the same value, but the argument is negative and so is not an accepted answer.

## 6. Gaussian Probability Tables and Quantiles

**Quantiles**: Let $\alpha$ in $(0,1)$, The quantile of order $1-\alpha$ of a random variable $X$ is the number $q_{\alpha}$ such that
$$
\mathbf{P}(X \leq q_{\alpha}) = 1-\alpha
$$
Let $F$ denote the CDF of $X$:

* $F(q_{\alpha}) = 1-\alpha$
* If $F$ is invertible, then $q_{\alpha} = F^{-1}(1-\alpha)$
* $\mathbf{P}(X > q_{\alpha}) = \alpha$
* If $X = Z \sim \mathcal{N}(0,1): \mathbf{P}(|X| > q_{\alpha/2}) = \alpha$

Some important quantiles of the $Z \sim \mathcal{N}(0,1)$ are:

| $\alpha$     | $2.5\%$ | $5\%$  | $10\%$ |
| ------------ | ------- | ------ | ------ |
| $q_{\alpha}$ | $1.96$  | $1.65$ | $1.28$ |

We get that $\mathbf{P}(|Z| > 1.96) = 5\%$.

> #### Exercise 8
>
> Graphed below is the pdf of the normal distribution with generic/unknown (but fixed) variance $σ^2$. If the total area of the two shaded regions is $0.03$, then what is $x$?
>
> ![gaussian_quantile](../assets/images/ex_gaussian_quantile.svg)
>
> **Answer**: $x = q_{\alpha} = q_{0.015}$.
>
> **Solution**: The total area of the two shaded regions equals $P(|X|≥x)=0.03$. By symmetry, the probability in the positive tail is $P(X≥x)=0.015$; hence $x=q_α$ with $α=0.015$.

> #### Exercise 9
>
> Which of the following is the correct ordering of the numbers $q_{0.05},2q_{0.5},q_{0.02}$, which are quantiles of a standard Gaussian variable?
>
> **Answer**: $2q_{0.5} < q_{0.05} < q_{0.02}$
>
> **Solution**: In general, the quantiles of any continuous random variable satisfies $q_{a}>q_{b}$ if $a < b$, since the definition of quantile $q_{\alpha}$ is $\mathbf{P}\left(X{{\geq }}  q_{\alpha }\right)=\alpha$. Note that $q_{0.5} = 0$.

> #### Exercise 10 
>
> The score distribution of the final exam in a data science course follows a normal distribution with mean 70 and standard deviation 10. According to this distribution, what score do you need to get in order to be at the 90th percentile of the class, that is, in order that 90% of all students in the class have a score less than or equal to your score?
>
> **Answer**: $82.82$
>
> **Solution**: 
>
> Given the final exam grade $X$ follows a normal distribution with $μ=70$ and $σ=10$, we can define a variable $Z=(X−70)/10$ that follows the standard normal distribution $N(0,1)$.
>
> $\mathbf{P}\left(Z\leq \frac{t-70}{10}\right) = 0.9 \quad \text {if and only if } \quad \frac{t-70}{10}=q_{0.1}=\Phi ^{-1}(0.9).$
>
> where $q_{0.1}=\Phi ^{-1}$ is the inverse of the cdf of the standard normal distribution. Since $\Phi^{-1}(0.9) = 1.282$ (by using `qnorm(0.9)` in R), we have
>
> $t = 1.282*10+70\, =\, 82.82.$

## 7. Modes of Convergence

### **Three types of convergence**: 

Suppose $(T_n)_{n \geq 1}$ is a sequence of random variables, $T$ is a random variables ($T$ may be deterministic). 

* Almost surely (a.s) convergence: [**Convergence almost surely (a.s)** is also known as **convergence with probability  (w.p.1)** and **strong convergence**.]
  $$
  T_n \xrightarrow[n \rightarrow \infty]{a.s.} T \quad \text{iff} \quad \mathbf{P}[\{w:T_n(w) \xrightarrow [n \rightarrow \infty]{}T(w) \}] = 1 \quad (\text{i.e. } \lim_{n \rightarrow \infty} T_n = T)
  $$

* Convergence in probability:
  $$
  T_n \xrightarrow[x\rightarrow \infty]{\mathbf{P}} T \quad \text{iff} \quad \mathbf{P}[|T_n - T| \geq \epsilon] \xrightarrow[n \rightarrow \infty]{} 0, \quad \forall \epsilon > 0 \quad (\text{i.e. } \lim_{n \rightarrow \infty} \mathbf{P}(|T_n - T| \geq \epsilon) =0)
  $$

* Convergence in distribution: [**Convergence in distribution** is also known as **convergence in law** and **weak convergence**.]
  $$
  T_n \xrightarrow[n \rightarrow \infty]{(d)} T \quad \text{iff} \quad \mathbb{E}[(f(T_n)] \xrightarrow[n \rightarrow \infty]{} \mathbb{E}[f(T)]
  $$
  for all continuous and bounded function $f$.

Properties:

* The following three types of convergence are from strong to weak

  * If $(T_n)_{n \geq 1}$ converges a.s., then it also converges in probability, and the two limits are equal a.s.
  * If $(T_n)_{n \geq 1}$ converges in probability, then it also converges in distribution.

* Convergence in distribution implies convergence of probabilities if the limit has a density (e.g. Gaussian)
  $$
  T_n \xrightarrow [n \rightarrow \infty]{(d)} T \implies \mathbf{P}(a \leq T_n \leq b) \xrightarrow[n \rightarrow \infty]{} \mathbf{P}(a \leq T \leq b)
  $$

### **Equivalent definition of convergence in distribution for real r.v.s**

**Convergence in distribution** is also known as **convergence in law** and **weak convergence**.

1. For all continuous and bounded function $f$
   $$
   T_ n\xrightarrow [n\longrightarrow \infty ]{(d)}T\hspace{3mm}\text{ iff }\hspace{3mm}\mathbb E[f(T_ n)]\xrightarrow [n\rightarrow \infty ]{}\mathbb E[f(T)]
   $$

2. For all $x∈R$ at which the CDF of $T$ is continuous,
   $$
   T_ n\xrightarrow [n\longrightarrow \infty ]{(d)}T \hspace{3mm}\text{ iff }\hspace{3mm} \mathbf{P}[T_ n\leq x]\xrightarrow [n\to \infty ]{}\mathbf{P}[T\leq x] \quad (\text{i.e. } F_{T_n}(x) \xrightarrow[n \rightarrow \infty]{} F_{T}(x))
   $$

> #### Exercise 11
>
> Let $\{X_1, X_2, ..., X_n\}$ be a sequence of r.v. such that $X_n \sim \rm{Ber}({1 \over n})$. What is the limit of the sequence $\mathbb{E}[\cos(X_n)]$ as $n$ tends to infinity?
>
> **Answer**:  1
>
> **Solution**: 
>
> $\mathbf{P}(\{X_n =1\})={1\over n} \xrightarrow[n \rightarrow \infty]{} 0 $, thus, $\{X_n\}$ converges in probability. The limit of $\{X_n\}$ (that is $X_n \xrightarrow[n \rightarrow \infty]{\mathbf{P}}X$) is $X = 0$.
>
> Since $\cos(x)$ is bounded and continuous, $\mathbb{E}[\cos(X_n)] \xrightarrow[n \rightarrow \infty]{}\mathbb{E}[\cos(0)] = 1$.

> #### Exercise 12
>
> Let $(T_n)_{n \geq 1} = T_1, T_2 ...$ be a sequence of r.v.s such that
> $$
> T_ n \sim \textsf{Unif}\left(5-\frac{1}{2n},5+\frac{1}{2n}\right).
> $$
> a) Given an arbitrary fixed number $0 < \delta < 1$, find the smallest number $N$ (in terms of $\delta$) such that $\mathbf{P}\left(\{ \left|T_ n-5\right|>\delta \} \right)=0$ whenever $n > N$. 
>
> b) What is the limit value of $\left(T_ n\right)_{n\geq 1}$?
>
> c) Let $F_n(t)$ be the cdf of $T_n$ and $F(t)$ be the cdf of the constant limit. For which values of $t$ does $\lim _{n\to \infty } F_ n(t)=F(t)$?
>
> **Answer**: 
>
> a) $N = {1 \over 2\delta}$ ;  b) $ \left(T_ n\right)_{n\geq 1}\stackrel{\mathbf{P}}{\longrightarrow } 5$ ; c) $t \neq 5$
>
> **Solution**:  
>
> a) $\frac{1}{2n}<\delta \implies N = {1 \over 2\delta}$
>
> b) By the definition of convergence in probability $T_ n \xrightarrow [n\to \infty ]{\mathbf{P}} 5.$
>
> c) The cdf of $F_n$ of $T_n$ is a piecewise linear function with value $0$ for all $t \leq 5 - {1 \over 2n}$, 1 for all $t\geq 5+\frac{1}{2n}$, and a line connecting the points $\left(t,F_ n(t)\right)= \left(5-\frac{1}{2n}, 0\right)$ and $\left(t,F_ n(t)\right)= \left(5+\frac{1}{2n}, 1\right)$ in the interval $5-\frac{1}{2n}\leq t\leq 5+\frac{1}{2n}$. In particular, $F_ n(5)=\frac{1}{2}$ for all $n$. On the other hand, the cdf $F$ of the constant 5 is $F(t)=0$ when $t < 5$, and $F(t)=1$ when $t \geq 5$. Therefore, $F_ n(t) \xrightarrow [n\to \infty ]{} F(t)$ for all $t \neq 5$.

> #### Exercise 13
>
> Let $(Y_n)_{n \geq 1}$ be a sequence of i.i.d. random variables with $Y_ n \sim \textsf{Unif}\left(0,1\right).$ Let $M_ n=\max (Y_1, Y_2,\ldots , Y_ n).$
>
> a) For any fixed number $0 < \delta < 1$, find $\mathbf{P}\left(\left|M_ n-1\right|>\delta \right)$.
>
> b) Does the sequence $(M_n)_{n \geq 1}$ converge in probability to a constant? If yes, enter the value of the constant limit.
>
> c) Find the CDF $F_{M_ n}(x)$ for $0 \leq x \leq 1$.
>
> **Answer**: 
>
> a) $\mathbf{P}\left(\left|M_ n-1\right|>\delta \right) = (1- \delta)^n$.
>
> b) $\left(M_ n\right)_{n\geq 1}\stackrel{\mathbf{P}}{\longrightarrow } 1$
>
> c) $F_{M_ n}(x) = \mathbf{P}(M_n \leq x) = x^n$
>
> **Solution**: 
>
> Since $M_n \leq 1$, 
> $$
> \begin{aligned}
> \mathbf{P}\left(\left| M_ n-1\right| \geq \delta \right) &=\mathbf{P}\left(1- M_ n \geq \delta \right)\\
> &=\mathbf{P}\left(M_ n\leq 1- \delta \right) \\
> &=\mathbf{P}\left(Y_1\leq 1- \delta \right)\mathbf{P}\left(Y_2\leq 1- \delta \right)\cdots \mathbf{P}\left(Y_ n\leq 1- \delta \right)\qquad \text {since }\, Y_ i \text { independent}\\
> &=\left(1- \delta \right)^ n\, \xrightarrow [n\to \infty ]{} 0\qquad \text {since }\, 0< (1-\delta )<1 .
> \end{aligned}
> $$
> Hence the sequence $(M_n)_{n \geq 1}$ converges in probability to the deterministic limit $M = 1$. This implies that is also converges in distribution to the same limit.
>
> The CDF is computed in the same way:
> $$
> F_{M_ n}(x)\, =\, \mathbf{P}(M_ n \leq x)\, =\,  \mathbf{P}(Y_1 \leq x) \cdots \mathbf{P}(Y_ n \leq x) \, = \, x^ n \qquad \text {for }0\leq x\leq 1.
> $$
> We already know that $ \left(M_ n\right)_{n\geq 1}$ converges in distribution to $M$; here we check directly through definition. As $n \rightarrow \infty$, $F_{M_n}(x)$ approaches the step function shown below. 
>
> ![u1s2_stepfunction](../assets/images/u1s2_stepfunction.svg)

**Exercise Remark**: In general, for a sequence $\left(T_ n\right)_{n\geq 1},\,$ if $\, \mathbb E[T_ n]\xrightarrow [n\to \infty ]{}\mu \,$ and $\, \textsf{Var}(T_ n)\xrightarrow [n\to \infty ]{} 0,\,$then $\, T_ n\xrightarrow [n\to \infty ]{\mathbf{P}}\mu .\, \,$ Both this problem and the previous one satisfy these conditions.

## 8. Operations on Sequences and Convergence

### Addition, Multiplication, Division for Convergence a.s. and in probability 

Assume
$$
{T_ n\xrightarrow [n\to \infty ]{\text{a.s.}/\mathbf{P}}T} \qquad \text {and}\qquad {U_ n\xrightarrow [n\to \infty ]{\text{a.s.}/\mathbf{P}}U}
$$
Then,

* ${T_ n+U_ n\xrightarrow [n\to \infty ]{\text{a.s.}/\mathbf{P}}T+U}$

* ${T_ nU_ n\xrightarrow [n\to \infty ]{\text{a.s.}/\mathbf{P}}TU}$

* If in addition, $U \neq 0$ a.s., then ${\frac{T_ n}{U_ n}\xrightarrow [n\to \infty ]{\text{a.s.}/\mathbf{P}}\frac{T}{U}}$

Note that these rules **do not** apply to convergence in distribution ($d$).

### Slutsky's theorem

For convergence in distribution, the Slutsky's Theorem will be our main tool.

Let $(T_n),(U_n)$ be two sequences of r.v., such that:

* $ T_ n\xrightarrow [n\to \infty ]{(d)}T$
* $ U_ n\xrightarrow [n\to \infty ]{\mathbf{P}}u$

where $T$ is a r.v. and $u$ is a given real number (deterministic limit $\mathbf{P}(U = u) = 1$). Then, 

* $ {T_ n+U_ n\xrightarrow [n\to \infty ]{(d)}T+u}$
* ${T_ nU_ n\xrightarrow [n\to \infty ]{(d)}Tu}$
* If in addition, $u\neq 0$, then ${\frac{T_ n}{U_ n}\xrightarrow [n\to \infty ]{(d)}\frac{T}{u}}$.

### Continuous Mapping Theorem

If $f$ is a **continuous** function:
$$
T_ n\xrightarrow [n\to \infty ]{\text{a.s.}/\mathbf{P}/(d)}T\hspace{3mm}\Rightarrow \hspace{3mm} f(T_ n)\xrightarrow [n\to \infty ]{{\text{a.s.}/\mathbf{P}/(d)}}f(T).
$$
**Example**: 

By LLN, $\bar{R}_n \xrightarrow[n \rightarrow \infty]{\mathbf{P},\,\,\, a.s.} p$. Therefore,
$$
f(\bar{R}_n) \xrightarrow[n \rightarrow \infty]{\mathbf{P}, \,\,a.s.} f(p) \quad \text{for any continuous }f
$$
(Only need $f$ to be continuous around $p: f(x) = 1/x$ works if $p > 0$)

We also have by CLT: $\sqrt{n }\frac{\bar{R}_n - p}{\sqrt{p(1-p)}} \xrightarrow[n \rightarrow \infty]{(d)} Z, Z \sim \mathcal{N}(0,1)$. So
$$
f(\sqrt{n} (\bar{R}_n - p))\xrightarrow[n \rightarrow \infty]{(d)} f(Y) \quad Y \sim \mathcal{N}(0,p(1-p))
$$
Note that not the limit of $\sqrt{n}[f(\bar{R}_n) - f(p)]$, which is what we usually need. To know $\sqrt{n}[f(\bar{R}_n) - f(p)]$ we resort to **Delta ($\Delta$) method**.

> #### Exercise 14
>
> Given the following:
>
> * $Z_1,Z_2,\ldots , Z_ n, \ldots$  is a sequence of random variables that converge in distribution to another random variable $Z$;
> * $Y_1,Y_2,\ldots , Y_ n, \ldots$  is a sequence of random variables each of which takes value in the interval $(0,1)$, and which converges in probability to a constant $c$ in $(0,1)$;
> * $f(x) = \sqrt{x(1-x)}$
>
> Does $Z_ n \frac{f(Y_ n)}{f(c)}$ converge in distribution? If yes, enter the limit.
>
> **Answer**: $Z_ n \frac{f(Y_ n)}{f(c)}\stackrel{\text {d}}{\longrightarrow } \quad Z$
>
> **Solution**: 
>
> Since $f$ is continuous in $(0,1), f(Y_n) $ converges in probability to $f(c)$ by the continuous mapping theorem. Since $f(c)$ is a constant, we have $\frac{f(Y_ n)}{f(c)}$ converges in probability to $1$. Finally, since $Z_n$ converges in distribution to $Z$ and $\frac{f(Y_ n)}{f(c)}$ converges in probability to a constant, by Slutsky's Theorem, $Z_n \frac{f(Y_ n)}{f(c)}$ converges in distribution to $Z$.



Problem set for Lec13-14.

# 1. Implicit hypothesis testing

Given $n$ i.i.d. samples $\,   X_1, \dots , X_ n \sim \mathcal{N}(\mu , \sigma ^2)  \, $ with $\mu \in \R$ and $\sigma^2 > 0$, we want to find a test with asymptotic level $5\%$ for the hypotheses
$$
H_0 \colon \mu \ge \sigma \quad \text {vs} \quad H_1 \colon \mu < \sigma .
$$

1. Define the MLE
   $$
   \hat\mu = \bar X_ n, \quad \widehat{\sigma ^2} = \frac{1}{n} \sum _{i=1}^{n} (X_ i - \bar X_ n)^2.
   $$
   Give a function $g(x,y)$​ such that
   $$
   g(\hat\mu , \widehat{\sigma ^2}) \xrightarrow [n \to \infty ]{\mathrm{\mathbf{P}}} \mu - \sigma .
   $$

2. What is the asymptotic variance $\,   V(g(\hat\mu , \widehat{\sigma ^2}))\,$​of $\,   g(\hat\mu , \widehat{\sigma ^2})  \,$​​ that you found in (1)?

3. Using the result in (2) together with a plug-in estimator for the asymptotic variance, give a test for 
   $$
   H_0 \colon \mu \ge \sigma \quad \text {vs} \quad H_1 \colon \mu < \sigma .
   $$
   that is with asymptotic level $5\%$ and of the form
   $$
     \psi = \mathbf{1} \{  f(\hat\mu , \widehat{\sigma ^2}) > 0\} ,
   $$
   where
   $$
   f(\hat\mu , \widehat{\sigma ^2}) = - T(\hat\mu , \widehat{\sigma ^2}) - q
   $$
   for some function $T$ And $q > 0$.

   Find $\,   f(\hat\mu , \widehat{\sigma ^2}) \,$​​.

4. Using the same test as in (3), give the (asymptotic) p-value of the test given observation $\hat{\mu}$​ and $\hat{\sigma}^2$​.

5. What is the (asymptotic) p-value if the sample size is $n=100,\,   \hat\mu = 3.28  \,$and$\,   \widehat{\sigma ^2} = 15.95  \,$?

   At level $10\%$, do you reject $H_0$? At level $5\%$, do you reject $H_0$?

> **Solution:**
>
> 1. Simply set
>    $$
>    g(x,y) = x - \sqrt{y}.
>    $$
>    By the consistency of the MLE and continuity of $g$, we get
>    $$
>    g(\hat\mu , \widehat{\sigma ^2}) \xrightarrow [n \to \infty ]{} g(\mu , \sigma ^2) = \mu - \sigma .
>    $$
>
> 2. By the **Theorem giving asymptotic normality for MLE**, we have
>    $$
>    \sqrt{n} \begin{pmatrix}  \hat\mu \\ \widehat{\sigma ^2} \end{pmatrix} - \sqrt{n} \begin{pmatrix}  \mu \\ \sigma ^2 \end{pmatrix} \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}\left( \begin{pmatrix}  0\\ 0 \end{pmatrix}, I(\mu , \sigma ^2)^{-1} \right),
>    $$
>    where $\,   I(\mu , \sigma ^2)  \,$ denotes the Fisher information that we computed earlier to be
>    $$
>    I(\mu , \sigma ^2) = \begin{pmatrix}  \frac{1}{\sigma ^2} &  0 \\ 0 &  \frac{1}{2 \sigma ^4} \end{pmatrix}.
>    $$
>    Hence,
>    $$
>    \sqrt{n} \begin{pmatrix}  \hat\mu \\ \widehat{\sigma ^2} \end{pmatrix} -\sqrt{n} \begin{pmatrix}  \mu \\ \sigma ^2 \end{pmatrix} \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}\left( \begin{pmatrix}  0\\ 0 \end{pmatrix}, \begin{pmatrix}  \frac{1}{\sigma ^2} &  0 \\ 0 &  \frac{1}{2 \sigma ^4} \end{pmatrix}^{-1} \right).
>    $$
>    Now, defining
>    $$
>    g \colon \mathbb {R}\times (0, \infty ) \to \mathbb {R}, \quad (x, y) \mapsto x - \sqrt{y},
>    $$
>    We can compute
>    $$
>    \nabla g (x, y) = \begin{pmatrix}  1 \\ - \frac{1}{2 \sqrt{y}}. \end{pmatrix}
>    $$
>    Then, apply the **multivariate Delta method** to obtain
>    $$
>    \sqrt{n}\left(\hat\mu - \sqrt{\widehat{\sigma ^2}} - (\mu - \sigma )\right) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, \nabla g(\mu , \sigma ^2)^ T I(\mu , \sigma ^2)^{-1} \nabla g(\mu , \sigma ^2)) = \mathcal{N}\left(0, \frac{3}{2} \sigma ^2\right).
>    $$
>    That means
>    $$
>    V\left(g\left(\hat\mu , \widehat{\sigma ^2}\right)\right) = \frac{3}{2} \sigma ^2.
>    $$
>
> 3. By **Slutsky's Theorem**, we know that
>    $$
>    \frac{\sqrt{n}}{\sqrt{\frac{3}{2} \widehat{\sigma ^2}}} (\hat\mu - \sqrt{\widehat{\sigma ^2}} - (\mu - \sigma )) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0,1).
>    $$
>    Therefore, set
>    $$
>    T = \sqrt{n} \frac{\hat\mu - \sqrt{\widehat{\sigma ^2}}}{\sqrt{\frac{3}{2} \widehat{\sigma ^2}}},
>    $$
>    and
>    $$
>    \psi = \mathbf{1}\{  - T - q > 0\} ,
>    $$
>    For some $q > 0$​​. If $\mu = \sigma$​​, the above means that we reject $H_0$​ if $T < -q$, which has probability
>    $$
>    \mathbf{P}_{\mu , \sigma ^2}(T < -q) \xrightarrow [n \to \infty ]{} 1 - \Phi (q).
>    $$
>    To achieve asymptotic level $5\%$​​​​​​​​​​, we therefore must set $q$​​​​​​​​​​ to be at least $q_{5\%}$​​​​​​​​​​. It turns out that this value is sufficient. Indeed, if $\mu > \sigma$​​​​​​​ (or equivalently, $\mu - \sigma>0$​​​​​​), then by the **LLN** $T$​​​​ has mean approaching $\infty$​​​​ due to the extra $\sqrt{n}$​​​ scaling:
>    $$
>    T = T_ n(\hat\mu , \widehat{\sigma ^2}) \to +\infty , \text { as } \quad n \to \infty ,
>    $$
>    so in that case
>    $$
>    \mathbf{P}_{\mu , \sigma ^2}(T < -1.65) \xrightarrow [n \to \infty ]{} 0.
>    $$
>    Hence the (asymptotic) supremum of rejecting $H_0$ over $(\mu ,\sigma ^2) \in \Theta _0$ is exactly $0.05$ if we set the threshold to be $q_{5\%}$. All in all, we have found a hypothesis test with asymptotic level $5\%$ of the form
>    $$
>    \psi \{  f(\hat\mu , \widehat{\sigma ^2}) > 0\} ,
>    $$
>    where
>    $$
>    f(\hat\mu , \widehat{\sigma ^2}) = -\sqrt{n} \frac{\hat\mu - \sqrt{\widehat{\sigma ^2}}}{\sqrt{\frac{3}{2} \widehat{\sigma ^2}}} - 1.65.
>    $$
>
> 4. By the same considerations as in (3), we only have to control the level under the assumption $\mu = \sigma$. Looking for a p-value means that we are looking for the smallest $\alpha$ such that the test with level $\alpha$ would have rejected the null-hypothesis. Note that in our notation, this is often an infimum, so we might not be rejecting the null-hypothesis at the p-value $\alpha$, but at any $\alpha + \epsilon, \epsilon > 0$.
>
>    As the asymptotic normality result in (3), if $\mu=\sigma$​​, we have
>    $$
>    \mathbf{P}_{\mu , \sigma ^2}(T < -q) \xrightarrow [n \to \infty ]{} 1 - \Phi (q).
>    $$
>    That means if we are looking for
>    $$
>    \alpha (T) = \inf \{ \alpha : \exists q > 0 \text { such that } T < -q \text { and } \alpha = 1 - \Phi (q)\} .
>    $$
>    By the monotonicity of $\Phi$, this comes down to
>    $$
>    \alpha (T) = 1 - \Phi (-T).
>    $$
>    Plugging in the form we found for $T$​ in (3) yields
>    $$
>    \text {p-value} = 1 - \Phi \left( -\sqrt{n} \frac{\hat\mu - \sqrt{\widehat{\sigma ^2}}}{\sqrt{\frac{3}{2} \widehat{\sigma ^2}}} \right).
>    $$
>
> 5. We compute 
>    $$
>    T = \sqrt{n} \frac{\hat\mu - \sqrt{\widehat{\sigma ^2}}}{\sqrt{\frac{3}{2} \widehat{\sigma ^2}}} \approx -1.46,
>    $$
>    Which leads to a p-value of roughly $0.07$​. 
>
>    This means that at $10\%$​, we can reject $H_0$, while at $5\%$, we cannot.

# 2. Student's T Test

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } X \sim \mathcal{N}(\mu _1, \sigma _1^2)$​. Consider the null and alternative hypotheses
$$
H_0: \mu_1 = 5\\
H_1: \mu_1 \neq 5
$$

1. Assume that $\mu_1$ is not known, but $\sigma^2$ is known. The test statistic $T_n'$ for the likelihood ratio test associated to the above hypothesis can be expressed in terms of $n$, $\bar{X}_n$, and $\sigma_1^2$. What is $T_n'$?

2. If $\sigma_1^2$ were unknown and we used the estimator $\widetilde{\sigma _1^2} = \frac{1}{n-1}\sum _{i} (X_ i - \overline{X}_ n)^2$ in both log-likelihoods, what would be the distribution of $\sqrt{T_ n'}$​ under the null hypothesis?

3. Let $Y_1, \ldots , Y_ m \stackrel{iid}{\sim } Y \stackrel{iid}{\sim } N(\mu _2, \sigma _2^2)$​ denote another sample, and assume that $X$​'S are independent of the $Y$​'s. What is the distribution of $\overline{X}_ n - \overline{Y}_ m$​?

4. Recall that $X_1, \ldots , X_ n \stackrel{iid}{\sim } N(\mu _1, \sigma _1^2)$, and the two samples are independent of one another. Consider the null and alternative hypotheses
   $$
   H_0: \mu_1 \leq \mu_2\\
   H_1: \mu_1 > \mu_2
   $$
   What is the test statistic $T_n$ for the two-sample student's T test associated to $H_0$ and $H_1$?

5. Suppose we observe $\overline{X}_ n = 6.2, \overline{Y}_ m = 6, \hat{\sigma _1}^2 = 0.1$, and $\hat{\sigma _2}^2 = 0.2$ with $n=50$ and $m=50$.

   Using the Welch-Satterthwaite formula, what is the approximate number of degrees of freedom for the test statistic $T_n$?

   What is the p-value for this test?

> **Solution:**
>
> 1. Recall that the MLE for a Gaussian statistical model is $(\overline{X}_ n, \hat{\sigma }^2)$.
>
>    Therefore, by the definition of the likelihood-ratio test,
>    $$
>    \begin{aligned}
>    T_n' &= 2 \left( \ell (X_1, \ldots , X_ n ; \overline{X}_ n, \hat{\sigma }^2) - \ell (X_1, \ldots , X_ n ; 5, \hat{\sigma }^2) \right)\\
>    & = 2 \left( \frac{-1}{2 \hat{\sigma }^2} \sum _{i = 1}^ n (X_ i - \overline{X}_ n)^2 + \frac{1}{2 \hat{\sigma }^2} \sum _{i =1 }^ n (X_ i - 5)^2 \right)\\
>    &= \frac{1}{\hat{\sigma }^2} \left(\sum _{i = 1}^ n ( - X_ i^2 + 2 X_ i \overline{X}_ n - \overline{X}_ n^2 + X_ i^2 - 10 X_ i + 25) \right)\\
>    &= \frac{1}{\hat{\sigma }^2} \left(2n \overline{X}_ n^2 - n \overline{X}_ n^2 - 10 n \overline{X}_ n + 25 n\right)\\
>    &= \frac{n}{\hat{\sigma }^2} \left(\overline{X}_ n^2 - 10 \overline{X}_ n + 25 \right)\\
>    &= \frac{n}{\sigma ^2} \left(\overline{X}_ n - 5\right)^2.
>    \end{aligned}
>    $$
>
> 2. Observe that
>    $$
>    \sqrt{T_ n'} = \frac{\sqrt{n}}{\sigma } |\overline{X}_ n - 5|.
>    $$
>    Plugging in the estimator for $\sigma_1^2$, since
>    $$
>    \frac{\sqrt{n}}{\widetilde{\sigma _1}} (\overline{X}_ n - 5) \sim t_{n-1},
>    $$
>    We conclude that $\sqrt{T_n'}= |t_{n-1}|$.
>    
> 3. Since $X_1, \ldots , X_ n, Y_1, \ldots , Y_ m$​ are mutually independent, we know that $\overline{X}_ n - \overline{Y}_ m$​ will have a normal distribution. It remains to compute the mean and variance.
>
>    By linearity of expectation,
>    $$
>    \mathbb {E}[\overline{X}_ n - \overline{Y}_ m] = \mu _1 - \mu _2
>    $$
>    By independence, the variances are additive, so
>    $$
>    \text {Var}(\overline{X}_ n - \overline{Y}_ m) = \text {Var}(\overline{X}_ n) + \text {Var}(\overline{Y}_ m) = \frac{\sigma _1^2}{n} + \frac{\sigma _2^2}{m}.
>    $$
>    Therefore, the distribution of $\overline{X}_ n - \overline{Y}_ m$ is $N\left(\mu _1 - \mu _2, \frac{\sigma _1^2}{n} + \frac{\sigma _2^2}{m}\right)$.
>
> 4. Under the null hypothesis, we observe that
>    $$
>    \mathbf{P}\left( \frac{\overline{X}_ n - \overline{Y}_ m - (\mu _1 - \mu _2)}{ \sqrt{\frac{\hat{\sigma _1}^2}{n} + \frac{\hat{\sigma _2}^2}{m}} } > \tau \right) \leq \mathbf{P}\left( \frac{\overline{X}_ n - \overline{Y}_ m}{\sqrt{ \frac{\hat{\sigma _1}^2}{n} + \frac{\hat{\sigma _2}^2}{m} }} > \tau \right)
>    $$
>    Therefore, we define the test statistic to be 
>    $$
>    T_ n = \frac{\overline{X}_ n - \overline{Y}_ m}{\sqrt{ \frac{\hat{\sigma _1}^2}{n} + \frac{\hat{\sigma _2}^2}{m} }}.
>    $$
>
> 5. Applying to the WS-formula, under $H_0$​, $T_n$​ is approximately distributed as $t_{88}$​ because
>    $$
>    \frac{\left( \frac{0.1}{50} + \frac{0.2}{50} \right)^2}{ \frac{0.1^2}{50^2(50-1)} + \frac{0.2^2}{50^2(50-1)} } \approx 88.2
>    $$
>    Hence, the approximate number of degrees of freedom for the test statistic is $88$​.
>
>    For the second question, we compute
>    $$
>    T_ n = \frac{6.2 - 6}{\sqrt{\frac{0.1}{50} + \frac{0.2}{50}}} \approx 2.582.
>    $$
>    Consulting a table for the student's T distribution, we observe that $P(t_{88} > 2.582 ) \approx 0.0057$. Therefore,  the p-value for this test is ${0.0057}$.
>

# 3. Likelihood Ratio Test

We consider a sample $X_1, \ldots , X_ n \stackrel{\text {iid}}{\sim } \text {ShiftExp}(\lambda , a)$​​, where $\text {ShiftExp}(\lambda , a)$​​ is a continuous probability distribution with parameters $\lambda > 0$, $a \in \R$ and PDF
$$
f_{\lambda , a}(x) = \lambda e^{-\lambda (x - a)} \mathbf{1}_{x \geq a}.
$$

1. What is the likelihood function $L(X_1, \ldots , X_ n ;\lambda , a)$​​ for the shifted exponential statistical model?

2. Let $(\hat{\lambda }, \hat{a})$ denote the MLE for the shifted exponential model. What is $\hat{a}$​? What is $\hat{\lambda}$?

3. While we cannot formally take the log of zero, it makes sense to define the log-likelihood of a shifted exponential to be
   $$
   \ell (\lambda , a) = (n \ln \lambda - \lambda \sum _{i = 1}^ n (X_ i - a)) \mathbf{1}_{\min _ i(X_ i) \geq a} + (-\infty )\mathbf{1}_{\min _ i(X_ i) < a} .
   $$
   Keep in mind that the likelihood is $0$​ when $\min _ i(X_ i) < a$​​, so that the log-likelihood is infinite and negative.

   Assume now that $a$​ is known and that $a=0$. Consider the hypotheses
   $$
   H_0: \lambda = 1 \quad \text{vs} \quad H_1: \lambda \neq 1
   $$
   What is the likelihood-ratio test statistic $T_n$​?

   Assume that Wilk's theorem applies. What is the distribution of $T_n$?

4. We assume that $\lambda =1$ and is known. The parameter $a \in \R$ is now unknown.

   As in the previous problem, you should use the following definition of the log-likelihood:
   $$
   \ell (\lambda , a) = (n \ln \lambda - \lambda \sum _{i = 1}^ n (X_ i - a)) \mathbf{1}_{\min _ i(X_ i) \geq a} + (-\infty )\mathbf{1}_{\min _ i(X_ i) < a} .
   $$
   Consider the following null and alternative hypotheses:
   $$
   \displaystyle  H_0 : a \leq 1 \quad \text {vs} \quad H_1 : a > 1.
   $$
   What is the log-likelihood ratio test statistic $\widetilde{T_ n}$ for the above hypotheses? Note that if we observe $\min _ i(X_ i) < 1$, then we should clearly fail to reject the null. Furthermore, the restricted and the unrestricted likelihoods for such samples are equal, and therefore have $\widetilde{T_ n} = 0$. Hence, we should assume $\min _ i(X_ i) \ge 1$. 

5. What is the distribution of $\hat{a} = \min _{i = 1, \ldots , n}(X_ i)$​ assuming that $a = 1$ and $\lambda =1$​?

   Recall the test statistic $\widetilde{T_ n} $ from (4). Suppose that $n=100$ and $\widetilde{T_{100}} = 1.03$. What is the p-value associated to this observation.

> **Solution:**
>
> 1. By definition, the likelihood is computed to be
>    $$
>    \begin{aligned}
>    L(X_1, \ldots , X_ n; \lambda , a) &= \prod _{i = 1}^ n \lambda e^{-\lambda (X_ i - a)} \mathbf{1}_{X_ i \geq a}\\
>    &= \lambda ^ n \exp \left( -\lambda \sum _{i = 1}^ n (X_ i - a) \right) \mathbf{1}_{\min _{i = 1, \ldots , n}(X_ i) \geq a}.
>    \end{aligned}
>    $$
>
> 2. Observe that the likelihood $L=0$ if $a > \min_i(X_i)$, so let's restrict to $a \leq \min_i(X_i)$. Taking the log, then we need to maximize the function
>    $$
>    \ell (\lambda , a) := n \ln \lambda - \lambda \sum _{i = 1}^ n X_ i + n \lambda a
>    $$
>    With respect to $\lambda$ and $a$.
>
>    Since $\lambda > 0$, we see that this function is monotone increasing in $a$, so we choose $a$​ to be as large as possible given the constraint $a \leq \min_i(X_i)$. Accordingly, we set
>    $$
>    \hat{a} = \min _{i =1, \ldots , n}(X_ i).
>    $$
>    To compute $\hat{\lambda}$, we set $a = \min _{i =1, \ldots , n}(X_ i)$, and need to maximize the function
>    $$
>    f(\lambda ) = n \ln \lambda - \lambda \sum _{i = 1}^ n X_ i + n \lambda \min _{i}(X_ i).
>    $$
>    Take the derivative and set it to $0$,
>    $$
>    f'(\lambda ) = \frac{n}{\lambda } - \sum _{i = 1}^ n X_ i + n \min _{i}(X_ i)
>    $$
>    We get
>    $$
>    \lambda = \frac{n}{\sum _{i = 1}^ n (X_ i - \min _{j}(X_ j))} = \frac{1}{\overline{X}_ n - \hat{a}}.
>    $$
>    Hence, we have
>    $$
>    \hat{\lambda } = \frac{1}{\overline{X}_ n - \hat{a}}.
>    $$
>
> 3. Since we are given that $a=0$ is known, we may write
>    $$
>    \ell (\lambda , 0) = (n \ln \lambda - \lambda \sum _{i = 1}^ n (X_ i)) \mathbf{1}_{\min _ i(X_ i) \geq 0} = n \ln \lambda - \lambda \sum _{i = 1}^ n (X_ i),
>    $$
>    Because the generated data will certainly satisfy $\min _ i(X_ i) \geq 0$.
>
>    The likelihood-ratio test statistic is
>    $$
>    \begin{aligned}
>    T_n &= 2 ( \ell (\hat{\lambda }, 0) - \ell (1, 0) )\\
>    &= 2( n \ln (1/\overline{X}_ n) - n - 0 + n \overline{X}_ n )\\
>    &= 2( - n \ln ( \overline{X}_ n ) - n + n \overline{X}_ n).
>    \end{aligned}
>    $$
>    By Wilks's theorem,
>    $$
>    T_ n \xrightarrow [(d)]{n \to \infty } \chi _1^2,
>    $$
>    Because the parameter $\lambda$​ is 1-dimensional.
>
> 4. We compute that
>    $$
>    \begin{aligned}
>    \widetilde{T_ n} &= 2( \ell (1, \hat{a}) - \ell (1, 1) )\\
>    & = 2\left(n \ln 1 - (1) \sum _{i = 1}^ n (X_ i - \hat{a})\right)\mathbf{1}_{\min _ i(X_ i) \geq \hat{a}} - 2\left( n \ln 1 - (1) \sum _{i = 1}^ n (X_ i - 1) \right) \mathbf{1}_{\min _ i(X_ i) \geq 1}.
>    \end{aligned}
>    $$
>    Recall that $\hat{a} = \min _ i(X_ i)$. Hence $\mathbf{1}_{\min _ i(X_ i) \geq \hat{a}} = 1$. Moreover, by our assumption $\mathbf{1}_{\min _ i(X_ i) \geq 1} =1$. We may further simplify
>    $$
>    \begin{aligned}
>    \widetilde{T_ n} &= 2\left(n \ln 1 - (1) \sum _{i = 1}^ n (X_ i - \hat{a})\right) - 2\left( n \ln 1 - (1) \sum _{i = 1}^ n (X_ i - 1) \right)\\
>    & = 2n(\hat{a} - 1).
>    \end{aligned}
>    $$
>
> 5. By independence, compute the CDF of $\hat{a} = \min _ i(X_ i)$:
>    $$
>    \begin{aligned}
>    P( \min _ i(X_ i) \geq t ) & = \left( \int _ t^\infty e^{-(x - 1)} \,  dx \right)^ n\\
>    & = e^{-n(t - 1)}\\
>    &= \int _ t^\infty -n e^{-n(x - 1)} \,  dx\\
>    & = \int _ t^\infty f_{n, 1}(x) \,  dx.
>    \end{aligned}
>    $$
>    Therefore, $\mathbf{\hat{a} \sim \text {ShiftExp}(n,1)}$ if we assume that $a= 1$ and $\lambda = 1$.
>
>    Compute the p-value:
>    $$
>    \begin{aligned}
>    P( \widetilde{T_{100}} > 1.03 ) & = P( \hat{a} > 1 + \frac{1.03}{200} )\\
>    & = \int _{ 1 + \frac{1.03}{200} }^\infty 100 e^{-100(x - 1)} \,  dx\\
>    & = e^{-1.03/2}\\
>    & \approx 0.5975.
>    \end{aligned}
>    $$

# 4. One-sided Test vs Wald's Test

Given $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Exp}(\lambda )$, where $\lambda > 0$ is an unknown parameter. In this series of problems, we will compare two tests for the following null and alternative hypotheses:
$$
\displaystyle  H_0 : \lambda \leq 1 \quad \text {vs} \quad H_1 : \lambda > 1.
$$

1. What is the MLE $\hat{\lambda}$ And the Fisher information $I(\lambda)$ for an exponential statistical model? 

2. Assume that the technical conditions hold so that the MLE $\hat{\lambda }_ n^{MLE}$ of an exponential statistical model is asymptotically normal. Then it follows that
   $$
   \frac{\sqrt{n} ( \hat{\lambda }_ n^{MLE} - \lambda ) }{g(\hat{\lambda }_ n^{MLE})} \xrightarrow [n \to \infty ]{(d)} N(0,1)
   $$
   where $g(\hat{\lambda }_ n^{MLE})$ is and expression that depends on $\hat{\lambda }_ n^{MLE}$. What is $g(\hat{\lambda }_ n^{MLE})$?

3. Let us define the test statistic
   $$
   T_ n = \frac{\sqrt{n} ( \hat{\lambda }_ n^{MLE} - 1) }{g(\hat{\lambda }_ n^{MLE})}
   $$
   where $g(\hat{\lambda }_ n^{MLE})$ is the expression from (2).

   We define the test $\psi = \mathbf{1}( T_ n > \tau )$​, where $\tau$​ is a chosen so that $\psi$​ is a test at asymptotic level $\alpha = 0.05$​. Suppose we observe $\overline{X}_ n = 0.83$​.

   Does the test $\psi$​ reject or fail to reject $H_0$​ on this data set? Use $n=100$​.

4. Recall the test statistic $T_n$​ from (3), and let $T_ n^{Wald}$​ denote the test statistic associated to Wald's test for the hypotheses $H_0$​ and $H_1$​. Express $T_ n^{Wald}$​ in terms of $T_n$​. What is $T_ n^{Wald}$​ asymptotically distributed to if we assume $\lambda = 1$​?

5. Consider the test $\psi ^{Wald} = \mathbf{1}( T_ n^{Wald} > \tau )$ where $\tau$ is set so that the test $\psi ^{Wald}$ has asymptotic level $0.05$. Suppose you observe $\overline{X}_ n = 0.83$.

   Does the test $\psi ^{Wald}$ Reject or fail to reject on the given data set? Use $n=100$.

> **Solution:**
>
> 1. Recall the MLE for an exponential statistical model which is precisely
>    $$
>    \hat{\lambda }_ n^{MLE} = \frac{1}{\overline{X}_ n}.
>    $$
>    To compute the Fisher information, the log-likelihood for a single observation is
>    $$
>    \ell (\lambda ) = \ln ( \lambda e^{-x \lambda } ) = \ln (\lambda ) - \lambda x.
>    $$
>    The second derivative is
>    $$
>    \ell ^{\prime \prime }(\lambda ) = - \frac{1}{\lambda ^2},
>    $$
>    and the Fisher information is given by
>    $$
>    \mathcal{I}(\lambda ) = - \mathbb {E}_\lambda [ \ell ^{\prime \prime }(\lambda ) ] = \frac{1}{\lambda ^2}.
>    $$
>
> 2. The asymptotic variance of the statistic
>    $$
>    \sqrt{n} ( \hat{\lambda }_ n^{MLE} - \lambda )
>    $$
>    is given by $\mathcal{I}(\lambda )^{-1} = \lambda ^2$. Therefore,
>    $$
>    \frac{\sqrt{n} ( \hat{\lambda }_ n^{MLE} - \lambda ) }{\lambda } \xrightarrow [n \to \infty ]{(d)} N(0,1)
>    $$
>    Moreover, by Slutsky's theorem,
>    $$
>    \frac{\sqrt{n} ( \hat{\lambda }_ n^{MLE} - \lambda ) }{\hat{\lambda }_ n^{MLE}} \xrightarrow [n \to \infty ]{(d)} N(0,1).
>    $$
>    Therefore ${g(\hat{\lambda }_ n^{MLE}) = \hat{\lambda }_ n^{MLE}}$​​.
>
> 3. Recall that the $5\%$​ quantile of $\mathcal{N}(0,1)$​ is approximately $1.65$​. If $\bar{X}_n = 0.83$, then
>    $$
>    T_ n = \frac{\sqrt{n} ( \frac{1}{0.83} - 1) }{1/0.83} \approx 1.7.
>    $$
>    Therefore, the test as designed above will reject $H_0$​.
>
> 4. By definition of Wald's Test, we have that
>    $$
>    T_ n^{Wald} = n (\hat{\lambda }_ n^{MLE} - 1)^ T I(\hat{\lambda }_ n^{MLE}) (\hat{\lambda }_ n^{MLE} - 1).
>    $$
>    Since the MLE is 1-dimensional,
>    $$
>    T_ n^{Wald} = n (\hat{\lambda }_ n^{MLE} - 1)^2 \cdot \frac{1}{(\hat{\lambda }_ n^{MLE})^2}.
>    $$
>    Observe that $T_ n^{Wald} = T_ n^2$.
>
>    Since 
>    $$
>    T_ n \xrightarrow [n \to \infty ]{(d)} N(0,1),
>    $$
>    We have that
>    $$
>    T_ n^{Wald} = T_ n^2 \xrightarrow [n \to \infty ]{(d)} \chi _1^2.
>    $$
>
> 5. Consulting a table of values, we see that the $0.05$​-quantile of $\chi^2_1$​ is $3.84$​. Now observe that
>    $$
>    T_ n^{Wald} = (T_ n^2) \approx (1.7)^2 \approx 2.89
>    $$
>    Therefore, using Wald's test, we would fail to reject $H_0$​ on observing $\overline{X}_ n = 0.83$​.



# Lecture 3. Parametric Statistical Models

There are 6 topics and 7 exercises.

## 1. The goals of statistics

Trinity of statistical inference: 

* Estimation
* Confidence intervals
* Hypothesis testing

The rationale behind statistical modeling

* Let $X_1, ..., X_n$ be $n$ independent copies of $X$. The goal of statistics is to learn the **distribution** of $X$. 
* If $X \in \{0,1\}$, it is **Bernoulli** and we only have to learn the parameter $p$.
* If $X \in \{0,1,2,3,4,5,6,7\}$, each with different probabilities $\mathbf{P}(X = x)$. We have to learn 8 parameters. More parameters requires more observations.
* Or we could assume that $X - 1 \sim \mathsf{Poiss}(\lambda)$. That's only $1$ parameter $\lambda$ to learn. 

## 2. Statistical model

Let the observed outcome of a statistical experiment be a sample $X_1, ..., X_n$ of $n$ **i.i.d** random variables in some measurable space $E$ (usually $E \subset \R$) and denote by $\mathbb{P}$ their common distribution. A statistical model associated to that statistical experiment is a pair
$$
{\left(E, \{ P_\theta \} _{\theta \in \Theta }\right)}
$$
where:

* $E$ is **sample space** for $X$, i.e. a set that contains all possible outcomes of $X$,
* $(\mathbb{P_\theta})_{\theta \in \Theta}$ is a family of **probability distributions** on $E$;
* $\Theta$ is a **parameter set**, i.e. a set consisting of some possible values of $\theta$.

## 3. Types of statistical models

* We usually assume that the statistical model is **well-specified**, i.e.,  $\exist ~\theta$ such that $\mathbb{P} = \mathbb{P}_\theta$.
* This $\theta$ is called the **true parameter**, and is unknown: The aim of the statistical experiment is to check $\theta$'s properties when they have special meaning, e.g. $\theta > 2, \theta \neq 1/2...$
* The model is **parametric** when we assume that $\Theta \subset \R^d$ for some $d \geq 1$.
* The model is **nonparametric** when we have $\Theta$ be **infinite dimensional**.
* The model is **semiparametric** if $\Theta = \Theta_1 \times \Theta_2$, where $\Theta_1$ is finite dimensional and $\Theta_2$ is infinite dimensional. In these models, we only care to estimate the finite dimensional parameter and the infinite dimensional one is called **nuisance parameter**. 

> #### Exercise 15
>
> Find the smallest sample space for each of the following random variables
>
> 1. $X_1 \sim \mathsf{Poiss}(\lambda)$, a **Poisson** random variable with parameter $\lambda$
> 2. $X_2\sim \mathcal{N}(0,1)$, a **standard Gaussian (or normal)** random variable with mean 0 and variance 1.
> 3. $X_3\sim \exp (\lambda )$, and **exponential** random variable with parameter $\lambda > 0$.
> 4. $X_4 \sim \mathcal{I}(Y > 0)$ where $Y$ is standard Gaussian and $\mathcal{I}$ is the **indicator function**.
>
> A. $\{0,1\}$
>
> B. $\{x \in \Z:x \geq 0\}$
>
> C. $[0,\infty)$
>
> D. $(-\infty, \infty)$
>
> **Answer**: 1-B, 2-D, 3-C, 4-A

#### Examples of parametric models

1. For $n$ Bernoulli trials:
   $$
   (\{0,1\}, (\mathsf{Ber}(p))_{p \in (0,1)})
   $$

2. If $X_1, ..., X_n \stackrel{iid}{\sim} \mathsf{Poiss} (\lambda)$ for some unknown $\lambda > 0$:
   $$
   (\N, (\mathsf{Poiss}(\lambda))_{\lambda > 0})
   $$
   
3. If $X_1, ..., X_n \stackrel{iid}{\sim} \mathcal{N}(\mu, \sigma^2)$, for some unknown $\mu \in \R$ and $\sigma^2 > 0$:
   $$
   (\R, (\mathcal{N}(\mu, \sigma^2))_{(\mu,\sigma^2) \in \R \times (0,\infty)})
   $$

4. If $X_1, .., X_n \stackrel{iid}{\sim} \mathcal{N}_d(\mu, I_d)$, for some unknown $\mu \in \R^d$:
   $$
   (\R^d, \mathcal{N}_d(\mu, I_d)_{(\mu \in \R^d)}
   $$

#### Examples of nonparametric models

1. If $X_1, ..., X_n \in \R$ are i.i.d with unknown unimodal pdf $f$:
   $$
   \begin{aligned}
   E &= \R \\ 
   \Theta &= \{\text{unimodal pdf } f\}\\
   \mathbb{P}_\theta &= \mathbb{P}_f = \text{distribution with pdf }f
   \end{aligned}
   $$

2. If $X_1, ..., X_n \in [0,1]$ are i.i.d with unknown invertible cdf $f$:
   $$
   E = [0,1]
   $$

> #### Exercise 16
>
> Let $\mathcal{U}([0,a])$ denote the uniform distribution on the interval $[0,a]$. Let $X_1,…,X_n\stackrel{iid}{∼}\mathcal{U}([0,a])$ for some unknown $a>0$. Which one of the following is *not* a statistical model associated with this statistical experiment?
>
> A. $\left([0,a],\left(\mathcal U([0,a])\right)_{a > 0}\right)$
>
> B. $\left(\mathbb {R}_{+},\left(\mathcal U([0,a])\right)_{a > 0}\right)$
>
> **Answer**: A.
>
> **Solution**: $\left([0,a],\left(\mathcal U([0,a])\right)_{a > 0}\right)$ is not a statistical model because the sample space depends on an unknown parameter $a$. $\left(\mathbb {R}_{+},\left(\mathcal U([0,a])\right)_{a > 0}\right)$ is a statistical model because for any value of $a$, the random variables $X_1,…,X_n$ will have sample space contained in the interval $[0,∞)=R_+$.

> #### Exercise 17
>
> Which of the following statistical models are parametric?
>
> A. $E=\{x \in \Z: x \geq 0\}; \{P_\theta\}_{\theta \in \Theta}$ is the set of all probability distributions with the sample space $\{x \in \Z:x \geq 0\}$.
>
> B. $E = \{0,1\}l \{P_\theta\}_{\theta \in [0,1]} = \{\mathsf{Ber}(\theta)\}_{\theta \in [0,1]}$
>
> C. $E = (-\infty, \infty); \{P_{\sigma^2}\}_{\sigma^2 \in (0,\infty)}$ is the set of all centered (mean 0) Gaussian distributions $\mathcal{N}(0, \sigma^2)$ where $\sigma^2 > 0$.
>
> D. $E = \{1,2,3,4\}; \{P_{(p_1, p_2,p_3,p_4)}\}_{(p_1, p_2,p_3,p_4) \in S}$ is defined in terms of 
>
> * $S$: the set of all $(p_1, p_2, p_3, p_4) \in \R^4$ such that $0 \leq p_i \leq 1$ for all $i=1, ..., 4$ and $\sum^4_{i=1} p_i = 1$
> * $P_{(p_1, p_2,p_3,p_4)}:$ the distribution defined by setting the probability of outcome $i $ to be $p_i$.
>
> E. $E \, =\,  (-\infty , \infty ); \{ P_{(\mu , \sigma ^2)}\} _{(\mu , \sigma ^2) \in \mathbb {R} \times (0, \infty )}$ is the set of all Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$ where $\mu \in \R$ and $\sigma^2 > 0$.
>
> F. $E = (0,\infty); \{ P_\theta \} _{\theta \in [0, \infty )} = \{ \mathcal{U}([0, \theta ])\} _{\theta \in (0, \infty )}$ is the set of all uniform distributions on the interval $[0,\theta]$ with $\theta > 0$
>
> G. $E=[0,1]; \{ P_\theta \} _{\theta \in \Theta }$ is the set of all probability distributions given by a probability density function $f:\R \rightarrow \R$ with $f$ continuous and $\int _{0}^1 f(x) \,  dx = 1$.
>
> **Answer**: BCDEF
>
> **Solution**: 
>
> A specifying the distribution requires us to know the probability of the outcome $i$ for all $i∈\Z$ such that $i≥0$. An infinite amount of information (or unknowns) is required, so this statistical model is non-parametric.
>
> BCF all require just a single unknown to specify the distribution. These models are parametric.
>
> D requires three unknowns to specify the distribution (once $p_1,p_2$, and $p_3$ are specified, $p_4$ is uniquely determined). This model is parametric.
>
> E requires only the specification of the mean and variance, so it is also parametric.
>
> G: the space of continuous density functions cannot be specified by a finite amount of information; we need to know the values of the function on an infinite subset of $[0,1]$ to be able to uniquely determine it. Hence, this statistical model is also non-parametric.

#### Statistical Model for a Censored Exponential

Let $X$ denote an **exponential** random variable with unknown parameter $λ>0$. Let $Y=\mathcal{I}(X>5)$, the indicator that $X$ is larger than $5$. 

We think of $Y$ as a **censored** version of the Exponential random variable $X$: we cannot directly observe $X$, but we are able to gather some information about it (in this case, whether or not $X$ is larger than 5.)

Observe that $Y$ is a **Bernoulli** random variable. Thus, the statistical model for $Y$ can be written $(\{ 0,1\} , \{ \text {Ber}(f(\lambda ))\} _{\lambda > 0})$ for some function $f$ for $\lambda$.  

- What is $f(\lambda)$?  $f(\lambda) = e^{-5\lambda}$.

We need to compute the probability that $X > 5$. Recall that the density of $\mathsf{Exp}(\lambda)$ is given by $\lambda e^{-\lambda x}$. we need to compute
$$
P(X > 5) = \int _{5}^\infty \lambda e^{- \lambda x} \,  dx = e^{-5 \lambda }.
$$
We conclude that if $X \sim \text {Exp}(\lambda )$, then $Y \sim \text {Ber}(e^{-5 \lambda })$. Hence, $f(\lambda ) = e^{-5 \lambda }$.

## 5. Further examples

#### Linear regression model

$(X_1, Y_1),...,(X_n,Y_n) \in \R^d \times \R$ are i.i.d. from the linear regression model $Y_i = \beta^T X_i + \epsilon_i$, $\epsilon_i \stackrel{iid}{\sim} \mathcal{N}(0,1)$ for an unknown $\beta \in \R^d$ and $X_i \sim \mathcal{N}_d(0,I_d)$ independent of $\epsilon_i$
$$
E = \R^d \times \R, \qquad \Theta = \R^d
$$
We know $X_i \sim \mathcal{N}_d(0,I_d)$. Conditioning on $X$, the probability distribution of $Y$ is $Y_i \sim \mathcal{N}_d(x^T \beta,1)$. 

The random ordered pair $(X,Y) \subset \R^d \times \R$ is distributed as $P_\beta$ if

* $X_i \sim \mathcal{N}_d(0,I_d)$
* $Y \sim \beta ^ T X + \varepsilon$, where $\epsilon_i \stackrel{iid}{\sim} \mathcal{N}(0,1)$ and $\epsilon$ is independent of $X$.

The set $\Theta$ in the ordered pair $(E, \{  P_\beta \} _{\beta \in \Theta } )$ denotes the parameter space for this model.

> #### Exercise 18
>
> Suppose that $\beta = 1 \in \R^d$, which denotes the $d$-dimensional vector with all entries equal to 1. What is the mean and variance of $Y_1$?
>
> **Answer**: $0; 1 + d$
>
> **Solution**: 
>
> We have $Y_1$:
> $$
> Y_1 = \beta ^ T X_1 + \varepsilon _1 = \mathbf{1}^ T X_1 + \varepsilon _1 = \varepsilon _1 + \sum _{j = 1}^ d X_{1,j}.
> $$
> where $X_{i,j}$ denotes the $j$th coordinate of $X_ i \sim \mathcal{N}(0, I_ d)$. By linearity of expectation,
> $$
> \mathbb {E}[Y_1] = \mathbb {E}[\varepsilon _1] + \sum _{j = 1}^ d \mathbb {E}[X_{1,j}] = 0
> $$
> Next we compute the variance. Since $X_{1,1}, \ldots , X_{1,d}, \varepsilon _ i$ are mutually independent, the variance is additive:
> $$
> \text {Var}[Y_1] = \text {Var}[\varepsilon _1] + \sum _{j = 1}^ d \text {Var}[X_{1,j}] = 1 + d
> $$
> because $X_{1,1}, \ldots , X_{1,d}, \varepsilon _1 \stackrel{iid}{\sim } N(0,1)$.

#### Cox proportional Hazard model

$(X_1, Y_1),...,(X_n,Y_n) \in \R^d \times \R:$ the conditional distribution of $Y$ given $X = x$ has CDF $F$ of the form:
$$
F(t) = 1 - \exp \left(-\int^t_0 h(u) e^{(\beta^Tx)}du\right)
$$
where $h$ is an unknown non-negative **nuisance** function and $\beta \in \R^d$ is the parameter of interest.

## 6. Identifiability

#### Injectivity

The notation $f: S \rightarrow T$ denotes that $f$ is a function, also called a **map**, defined on all of a set $S$ and whose outputs lie in a set $T$. A function $f: S \rightarrow T$ is **injective** if for all $x,y \in S, f(x) = f(y)$ implies that $x = y$.

Alternatively: a function is injective if we can **uniquely** recover some input $x$ based on an output $f(x)$.

Intuitively, injectivity means if we have two parameters to start from, we are going to end up with two distributions that are different.

> #### Exercise 19
>
> Which of the following functions are injective? 
>
> A. $f_1: \R \rightarrow \R,$ given by $f_1(x)=x$.
>
> B. $f_2:\R \rightarrow \R$, given by $f_2(x) = x^2$
>
> C. $f_3:\R \rightarrow \R$, given by $f_3(x) = \sin(x)$.
>
> D. $f_4: [0,1] \rightarrow \{\text{probability distribution on } \{0,1\} \}, $ given by $f_4(p)=\mathsf{Ber}(p)$
>
> **Answer**: AD

#### Identifiability

The parameter $\theta$ is called **identifiable** iff the map $\theta \in \Theta \rightarrow \mathbb{P}_\theta$ is **injective**, i.e.,
$$
\theta \neq \theta' \implies \mathbb{P}_\theta \neq \mathbb{P}_{\theta'}
$$
Or equivalently,
$$
\mathbb{P}_\theta = \mathbb{P}_{\theta'} \implies \theta = \theta'
$$
Intuitively, if we the parameter $\theta$ is identifiable, event with an infinite number of data, we can go from $\mathbb{P}_\theta$ to $\theta$. If parameter $\theta$ is not identifiable, we could have multiple $\theta$ that give the same $\mathbb{P}$, and there is no way to decide which one is. In other words, identifiability means that, given the true distribution $P_θ$, we can uniquely recover the true parameter $θ$.

The formal mathematical way of convincing someone that the model is NOT identifiable, is to take two sets of parameters and obtain the same distribution.

**Example**: 

If $X_i = \mathcal{I}_{Y_i\geq 0}$ (indicator function), $Y_1, ..., Y_n \stackrel{iid}{\sim} \mathcal{N}(\mu, \sigma^2),$ for some unknown $\mu \in \R$ and $\sigma^2 > 0$, are unobserved: $\mu$ and $\sigma^2$ are NOT identifiable from the common distribution of the $X_i$'s.

Since $X_i \sim \mathsf{Ber}({P}(Y \geq 0))$, the probability is
$$
{P}(Y \geq 0) = {P}({Y-\mu \over \sigma} \geq - {\mu \over \sigma}) = 1 - \Phi(-{\mu \over \sigma})
$$
Thus, parameter $\theta = {\mu \over \sigma} = - \Phi^{-1}(1-p)$ is identifiable but not $\mu$ and $\sigma^2$.

Intuitively, only one parameter that determines the Bernoulli is a single $p$, we cannot split it into two numbers.

To prove that $\mu$ and $\sigma^2$ are not identifiable: create two sets of parameters ($\mu, \sigma^2$): $(1,4)$ and $(2,16)$, both have ${\mu \over \sigma} = {1\over 2}$, so these two parameters are not identifiable.

**Example**: 

The followings are all identifiable.

* One observes $n$ i.i.d. Poisson random variables with unknown parameter $λ$.

  since $\,   \mathbb E[X_ i] = \lambda  \,$.

* One observes $n$ i.i.d. exponential random variables with parameter $λ$, which is unknown but a priori known to be no larger than $10$.

  since $\,   \mathbb E[X_ i] = \lambda ^{-1}  \,$.

* One observes $n$ i.i.d. uniform random variables in the interval $[0,θ]$, where $θ$ is unknown.

  since $\,   \mathbb E[X_ i] = \theta /2  \,$.

* One observes $n$ i.i.d. Gaussian random variables with unknown parameters $μ,σ^2$.

  since $\,   \mathbb E[X_ i] = \mu  \,,   \textsf{Var}(X_ i) = \sigma ^2  \,$.

* The US Census Bureau is interested in finding out the average commute time of Bostonians. To that end, it randomly selects $n$ individuals, with replacement, among the people who work and live in the Boston area, and asks to each if their commute time is at least 20 minutes. The commute time of a random person is assumed to follow an exponential distribution with parameter $λ$.

  since what we collect can be seen as Bernoulli random variables $Y_i$ with hitting probability $p=\exp(−20λ)$, hence $λ$ can be reconstructed by $\lambda = - \frac{\log \mathbb E[Y_ i]}{20}.$

> #### Exercise 20
>
> Which of the following families of distributions has an identifiable parameter?
>
> A. $\{\mathsf{Ber}(p)\}_{p \in [0,1]}$
>
> B. $\{\mathsf{Ber}(p^2)\}_{p \in [-1,1]}$
>
> C. $\{\mathsf{Ber}(\sin(p))\}_{p \in [0,{\pi \over 2}]}$
>
> D. $\{\mathsf{Ber}(\sin(p))\}_{p \in [0,\pi]}$
>
> **Answer**: AC

> #### Exercise 21
>
> Let $X_ i = \mathcal{I}(Y_ i \geq a/2)$ where $Y_1, \ldots , Y_ n\stackrel{iid}{\sim }\mathcal U([0,a])$ for some unknown parameter $a$. We observe the independent samples $X_1, ..., X_n$ but not the $Y_i$'s themselves. Is the parameter $a$ identifiable from the common distribution of the $X_i$'s?
>
> **Answer**: No
>
> **Solution**: 
>
> $X$ is a Bernoulli random variable with parameter $p := P\left(\mathcal{I}\left(Y_ i \geq \frac{a}{2}\right) = 1\right) = P\left(Y_ i \geq \frac{a}{2}\right)$.
>
> For any choice of $a$, we have by the distribution of $Y_i$ that $p = P(Y_ i \geq a/2) = 1/2$. Hence, for any choice of $a$, the random variable $X$ is distributed as $\text {Ber}(1/2)$. The parameter $a$ is not identifiable.



Problem set for Lec6-7.

# 1. True or False

Suppose the following: According to a fixed statistical model $\{\mathbf{P}_\theta\}_{\theta \in \Theta}$, a pair of hypotheses $H_0: \theta \in \Theta_0$ and $H_1 : \theta \in \Theta_1$, and a $0.05$-level test $\psi_{0.05}$ of the form
$$
\psi _{\alpha } = \mathbf{1}(T_ n > c_\alpha ), \quad T_ n = T_ n(X_1,\ldots ,X_ n), \quad c_\alpha \in \mathbb {R},
$$
We observe the sample $x_1, ..., x_n$ and compute the p-value to be $0.01$. Here $T_n$ is a test statistic, and $c_\alpha$ is a threshold constant.

For each of the following groups of statements, select the one that is necessarily true. If there is none, select "None of the above."

Which of the following is necessarily true?

a. Any test $\psi_\alpha$ that rejects $H_0$ for this observation will have a Type 1 error of at most $0.05$.

b. Any test $\psi_\alpha$ that rejects $H_0$ for this observation will have a Type 2 error of at most $0.05$.

c. Any test $\psi_\alpha$ that does not reject $H_0$ for this observation will have a Type 1 error of at most $0.01$.

d. Any test $\psi_\alpha$ that does not reject $H_0$ for this observation will have a Type 2 error of at most $0.01$.

e. None of the above.

> **Answer**: $c.$
>
> **Solution**:
>
> Note that $\psi _{0.05}(x_1,\ldots ,x_ n)=1$ since the p-value $0.01$ of this sample is lower than $0.05$, the level of the test. Thus, $T_ n(x_1,\ldots ,x_ n) > c_{0.05}$. In fact, $T_ n(x_1,\ldots ,x_ n) = c_{0.01}$ by the definition of p-value. If another test of the form $\psi _\alpha = \mathbf{1}(T_ n > C_\alpha )$ does not reject $H_0$ at the given sample $x_1, ...,x_n$, then its threshold must satisfy
> $$
> c_\alpha > T_ n(x_1,\ldots ,x_ n) = c_{0.01}.
> $$
> Since $\psi_\alpha$ has a higher threshold than $\psi_{0.01}$, we always have
> $$
> \{ T_ n > c_\alpha \}  \subset \{ T_ n > c_{0.01}\}  \quad \text {and}\quad \mathbf{P}_{\theta }\{ T_ n > c_\alpha \}  \le \mathbf{P}_{\theta }\{ T_ n > c_{0.01}\}  \quad \forall \theta \in \Theta _0 .
> $$
> Thus, its level must be $\alpha < 0.01$.
>
> On the other hand, any test of this form that reject $H_0$ for this sample will have a type 1 error rate of at least $0.01$.
>
> This problem does not provide any information about the Type 2 errors.

# 2. Hypothesis Test Using a Single Observation

Let $X$ be a single (i.e. $n =1$) Gaussian random variable with unknown mean $\mu$ and variance $1$. Consider the following hypotheses:
$$
H_0: \mu =0 \quad \text {vs} \quad H_1: \mu \neq 0.
$$

1. Define a test $\psi_\alpha:\R \rightarrow\{0,1\}$ with level $\alpha$ that is of the form
   $$
   \psi _\alpha = \mathbf{1}\{  f_\alpha (X) > 0 \} ,
   $$
   For some function $\,   f_\alpha : \mathbb {R}\to \mathbb {R} \,$.

   We want our test $\psi$ above to satisfy the following:

   * Symmetric in the value of $X$ about $0$, so $f(X)=f(-X)$.
   * Its “acceptance region" is an interval. (The **acceptance region** of a test is the region in which the null hypothesis is **not rejected**, i.e. the complement of its rejection region.)

   What's the function $f_\alpha(X)$ in terms of $\alpha$?

2. Assume you observe $X = 1.32$, and what is the value of your test $\psi_\alpha$ with level $\alpha = 0.05$?  

3. As above, what is the p-value of your test (keeping in mind the symmetry and interval requirements)? 

4. As above, what is the conclusion of the test?

> **Answer**:
>
> $1.\quad f_\alpha(X)=|X| - q_{\alpha/2};\\ 2. \quad \psi(X)=0; \\3. \quad \text{p-value} = 2 (1-\Phi(1.32))$
>
> **Solution**:
>
> 1. Since our test should be symmetric about zero and its “acceptance region" an interval, it must be of the form
>    $$
>    \psi = \mathbf{1}\{  |X| - q_{\alpha /2} > 0 \} .
>    $$
>
> 2. First, determine $q_{\alpha}$ for our test $\psi_\alpha$ when $\alpha = 0.05$:
>    $$
>    \begin{aligned}
>    \mathbf{P}_{\mu = 0}(|X| > q) &= 0.05\\
>    \implies 2(1-\Phi(q)) &= 0.05\\
>    \implies \Phi(q) &= 0.975\\
>    \implies q = q_{0.025} &\approx 1.96
>    \end{aligned}
>    $$
>    Hence, the test $\Phi_{0.05}(X) = \mathbf{1}\{f_{0.05}(X) > 0\}$ where
>    $$
>    f_{0.05}(X) = |X| - 1.96.
>    $$
>    Since $|1.32| < 1.96, \psi_{0.05} (1.32) = 0$.
>
> 3. The p-value is defined as
>    $$
>    \inf \{ \alpha : \psi _\alpha (X) = 1\} ,
>    $$
>    where
>    $$
>    \psi _\alpha (X) = \mathbf{1}\{ |X| > q(\alpha )\} .
>    $$
>    In other words, the p-value is the smallest value so that we could still reject $H_0$ given the observation, when picking our hypothesis test from a family of hypothesis tests indexed by $\alpha$. In this case, by the requirement of $\psi_\alpha$ having confidence level $\alpha$,
>    $$
>    \mathbf{P}_{\mu = 0}(\psi _\alpha (X) > q(\alpha )) = 2(1-\Phi (q)) = \alpha\\
>    \implies \Phi(q) = 1 - {\alpha \over 2}
>    $$
>    and hence
>    $$
>    q(\alpha) = q_{\alpha/2}
>    $$
>    The $1 - \alpha/2$ quantile of a Normal variable. Now, by the form of the test $\psi_\alpha$, we see that we get the infimum of $\alpha$ if $|X| = q_{\alpha/2}$, i.e., if
>    $$
>    \alpha = 2(1 - \Phi (|X|)) = 2 - 2 \Phi (1.32) \approx 0.19.
>    $$
>
> 4. We do not reject $H_0$ because there is not enough evidence for doing so. That does not necessarily mean that we think $H_0$ true, so we should not "accept" it.

# 3. Simple Testing

Let $X_1, ..., X_n$ be i.i.d. $\mathcal{N}(\theta,1)$. Consider testing
$$
H_0: \theta = 0 \quad \text { v.s. } \quad H_1: \theta = 1.
$$

1. What should a Type 1 error and a Type 2 error be in this test?
2. Suppose that the rejection region of a test $\psi$ has the form $R = \{ \overline{X}_ n : \overline{X}_ n > c\}$. Find the smallest $c$ such that $\psi$ has level $\alpha$.
3. Suppose that the test $\psi$ has level $\alpha = 0.05$. What is the power of $\psi$? What does the power of $\psi$ approach as $n \rightarrow \infty$?

> **Answer**:
>
> 1. Type 1 error is rejecting $H_0$ when $\theta = 0$. Type 2 error is not rejecting $H_0$ when $\theta = 1$.
>
> 2. $c \geq {q_\alpha \over \sqrt n}$
>
> 3. Power of $\psi:$ $1 - \Phi(q_{0.05} - \sqrt n)$
>
>    $\lim\limits_{n \rightarrow \infty}$Power$= 1$.
>
> **Solution**:
>
> 1. /
>
> 2. Since $X_i$ are Gaussian, for $\theta = 0$,
>    $$
>    \sqrt{n}\overline{X}_ n\sim \mathcal{N}(0,1).
>    $$
>    Given the rejection region $R = \{ \overline{X}_ n : \overline{X}_ n > c\}$, the corresponding test $\psi _{n,\alpha }=\mathbf{1}\left(\overline{X}_ n\in R \right)$ has level $\alpha$ for any $c$ such that
>    $$
>    \mathbf{P}_0\left(\overline{X}_ n > c\right)\, =\, \mathbf{P}_0\left(\sqrt{n}\overline{X}_ n > \sqrt{n}c\right)
>    $$
>    Hence, the smallest such $c$ is $c=\frac{q_{\alpha }}{\sqrt{n}}$.
>
> 3. Since $H_1$ consists of a single point, the type 2 error of $\psi$ is
>    $$
>    \begin{aligned}
>    \mathbf{P}_{\theta =1}(\psi =0)\, =\, \mathbf{P}_{\theta =1}\left(\overline{X}_ n\leq \frac{q_{0.05}}{\sqrt{n}}\right) &= \mathbf{P}_{\theta =1}\left(\sqrt{n}\left(\overline{X}_ n-1\right)\, \leq \,  q_{0.05}-\sqrt{n}\right)\\
>    &=\Phi \left(q_{0.05}-\sqrt{n}\right).
>    \end{aligned}
>    $$
>    Hence, the power is $1 - \Phi(q_{0.05} - \sqrt n)$. As $n \rightarrow \infty$, this goes to $1$.

# 4. Relating Hypothesis Tests and Confidence Intervals

Consider an i.i.d. sample $\,   X_1, \dots , X_ n \sim \textsf{Poiss}(\lambda ) \,$ for $\lambda > 0$.

1. Starting from the CLT, find a C.I. $I=[A,B]$ with asymptotic level $1- \alpha$ that is centered about $\overline{X}_n$ using the **plug-in** method.

2. Continuing the problem above, now consider the following hypothesis with a fixed number $\lambda_0 > 0$:
   $$
   H_0 : \lambda = \lambda _0 \quad \text {vs} \quad H_1 : \lambda \neq \lambda _0.
   $$
   Define a test for the above hypotheses with asymptotic level $\alpha$, and rewrite it in the form
   $$
   \psi = \mathbf{1}\{ \lambda _0 \notin J\} ,
   $$
   Find C.I. $J = [C,D]$ obtained through the plug-in method.

> **Answer**: 
>
> $1. \quad I = \left[ \overline{X}_ n - \frac{q \sqrt{\overline{X}_ n}}{\sqrt{n}}, \overline{X}_ n + \frac{q \sqrt{\overline{X}_ n}}{\sqrt{n}} \right];\\2. \quad J = \left[ \overline{X}_ n - \frac{q_{\alpha /2} \sqrt{\overline{X}_ n}}{\sqrt{n}}, \overline{X}_ n + \frac{q_{\alpha /2} \sqrt{\overline{X}_ n}}{\sqrt{n}} \right]$
>
> **Solution**:
>
> 1. By the CLT, 
>    $$
>    \sqrt{n} \frac{\overline{X}_ n - \lambda }{\sqrt{\lambda }} \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0,1).
>    $$
>    Since 
>    $$
>    \overline{X}_ n \xrightarrow [n \to \infty ]{\mathrm{\mathbf{P}}} \lambda ,
>    $$
>    By **Slutsky's Theorem**, we get 
>    $$
>    \sqrt{n} \frac{\overline{X}_ n - \lambda }{\sqrt{\overline{X}_ n}} \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0,1).
>    $$
>    That means for $q > 0$ that with
>    $$
>    I = \left[ \overline{X}_ n - \frac{q \sqrt{\overline{X}_ n}}{\sqrt{n}}, \overline{X}_ n + \frac{q \sqrt{\overline{X}_ n}}{\sqrt{n}} \right],
>    $$
>    we have
>    $$
>    \mathbf{P}_\lambda \left( \lambda \in I \right) \xrightarrow [n \to \infty ]{} 1 - 2 \Phi (-q).
>    $$
>    If we want this quantity to be $1 - \alpha$ to guarantee level $1-\alpha$ of the interval, that leads to
>    $$
>    \Phi (q) = 1 - \frac{\alpha }{2} \iff q = q_{\alpha /2} = \Phi ^{-1}(1 - \frac{\alpha }{2}).
>    $$
>
> 2. By setting
>    $$
>    J = I = \left[ \overline{X}_ n - \frac{q_{\alpha /2} \sqrt{\overline{X}_ n}}{\sqrt{n}}, \overline{X}_ n + \frac{q_{\alpha /2} \sqrt{\overline{X}_ n}}{\sqrt{n}} \right]
>    $$
>    From (1), the fact that $I$ is a C.I. with asymptotic level $\alpha$ means that
>    $$
>    \mathbf{P}_\lambda (\lambda \in I) \xrightarrow [n \to \infty ]{} 1 - \alpha \quad \lambda > 0,
>    $$
>    so
>    $$
>    \mathbf{P}_\lambda (\lambda \notin I) \xrightarrow [n \to \infty ]{} \alpha \quad \lambda > 0.
>    $$
>    In particular, if we set
>    $$
>    \psi = \mathbf{1}\{ \lambda _0 \notin I\} ,
>    $$
>    This means that 
>    $$
>    \mathbf{P}_{\lambda _0}(\psi = 1) = \mathbf{P}_{\lambda _0}(\lambda _0 \notin I) \xrightarrow [n \to \infty ]{} \alpha ,
>    $$
>    yielding a hypothesis test with asymptotic level $\alpha$.

# 5. P-Values Formulas

In each of the following questions, you are given an i.i.d. sample and two hypotheses. For any $\alpha \in (0,1)$ , use the Central Limit Theorem to define a test with asymptotic level $\alpha$, then give a formula for the asymptotic p-value of your test.

1. $X_1,\ldots ,X_ n\stackrel{i.i.d.}{\sim } \textsf{Poiss}(\lambda )$ For some unknown $\lambda > 0$;
   $$
   H_0: \lambda = \lambda _0 \quad \text { v.s. }\quad H_1: \lambda \neq \lambda _0\qquad \text {where }\,  \lambda _0 > 0.
   $$
   What is the asymptotic p-value?

2. $X_1,\ldots ,X_ n\stackrel{i.i.d.}{\sim } \textsf{Poiss}(\lambda )$ For some unknown $\lambda>0$;
   $$
   H_0: \lambda {\color{blue}{\geq }}  \lambda _0 \quad \text { v.s. } \quad H_1: \lambda {\color{blue}{<}} \lambda _0 \qquad \text {where }\,  \lambda _0 > 0.
   $$
   What is the asymptotic p-value?

3. $X_1,\ldots ,X_ n\stackrel{i.i.d.}{\sim } {{\textsf{Exp}(\lambda )}}$ for some unknown $\lambda > 0$;
   $$
   H_0: \lambda =\lambda _0 \quad \text { v.s. } \quad H_1: \lambda \neq \lambda _0 \qquad \text {where }\,  \lambda _0 > 0.
   $$
   What is the asymptotic p-value?

> **Answer**:
>
> 1. $2\left(1-\Phi \left( \sqrt{n}\left|\frac{\overline{X}_ n-\lambda _0}{{{\sqrt{\lambda_ 0}}} }\right| \right)\right)$
> 2. $\Phi \left(  \sqrt{n}\left(\frac{\overline{X}_ n-\lambda _0}{{{\sqrt{\lambda_ 0}}} }\right)  \right)$
> 3. $2\left(1-\Phi \left(\sqrt{n}\left|\frac{1/\overline{X}_ n-\lambda _0}{\lambda _0}\right|\right)\right)$
>
> **Solution**:
>
> 1. Since $X_ i\sim \textsf{Poiss}(\lambda )$, $\mathbb E[X_ i]=\lambda$ and $\sigma =\sqrt{\lambda }$. Hence, under $H_0: \lambda =\lambda _0$, the CLT gives
>    $$
>    T_{n,\lambda _0}(\overline{X}_ n) = \sqrt{n}\left(\frac{\overline{X}_ n-\lambda _0}{\sqrt{\lambda _0}}\right) \xrightarrow [n\to \infty ]{(d)} \mathcal{N}\left(0,1\right).
>    $$
>    A test $\psi$ with asymptotic level $\alpha$ is therefore
>    $$
>    \psi _{n,\lambda _0,\alpha } = \mathbf{1}\left( \left| T_{n,\lambda _0}(\overline{X}_ n) \right| > q_{\alpha /2} \right).
>    $$
>    The **asymptotic p-value is the smallest level $\alpha$ such that the test $\psi _{n,\lambda _0,\alpha }$ rejects the null hypothesis for a given sample** (here, for a given realization of $\overline{X}_n$), hence:
>    $$
>    \begin{aligned}
>    \text{p-value} &= \mathbf{P}\left(\left|  Z \right| > \left|  T_{n,\lambda _0}(\overline{X}_ n) \right|\right) \quad \text {where}\quad Z \sim \mathcal{N}(0,1)\\
>    &=2\left(1-\Phi \left( \left| T_{n,\lambda _0}(\overline{X}_ n) \right| \right)\right).
>    \end{aligned}
>    $$
>    **Alternatively**, define the test $\psi$ and the p-value using
>    $$
>    T_{n,\lambda _0}(\overline{X}_ n) = \sqrt{n}\left(\frac{\overline{X}_ n-\lambda _0}{{{\sqrt{\overline{X}_ n}}} }\right).
>    $$
>    By Slutsky's theorem and the CLT, $T_{n,\lambda _0}(\overline{X}_ n) \xrightarrow [n\to \infty ]{(d)} \mathcal{N}(0,1)$.
>
> 2. As in the previous problem, since $X_ i\sim \textsf{Poiss}(\lambda )$, $\mathbb E[X_ i]=\lambda$ and $\sigma = \sqrt \lambda$. Hence, assuming $\lambda = \lambda_o$, which is at the boundary of $\Theta_0$ and $\Theta_1$, the CLT gives again
>    $$
>    T_{n,\lambda _0}(\overline{X}_ n) = \sqrt{n}\left(\frac{\overline{X}_ n-\lambda _0}{\sqrt{\lambda _0}}\right) \xrightarrow [n\to \infty ]{(d)} \mathcal{N}(0,1).
>    $$
>    A candidate test $\psi$ with asymptotic level $\alpha$ is therefore
>    $$
>    \psi _{n,\lambda _0,\alpha } = \mathbf{1}\left(T_{n,\lambda _0}(\overline{X}_ n) {{<}}  -q_{\alpha }\right).
>    $$
>    This is because 
>    $$
>    \mathbf{P}_{\lambda }\left(T_{n,\lambda _0}(\overline{X}_ n) {{<}}  -q_{\alpha }\right) \le \mathbf{P}_{\lambda }\left(T_{n,\lambda _0}(\overline{X}_ n) {{<}}  -q_{\alpha }\right) \quad \text {for} \quad \lambda \ge \lambda _0
>    $$
>    Recall that the **(asymptotic) level $\alpha$ is an upper bound of the type 1 error.** The maximum of the type 1 error is achieved at the boundary of $\Theta_0$ and $\Theta_1$ for a one-sided tests, where the parameter space is 1-dimensional.
>
>    The asymptotic p-value is 
>    $$
>    \begin{aligned}
>    \text{p-value} &= \mathbf{P}\left(  Z  <   T_{n,\lambda _0}(\overline{X}_ n) \right) \quad \text {where}\quad Z \sim \mathcal{N}(0,1)\\
>    &=\Phi \left(  T_{n,\lambda _0}(\overline{X}_ n)  \right).
>    \end{aligned}
>    $$
>    **Alternatively**, again define the test $\psi$ and the p-value using
>    $$
>    T_{n,\lambda _0}(\overline{X}_ n) = \sqrt{n}\left(\frac{\overline{X}_ n-\lambda _0}{{{\sqrt{\overline{X}_ n}}} }\right).
>    $$
>    By Slutsky's theorem and the CLT, $T_{n,\lambda _0}(\overline{X}_ n)\xrightarrow [n\to \infty ]{(d)} \mathcal{N}(0,1).$
>
> 3. Since $X_ i\sim \textsf{Exp}(\lambda )$, $\mathbb E[X_ i]=\sigma =\frac{1}{\lambda }$. Hence, assuming $H_0: \lambda =\lambda _0$, the central limit theorem and the delta method gives
>    $$
>    \begin{aligned}
>    T_{n,\lambda _0}\left(\overline{X}_ n\right)\, =\, \sqrt{n}\left(\frac{g\left(\overline{X}_ n\right)-g\left(1/\lambda _0\right)}{\lvert g'(1/\lambda _0)\rvert (1/\lambda _0)}\right)&\xrightarrow [n\to \infty ]{(d)}\mathcal{N}\left(0,1\right)\qquad \text {where }\, g(x):=1/x.\\
>    \iff\sqrt{n}\left(\frac{1/\overline{X}_ n-\lambda _0}{\lambda _0}\right)&\xrightarrow [n\to \infty ]{(d)}\mathcal{N}\left(0,1\right)\qquad \text {since }\, g'(1/\lambda )=-\lambda ^2.
>    \end{aligned}
>    $$
>    As in (1), a test $\psi$ with asymptotic level $\alpha$ is therefore
>    $$
>    \psi _{n,\lambda _0,\alpha } = \mathbf{1}\left(\lvert T_{n,\lambda _0}\left(\overline{X}_ n\right)\rvert {\color{blue}{>}} \, q_{\alpha /2}\right).
>    $$
>    With asymptotic p-value
>    $$
>    \begin{aligned}
>    \text{p-value }&=\mathbf{P}\left( \lvert Z\rvert > \lvert T_{n,\lambda _0}\left(\overline{X}_ n\right)\rvert \right)\qquad \text {where } Z\sim \mathcal{N}(0,1)\\
>    &=2\left(1-\Phi \left(\lvert T_{n,\lambda _0}\left(\overline{X}_ n\right)\rvert \right)\right).
>    \end{aligned}
>    $$
>    **Alternatively**, define the test $\psi$ and the p-value using
>    $$
>    T_{n,\lambda _0}\left(\overline{X}_ n\right)=\sqrt{n}\left(\frac{1/\overline{X}_ n-\lambda _0}{{{1/\overline{X}_ n}} }\right).
>    $$
>    Where we plug-in the estimator $1/\overline{X}_n$ for $\lambda_0$.

# 6. A Two-Sample Test on Standardized Test Scores

The National Assessment of Educational Progress tested a simple random sample of 1000 thirteen year old students in both 2004 and 2008 and recorded each student's score. The average and standard deviation in 2004 were 257 and 39, respectively. In 2008, the average and standard deviation were 260 and 38, respectively.

Your goal as a statistician is to assess whether or not there were statistically significant changes in the average test scores of students from 2004 to 2008. To do so, you make the following modeling assumptions regarding the test scores:

* $X_1, \ldots , X_{1000}$ Represent the scores in 2004.
* $X_1, \ldots , X_{1000}$ Are iid Gaussians with standard deviation $39$.
* $\mathbb {E}[X_1] = \mu _1$, which is an unknown parameter.
* $Y_1, \ldots , Y_{1000}$ represent the scores in 2008.
* $Y_1, \ldots , Y_{1000}$ are iid Gaussians with standard deviation $38$.
* $\mathbb {E}[Y_1] = \mu _2$, which is an unknown parameter.
* $X_1, \ldots , X_ n$ Are independent of $Y_1, \ldots , Y_ n$.

You define your hypothesis test in terms of the null $H_0: \mu _1 = \mu _2$ (signifying that there were not significant changes in test scores) and $H_1: \mu _1 \neq \mu _2$. You design the test
$$
\psi = \mathbf{1}\left( \sqrt{n}\bigg| \frac{\overline{X}_ n - \overline{Y}_ n}{\sqrt{38^2 + 39^2}} \bigg| \geq q_{\eta /2} \right).
$$
where $q_\eta$ represents the $1-\eta$ quantile of a standard Gaussian.

Under $H_0: \mu _1 = \mu _2$, the test statistic is distributed as a standard Gaussian:
$$
\sqrt{n} \frac{\overline{X}_ n - \overline{Y}_ n}{\sqrt{38^2 + 39^2}} \sim N(0,1)
$$

1. What is the largest possible value of $\eta$ so that $\psi$ has level $10\%$?
2. If $\psi$ is designed to have level $10\%$, would you reject or fail to reject the null hypothesis given these data?
3. What is the p-value for this data set?

> **Answer**:
>
> 1. $\eta = 0.1$
> 2. Reject
> 3. $0.0815$
>
> **Solution**:
>
> 1. $q_\eta$ Is the number such that
>    $$
>    \eta = P(Z \geq q_\eta )
>    $$
>    where $Z \sim N(0,1)$. Observe that under the null hypothesis,
>    $$
>    \sqrt{n}\bigg| \frac{\overline{X}_ n - \overline{Y}_ n}{\sqrt{38^2 + 39^2}} \bigg| \sim N(0,1).
>    $$
>    By symmetry,
>    $$
>    P(|Z| \geq q_{\eta /2}) = 2 P(Z \geq q_{\eta /2}).
>    $$
>    Thus our goal is to choose the smallest $\eta$ such that $P(|Z| \geq q_{\eta /2}) \leq 0.1 \%$. We get
>    $$
>    2 P(Z \geq q_{\eta /2}) = 0.1 \Rightarrow \eta = 0.1.
>    $$
>
> 2. To determine if we should reject or accept the null based on the 2008 data, we need to compute $q_{0.1/2} = q_{0.05}$. Using computational software or a table, we find that $q_{0.05} \approx 1.64$. Now we evaluate our test statistic on the 2008 data:
>    $$
>    \sqrt{n}\bigg| \frac{\overline{X}_ n - \overline{Y}_ n}{\sqrt{38^2 + 39^2}} \bigg| = \sqrt{1000}\bigg| \frac{260-257}{\sqrt{38^2 + 39^2}} \bigg| \approx 1.7422
>    $$
>    Hence, $\psi = 1$, and we would reject the hypothesis that there were no changes in test scores between 2004 and 2008.
>
> 3. To compute the p-value for this data set, we let $Z \sim N(0,1)$ and compute using a table.
>    $$
>    P(|Z| > 1.7422) = 2 P(Z > 1.7422) \approx 0.0815
>    $$

# 7. Select a Test

Select a test with asymptotic level $\alpha$, in terms of the function $T_{n,p}\left(\overline{X}_ n\right),\,$ for each of the following pairs of hypotheses: 

* $H_0 : p = 0.5 \quad \text {vs} \quad H_1 : p \neq 0.5 :$

  $\mathbf{1}\left(\left|  T_{n,0.5}\left(\overline{X}_ n\right) \right| {{>}} q_{{{\alpha /2}} }\right)$

* $H_0 : p \leq 0.5 \quad \text {vs} \quad H_1 : p > 0.5 :$

  $\mathbf{1}\left(T_{n,0.5}\left(\overline{X}_ n\right){{>}} q_{{{\alpha }} }\right)$

* $H_0 : p \geq 0.5 \quad \text {vs} \quad H_1 : p < 0.5 :$

  $\mathbf{1}\left(T_{n,0.5}\left(\overline{X}_ n\right){{<}} {{-}} q_{{{\alpha }} }\right)$

# 8. A Union-Intersection Test

Let $\,   X_1, \dots , X_ n  \,$ be i.i.d. Bernoulli random variables with unknown parameter $p \in (0,1)$. Suppose we want to test
$$
H_0: p \in [0.48, 0.51] \quad \text {vs} \quad H_1: p \notin [0.48, 0.51]
$$
We want to construct an asymptotic test $\psi$ for these hypotheses using $\overline{X}_n$. For this problem, we specifically consider the family of tests $\psi_{c_1, c_2}$ where we reject the null hypothesis if either $\,  \overline{X}_ n < c_1 \leq 0.48 \,$ or $\,  \overline{X}_ n > c_2 \geq 0.51 \,$ for some $c_1$ and $c_2$ that may depend on $n$, i.e.,
$$
\psi _{c_1,c_2}=\mathbf{1}\left((\overline{X}_ n < c_1)\, \cup \, (\overline{X}_ n > c_2 ) \right)\qquad \text {where } c_1<0.48<0.51<c_2.
$$
Throughout this problem, we will discuss **possible choices for constants $c_1$ and $c_2$ , and their impact to both the asymptotic and non-asymptotic level of the test**.

1. What is the expression of the (smallest asymptotic) level $\alpha$ of this test?

2. Use the central limit theorem and the approximation $\sqrt{p(1-p)} \approx \frac{1}{2}$ for $p \in [0.48, 0.51]$ to approximate $\mathbf{P}_ p(\overline{X}_ n < c_1)$ and $\mathbf{P}_ p(\overline{X}_ n > c_2)$ for large $n$. Express your answers as a formula in terms of $c_1, c_2, n$ and $p$. For what value of $p \in [0.48, 0.51]$ is the expression for $\mathbf{P}_ p\left(\overline{X}_ n < c_1\right)\,$ and $\mathbf{P}_ p(\overline{X}_ n > c_2)$ maximized?

3. Next, we combine the results from (1) and (2). Apply the inequality
   $$
   \max _{x} \left(f(x) + g(x)\right) \leq \max _ x f(x) + \max _ x g(x)\,
   $$
   To the expression for the (asymptotic) level $\alpha$ obtained in part (1) and use the results from part (b) to give an upper bound on $\alpha$. Express your answer as a formula in terms of $c_1, c_2,$ and $n$.

4. Suppose that we wish to have a level $\alpha = 0.05$. What $c_1$ and $c_2$ will achieve $\alpha = 0.05$? Choose $c_1$ and $c_2$ by setting the expressions you obtained above for $\max _{p\in [0.48,0.51]} \mathbf{P}_ p\left(\overline{X}_ n<c_1\right)$ and $\max _{p\in [0.48,0.51]} \mathbf{P}_ p\left(\overline{X}_ n>c_2\right)$ to both be $0.025$.

5. We will now show that the values we just derived for $c_1$ and $c_2$ are in fact too conservative. 

   For $p > 0.48$ (note the strict inequality), find $\lim _{n\to \infty } \mathbf{P}_ p(\overline{X}_ n < c_1)$.

   For $p < 0. 51$ (note the strict inequality), find $\lim _{n\to \infty }\mathbf{P}_ p(\overline{X}_ n > c_2)$. 

6. Next, we analyze the asymptotic test given different possible values of $p$, in order to choose suitable and sufficiently-tight $c_1$ and $c_2$. Looking more closely at part (d), we may note that the asymptotic behavior of the expressions for the errors are different depending on whether $p=0.48$, $0.48 <p  < 0.51$, or $p=0.51$.

   Based on your answers and work from the previous part, evaluate the asymptotic Type 1 error
   $$
   \mathbf{P}(\overline{X}_ n < c_1) + \mathbf{P}(\overline{X}_ n > c_2).
   $$
   on each of the three cases for the value of $p$ in terms of $c_1$, $c_2$, and $n$, and determine in each case which component(s) of the Type 1 error will converge to zero.

   This would allow you to come up with a new set of conditions for $c_1$ and $c_2$ in terms of $n$, given the desired level of $5\%$. 

> **Answer**:
>
> 1. $\alpha \, =\, \max _{p \in [0.48, 0.51]} (\mathbf{P}_ p(\overline{X}_ n < c_1) + \mathbf{P}_ p(\overline{X}_ n > c_2)) .$
> 2. $\mathbf{P}_ p(\overline{X}_ n < c_1) \approx \Phi \left(2(c_1 - p)\sqrt{n}\right).\\ \mathbf{P}_ p(\overline{X}_ n < c_1)  \text{ is max at p = } 0.48.\\\mathbf{P}_ p(\overline{X}_ n > c_2) \approx 1 - \Phi (2(c_2 - p)\sqrt{n}).\\\mathbf{P}_ p(\overline{X}_ n > c_2) \text{ is max at p = } 0.51.$
> 3. $\alpha \leq \Phi ( 2(c_1 - 0.48)\sqrt{n})+ 1 - \Phi ( 2(c_2 - 0.51)\sqrt{n}) $
> 4. $c_1 = -\frac{0.98}{\sqrt{n}}+0.48\\c_2 = \frac{0.98}{\sqrt{n}}+0.51$
>
> **Solution**:
>
> 1. The type 1 error is 
>    $$
>    \mathbf{P}_ p\left((\overline{X}_ n < c_1)\,  \cup \, (\overline{X}_ n > c_2)\right)
>    $$
>    Since $c_1 < 0.48 < 0.51 < c_2$, we have
>    $$
>    \mathbf{P}_ p\left((\overline{X}_ n < c_1)\,  \cup \, (\overline{X}_ n > c_2)\right) = \mathbf{P}_ p\left(\overline{X}_ n < c_1\right)+ \mathbf{P}_ p\left(\overline{X}_ n > c_2)\right).
>    $$
>    Maximizing this over $p \in [0.48, 0.51]$, we get that the **maximum Type 1 error of this test, i.e., the smallest level**, is
>    $$
>    \alpha \, =\, \max _{p \in [0.48, 0.51]} (\mathbf{P}_ p(\overline{X}_ n < c_1) + \mathbf{P}_ p(\overline{X}_ n > c_2)) 
>    $$
>
> 2. Consider a specific $p\in [0.48, 0.51]$, then
>    $$
>    \mathbf{P}_ p(\overline{X}_ n < c_1) = \mathbf{P}_ p\left(\frac{\overline{X}_ n - p}{\sqrt{p(1-p)}}\sqrt{n} < \frac{c_1 - p}{\sqrt{p(1-p)}}\sqrt{n}\right).
>    $$
>    By the CLT and noting that the variance of $X_1$ is $\sqrt{p(1-p)}$, we see
>    $$
>    {\overline{X}_n -p  \over \sqrt{p(1-p)} }\sqrt n \xrightarrow [n\rightarrow \infty]{(d)} \mathcal{N}(0,1)
>    $$
>    So 
>    $$
>    \mathbf{P}_ p(\overline{X}_ n < c_1) = \Phi \left(\frac{c_1 - p}{\sqrt{p(1-p)}}\sqrt{n})\right) \approx \Phi \left(2(c_1 - p)\sqrt{n}\right).
>    $$
>    As $\Phi(x)$ is an increasing function, $\Phi(2(c_1-p)\sqrt n)$ is maximized at the minimum possible $p$ in the range, which is $p=0.48$. Hence, $\max _{p \in [0.48, 0.51]} \mathbf{P}_ p(\overline{X}_ n < c_1) = \Phi (2(c_1 - 0.48)\sqrt{n})$.
>
>    Similarly, for a specific $p\in [0.48, 0.51]$,
>    $$
>    \mathbf{P}_ p\left(\overline{X}_ n > c_2\right) = \mathbf{P}_ p\left(\frac{\overline{X}_ n - p}{\sqrt{p(1-p)}}\sqrt{n} > \frac{c_2 - p}{\sqrt{p(1-p)}}\sqrt{n}\right)
>    $$
>    Applying the CLT and the approximation $\sqrt{p(1-p)} \approx \frac{1}{2}$ gives 
>    $$
>    \mathbf{P}_ p(\overline{X}_ n > c_2) \approx 1 - \Phi (2(c_2 - p)\sqrt{n}).
>    $$
>    As $\Phi(x)$ is an increasing function, $1 - \Phi ( 2(c_2 - p)\sqrt{n})$ is maximized at the maximum possible $p$ in the range, which is $p=0.51$. Hence,
>    $$
>    \max _{p \in [0.48, 0.51]} \mathbf{P}_ p(\overline{X}_ n > c_2) = 1 - \Phi ( 2(c_2 - 0.51)\sqrt{n})
>    $$
>
> 3. Recall that **the (smallest) asymptotic level $\alpha$ of a test is equal to the maximum Type 1 error rate.** We have
>    $$
>    \begin{aligned}
>    \max _{p \in [0.48, 0.51]} (\mathbf{P}_ p(\overline{X}_ n < c_1) + \mathbf{P}(\overline{X}_ n > c_2)) &\leq \max _{p \in [0.48, 0.51]} \mathbf{P}(\overline{X}_ n < c_1) + \max _{p \in [0.48, 0.51]} \mathbf{P}(\overline{X}_ n > c_2)\\
>    &\approx \Phi ( 2(c_1 - 0.48)\sqrt{n})+ 1 - \Phi ( 2(c_2 - 0.51)\sqrt{n})
>    \end{aligned}
>    $$
>    **(This bound is not tight because the the maxima for the two summands are not obtained at the same $p$.)**
>
> 4. To have a test of level $0.05$ and equal left and right tails, solve the equations
>    $$
>    \Phi (2*(c_1-0.48)*\sqrt{n}) = 0.025\\
>    2*(c_1-0.48)*\sqrt{n} = -1.96\\
>    c_1 = -\frac{0.98}{\sqrt{n}}+0.48
>    $$
>
>    $$
>    \Phi (2*(c_2-0.51)*\sqrt{n}) = 0.975\\
>    2*(c_2-0.51)*\sqrt{n} = 1.96\\
>    c_2 = \frac{0.98}{\sqrt{n}}+0.51
>    $$
>
> 5. From (2), 
>    $$
>    \mathbf{P}(\overline{X}_ n < c_1) = \Phi (\frac{c_1 - p}{\sqrt{p(1-p)}}\sqrt{n})) \approx \Phi (2(c_1 - p)\sqrt{n}).
>    $$
>    If $p > 0.48$, then
>    $$
>    \Phi (2(c_1 - p)\sqrt{n}) < \Phi (2(0.48 - p)\sqrt{n}).
>    $$
>    This argument in $\Phi$ on the right is a negative constant times $\sqrt n$, so the argument tends to negative infinity as $n \rightarrow \infty $ and thus,
>    $$
>    \Phi (2(c_1 - p)\sqrt{n}) \rightarrow 0.
>    $$
>    For the other side, $c_2$, we obtained in (2) that
>    $$
>    \mathbf{P}(\overline{X}_ n > c_2) \approx 1 - \Phi (2(c_2 - p)\sqrt{n}).
>    $$
>    If $ p < 0.51$, then
>    $$
>    1 - \Phi (2(c_2 - p)\sqrt{n}) < 1 - \Phi (2(c_2 - 0.51)\sqrt{n}).
>    $$
>    Taking $n \rightarrow \infty$, as $c_2 > 0.51$,
>    $$
>    2(c_2 - 0.51)\sqrt{n} \rightarrow +\infty ;
>    $$
>    So
>    $$
>    1 - \Phi (2(c_2 - 0.51)\sqrt{n}) \rightarrow 0
>    $$
>    And thus,
>    $$
>    1 - \Phi (2(c_2 - p)\sqrt{n}) \rightarrow 0.
>    $$
>
> 6. For this solution, we write $c_1(n)$ and $c_2(n)$ for $c_1$ and $c_2$ respectively as they are in practice functions of $n$.
>
>    From the previous part, $\mathbf{P}(\overline{X}_ n < c_1(n))$ for any $c_1(n) < 0.48$ will definitely converge to $0$ for $0.48 < p < 0.51$ and for $p = 0.51$, but not for $p=0.48$. When $p = 0.48$, we could write
>    $$
>    \mathbf{P}(\overline{X}_ n < c_1(n)) = \Phi (2(c_1(n) - 0.48)\sqrt{n}).
>    $$
>    Similarly, $\mathbf{P}(\overline{X}_ n > c_2(n))$ for any $c_2(n) > 0.51$ will converge to $0$ for $p=0.48$ and $0.48 < p < 0.51$, while for $p=0.51$,
>    $$
>    \mathbf{P}(\overline{X}_ n > c_2(n)) = 1 - \Phi (2(c_2(n) - 0.51)\sqrt{n}).
>    $$
>    Summarizing the above observations, we get
>    $$
>    \lim _{n\to \infty } (\mathbf{P}_(\overline{X}_ n < c_1(n)) + \mathbf{P}_(\overline{X}_ n > c_2(n))) = \begin{cases}  \lim _{n\to \infty } \Phi (2(c_1(n) - 0.48)\sqrt{n}), \quad &p=0.48\\ 0, \quad &0.48 < p < 0.51\\ \lim _{n\to \infty } 1 - \Phi (2(c_2(n) - 0.51)\sqrt{n}), \quad &p=0.51 \end{cases}
>    $$
>    Hence our constraints (from the first and third cases above) are
>    $$
>    \lim _{n\to \infty } \Phi (2(c_1(n) - 0.48)\sqrt{n}) \leq 0.05, \quad \lim _{n\to \infty } \Phi (2(c_2(n) - 0.51)\sqrt{n}) \geq 0.95.
>    $$
>    Taking the simplest (or broadest) case, we could set equality everywhere, which gives
>    $$
>    \Phi \left(2(c_1(n) - 0.48)\sqrt{n}\right) = 0.05,\quad \Phi \left((2(c_2(n) - 0.51)\sqrt{n}\right) = 0.95.
>    $$
>    Finally, taking $\Phi ^{-1}$ of both sides then rearranging gives our answer:
>    $$
>    c_1(n) = \frac{-q_{0.05}}{2\sqrt{n}} + 0.48,\quad c_2(n) = \frac{q_{0.05}}{2\sqrt{n}} + 0.51
>    $$







# Lecture 7. Hypothesis Testing (Continued): Levels and P-values

There are 7 topics and 5 exercises.

## 1. Review of Parametric Hypothesis Testing

### Hypothesis testing

You have i.i.d. data $X_1, ..., X_n$  generated by a distribution $\mathbf{P}_\theta$ for some unknown parameter $\theta \in \R$. You would like to test some null hypothesis $H_0$ against an alternative hypothesis $H_1$. What is the purpose of hypothesis testing?

**Answer**: To decide with a quantified probability of error whether or not $\theta$ lies in a certain region of the parameter set.

**Solution**: The null and the alternative hypotheses describe disjoint subsets of the parameter set. A statistical test is a data dependent rule that decides whether or not to reject the statement (hypothesis) that the unknown true parameter belongs to the subset described by $H_0$ or fail to reject it. In designing a statistical test, we must quantify how likely it is that the observed sample is generated by a probability distribution $\mathbf{P}_\theta$ for $\theta$ in $H_0$.

### Rejection region

You have samples $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Ber}(p^*)$ for some true parameter $p^* \in (0,1)$. Let $(\{ 0,1\} , \{ \mathbf{P}_ p\} _{p \in (0,1)})$ denote the associated statistical model, where $\mathbf{P}_p = \mathsf{Ber}(p).$

You conduct a hypothesis test between

* A null hypothesis $H_0: p^* \in \Theta _0$ and
* An alternative hypothesis $H_1: p^* \in \Theta _1$,

where $\Theta _0, \Theta _1 \subset (0,1)$ and $\Theta_0$ and $\Theta_1$ are disjoint.

You construct a statistical test
$$
\psi :\{ 0,1\} ^ n \to \{ 0, 1\}
$$
which takes as input the sample $(X_1, ...,X_n)$. If $\psi(X_1, ...,X_n)=1$, you will reject the null $H_0$ in favor of the alternative $H_1$, and otherwise you will fail to reject the null.

Recall that the rejection region $R_\psi$ describes which outcomes $(x_1, ..., x_n)$ will result in $\psi(x_1, ..., x_n)=1$ and, hence, rejection of the null.

The rejection region is a subset of ...

a. $(\Theta_0)^n$ where $\Theta_0$ defines the null hypothesis $H_0$ in the parameter space $\Theta$.

b. $(\Theta_1)^n$ where $\Theta_1$ defines the alternative hypothesis $H_1$ in the parameter space $\Theta$.

c. $(\Theta)^n$ where $\Theta$ is the parameter space

d. $E^n$ where $E$ is the sample space of $X_i$

**Answer**: d

**Solution**: The rejection region is by definition the set of all observed outcomes for which $H_0$ will be rejected by the test $ \psi =\mathbf{1}\left((X_1,\ldots ,X_ n)\in R_\psi \right)$. It is a subset of $E^n$, where $E$ is the sample space of $X_i$.

### Type 1/2 error and power

Setup as above, let $\alpha_\psi$ and $\beta_\psi$ denote the type 1 error and type 2 error, respectively. Determine which type of mathematical object each of the following is.

1. Rejection region
2. Type 1 or type 2 error
3. Level
4. Power

a. A number

b. A set

c. A function

**Answer**: $1-b,\quad 2-c,\quad 3-a,\quad 4-a$

**Solution**: 

Recall the definition of each object in the context of the statistical model $(\{ 0,1\} , \{ \mathbf{P}_ p\} _{p \in (0,1)})$

1) The rejection region is defined to be 
$$
R_\psi := \{  \mathbf{x} \in \{ 0,1\} ^ n: \,  \psi (\mathbf{x}) = 1 \} ,
$$
So this is a **set**.

2) Type 1 and 2 errors are defined to be 
$$
\begin{aligned}
\alpha _\psi : \Theta _0 &\rightarrow [0,1]\\
p &\mapsto P_p(\psi = 1)\\
\beta _\psi : \Theta _1 &\rightarrow [0,1]\\
p &\mapsto P_p(\psi = 0)\\
\end{aligned}
$$
So this is a **function** of $p$.

3) The power $\pi_\psi$ is defined as
$$
\pi _\psi := \inf _{p \in \Theta _1} (1 - \beta _\psi (p)).
$$
This greatest lower bound of $(1 - \beta _\psi (p))$ over a set of $p$ values is a **number**.

### Type 1 vs. Type 2 Error

Recall type 1 and type 2 error

| Test \ Truth | $H_0$  | $H_1$  |
| ------------ | ------ | ------ |
| $\psi = 0$   | \      | Type 2 |
| $\psi = 1 $  | Type 1 | \      |

Setup as above, you would like to hypothesis test between two simple hypotheses:
$$
H_0:p^* \in \Theta _0=\{ 1/2\}\\
H_1:p^* \in \Theta _1=\{ 3/4\}\\
$$
That is, each of the regions defined by the null and alternative hypotheses consists of a single point in the parameter space $\Theta = [0,1].$

You constructed a statistical test $\psi$, and let $\alpha_\psi$ and $\beta_\psi$ denote the type 1 error and type 2 error, respectively, associated to this test. What do $\alpha_\psi(1/2)$  and $\beta_\psi(3/4)$ represent?

**Answer**: 

$\alpha_\psi(1/2)$: The probability that we **reject** $p^* = 1/2$ in favor of $p^* = 3/4$ even though in fact $p^* =  1/2$.

$\beta_\psi(3/4)$: The probability that we **fail to reject** $p^* = 1/2$ in favor of $p^* = 3/4$ even though in fact $p^* = 3/4$.

**Solution**:

1) If $\psi = 1$, then we would reject the null-hypothesis $p^* \in \Theta_0 = \{1/2\}$. Therefore $\alpha_\psi(1/2)=\mathbf{P}_{1/2}(\psi =1)$ is the probability of rejecting $p^* \in \Theta_0 = \{1/2\}$ in favor of $p^* \in \Theta_1 = {3/4}$ even when in fact $p^* \in \Theta_0 = \{1/2\}$.

2) If $\psi = 0$, then we would fail to reject the null-hypothesis $p^* \in \Theta_0 = \{1/2\}$ in favor of the alternative hypothesis $p^* \in \Theta_0 = \{3/4\}$. Therefore $\beta_\psi(3/4)=\mathbf{P}_{3/4}(\psi =0)$ is the probability of not rejecting $H_0: p^* \in \Theta_0 = \{1/2\}$ even when in fact $p^* \in \Theta_0 = \{3/4\}$.

### Interpreting the level

What is a correct interpretation of the (smallest) **level** of a test?

**Answer**: 

* The level of a test is an **upper bound** on the type 1 error. 
* The level of a test gives an **upper bound** on the **worst**-case probability of making an error under the null hypothesis.

**Solution**: 

Recall the definition of the level of a test $\psi$. Let 
$$
\begin{aligned}
\alpha _\psi : \Theta &\rightarrow [0,1]\\
\theta& \mapsto P_\theta(\psi = 1)
\end{aligned}
$$
denote the type 1 error. Then the level of $\psi$ is any real number $\alpha$ such that
$$
\alpha _\psi (\theta ) \leq \alpha , \quad \text {for all} \,  \,  \theta \in \Theta _0.
$$

### Test Statistics

Recall the statistical experiment in which you flip a coin $n$ times to decide the coin is fair.

You model the coin flip as $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Ber}(p)$ where $p$ is an unknown parameter, and formulate the hypothesis:
$$
H_0: p = 0.5\\
H_1:p \neq 0.5
$$
And design the test $\psi$ using the statistic $T_n$:
$$
\psi_n = \mathbf{1}(T_n > C)\\
\text{where } T_n = \sqrt{n}{\left\vert \overline{X}_n - 0.5 \right\vert \over \sqrt{0.5(1-0.5)}}
$$
where the number $C$ is the threshold. Note that absolute value in $T_n$ for this two sided test.

If it is true that $p = 1/2$, which of the following are true about $T_n$?

a. $T_n$ Is a consistent estimator of the true parameter $p = 1/2$.

b. $\lim _{n \to \infty } T_ n \xrightarrow [n \to \infty ]{(d)} |Z|$ where $Z \sim \mathcal{N}(0,1)$ is a standard Gaussian.

c. $T_n$ Involves a shift and rescaling of the sample average so that as $n \rightarrow \infty$, this random variable will converge in distribution.

d. The limiting distribution of $T_n$ can be understood using computational software or tables.

**Answer**: bcd

**Solution**:

a. The first choice is incorrect. The statistic $T_n$ does NOT converge to a real number as $n \rightarrow \infty$. By the CLT, $T_n$ converges in **distribution**, meaning that asymptotically, it is a **random variable**.

**Remark**: This example illustrates one of the main strategies involved in hypothesis testing. Namely, we want to work with a test statistic, that, asymptotically, tends to a distribution that we can easily work with. In many cases, this will involve shifting and rescaling the sample mean so that the CLT applies and we can just work with a standard Gaussian $\mathcal{N}(0,1)$.

### Design a test to have a given asymptotic level

Setup as above, recall that the test $\psi$ has asymptotic level $\alpha$ if
$$
\lim _{n \to \infty } P_{1/2}( \psi = 1) \leq \alpha .
$$
This is a graph of the standard normal distribution $\mathcal{N}(0,1)$, along with the lines $Z = \pm C$. The letters $A,B$ denote the areas of the corresponding shaded regions:
$$
\begin{aligned}
A &= \mathbf{P}(Z < -C) = \mathbf{P}(Z > C) \quad \text{by symmetry}\\
B &= \mathbf{P}(-C \leq Z \leq C)
\end{aligned}
$$
where $\mathbf{P}$ is the probability distribution of $\mathbf{N}(0,1)$.

![images_u2s5_normalTn_areas](../assets/images/images_u2s5_normalTn_areas.svg)

1. What is the smallest $C$ such that the test $\psi(T_n > C)$ has asymptotic level $\alpha$? 

2. As a function of $\alpha$, what is $C_\alpha$?

3. Let the rejection region for the test $\psi(T_n > C_\alpha)$ be 
   $$
   R_\alpha = \{(X_1, ...,X_n) \in \{0,1\}^n: \overline{X}_n < L \cup \overline{X}_n > R \}.
   $$
   What are $L$ and $R$?

**Answer**: 

$1. \quad 2A;\\2. \quad q_{\alpha/2};\\3. \quad L = 0.5-q_{\alpha /2}\frac{\sqrt{0.5(1 - 0.5)}}{\sqrt{n}}; \quad R = 0.5+q_{\alpha /2}\frac{\sqrt{0.5(1 - 0.5)}}{\sqrt{n}}$

**Solution**:

1) By CLT, if $\mathbb{E}[X] = p^* = 0.5$, then
$$
\sqrt{n}\frac{\overline{X}_ n - 0.5}{\sqrt{0.5(1 - 0.5)}} \xrightarrow [n \to \infty ]{(d)} N(0,1).
$$
Let $\mathbf{P}_{1/2} = \text {Ber}(1/2)$ for notational convenience. Then for the test statistics
$$
T_n = \left|\sqrt{n}\frac{\overline{X}_ n - 0.5}{\sqrt{0.5(1 - 0.5)}}\right|,
$$
We have (by observing the graph)
$$
\mathbf{P}_{1/2}\left(T_ n>C\right)\xrightarrow [n \to \infty ]{}A + A = 2A
$$
where $2A$ are the total area of the shaded regions under the graph of the normal distribution.

Since $H_0$ is defined by a single value $p=1/2$, the asymptotic level is equal to the asymptotic type 1 error at $p=1/2$, which is $\mathbf{P}_{1/2}(T_n > C)$. Therefore, given a desired asymptotic level $\alpha$, choosing a threshold $C_\alpha$ such that
$$
P(Z<-C_\alpha )+P(Z>C_\alpha )\, =\, A+A=2A\qquad Z\sim \mathcal{N}(0,1)
$$
will result in a test $\psi = \mathbf{1}(T_n > C_\alpha)$ that has asymptotic level $\alpha$. Furthermore, for any threshold $C < C_\alpha$ will yield a larger asymptotic type 1 error, as shown in the figure below.

![images_u2s5_normalTn_smallerC](../assets/images/images_u2s5_normalTn_smallerC.svg)

For $C < C_\alpha$, the type 1 error for $\psi = \mathbf{1}(T_n > C)$ (shaded blue) is larger than the type 1 error for $\psi = \mathbf{1}(T_n > C_\alpha)$ (shaded orange).

2) Since $\alpha =P(Z<-C_\alpha )+P(Z>C_\alpha )=2 P(Z>C_\alpha )$ by symmetry, we have $C_\alpha = q_{\alpha/2}$.

3) The rejection region of $\psi = \mathbf{1}(T_n > q_{\alpha/2})$ is defined by
$$
T_ n\, =\, \left|\sqrt{n}\frac{\overline{X}_ n - 0.5}{\sqrt{0.5(1 - 0.5)}}\right| > q_{\alpha/2}\\
\implies \overline{X}_ n\, <\,  0.5-q_{\alpha /2}\frac{\sqrt{0.5(1 - 0.5)}}{\sqrt{n}} \cup \overline{X}_ n\, >\, 0.5+q_{\alpha /2}\frac{\sqrt{0.5(1 - 0.5)}}{\sqrt{n}}.
$$

### Rejection region

1. **True or False:** The rejection region of a test $\psi$ depends on the value of the true unknown parameter $\theta^*$ explicitly, in the sense that we need to specify the value of $\theta^*$ in order to compute the rejection region. 

   False. The rejection region cannot depend on the parameter value $\theta^*$ because it is **unknown**. Instead, we use the **sample** (and **implicitly** the true unknown distribution $\mathbf{P}_{\theta^*}$ of the data) to design the test.

2. **True or False:** To define a statistical test $\psi$, it is enough to define the rejection region $R_\psi$.

   True. A test is by definition an indicator function of its rejection region.
   $$
   \psi = \mathbf{1}((X_1,..., X_n)\in R_\psi)
   $$
   Hence, yes, to define a test, all that is needed is to define its rejection region.

## 2. Worked Example: A Two-Sided Test Associated to a Bernoulli Experiment

* Let $X_1, ...X_n \stackrel{iid}{\sim} \mathsf{Ber}(p),$ for some unknown $p \in (0,1)$.

* We want to test
  $$
  H_0: p = 1/2 \text{ vs. }H_1: p \neq 1/2
  $$
  With asymptotic level $\alpha \in (0,1).$

* Let the test statistic to be $T_n  = \left\vert  \sqrt{n} {\hat{p}_n - 0.5 \over\sqrt{0.5(1-0.5)} }\right\vert$, where $\hat{p}_n = \overline{X}_n$

  We know that
  $$
  \sqrt{n} {\hat{p}_n - p \over\sqrt{p(1-p)} } \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
  $$
  We do the standardization in order to calculate the probability: $\mathbb{P}_\theta [\psi = 1]$, where $\theta = \Theta_0$.

* The probability of reject the null when $p$ is true is
  $$
  \mathbb{P}_p \left[ \sqrt{n} {\hat{p}_n - p \over\sqrt{p(1-p)} } \in \mathcal{R} \right] = \alpha(p)
  $$
  Since $p \in \Theta_0 = \{1/2\}$,
  $$
  \mathbb{P}_{1/2} \left[ \sqrt{n} {\hat{p}_n - 0.5 \over\sqrt{0.5(1-0.5)} } \in \mathcal{R} \right] = \alpha(1/2)
  $$

* We are going to reject ($\psi = 1$) if $ \left\vert  \sqrt{n} {\hat{p}_n - 0.5 \over\sqrt{0.5(1-0.5)} }\right\vert> C$. In other words,
  $$
  \mathbb{P}_{1/2}\left[ \sqrt{n} {|\hat{p}_n - 0.5| \over 0.5 } > C\right] \xrightarrow[n \rightarrow \infty]{} \alpha
  $$
  We would replace $C$ with $q_{\alpha/2}$, so
  $$
  \mathbb{P}_{1/2}\left[ \sqrt{n} {|\hat{p}_n - 0.5| \over 0.5 } > q_{\alpha/2}\right] \xrightarrow[n \rightarrow \infty]{} \alpha
  $$
  In this problem, if $H_0$ is true, then by CLT,
  $$
  \mathbb{P}[T_n > q_{\alpha/2}] \xrightarrow[n\rightarrow \infty]{} 0.05
  $$
  So the test $\psi$ with reject region
  $$
  R_\psi = \{ \sqrt{n} |\bar{X}_n - 0.5| \times 2 > q_{\alpha/2}\}
  $$
  has asymptotic level $\alpha.$

* Let $\psi_\alpha = \mathbf{1}\{T_n > q_{\alpha/2}\}$

## 3. Worked Examples: Two-sided and one-sided test

1. **Fair coin**: Recall Lecture 6 Example: Test the fairness of a coin: Our data gives $\sqrt{n}{\bar{X}_n - 0.5 \over\sqrt{0.5(1-0.5)} } \approx -0.77$.

   For $\alpha = 5\%, \, q_{\alpha/2} = 1.96$, $H_0$ is not rejected at the asymptotic level $5\%$ by the test $\psi_{5\%}$, since $0.77 < 1.96 $.

2. **News on YouTube**: 

   * $H_0: p \geq 0.33$ vs. $H_1: p < 0.33$. This is a one-sided test.

   * We reject if
     $$
     \sqrt{n} {\hat{p}_n - p \over \sqrt{p(1-p)} } < C
     $$
     Since we want to test whether $\hat{p} $ is small enough. 

     So we want
     $$
     \mathbb{P}_p \left[ \sqrt{n} {\hat{p} - p \over \sqrt{p(1-p)} } < C \right] \xrightarrow[n \rightarrow \infty]{} \alpha(p)\\
     \sup_{p \in \Theta_0} \alpha(p) = \alpha
     $$
     We can replace $C$ with $-q_\alpha$ and re-write it as 
     $$
     \mathbb{P}_p \left[ \sqrt{n} {\hat{p} - p \over \sqrt{p(1-p)} } < -q_\alpha \right] \xrightarrow[n \rightarrow \infty]{} \alpha
     $$

   * Common sense: Reject the null hypothesis if $\hat{p}_n = \overline{X}_n < \lambda$, for $\lambda$ to be chosen later.
     $$
     \sup_{p \leq 0.33} \mathbb{P}_p\left[ \overline{X}_n < \lambda \right] \xrightarrow[n \rightarrow \infty]{} \alpha\\
     
     \sup_{p \leq 0.33} \mathbb{P}_p\left[ \sqrt{n} {\overline{X}_n - p \over \sqrt{p (1 - p)}} < \sqrt{n} {\lambda - p \over \sqrt{p (1 - p)}} \right] \xrightarrow[n \rightarrow \infty]{} \alpha
     $$
     where $\sqrt{n} {\overline{X}_n - p \over \sqrt{p (1 - p)}} \sim \mathcal{N}(0,1)$ and $\sqrt{n} {\lambda - p \over \sqrt{p (1 - p)}} = C$.

   * But what value for $p \in \Theta_0 =  [0.33,1]$ should we choose?

     Let's pick $p_o$ that for any $p$ in $H_0$, the probability is asymptotically less than $\alpha$:
     $$
     \mathbb{P}_p \left[ \sqrt{n} {\hat{p} - p_o \over \sqrt{p_o(1-p_o)} } < -q_\alpha \right] = f(p,p_o)  \leq \alpha, \quad \forall p \geq 0.33
     $$
     In other words, we want to choose $p_o$ such that
     $$
     f_n(p_o) = \sup_{p} \mathbb{P}_p\left[ \sqrt{n} {\overline{X}_n - p_o \over \sqrt{p_o (1 - p_o)}} < -q_{\alpha} \right] \xrightarrow[n \rightarrow \infty]{} \alpha
     $$

   * Type 1 error is the function $p \mapsto \mathbb{P}_p[\psi = 1]$. To control the level we need to find the $p$ that maximizes it over $\Theta_0$

     $\rightarrow$ No need for computations it's clearly $p = 0.33$, which is the one that is at the boundary, **maximizes type 1 error**.

     $H_0$ is not reject at the asymptotic level $5\%$ by the test $\psi_{5\%}$, since with $q_\alpha = 1.645$, $\sqrt{n}{\hat{p} -  0.33\over\sqrt{0.33(1-0.33)} } =  -1.50 > -1.645$.

> #### Exercise 39
>
> If the test $\psi = \mathbf{1}(T_n > q_{\alpha/2})$ is designed to have asymptotic level $5\%$ and the null hypothesis is rejected, what if the asymptotic level is $10\%$?
>
> **Answer**: the null hypothesis is also rejected.
>
> **Solution**: A test with a smaller (asymptotic) level is more “stringent" than a test of the same form with a greater (asymptotic) level.

> #### Exercise 40 
>
> Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Ber}(p^*)$ for some true parameter $p^* \in (0,1)$, and let $(\{ 0,1\} , \{ P_ p\} _{p \in (0,1)})$ denote the associated statistical model where $P_ p = \text {Ber}(p)$.
>
> Suppose the null hypothesis is $H_0: p^* \leq 1/2$ and the alternative hypothesis is $H_1: p^* > 1/2$. Let $\psi$ continue to denote the statistical test we will use. (Recall that a test takes value either $0$ or $1$.) Usually it is of the form $\mathbf{1}(T_n>C)$ where $C$ is a threshold to be specified and $T_n$ is known as a test statistic. 
>
> Consider the following graph of this hypothesis testing set-up.
>
> ![images_u2s4_hypotest_graph](../assets/images/images_u2s4_hypotest_graph.svg)
>
> * Continuous curve on the left: type 1 error, $\alpha_\psi$, graphed as a function of $\theta$.
> * Continuous curve on the right: type 2 error, $\beta_\psi$, graphed as a function of $\theta$.
> * Horizontal axis: the parameter space $\Theta = (0,1)$.
>
> 1. Which letter indicates $\Theta_0$ the region defined by the null hypothesis?
>
> 2. Which letter indicates $\Theta_1$ the region defined by the alternative hypothesis?
>
> 3. Let $p \in (0,1)$ denote the point where the power is attained, i.e., the point where
>    $$
>    \pi _\psi = \inf _{\Theta _1} (1 - \beta _\psi (p)).
>    $$
>    Which letter indicates the ordered pair $(p, \pi_\psi)$?
>
> 4. Which of the following are levels of $\psi$?
>
>    a. $5\%$
>
>    b. $10\%$
>
>    c. $20\%$
>
> **Answer**:
>
> $1. \quad B; \\2. \quad C; \\3. \quad A.\\4. \quad c.$
>
> **Solution**:
>
> 1) We are given that $\Theta_0:p\leq 1/2$, then the interval $(0,1/2]$ defines $\Theta_0$.
>
> 2) We are given that $\Theta_1:p > 1/2$, then the interval $(1/2,1]$ defines $\Theta_1$.
>
> 3) The continuous curve on the right, which graphs $\beta_\psi$, attains its maximum at $p = 1/2$, and this maximum is given by $\beta_\psi(1/2) = 0.8$. Therefore,
> $$
> \pi _\psi = \inf _{p \in (0,1)} (1 - \beta _\psi (p)) =1 - 0.8 = 0.2,
> $$
> 4) The level of $\psi$ is given by any real $\alpha \in \R$ such that
> $$
> \alpha _{\psi }(p) \leq \alpha , \quad \text {for all} \,  \,  p \in \Theta _0 = (0,1/2]
> $$
> That is, the type 1 error is uniformly bounded above by $\alpha$. According to the graph, the continuous curve on the left curve stays below $0.2$.

## 4. Behavior of Type 1 and Type 2 Errors for One-Sided Tests

**Setup**:

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } X\sim \mathbf{P}_{\mu }$ where $\mu \in \R$ is the true unknown mean of $X$, and the variance $\sigma^2$ of $X$ is fixed. The associated statistical model is $\left( E, \{\mathbf{P}_\mu\}_{\mu \in \R} \right)$ where $\mathbb{E}$ is the sample space of $X$.

We conduct a one-sided hypothesis test with the following hypotheses:
$$
H_0: \mu \leq \mu _0\qquad \Longleftrightarrow \Theta _0\, =\, (-\infty , \mu _0]\\
H_1: \mu > \mu _0\qquad \Longleftrightarrow \Theta _1\, =\, (\mu _0,+\infty )
$$
Note the boundary between $\Theta_0$ and $\Theta_1$. You use the statistical test:
$$
\psi_n = \mathbf{1}(T_n > q_\alpha)\\
\text{where }T_n = \sqrt{n}\frac{\overline{X}_ n - \mu _0}{\sigma }.
$$
The shaded region corresponds the type 1 error $\alpha_{\psi_n}(\mu_0)$ for large $n$.

![lec7-4-type1error](../assets/images/lec7-4-type1error.png)

At $\mu = \mu_0$ and when $n$ is large, $T_n \sim \mathcal{N}(0,1)$ by the CLT. Therefore, when $n$ is large, the type 1 error $\mathbf{P}_{\mu _0}\left(T_ n>q_{\alpha }\right)$ is geometrically approximately the area of the "right tail" of standard normal distribution defined by the line $T_n = q_\alpha$.

Alternatively, since
$$
T_ n\, =\,  \sqrt{n}\frac{\overline{X}_ n - \mu _0}{\sigma } \, >\, q_{\alpha } \iff   \overline{X}_ n>\, \mu _0+q_{\alpha }\frac{\sigma }{\sqrt{n}},
$$
we have
$$
\mathbf{P}_{\mu _0}\left(T_ n>q_{\alpha }\right) = \mathbf{P}_{\mu _0}\left(\overline{X}_ n>\mu _0+q_{\alpha }\frac{\sigma }{\sqrt{n}}\right),
$$
which is the area of the "right tail" of the distribution of $\overline{X}_n$ to the right of $\overline{X}_ n=\mu _0+q_{\alpha }\frac{\sigma }{\sqrt{n}}$. By CLT, for $n$ large, the distribution of $\overline{X}_n$ is approximately Gaussian, with mean $\mathbb{E}[X]$ and variance ${\sigma^2 \over n }$.

The graph of the distribution of $\overline{X}_n$ for $\mu < \mu_0$ is as follows. It is a single shift without rescaling, since the variance of $X$ is fixed at $\sigma$.

![images_u2s5_errortrend_Xnbar_shiftleft](../assets/images/images_u2s5_errortrend_Xnbar_shiftleft.svg)

As $\mu$ decreases from $\mu_0$ (i.e. moving away from the boundary of $\Theta_0$ and $\Theta_1$), the type 1 error $\alpha_{\psi_n}(\mu)$ decreases. The threshold is
$$
\tau _{n,\alpha }=\mu _0+q_{\alpha }\frac{\sigma }{\sqrt{n}}
$$
Since the type 1 error $\alpha _{\psi _ n}(\mu )=\mathbf{P}_{\mu }(\overline{X}_ n>\tau )$ is the area of the tail to the right of $\tau$, we see that the type 1 error continues to decrease as $\mu$ (and the distribution of $\overline{X}_n$) moves to the left.

![images_u2s5_errortrend_Xnbar_shiftleft_witherror](../assets/images/images_u2s5_errortrend_Xnbar_shiftleft_witherror.svg)

**Remark**: The type 2 error $\beta _{\psi _ n}(\mu )=1-\mathbf{P}_{\mu }(\overline{X}_ n>\tau )$ decreases as $\mu$ increases from $\mu_0$.

## 5. The p-value of a Statistical Test

* Definition of p-value:

  The **(asymptotic) p-value** of a test $\psi_\alpha$ is the smallest (asymptotic) level $\alpha$ at which $\psi_\alpha$ rejects $H_0$. It is random, it depends on the sample.

* Golden rule

  P-value $\leq \alpha \iff H_0$ is rejected by $\psi_\alpha$, at the (asymptotic) level $\alpha$.

  The smaller the p-value, the more confidently one can reject $H_0$.

## 6. Worked Example: Find the P-value

**Setup:**

We have sample $X_1, \ldots , X_{n} \stackrel{iid}{\sim } \text {Ber}(p^*)$ and associated statistical model $(\{ 0,1\} , \{  \text {Ber}(p) \} _{p \in (0,1)})$. The null and alternative hypotheses are 
$$
H_0: p^* = 1/2\\H_1: p^* \neq 1/2
$$
Let
$$
T_ n = \sqrt{n} \left|  \frac{\left( \overline{X}_ n - 0.5\right)}{\sqrt{0.5(1 - 0.5)}}  \right|
$$
Denote the test statistic and let
$$
\psi = \mathbf{1}\left( T_ n \geq q_{\eta /2} \right).
$$
Denote the test where $q_\eta$ is the $1 - \eta$ quantile of a standard Gaussian.

**Questions:**

1. In one run of the experiment, you obtain the data set consisting of $80$ Heads, and evaluated test statistics $T_n$ at this data set to be $T_n = 2.82842$.

   What is the asymptotic p-value for this data set?

2. In another run of the experiment, you obtain the data set consisting of $106$ Heads, and evaluated test statistics $T_n$ at this data set to be $T_n = 0.8485$.

   What is the asymptotic p-value for this second data set?

3. Now let's generalize our findings above. In this two-sided test, as the test statistic $T_n$ increases, the p-value ...

   a. increases

   b. decreases

**Answer**:

$1.\quad 0.0047; \\2. \quad 0.3962;\\3. \quad b.$

**Solution**:

1) In Lec 6 Exercise 35, we observed that $T_n = |-2.82842|$. For notational convenience, let $P_{1/2} = \mathsf{Ber}(1/2)$. Recall that the asymptotic level is given by
$$
\lim _{n \to \infty } P_{1/2}(T_ n \geq q_{\eta /2}) = P(|Z| > q_{\eta /2}) = \eta
$$
where $Z \sim \mathcal{N}(0,1)$. Hence, we need to find the smallest level $\eta$ such that $\psi$ rejects, i.e., such that
$$
T_ n \geq |-2.82842|.
$$
Hence, we should set $q_{\eta /2} = 2.82842$ and solve for $\eta$. Using computational tools or a table of the standard Gaussian, we find that
$$
\eta = 2 P(Z \geq 2.82842) \approx 2(0.002339)=0.00467.
$$
2) We observed $T_ n = 0.8485$. Following the same procedure as above, we set $T_ n = 0.8485$, and using computational tools or a table of the standard Gaussian, we find that
$$
\eta = 2 P(Z \geq 0.8485) \approx 0.3961596
$$
3) As the test statistic increases, the p-value will decreases. Note that $T_n$ measures (up to some rescaling) the deviation from the true mean under $H_0: p^* = 0.5$. As this value grows, our observation moves further into the tails of the distribution $\mathcal{N}(0,1)$. Since the asymptotic p-value for this problem is given by $1-\Phi(T_n)$ where $\Phi$ is the CDF of $\mathcal{N}(0,1)$, this implies that the asymptotic p-value decreases as $T_n$ increases.

**Remark 1**: As a rule of thumb, a smaller p-value implies that one can more confidently reject the null hypothesis. Hence, in this scenario, we can more confidently reject the null for experiment $I$ than the null from experiment $II$. You can think of a p-value as a measure of '**how surprised**' you are to observe the given data set under the assumption that the null hypothesis holds. In particular, the smaller the p-value is, the more surprised you should be.

**Remark 2**: A very large value of $T_n$ indicates a rare event under the null hypothesis, so we should be ‘more surprised' at the data if we observe a very large value of $T_n$ as opposed to a small one. The fact that the p-value decreases as $T_n$ increases is consistent with that intuition, since our heuristic is to be more surprised at very small p-values than large ones under $H_0$.

**Remark 3:** Here is an explanation of heuristic of p-value. As the p-value gets smaller, this means we can set the level of a test smaller and smaller and will still reject the null hypothesis based on the data. Since a smaller type 1 error tolerates rarer events under the null, this means that a small p-value lends evidence that the observation was a rare event under $H_0$. Therefore, a smaller p-value suggests more evidence against $H_0$.

## 7. Worked Example: the P-value of a One-Sided Test

Students are asked to count the number of chocolate chips in $15$ cookies for a class activity. They found that the cookies on **average** had $16.5$ chocolate chips with a **standard deviation** of $5.2$ chocolate chips. The packaging for these cookies claims that there are at least $20$ chocolate chips per cookie.

One student thinks this number is unreasonably high since the average they found is significantly lower. Another student claims the difference might be due to chance.

As a statistician, you decide to approach this question with the tools of hypothesis testing. You make the following modeling assumptions on the cookies:

* $X_1, ..., X_n$ are i.i.d. Gaussian random variables,
* $\sqrt{\mathsf{Var}(X_1)} = 5.2$, and
* $\mathbb{E}[X_1] = \mu$ Is an unknown parameter.

You defined the hypotheses as follows
$$
H_0: \mu \geq 20, \quad H_1: \mu < 20.
$$
and specify the test
$$
\psi _ n := \mathbf{1}\left( \sqrt{n}\frac{\overline{X}_ n - 20}{5.2} < -q_{\eta } \right),
$$
where $q_\eta$ is the $1 -\eta $ quantile of a standard Gaussian. (Note that if $Z \sim \mathcal{N}(0,1)$), then $P(Z < - q_\eta ) = P(Z > q_\eta ) = \eta$. Also, since this is a **one-sided test**, we will not use an absolute value to define our test statistic.)

If $\mu =20$ and $X_1, ..., X_n \stackrel{iid}{\sim} \mathcal{N}(\mu, \sigma^2)$, the given test statistic is a standard Gaussian:
$$
\sqrt{n}\left(\frac{\overline{X}_ n - 20}{5.2}\right) \sim N(0,1).
$$
The above holds for *any* value of $n$, not just asymptotically.

For this test and the observed sample mean $\overline{X}_n = 16.5$, the associated p-value is calculated as follows.

**Answer**: $0.00466$

**Solution**:

For notational convenience, let $\mathbf{P}_\mu$ denote the distribution $\mathcal{N}(\mu, 5.2^2)$. Recall that the level $\alpha$ is a bound on the type 1 error. i.e., $\alpha$ is a level of $\psi$ if 
$$
\alpha _\psi (\mu ) = \mathbf{P}_\mu (T_ n <- q_{\eta }) \leq \alpha \quad \text {for all} \,  \,  \mu \geq 20,
$$
where,
$$
T_n = \sqrt{n}\frac{\overline{X}_ n - 20}{5.2}.
$$
Observe that is $X_1,...,X_n \sim P_\mu$ and $\mu > 20$, then
$$
\begin{aligned}
T_n &= \sqrt{n}\frac{\overline{X}_ n - \mu +\left(\mu -20\right)}{5.2}\\
& \sim Z+ \frac{\sqrt{n}}{5.2}(\mu -20).
\end{aligned}
$$
In particular, the distribution of $T_n$ is normal with mean shifted to the right of $\mathcal{N}(0,1)$. Comparing the tails visually (as in previous problems) shows the inequality
$$
\mathbf{P}_\mu (T_ n < -q_{\eta }) < \mathbf{P}_{20}(T_ n < - q_\eta ) = \eta .
$$
Therefore, $\mu = 20$ is the "worse-case" possibility under the null, and $\psi$ is a test of level $\eta$. To compute the p-value, we just need to find the smallest possible $\eta$ such that $\psi$ rejects $H_0$. Hence, we set
$$
-q_\eta = \sqrt{15}\left( \frac{16.5 - 20}{5.2} \right) \approx -2.6068
$$
and compute
$$
P\left(Z < \sqrt{15}\left( \frac{16.5 - 20}{5.2} \right)\right) = P\left(Z > -\sqrt{15}\left( \frac{16.5 - 20}{5.2} \right)\right) \approx 0.0047
$$
where $Z \sim \mathcal{N}(0,1)$. This gives a p-value of $\approx 0.0047$ or roughly $0.5\%$.

**Remark**: A p-value less than $1\%$ indicates that observing a sample mean smaller than $16.5$ is a less than $1\%$ chance event if $\mu = 20$ (which is the worst-case scenario under $H_0$). This indicates a fairly rare event, so it seems reasonable, given our modeling assumptions, to doubt the second student's claim that the low number of chocolate chips was due to chance.

> #### Exercise 41
>
> Suppose we have a test statistic $T_n$ such that $T_ n \sim |Z|$ where $Z \sim \mathcal{N}(0,1)$. In particular, for this problem we know the distribution of $T_n$ for any fixed $n$ and not just asymptotically. You design the test
> $$
> \psi _ n = \mathbf{1}(T_ n \geq q_{\eta /2})
> $$
> where $q_\eta$ is the $1-\eta$ quantile of a standard Gaussian (i.e. if $Z \sim \mathcal{N}(0,1)$, then $P(Z > q_\eta) = \eta$). If $\psi  =1$, we will reject $H_0$, and if $\psi = 0$, we will fail to reject $H_0$.
>
> With this setup, you observe a data set and compute $T_n$. Consider the following figure.
>
> ![images_u2s5_visualizing_pvalue](../assets/images/images_u2s5_visualizing_pvalue.svg)
>
> 1. On which side, **to the left** or **to the right**, of $T_n$ should the value $q_{\eta/2}$ be such that $\psi_n$ rejects on our data set?
>
> 2. What is the largest value of $q_{\eta/2}$ such that $\psi_n$ rejects on our data set?
>
> 3. What is the smallest value of $\eta$ so that $\psi_n$ rejects on our data set?
>
> 4. Now you observe a new data set and compute a new value of the test statistic, which we denote by $T_n'$ . Suppose that $T_ n' < T_ n$, *i.e.*, the test statistic has a smaller value than from before.
>
>    Will the new p-value be **larger** or **smaller** than the p-value from the previous data set considered in this problem?
>
> **Answer**: 
>
> $1.\quad \text{To the left};\\ 2. \quad B;\\ 3. \quad \eta =2\times (\text {the area under the curve to the right of B});\\ 4. \quad \text{Larger}.$
>
> **Solution**:
>
> 1) If $q_{\eta/2}$ is to the left of $T_n$ (i.e., $q_{\eta /2} < T_ n$), then we see that $\psi =\mathbf{1}(T_ n \geq q_{\eta /2}) = 1$. Hence, we would reject in this situation.
>
> 2) We know that $\psi$ rejects if $q_{\eta /2}$ is to the left of $T_n$. Hence, we should make $q_{\eta /2}$ as large as possible so that we still reject.
>
> 3) Note that $\eta/2$ is the area under the curve to the right of $q_{\eta /2}$. 
>
> 4) If $T_ n' < T_ n$, then we know that new p-value is the area under the curve to the right of $T_ n'$ and to the left of $-T_ n' $. Referring to the graphic in this problem, we see that this means the p-value for $T_ n'$ will be larger than the p-value for $T_n$.

> #### Exercise 42
>
> Consider a statistical experiment with iid $X_1, \ldots , X_ n \sim N(\mu , 1)$, where $\mu$ is an unknown parameter. We will hypothesis test on the parameter $\mu$ by setting $H_0: \mu = 0$ and $H_1: \mu \neq 0$. Our test is designed to be 
> $$
> \psi _ n = \mathbf{1}\left( |\sqrt{n} \overline{X}_ n |\geq q_{\eta /2} \right)
> $$
> where $q_\eta$ denotes the $1-\eta$ quantile of a standard Gaussian.
>
> What is the p-value?
>
> **Answer**:  $2(1 - \Phi (|\sqrt{n} \overline{X}_ n |)$.
>
> **Solution**:
>
> Let $T_ n = \sqrt{n} |\overline{X}_ n|$, the p-value is obtained by setting $q_{\eta /2} = T_ n$ and solving for $\eta$. Since $\eta = P(Z \geq q_{\eta })$ for $Z \sim N(0,1)$, we have that the p-value is given by
> $$
> \eta = 2 P( Z > q_{\eta /2}) = 2 P( Z > T_ n) = 2( 1 - \Phi (T_ n)).
> $$

> #### Exercise 43
>
> Which of the following are true statements regarding p-values? 
>
> a. The p-value represents a **tipping point** in the sense that for any level smaller than the p-value, our test would fail to reject the null hypothesis based on the data.
>
> b. The smaller the p-value, the more confidently one can reject the null hypothesis. 
>
> c. The p-value is computed based on the sample that we observe. 
>
> d. One way that scientists and companies have, in some instances, artificially lowered p-values is by specifying the null and alternative hypotheses *after* observing the data. 
>
> **Answer**: abcd
>
> **Solution**:
>
> a) The first choice elaborates on the definition of the (asymptotic) p-value, which is the smallest (asymptotic) level at which a test $\psi$ will reject the null hypothesis. Hence, for any asymptotic level below the p-value, our test will **fail to reject** on our observed sample.
>
> d) It is very important in practice that one specifies the null and alternative hypotheses *before* conducting the experiment. Otherwise, it is possible to ‘tweak' the hypotheses. This can artificially result in a lower p-value, which would favor the scientist or company's desired conclusion.



# Lecture 15. Goodness of Fit Test for Discrete Distributions

There are 8 topics and 3 exercises.

## 1. Introduction to Goodness of Fit Tests

#### Recap of Parametric Hypothesis Testing: The Uniform Statistical Model

Let $X$ be a uniform random variable with the distribution $\text {Unif}\left[0,\theta ^*\right]$.

We would like to test whether $H_0: \theta ^* = 2$ or $H_1: \theta ^* \ne 2$, with $\Theta = (0,\infty )$.

Let $X_1,\dots ,X_ n$ be i.i.d. samples of $X$.

* Let $\overline{X_ n}$ denote sample mean.
* Let $\widetilde{S}_ n$ denote the unbiased sample variance of $X_1, \dots , X_ n$.
* Let $\widehat{\theta _ n}^{\text {MLE}}$ denote the maximum likelihood estimator of $\theta$.
* Let $\ell _ n\left(\widehat{\theta _ n}^{\text {MLE}}\right)$​​ denote the log-likelihood of $n$​​ samples evaluated at the maximum likelihood estimator and $\ell _ n\left(2\right)$​​ denote the log-likelihood of $n$​​ samples under $H_0$.

Select from the tests that are technically correct (that is, can be applied under this scenario) and that have the required level $\alpha \in [0,1]$.

**Note:** By asymptotic level of $\alpha$​, we require the probability of type-1 error under $H_0$​ be at most $\alpha$​ as $n \rightarrow \infty$​. By non-asymptotic level of $\alpha$​, we require the probability of type-1 error under $H_0$​ be at most $\alpha$​ for every $n$.

A. $\mathbf{1}\left\{ \frac{\left| \overline{X_ n} - 1 \right|}{\sqrt{\widetilde{S}_ n/n}} > q_{\alpha /2}\right\}$​ for non-asymptotic level $\alpha$​, where $q_{\alpha /2}$​ is the $(1-\alpha/2)$​-quantile of the Student's T distribution with $n-1$​ degrees of freedom.

B. $\mathbf{1}\left\{ \sqrt{n} \frac{\left| 2 \overline{X_ n} - 2 \right|}{\sqrt{4/3}} > q_{\alpha /2}\right\}$ for asymptotic level $\alpha$, where $q_{\alpha}$ is the $(1-\alpha)$-quantile of the standard normal random variable.

C. $\mathbf{1}\left\{  \widehat{\theta _ n}^{\text {MLE}} > 2 \text { or } \widehat{\theta _ n}^{\text {MLE}} \le 1 \right\}$ for asymptotic level $\alpha$.

D. $\mathbf{1}\left\{ 2\left(\ell _ n\left(\widehat{\theta _ n}^{\text {MLE}}\right) - \ell _ n\left(2\right)\right) > q_{\alpha }\right\}$ for asymptotic level $\alpha$, where $q_\alpha$ is the $(1-\alpha)$-quantile of $\chi_1^2$​.

**Answer:**

BC is technically CORRECT.

A. The first choice is attempting a Student's T test for non-asymptotic level $\alpha$​ and this is technically incorrect because $X$​ is not a Gaussian random variable. **Only if $X$​ is a Gaussian random variable will the test statistic follow a Student's T distribution for a finite number of sample $n$.**

B. This is a test that is both technically correct and has an asymptotic level $\alpha$​. This can be seen from the following:

* $2\overline{X_ n}$​ has an expectation equal to $\theta^*$.

* The variance of $2\overline{X_ n}$ under $H_0$ is
  $$
  \begin{aligned}
  \textsf{Var}_{H_0}\left(2 \overline{X_ n}\right) &= \frac{4}{n}\textsf{Var}_{H_0}(X)\\
  &= \frac{4}{n} \frac{4}{12}\\
  &=\frac{4}{3n}.
  \end{aligned}
  $$

* An application of CLT.

**Remark**: Wald's test cannot be written out because Fisher information does not exist for the uniform random variable.

C. This choice has an asymptotic level 0 because of the following:
$$
\begin{aligned}
P_{H_0}\left[\widehat{\theta _ n}^{\text {MLE}} > 2 \text { or } \widehat{\theta _ n}^{\text {MLE}} \le 1\right] &= P_{H_0}\left[\widehat{\theta _ n}^{\text {MLE}} > 2\right] + P_{H_0}\left[\widehat{\theta _ n}^{\text {MLE}} \le 1\right]\\
&= P_{H_0}\left[\widehat{\theta _ n}^{\text {MLE}} \le 1\right]\\
&= P_{H_0}\left[\max _{i=1,\dots ,n}X_ i \le 1\right]\\
&= \left(\frac{1}{2}\right)^ n \to 0.
\end{aligned}
$$
D. This choice is attempting a likelihood ratio test using the log-likelihoods evaluated at the MLE and under $H_0$ and this choice is also technically incorrect because the MLE technical conditions are not satisfied for the uniform statistical model (recall the technical conditions for asymptotic normality of the MLE). **Only if the MLE conditions are satisfied can this test be applied according to Wilk's theorem.** 

#### Intuition for Goodness of Fit Tests

Let $X$ be a r.v. Given i.i.d. copies of $x$ we want to answer the following types of questions:

* Does $X$ have distribution $\mathcal{N}(0,1)$? (Cf. Student's T distribution)
* Does $X$ have distribution $\mathcal{U}([0,1])$?
* Does $X$ have PMF $p_1 = 0.3, p_2 = 0.5, p_4 = 0.2$?

These are all *goodness of fit* (GoF) tests: we want to know if the hypothesized distribution is a good fit for the data.

Key characteristic of GoF tests: non-parametric modeling.

In practice, a useful tool for making such a decision is to use a **histogram** of the data set. The x-axis, which represents the sample space, is divided into the intervals $[i, i+1]$​​​ for all $i \in \Z$​​. The bar over the interval $[i,i+1]$​ represents how many data points took values in that interval.

#### Terminology

Suppose you observe i.i.d. samples $X_1, \ldots , X_ n \sim P$ from some unknown distribution $\mathbf P$. Let $\mathcal{F}$ denote a parametric family of probability distributions (e.g., $\mathcal{F}$ could be the family of normal distribution $\{  \mathcal{N}(\mu , \sigma ^2) \} _{\mu \in \mathbb {R}, \sigma ^2 > 0}$).

In the topic of **goodness of fit testing**, our goal is to answer the question "**Does** $\mathbf{P}$​​​ **belong to the family** $\mathcal{F}$​​​, **or is** $\mathbf{P}$​​ **any distribution outside of**  $\mathcal{F}$​ **?**"

Parametric hypothesis testing is a particular case of goodness of fit testing (why?). However, in the context of parametric hypothesis testing, we assume that the data distribution $\mathbf{P}$ comes from some **parametric** statistical model $\{ \mathbf{P}_\theta \} _{\theta \in \Theta }$, and we ask if the distribution $\mathbf{P}$ belongs to a submodel $\{ \mathbf{P}_\theta \} _{\theta \in \Theta_0 }$or its complement $\{ \mathbf{P}_\theta \} _{\theta \in \Theta_1 }$. In parametric hypothesis testing, we allow only a small set of alternatives $\{ \mathbf{P}_\theta \} _{\theta \in \Theta_1 }$, where as in the goodness of fit testing, we allow the alternative to be anything.

**Remark**: In general, goodness of fit testing is considered a topic in **non-parametric statistics**, in contrast to the material we have covered so far. You should keep in mind though that the topic of parametric hypothesis testing is a special case of goodness of fit testing. However, to handle non-parametric models we will need to develop new techniques.

#### Goodness-of-Fit

Let $X_1, ..., X_n \stackrel{iid}{\sim}\mathbb{P}_\mathbf{p}$​​, for some unknown $\mathbf{P}\in \Delta_K$​​, and let $\mathbf{p}^0 \in \Delta_K$​​ be fixed.​

We want to test:
$$
H_0: \mathbf{p}=\mathbf{p}^0 \quad \text{vs.} \quad H_1: \mathbf{p} \neq \mathbf{p}^0
$$
With asymptotic level $\alpha \in (0,1)$.

Example: If $\mathbf{p}^0 = (1/K, 1/K, ..., 1/K)$​​, we are testing whether $\mathbb{P}_\mathbf{p}$​​ is the **uniform distribution** on $E$​.

## 2. The Probability Simplex of Discrete Distributions

The probability simplex in $\R^K$​, denoted by $\Delta_K$​, is the set of all vectors $\mathbf{p} = \left[p_1, \dots , p_ K\right]^ T$​ (Note that we are using subscripts for vector indices for simplicity) such that
$$
\displaystyle  \displaystyle \mathbf{p}\cdot \mathbf{1}\, =\, \mathbf{p}^ T \mathbf{1} = 1, ~ ~  p_ i \ge 0 \text { for all } i= 1,\ldots , K
$$
where $\mathbf{1}$ denotes the vector $\, \mathbf{1}=\begin{pmatrix} 1& 1& \ldots & 1\end{pmatrix}^ T$​. 

Equivalently, in more familiar notation, let $E = \{a_1, ..., a_K\}$​​ be a finite space and $(\mathbb{P}_\mathbf{P})_{\mathbf{P} \in \Delta_K}$​​​ be the family of all probability distributions on $E$:

* A set of PMFs: 
  $$
  \Delta_K = \Big\{\mathbf{p} = (p_1, ..., p_K) \in (0,1)^{K}: \sum^K_{j=1} p_j = 1\Big\}
  $$

* For $\mathbf{p} \in \Delta_K$​ and $X \sim \mathbb{P}_\mathbf{P}$​,
  $$
  \mathbb{P}_{\mathbf{P}}[X = a_j] = p_j, \qquad j =  1, ..., K.
  $$

## 3. Goodness of Fit Test - Discrete Distributions

#### Multinomial Distribution

The **multinomial distribution** with $K$​​​ modalities (or equivalently $K$​​ possible outcomes in a trial) is a generalization of the binomial distribution. It models the probability of counts of the $K$ possible outcomes of the experiment in $n'$ i.i.d trials of the experiment.

It is parameterized by the parameters $n', p_1, ..., p_K$ where

* $n'$ is the number of i.i.d trials of the experiment
* $p_i$​ is the probability of observing outcome $i$​ in any trial, and hence the $p_i$​'s satisfy $p_i \geq 0$​ for all $i=1, ..., K$​, and $\displaystyle \sum _{i=1}^ K p_ i = 1$​.

Let $\mathbf{p} \triangleq [p_1~ ~ p_2~ ~ \cdots ~ ~ p_ K]^ T$ and note that $\mathbf{p} \in \Delta _ K$.

The multinomial distribution can be represented by a random vector $\mathbf N\in \mathbb {Z}^ K$ to represent the number of instances $N^{(i)}$ of the outcome $i =1, ..., K$. Note that $\sum _{i=1}^ K N^{(i)} = n'$. The multinomial PMF for all $n$ such that $\sum _{i=1}^ K n^{(i)} = n'$,
$$
\displaystyle  p_{\mathbf N}\left(N^{(1)} = n^{(1)}, \dots , N^{(K)} = n^{(k)}\right) = \frac{n'!}{n^{(1)}! n^{(2)}! \cdots n^{(K)}!} \prod _{i=1}^ K p_ i^{n^{(i)}}.
$$

#### Categorial (Generalized Bernoulli) Distribution and its Likelihood

The multinomial distribution, when specialized to $n'=1$​​​​​ for any $K$​​​​​ gives the **categorical distribution**. When $K=2$​​​​ and the two outcomes are $0$​​​​ and $1$​​​​ the categorical distribution is the Bernoulli distribution, and for any $K > 2$​​ the categorical distribution is also known as the **generalized Bernoulli distribution**.

The categorical distribution, therefore, models the probability of counts of the $K$​​​​ possible outcomes of a discrete experiment in a single trial. Since the total count is equal to $1$​ (only one trial), we can use a random variable $X$ to represent the outcome of the trial. This means the sample space of a categorical random variable $X$ is
$$
  E = \{  a_1, \ldots , a_ K \} .
$$
The vector $\mathbf{p}$​​ is the parameter of a categorical random variable. The PMF of a categorical distribution can be given as
$$
P(X=a_ j) = \prod _{i=1}^ K p_ i^{\mathbf{1}(a_ i=a_ j)} = p_ j, ~ ~ j=1,\dots ,K.
$$
Let $\mathbf{P}_\mathbf {p}$​ denote the distribution of a categorical random variable with sample space $E= \{  a_1, \ldots , a_ K \}$​ and parameter vector $\mathbf{p}$​. Let $\Delta_K$ denote the **probability simplex** in $\R^K$, $p \in \R^K$. The **categorical statistical model** can thus be written as the tuple $\left( \{  a_1, \ldots , a_ K \} , \{ \mathbf{P}_{\mathbf{p}} \} _{\mathbf{p} \in \Delta _ K}\right)$​.

In goodness of fit testing for a discrete distribution, we observe $n$ i.i.d. samples $X_1, \dots , X_ n$ of a categorical random variable $X$ and it is our aim to find statistical evidence of whether a certain distribution $\mathbf{p}^0 \in \Delta _ K$ could have generated $X_1, \dots , X_ n$.

The **categorical likelihood** of observing a sequence of $n$​ i.i.d outcomes $X_1, X_2, \dots , X_ n \sim X$​ can be written using the number of occurrences $N_ i, i=1,\dots ,K$​ of the $K$​ outcomes as
$$
L_ n(X_1,\dots ,X_ n,p_1,\dots ,p_ K) = p_1^{N_1}p_2^{N_2} \cdots p_ K^{N_ K}.
$$
The categorical likelihood of the random variable $X$​, when written as a random function, is
$$
L(X,p_1,\dots ,p_ K) = \prod _{i=1}^ K p_ i^{\mathbf{1}(X = a_ i)}.
$$

#### Maximum Likelihood Estimator for the Categorical Distribution

The log likelihood is
$$
\log L_n(X_1,\dots ,X_ n,p_1,\dots ,p_ K) = N_1 \log(p_1) + ... + N_K \log(1-\sum^{k-1}_{j=1} p_j)
$$
Take the derivative and set it to zero
$$
{\partial \over \partial p_j} \log L_n(X_1,\dots ,X_ n,p_1,\dots ,p_ K) = {N_j \over p_j} + {N_k \over 1- \sum\limits^{k-1}_{j=1} p_j} = 0\\
\implies  {N_j \over p_j} =- {N_k \over 1- \sum\limits^{k-1}_{j=1} p_j}
$$
Let $- {N_k \over 1- \sum\limits^{k-1}_{j=1} p_j} = \gamma$​, we have
$$
p_j  = {N_j \over \gamma}
$$
We know that $\sum\limits_{j=1}^K p_j = 1$​, so that
$$
{\sum\limits^K_{j=1} N_j \over \gamma} =1 \quad \implies \gamma = \sum^K_{j=1} N_j = n
$$
Let $\hat{\mathbf{p}}$​ be the MLE:
$$
\hat{\mathbf{p}}_j = {N_j \over n}, \quad j=1,..., K
$$
$\hat{\mathbf{p}}$​​​​ maximizes $\log L_n(X_1, ..., X_n, \mathbf{p})$​​ **under the constraint**.

> #### Exercise 86
>
> Consider the distribution $\text {Ber}(0.25)$. Consider the categorical statistical model $\left( \{  a_1, \ldots , a_ K \} , \{ \mathbf{P}_{\mathbf{p}}\} \right)$ for this Bernoulli distribution. 
>
> 1. If we get $a_1 = 1$​ and $a_2 = 0$​, then this corresponds to a categorical distribution $\mathbf{P}_{\mathbf{p}}$​ with parameter vector $\mathbf{p}$​ given by ...
>
> 2. Let $a_i = i$​ for $i = 1,...,K$​. The uniform distribution on $E = \{1,2,..., K\}$​ can be expressed asa  categorical distribution $\mathbf{P}_{\mathbf{p}}$​ for some choice of parameter $\mathbf{p}$.
>
>    What is $\sum _{i = 1}^ K p_ i^2$​? 
>
> **Solution:**
>
> 1. Let $X \sim \text{Ber}(0.25)$​. Observe that
>    $$
>    p_1 = P(X = a_1) = P(X = 1) = 0.25\\
>    p_2 = P(X = a_2) = P(X = 0) = 0.75.
>    $$
>    Hence, $\mathbf{p} = [0.25~ ~ 0.75]^ T$​.
>
>    **Remark:** Observe that $\text {Ber}(p)$​​ has a one-dimensional parameter and $\mathbf{P}_{\mathbf{p}}$​​ for this example involves a parameter that is two-dimensional, but such that the second parameter depends on the first one $(p_1 = 1 - p_2)$​​. In general, the categorical distribution for $\mathbf{p} \in \Delta _ K$​​ involves a $K$​​-dimensional parameter, but **categorical distribution has only $K-1$​​ degrees of freedom. This will make our analysis more challenging: the extra constraint on the parameter $\sum _{i = 1}^ K p_ i = 1$​​​​ implies that the Fisher information for the model as specified does not exist. Hence, we cannot apply Wald's test directly.**
>
> 2. By definition, the uniform distribution weighs all elements in $\{ 1, \ldots , K\}$​ equally. Let $\mathbf{P}$​ denote the parameter vector of the uniform distribution on $\{ 1, 2, \ldots , K\}$​. Then
>    $$
>    p_ i = P(X = i) = \frac{1}{K}.
>    $$
>    Thus,
>    $$
>    \sum _{i = 1}^ K p_ i^2 = \sum _{i = 1}^ K \frac{1}{K^2} = \frac{1}{K}.
>    $$

## 4. The Goodness of Fit Test for Discrete Distributions: Chi-Squared Test

Note that 
$$
\sqrt{n}\left(\widehat{\mathbf{p}} - \mathbf{p}^0\right)^ T \mathbf{1} = \sqrt{n}\sum^K_{i=1} \left(\hat{p}_i - p_i^0\right) = \sqrt{n} \ \left(\sum _{i=1}^ K \widehat{p}_ i - \sum _{i=1}^ K p_ i^0\right) = \mathbf{0} \xrightarrow[n \rightarrow 0]{} 0
$$
where $\mathbf{p}^0$​​​​​​​ is the discrete PMF that we wish to test to goodness of fit for an observed sequence of iid samples, and $\hat{\mathbf{p}}$​​​​​​​ is the MLE upon observing the iid samples.

Since all linear combination of a Gaussian vector gives a Gaussian random variable, $\mathbf{0}$​​​ is actually a Gaussian random variable, or say a degenerated Gaussian. 

#### $\chi^2$ test

If $H_0$​​ is true, then $\sqrt{n}(\hat{\mathbf{p} } - \mathbf{p}^0)$​​ is asymptotically normal, and the following holds.

**Theorem:**

Under the null hypothesis:
$$
T_n = n\sum^K_{j=1} {(\hat{\mathbf{p}} - \mathbf{p}^0)^2 \over \mathbf{p}_j^0 } \xrightarrow[n \rightarrow \infty]{(d)} \chi^2_{K-1}
$$

* $\chi^2$​​​ test with asymptotic level $\alpha$: $\psi_\alpha = \mathbb{1}\{T_n > q_\alpha\}$​​​, where $q_\alpha$​ is the $(1-\alpha)$​-quantile of $\chi_{K-1}^2$​.
* Asymptotic p-value of this test: p-value = $\mathbb {P}[Z > T_n | T_n] $​​, where $Z \sim \chi^2_{K-1}$ and $Z \perp T_n$​.

**Informal Ideas with the Theorem above:**



> #### Exercise 87
>
> Let's consider a statistical model with parameter $\theta \in \mathbb {R}^ d$. Let $\theta^*$ be the parameter that generates the $n$ Iid samples $\mathbf X_1,\dots , \mathbf X_ n$. Let $I(\theta )$ be the Fisher information and assume that the MLE $\hat{\theta }_ n^{\text {MLE}}$ is asymptotically normal. Assume that $I(\theta ^0)$ is a diagonal matrix with positive entries $1/t_1, \dots , 1/t_ d$. We wish to perform a test for the hypotheses $H_0: \theta ^* = \theta ^0$ and $H_1: \theta ^* \ne \theta ^0$.
>
> Let the test statistic $T_n$ be
> $$
>   T_ n = n\sum _{i=1}^ d \frac{\left(\theta _ i^0 - \hat{\theta }_{i}\right)^2 }{t_ i},
> $$
> where $\left[\hat{\theta }_{1} ~ ~  \hat{\theta }_{2}~ ~  \cdots ~ ~  \hat{\theta }_{d}\right]^ T = \hat{\theta }_ n^{\text {MLE}}$.
>
> 1. What distribution does the test statistic $T_n$​​​ converge to under $H_0$​​​ as $n \rightarrow \infty$​​​?
> 2. What is the number of degrees of freedom of the asymptotic distribution of $T_n$​? 
>
> **Answer:**
>
> 1. $\chi _ d^2$.
> 2. $d$.
>
> **Solution:**
>
> The test statistic $T_n$ can be seen to be equivalent to
> $$
> n\left(\hat{\theta }_ n^{\text {MLE}} - \theta ^0\right)^ T I(\theta ^0) \left(\hat{\theta }_ n^{\text {MLE}} - \theta ^0\right),
> $$
> which is the test statistic for **Wald's test**, Therefore,
> $$
> T_ n \xrightarrow [n\to \infty ]{(d)} \chi _ d^2.
> $$

## 5. The Chi-Squared Test - Main / Informal Ideas

* Scaling by $\mathbf{p}^0_j$​​​​​​​ but not $(\mathbf{p}^0_j)^2$​​​​​​​​​​​ is coming from the half Fisher information matrix. We have some issues with this **Fisher information matrix: it is not well-defined in the sense of it is not invertible.** 

  The WRONG calculation of Fisher information: 
  $$
  L_n(X_1, ..., X_n ;\mathbf{p}) = p_1^{\sum\limits^n_{i=1}\mathbb{1}_{(X_1 = \theta_1)}} \cdot p_2^{\sum\limits^n_{i=1}\mathbb{1}_{(X_2 = \theta_2)}} ... p_K^{\sum\limits^n_{i=1}\mathbb{1}_{(X_K = \theta_K)}} \\
  \log L_1(X_1, \mathbf{p}) = \log({p_1}) \mathbb{1}_{(X =\theta_1)} + ... + \log({p_K}) \mathbb{1}_{(X =\theta_K)} \\
  I(\mathbf{p})= \begin{pmatrix} {\partial^2\over \partial p_j^2} & {\partial^2\over \partial p_j p_k} \\{\partial^2\over \partial p_k p_j} & {\partial^2\over \partial p_k^2}   \end{pmatrix} = \begin{pmatrix} {1\over p_j} & 0 \\0 & {1\over  p_k}   \end{pmatrix}\\
  $$
  Plugging in and we get
  $$
  n (\hat{\mathbf{p}}- \mathbf{p}^0) I(\mathbf{p}^0)(\hat{\mathbf{p}}- \mathbf{p}^0) = n \sum^K_{i=1} {(\hat{\mathbf{p}}_j - \mathbf{p}_j)^2\over \mathbf{p}_j}
  $$
  However, the LHS is wrong while the RHS is true. The LHS is wrong because the **additional restriction: $\sum^K_{i=1} p_j = 1$​​​ ($p_j$​​​s are dependent)​**, so we cannot take the derivative with respective to $p_j$​​ in this way. 

  The CORRECT calculation of Fisher information:
  $$
  \log L_1(X_1, \mathbf{p}) = \log(\mathbf{p}) \mathbb{1}_{(X =\theta_1)} + ... + \log(1-\mathbf{p}) \mathbb{1}_{(X =\theta_2)} 
  $$
  Take the derivative and take the expectation,
  $$
  {\partial^2 \over \partial \mathbf{p}^2} = -{\mathbb{1}_{(X = \theta_1)}\over \mathbf{p}^2} - {\mathbb{1}_{(X = \theta_2)}\over (1- \mathbf{p})^2} \xrightarrow[\mathbb{E}]{} -{1\over \mathbf{p}} -{1\over1- \mathbf{p}} = - {1\over \mathbf{p}(1-\mathbf{p})}\\
  {\partial^2 \over \partial (1-\mathbf{p})^2} = - {\mathbb{1}_{(X = \theta_2)}\over (1-\mathbf{p})^2} -{\mathbb{1}_{(X = \theta_1)}\over \mathbf{p}^2} \xrightarrow[\mathbb{E}]{} - {1\over \mathbf{p}(1-\mathbf{p})}\\
  {\partial^2 \over \partial \ \mathbf{p}(1- \mathbf{p})} = {\mathbb{1}_{(X = \theta_1)}\over \mathbf{p}^2} + {\mathbb{1}_{(X = \theta_2)}\over (1- \mathbf{p})^2} = {1\over \mathbf{p}(1-\mathbf{p})}\\
  $$
  The Fisher information is
  $$
  I\left(\begin{pmatrix}\mathbf{p}\\1-\mathbf{p} \end{pmatrix}\right) = \begin{pmatrix} {1\over \mathbf{p}(1-\mathbf{p})} & -{1\over \mathbf{p}(1-\mathbf{p})} \\-{1\over \mathbf{p}(1-\mathbf{p})} & {1\over \mathbf{p}(1-\mathbf{p})} \end{pmatrix} = {1\over \mathbf{p}(1-\mathbf{p})} \begin{pmatrix}1 & -1 \\ -1 & 1 \end{pmatrix}
  $$
  It is not invertible since 
  $$
  I\left(\begin{pmatrix}\mathbf{p}\\1-\mathbf{p} \end{pmatrix}\right) \begin{pmatrix}1\\ 1 \end{pmatrix} = \begin{pmatrix}0 \\ 0 \end{pmatrix}
  $$
  Geometrically, this means the MLE estimator does not fluctuate in all $K$​ direction in $\R^K$​.

* As for $\hat{\mathbf{p}}_j$​​, we know $\hat{\mathbf{p}}_j = {N_j \over n} \sim {\text{Bin}(n, p_j) \over n}$​​​​​. By CLT, the following is true:
  $$
  \sqrt{n} {\hat{\mathbf{p}}_j-\mathbf{p}_j^0 \over \sqrt{\mathbf{p}_j^0(1- \mathbf{p}_j^0)}} \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
  $$
  and
  $$
  n \ {(\hat{\mathbf{p}}_j-\mathbf{p}_j^0)^2 \over \mathbf{p}_j^0(1- \mathbf{p}_j^0)} \xrightarrow[n \rightarrow \infty]{(d)} \chi^2_1\\
  n \sum_{i=1}^K {(\hat{\mathbf{p}}_j-\mathbf{p}_j^0)^2 \over \mathbf{p}_j^0(1- \mathbf{p}_j^0)} \xrightarrow[n \rightarrow \infty]{(d)} \chi^2_K
  $$
  which is true but different from the Theorem given above (not the right normalization and not the right degrees of freedom).

  **Therefore, looking at the joint convergence of all of them is not the same as looking at individual convergences. The key is the dependence between individual estimators should not be ignored, otherwise, it would lead to an erroneous conclusion.**

  **Dividing by $\mathbf{p}^0 (1-\mathbf{p}^0)$​​​​ makes the test statistic $T_n$​​​ larger, this would lead us to reject more, reject when we should not. This is a serious mistake because we may conclude to $H_1$​​ when we should not.  So this test is the kind of test where you do not want to conclude to $H_1$.**

* We only have $K-1$​​​​​​​​​ degrees of freedom, since the asymptotic Gaussian vector we have actually lives in $K-1$​​​​​​​​ dimension not $K$​​​​​​​​ dimension. It lives on the linear space that's orthogonal to the $\begin{pmatrix}1\\1\\ \vdots \\ 1\end{pmatrix}$​​​​​​​​​​. **Having $K-1$​​​​ degrees of freedom is more conservative, since we have a smaller critical value $q_\alpha$​, so we are going to reject less.**

## 6. The Chi-Squared Test - Example Problems I

#### Chi-squared Test I

Let $\widehat{\mathbf{p}}$ denote the MLE for a categorical statistical model $( \{  a_1, \ldots , a_ K \} , \{ \mathbf{P}_{\mathbf{p}} \} _{\mathbf{p} \in \Delta _ K})$. Let $\mathbf{p}^*$ denote the true parameter. Then $\sqrt{n}(\widehat{\mathbf{p}} - \mathbf{p}^*)$ is asymptotically normal and
$$
n \sum _{i = 1}^ K \frac{ ( \widehat{ p_ i } - p_ i^*)^2 }{p_ i^*} \xrightarrow [n \to \infty ]{(d)} \chi _{K -1}^2.
$$
Consider the particular categorical distribution, where we have the statistical experiment $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathbf{P}_{\mathbf{p}}$ and associated statistical model $(\{ 1,2,3\} , \{  \mathbf{P}_{\mathbf{p}} \} _{\mathbf{p} \in \Delta _3})$. We will use the above fact to hypothesis test between the following null and alternative:
$$
H_0: \mathbf{p}^* = [1/3 \quad 1/3 \quad  1/3]^T \\
H_1: \mathbf{p}^* \neq [1/3 \quad 1/3 \quad  1/3]^T.
$$
Consider the test,
$$
\psi = \mathbf{1}\left( n \sum _{i = 1}^3 \frac{ ( \widehat{ p_ i } - \frac{1}{3})^2 }{1/3} >C \right),
$$
for a threshold $C$.

Compute the asymptotic p-value of the test $\psi$ on the data set
$$
\mathbf{x} = 1, 3, 1, 2, 2, 2, 1, 1, 3, 1, 1, 2.
$$
**Solution:**

Since $K=3$​ and $E = \{1,2,3\}$​,
$$
f_{\mathbf{p}}(i) = p_ i, \quad i = 1,2,3.
$$
Next,
$$
L_ n( X_1, \ldots , X_ n, \mathbf{p}) = \prod _{i = 1}^ n f_{\mathbf{p}}(X_ i) = p_1^{N_1} p_2^{N_2} p_3^{N_3}
$$
where 
$$
N_ i = \text {number of times} \,  \,  i \,  \,  \text {appears in } (X_1, \ldots , X_ n) , \quad i = 1,2, 3.
$$
Given the data, we define
$$
L_{12}(\mathbf{x}, \mathbf{p}) = p_1^ A p_2^ B p_3^ C
$$
where $A = N_1 = 6, B = N_2 = 4,$ and $C = N_3 = 2$.

Thus, the log likelihood is 
$$
\log L_ {12}(\mathbf{x}, \mathbf{p}) = 6 \log p_1 + 4 \log p_2 + 2 \log p_3.
$$
Recall the MLE is given by
$$
\widehat{\mathbf{p}}^{MLE}_ n = \text {argmax}_{\mathbf{p} \in \Delta _3} \log L_ n(X_1, \ldots , X_ n, \mathbf{p}).
$$
Hence,
$$
\nabla \log L_ n(\mathbf{x}, \mathbf{p}) = \begin{bmatrix}  \frac{6}{p_1} \\ \frac{4}{p_2} \\ \frac{2}{p_3} \end{bmatrix}.
$$
By the theory of **Lagrange multipliers**, one can show that the maximum occurs at the point $\mathbf{p}$​​ such that there exists $\lambda \neq 0$​ so that
$$
\nabla \log L_ n (X_1, \ldots , X_ n, \mathbf{p}) = \lambda \begin{bmatrix}  1 \\ 1 \\ 1 \end{bmatrix}.
$$
Hence,
$$
\nabla \log L_ n(\mathbf{x}, \mathbf{p}) = \begin{bmatrix}  \frac{6}{p_1} \\ \frac{4}{p_2} \\ \frac{2}{p_3} \end{bmatrix}\\
\begin{bmatrix}  \frac{6}{p_1} \\ \frac{4}{p_2} \\ \frac{2}{p_3} \end{bmatrix} = \lambda \begin{bmatrix}  1 \\ 1 \\ 1 \end{bmatrix}.
$$
Therefore,
$$
p_1 = \frac{6}{\lambda }, \,  \,  p_2 = \frac{4}{\lambda }, \,  \,  p_3 = \frac{2}{\lambda }.
$$
By the constraint $p_1 + p_2 + p_3 = 1$, we see that
$$
\lambda = 6 + 4 + 2 = 12.
$$
Therefore, the MLE is
$$
\widehat{\mathbf{p}}^{MLE}_{12} = \begin{bmatrix}  \frac{1}{2} \\ \frac{1}{3} \\ \frac{1}{6} \end{bmatrix}.
$$
Therefore the test statistic is
$$
n \sum _{i = 1}^3 \frac{ ( \widehat{ p_ i } - \frac{1}{3})^2 }{1/3} = 12\cdot 3 \left( \frac{1}{6^2} + 0 + \frac{1}{6^2} \right) = 2.
$$
By the asymptotic normality, we have
$$
n \sum _{i = 1}^3 \frac{ ( \widehat{ p_ i } - \frac{1}{3})^2 }{1/3} \xrightarrow [n \to \infty ]{(d)} \chi _2^2.
$$
Consulting the link provided, we see that if $X \sim \chi _2^2$ , then $P(X \geq 2) \approx 36.79 \%$. Hence, the asymptotic p-value for the test $\psi$ on this dataset is approximately equal to $0.3679$.

#### Playing Dice

You and your friend play dice games for fun, but one day you suspect that the die your friend uses is not fair. You will gather data and use the tools of hypothesis testing in this problem to provide a plausible answer as to whether or not the die is fair.

Your statistical model is $(\{ 1,2,3,4,5,6\} , \{  \mathbf{P}_{\mathbf{p}} \} _{\mathbf{p} \in \Delta _6})$. You roll the die $15$ times, writing the sample as random variables $X_1, \ldots , X_{15} \stackrel{iid}{\sim } \mathbf{P}_{\mathbf{p}^*}$ where $\mathbf{p}^*$ is the true parameter. Your null and alternative hypothesis are, respectively,
$$
H_0: \mathbf{p}^* = [1/6~ ~ 1/6~ ~ 1/6~ ~ 1/6~ ~ 1/6~ ~ 1/6]^ T\\
H_1: \mathbf{p}^* \neq [1/6~ ~ 1/6~ ~ 1/6~ ~ 1/6~ ~ 1/6~ ~ 1/6]^ T.
$$
Let $\widehat{\mathbf{p} }$​ denote the MLE for the true parameter $\mathbf{p}^*$. You use the following test statistic to test the hypotheses:
$$
T_ n = n \sum _{j = 1}^6 \frac{(\widehat{p}_ j - \frac{1}{6} )^2}{\frac{1}{6} }.
$$

1. What is the distribution does $T_n$​​​ converge to?

2. Use the test of the form
   $$
   \psi _ n = \mathbf{1}\left( T_ n > C \right).
   $$
   What value of $C$​​ should be chosen so that $\psi$ is a test of asymptotic level $5\%$?

3. Suppose you observe that data set
   $$
   5,6,1,6,4,1,2,4,6,6,1,6,6,3,5.
   $$
   Do you reject or fail to reject the null hypothesis that the die is fair?

**Solution:**

1. If the sample space consists of $K$ Elements, and $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathbf{P}_{\mathbf{p}^0}$, then
   $$
   T_ n = n \sum _{j = 1}^ K \frac{(\widehat{p }_ j - p_ j^0 )^2}{p_ j^0 } \xrightarrow [n \to \infty ]{(d)} \chi _{K -1}^2.
   $$
   Our sample space consists of $6$ elements (recall $E = \{ 1, 2,3,4,5,6\}$), so we conclude that the limiting distribution is $\chi_5^2$.

2. Consulting a table, we see that if $X \sim \chi _5^2$, then if $C=11.070$.
   $$
   P(X > C) = 0.05.
   $$
   Therefore,
   $$
   \lim _{n \to \infty } P_{H_0}[T_ n > 11.070] = 0.05.
   $$
   By definition, $\psi_n$ is a test with asymptotic level $5\%$.

3. The MLE is 
   $$
   \widehat{\mathbf{p} } = \frac{1}{15} [3~ ~  1~ ~  1~ ~  2~ ~  2~ ~  6]^ T.
   $$
   We compute that for this data set,
   $$
   T_{15} = 15\left( \frac{( \frac{3}{15} - \frac{1}{6} )^2}{1/6} + \frac{( \frac{1}{15} - \frac{1}{6} )^2}{1/6} + \frac{( \frac{1}{15} - \frac{1}{6} )^2}{1/6} + \frac{( \frac{2}{15} - \frac{1}{6} )^2}{1/6} + \frac{( \frac{2}{15} - \frac{1}{6} )^2}{1/6} + \frac{( \frac{6}{15} - \frac{1}{6} )^2}{1/6} \right)
    = 7
   $$
   Since $C = 11.070$​​, by the previous problem, $\psi$​ fails to reject on the given data set.

**Remark:** This is a rather surprising result given that the number $6$​​​​​ Has appeared an overwhelming $6$​​​​​ times out of $15$​​​ trials and the number $2$​​ and $3$​​ have each appeared only once. Without performing this test, one would have probably concluded that the die is likely not a fair die (would have rejected the null hypothesis).

## 7. The Chi-Squared Test for Two Modalities

Consider the $\chi^2$ test statistic for $K=2$:
$$
T_ n = n \sum _{j = 1}^2 \frac{(\widehat{p}_ j - p_ j^0 )^2}{p_ j^0 }.
$$
We can use this statistic in a chi-squared test with $1$ degree of freedom to determine, with an asymptotic level $\alpha$, whether the observed iid samples follow the distribution $\text {Ber}(p_2^0)$ under the null hypothesis $H_0$, with the sample space being the two values $a_1 = 0$ and $a_2=1$. The chi-squared test with asymptotic level $\alpha$ is
$$
\mathbf{1}\left\{  T_ n > q_\alpha \right\} ,
$$
where $q_\alpha$ is the $(1-\alpha)$-quantile of the chi-squared distribution with $1$ degree of freedom.

**This test is identical (asymptotically) to Wald's test of the Bernoulli statistical model with parameter $p$, null hypothesis $H_0: p = p_2^0$ and alternative hypothesis $H_1 : p \neq p_2^0$, where $p_2^0$, as defined above, is the probability of $a_2=1$​ under the null hypothesis.**

Wald's test in the above statement is:
$$
\displaystyle  \mathbf{1}\left\{  n \frac{\left(\widehat{p}_2 - p_2^0 \right)^2}{p_2^0\left(1 - p_2^0\right)} > q_\alpha \right\} ,
$$
where $q_\alpha$ is the $(1-\alpha)$-quantile of the chi-squared distribution with $1$ degree of freedom.

The chi-squared test statistic can be re-written as
$$
\begin{aligned}
T_n &=  n \sum _{j = 1}^2 \frac{(\widehat{p}_ j - p_ j^0 )^2}{p_ j^0 }\\
&= n \frac{(\widehat{p}_1 - p_1^0 )^2}{p_1^0 } + n\frac{(\widehat{p}_2 - p_2^0 )^2}{p_2^0 }\\
&= n \frac{((1-\widehat{p}_2) - (1-p_2^0) )^2}{1-p_2^0 } + n\frac{(\widehat{p}_2 - p_2^0 )^2}{p_2^0 }\\
&= n \frac{\left(\widehat{p}_2 - p_2^0 \right)^2 (p_2^0 + 1 - p_2^0)}{p_2^0\left(1 - p_2^0\right)}\\
&= n \frac{\left(\widehat{p}_2 - p_2^0 \right)^2}{p_2^0\left(1 - p_2^0\right)},
\end{aligned}
$$
which is the same as the test statistic for Wald's test.

## 8. Chi-Squared Test for a Family of Discrete Distributions

In the following problems, we will apply the $\chi^2$ goodness of fit test to determine whether or not a sample has a binomial distribution. 

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } X\sim \mathbf{P}$ denote iid discrete random variables supported on $\{0,..., K\}$. We will decide between the following null and the alternative hypotheses:
$$
H_0:  \mathbf{P}\in \{  \text {Bin}(K, \theta ) \} _{\theta \in (0,1)}\\
H_1:  \mathbf{P}\notin \{  \text {Bin}(K, \theta ) \} _{\theta \in (0,1)},
$$
where the null hypothesis can be rephrased as
$$
H_0: \quad \text {there exists } \, \theta \in (0,1)\, \text {such that for all }\, j = 0, \ldots , K, \, \text {we have } P(X = j) = \binom {K}{j} \theta ^{j} (1 - \theta )^{K -j}.
$$
Let $(\{ 0, \ldots , K\} , \{  \text {Bin}(K, \theta ) \} _{\theta \in (0,1)})$ denote a binomial statistical model. Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Bin}(K, \theta ^*)$ for some unknown parameter $\theta ^* \in (0,1)$.

The PMF of $\text {Bin}(K, \theta )$ is
$$
j \mapsto \binom {K}{j} \theta ^ j (1 - \theta )^{K - j}
$$
for $j \in \{0, ..., K\}$​.

Therefore, the likelihood is given by
$$
\begin{aligned}
L_ n(X_1, \ldots , X_ n, \theta ) &= \prod _{i = 1}^ n \left(\binom {K}{X_ i} \theta ^{X_ i} (1 - \theta )^{K - X_ i} \right)\\
&=  \left( \prod _{i = 1}^ n \binom {K}{X_ i} \right) \theta ^{\sum _{i = 1}^ n X_ i} (1 - \theta )^{nK - \sum _{i = 1}^ n X_ i }.
\end{aligned}
$$
Taking the logarithm, we have
$$
\begin{aligned}
\log L_ n(X_1, \ldots , X_ n, \theta ) &= \log \left( \prod _{i = 1}^ n \binom {K}{X_ i} \right) + \left( \sum _{i = 1}^ n X_ i \right) \log \theta + \left( nK - \sum _{i = 1}^ n X_ i \right) \log (1 - \theta )\\
&= C + \left( \sum _{i = 1}^ n X_ i \right) \log \theta + \left( nK - \sum _{i = 1}^ n X_ i \right) \log (1 - \theta ).
\end{aligned}
$$
where $C$ does not depend on $\theta$.

To compute the MLE, we maximize the above with respect to the parameter $\theta$. We set the derivative to be $0$.
$$
0 = \frac{\sum _{i = 1}^ n X_ i}{\theta } - \frac{nK - \sum _{i = 1}^ n X_ i}{1 - \theta }.
$$
The above holds when
$$
p = \frac{1}{nK} \sum _{i = 1}^ n X_ i.
$$
Therefore, the right-hand side is the MLE for this statistical model.

#### $\chi^2$-Test for a Family of Distribution

Now, we return to the following more general statistical set-up.

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathbf{P}$ denote iid discrete random variables supported on ${0,..., K}$. We will decide between the following null and alternative hypotheses.
$$
H_0:  \mathbf{P}\in \{  \text {Bin}(K, \theta ) \} _{\theta \in (0,1)}\\
H_1:  \mathbf{P}\notin \{  \text {Bin}(K, \theta ) \} _{\theta \in (0,1)},
$$
Let $f_\theta$​ denote the PMF of the distribution $\text {Bin}(K, \theta )$​, and let $\hat{\theta}$​ denote the MLE of the parameter $\theta$ from the previous problem.

Further, let $N_j$​ denote the number of times that $j (j \in \{0,1,..., K\})$​ appears in the data set $\, X_1, \ldots , X_ n\,$​ (So that $\sum _{j=0}^{K} N_ j =n$​) The $\chi^2$ test statistic for this hypothesis test is defined to be 
$$
T_ n := n\sum _{j =0}^ K \frac{\left( \frac{N_ j}{n} - {{f_{\widehat{\theta }}(j)}}  \right)^2}{{{f_{\widehat{\theta }}(j)}}  }.
$$
This statistic is different from before. Previously, under the null hypothesis, $\mathbf{P}(X=j)=p_ j\,$ for some fixed $p_j$. Here, instead, we use $f_{\widehat{\theta }}(j)\,$ to estimate $\mathbf{P}(X=j)\,$. This statistic still converges in distribution to a $\chi^2$​ distribution, but the number of degrees of freedom is smaller.

#### Degrees of Freedom for Test for a Family of Distribution

More generally, to test if a distribution $\mathbf{P}$​ is described by some member of a family of discrete distributions $\{  \mathbf{P}_{\theta } \} _{\theta \in \Theta \subset \mathbb {R}^ d}$​ where $\Theta \subset \mathbb {R}^ d$​ is $d$​-dimensional, with support $\{0,1,2,...,K\}$​ and PMF $f_\theta$​, i.e. to test the hypotheses:
$$
H_0: \mathbf{P}\in \{  \mathbf{P}_\theta \} _{\theta \in \Theta }\\
H_1: \mathbf{P}\notin \{  \mathbf{P}_\theta \} _{\theta \in \Theta }\\
$$
then if indeed $\mathbf{P}\in \{  \mathbf{P}_{\theta } \} _{\theta \in \Theta \subset \mathbb {R}^ d}$​ (i.e. the null hypothesis $H_0$ holds), and if in addition some technical assumption hold, then we have that
$$
T_ n:= n\sum _{j =0}^ K \frac{\left( \frac{N_ j}{n} - f_{\widehat{\theta }}(j) \right)^2}{ f_{\widehat{\theta }}(j) } \xrightarrow [n \to \infty ]{(d)} \chi ^2_{(K+1) - d - 1}.
$$
Note that $K+1$​​​ is the support size of $\mathbf{P}_\theta$​​​ (for all $\theta$​​).

In our example testing for a binomial distribution, the parameter $\theta$ is one-dimensional, i.e. $d=1$. Therefore, under the null hypothesis $H_0$, it holds that 
$$
T_ n \xrightarrow [n \to \infty ]{(d)} \chi ^2_{(K+1) - 1 - 1} = \chi ^2_{K-1}.
$$

> #### Exercise 88
>
> Consider the same statistical set-up as above. In particular, we have the test statistic
> $$
> T_ n := n \sum _{j =0}^ K \frac{\left( \frac{N_ j}{n} - f_{\widehat{\theta }}(j) \right)^2}{ f_{\widehat{\theta }}(j) }.
> $$
> where $\hat{\theta}$ is the MLE for the binomial statistical model $(\{ 0,1, \ldots , K\} , \{  \text {Bin}(K, \theta ) \} _{\theta \in (0,1)})$.
>
> We define our test to be
> $$
> \psi _ n = \mathbf{1}( T_ n > \tau ),
> $$
> where $\tau$​ is a threshold that you will specify. For the remainder of this page, we will assume that $K=3$​ (The sample space is $\{0,1,2,3\}$​).
>
> 1. What value of $\tau$​​ should be chosen so that $\psi_n$​​ is a test of asymptotic level $5\%$​​​?
>
> 2. Suppose we observe a data set consisting of $1000$​​ Observations as described in the following format: $N_i$​​, number of observations of $i$​):
>    $$
>    \begin{aligned}
>    i \quad & \quad N_i\\
>    0 \quad & \quad 339\\
>    1 \quad & \quad 455\\
>    2 \quad & \quad 180\\
>    3 \quad & \quad 26\\
>    \end{aligned}
>    $$
>    What is the value of the test statistic $T_n$​ for this data set? What is the p-value of this data set with respect to the test $\psi_{1000}$​? If $\psi_n$ is designed to have level $5\%$, would you reject or fail to reject on the given data set?
>
> **Solution:**
>
> 1. Since $K=3$​ and $d=1$​, we know that the limiting distribution of $T_n$​ is $\chi^2_2$​. Thus, the asymptotic level is the value $\tau$​ such that 
>    $$
>    \lim _{n \to \infty } P( T_ n > \tau ) = P( Z > \tau ) = 0.05
>    $$
>    Where $Z \sim \chi_2^2$. Hence, $\tau$ should be chosen to be $5.991$.
>
> 2. Observe that the MLE is given by
>    $$
>    \widehat{\theta } = \frac{1}{3 \cdot 1000} (455 + 2 \cdot 180 + 3 \cdot 26 ) \approx 0.29767.
>    $$
>    Thus for this data set,
>    $$
>    \begin{aligned}
>    T_n &= 1000 \cdot \bigg( \frac{\left(\frac{339}{1000} - \binom {3}{0} (0.2977^0) (0.7023)^{3 - 0} \right)^2}{\binom {3}{0} (0.2977^0) (0.7023)^{3 - 0} } + \frac{\left(\frac{455}{1000} - \binom {3}{1} (0.2977^1) (0.7023)^{3 - 1} \right)^2}{\binom {3}{1} (0.2977^1) (0.7023)^{3 - 1} } + \\
>    &\frac{\left(\frac{180}{1000} - \binom {3}{2} (0.2977^2) (0.7023)^{3 - 2} \right)^2}{\binom {3}{2} (0.2977^2) (0.7023)^{3 - 2} } + \frac{\left(\frac{26}{1000} - \binom {3}{3} (0.2977^3) (0.7023)^{3 - 3} \right)^2}{\binom {3}{3} (0.2977^3) (0.7023)^{3 - 3} } \bigg)\\
>    & \approx 0.8829
>    \end{aligned}
>    $$
>    The asymptotic p-value for this data set is given by
>    $$
>    \lim _{n \to \infty } P(T_ n > 0.8829) = P(Z > 0.8829).
>    $$
>    where $Z \sim \chi _2^2$. Consulting the suggested link, we see that $P(Z > 0.8829) \approx 0.6431$.
>
>    According to the golden rule of p-values, since $0.6431 > 0.05$​, we should **fail to reject** the null hypothesis that $X_1, ..., X_{1000}$​ are distributed as $\text {Bin}(3, \theta )$​ for some value of the parameter $\theta$​.



# Recitation 13. Wald's Test versus Likelihood Ratio Test

Suppose we have a dataset $X_1, ..., X_n$ that is i.i.d. $f_\theta (x) = \theta x^{-\theta - 1} \mathbf{1}_{x > 1}$.

1. Calculate the MLE and Fisher Information for $\theta$.
2. Give the **likelihood ratio test** with level $\alpha = 0.05$ to test if $\theta= 2$.
3. Give **Wald's test** with level $\alpha = 0.05$ to test if $\theta=2$.
4. Assume $n=100$ and $\hat{\theta} = 2.45$. Compute the asymptotic $p$-values of both tests.

> **Review:**
>
> Recall that the **Wald's test** is based on the **asymptotic normal approximation** to the MLE. We have
> $$
> \sqrt{n} \left(\hat{\theta}_{MLE} - \theta\right) \xrightarrow[]{(d)} \mathcal{N}(0, I^{-1}(\theta))\\
> \sqrt{n} \sqrt{I(\theta)} \left(\hat{\theta}_{MLE} - \theta\right) \xrightarrow[]{(d)} \mathcal{N}(0, 1)\\
> {n} {I(\theta)} \left(\hat{\theta}_{MLE} - \theta\right)^2 \xrightarrow[]{(d)} \chi_1^2\\
> {n} {I(\hat{\theta}_{MLE})} \left(\hat{\theta}_{MLE} - \theta\right)^2 \xrightarrow[]{(d)} \chi_1^2
> $$
> The **likelihood ratio test** looks at how much more likely is the alternative hypothesis when compared to the null hypothesis. If
> $$
> \frac{\sup_{\Theta \in \Theta_1} L(X_1, \ldots , X_ n; \Theta )}{L(X_1, \ldots , X_ n; \Theta _0 )} > C
> $$
> We reject the null hypothesis in favor of the alternatives.
>
> Note that
> $$
> \begin{aligned}
> &\frac{\sup_{\Theta \in \Theta_1} L(X_1, \ldots , X_ n; \Theta )}{L(X_1, \ldots , X_ n; \Theta _0 )} > C\\
> \implies & \frac{ L(X_1, \ldots , X_ n; \hat{\Theta}_{MLE} )}{L(X_1, \ldots , X_ n; \Theta _0 )} > C\\
> \implies & 2(\ell(X_1, ..., X_n; \hat{\Theta}_{MLE}) - \ell (X_1, ..., X_n ; \Theta_0)) >  \log C \quad \text{(take the log and multiply by 2)}
> \end{aligned}
> $$
> **Wilk's theorem** states that
> $$
> 2(\ell(X_1, ..., X_n; \hat{\Theta}_{MLE}) - \ell (X_1, ..., X_n ; \Theta_0)) \xrightarrow[]{(d)} \chi_1^2
> $$
> **Solution:**
>
> 1. Find the **MLE**:
>
>    The likelihood is 
>    $$
>    \begin{aligned}
>    L(x_1, ..., x_n; \Theta) &= \prod_{i=1}^n f(x_i ; \Theta)\\
>    &= \prod^n_{i=1} \Theta x_i^{-\Theta-1}\\
>    &= \Theta^n (\prod^n_{i=1}x_i)^{-\Theta-1}\\
>    \end{aligned}
>    $$
>    The log likelihood is
>    $$
>    \begin{aligned}
>    \ell(X_1, ..., X_n ; \Theta) &= \log \left(\Theta^n \left(\prod^n_{i=1}X_i\right)^{-\Theta-1}\right)\\
>    &= n \log \Theta + (-\Theta-1) \sum^n_{i=1} \log X_1\\
>    \end{aligned}
>    $$
>    Take the first derivative and set it to zero
>    $$
>    {d \ell \over d\Theta } = {n \over \Theta} - \sum^n_{i=1} \log X_i = 0\\ 
>    \implies \hat{\Theta} = {n \over \sum^n_{i=1} \log X_i}
>    $$
>    Take the second derivative
>    $$
>    {d ^2 \ell\over d \Theta^2} = - {n \over \Theta^2} < 0
>    $$
>    This tells us that the log likelihood is concave and $\hat{\Theta}$ is a maximizer.
>    $$
>    \hat{\Theta}_{MLE} = {n \over \sum^n_{i=1} \log X_i}
>    $$
>    Find the **Fisher information**:
>    $$
>    I(\Theta) = \mathbb{E}\left[- {d ^2 \ell (X; \Theta)\over d \Theta^2}\right] = \mathbb{E}\left[- \left(- {1\over \Theta^2}\right)\right] = {1\over \Theta^2}
>    $$
>    
> 2. **Likelihood ratio test:**
>
>    Under the null hypothesis,
>    $$
>    2(\ell(X_1, ..., X_n; \hat{\Theta}_{MLE}) - \ell (X_1, ..., X_n ; \Theta_0)) \xrightarrow[]{(d)} \chi_1^2
>    $$
>    The test statistic $T_n$ is
>    $$
>    T_n = 2(\ell(X_1, ..., X_n; \hat{\Theta}_{MLE}) - \ell (X_1, ..., X_n ; \Theta_0))
>    $$
>    Our test is 
>    $$
>    \psi = \mathbb{1}_{(T_n \geq \chi_{1, 0.05}^2)}=\mathbb{1}_{(T_n \geq 3.84)}
>    $$
>    Now we do the test
>    $$
>    H_0: \Theta = 2 \ \ \text{vs.} \ \ H_1: \Theta \neq 2\\
>    $$
>    with the given data:  $\hat{\Theta}_{MLE} = 2.45,\ \  n = 100$.​
>
>    $$
>    \begin{aligned}
>    T_n &= 2(\ell(X_1, ..., X_n; \hat{\Theta}_{MLE}) - \ell (X_1, ..., X_n ; \Theta_0))\\
>    &= 2 \left(n \log \hat{\Theta}_{MLE} + \left(- \hat{\Theta}_{MLE} - 1\right) {n\over \hat{\Theta}_{MLE}} -\left(n \log \Theta_0 + \left(- \Theta_0 - 1\right) {n\over \hat{\Theta}_{MLE}}\right)\right)\\
>    &= 3.85 > 3.45
>    \end{aligned}
>    $$
>    So we reject the null hypothesis.
>
> 3. **Wald's test:** 
>
>    Under the null hypothesis,
>    $$
>    n I(\Theta) (\hat{\Theta}_{MLE} - \Theta_0)^2 \xrightarrow[]{(d)} \chi_1^2\\
>    \implies
>    n {1\over \hat{\Theta}_{MLE}^2} (\hat{\Theta}_{MLE} - \Theta_0)^2 \xrightarrow[]{(d)} \chi_1^2
>    $$
>    Where the test statistic $T_n'$​ is
>    $$
>    T_n'  =n {1\over \hat{\Theta}_{MLE}^2} (\hat{\Theta}_{MLE} - \Theta_0)^2
>    $$
>    Our test is to reject the null when $T_n$ is greater than or equal to some quantile of a $\chi^2_1$.
>    $$
>    \psi' = \mathbb{1}_{(T_n' \geq \chi_{1,0.05}^2)}= \mathbb{1}_{(T_n' \geq 3.84)}
>    $$
>    where $\chi^2_{1, 0.05}$ is $1-0.05$ quantile of a $\chi_1$.
>
>    If we want a probability of $0.05$​​​​ of rejecting the null hypothesis under the null hypothesis (null is true), we mean we have a probability of $0.05$​​ of making a type I error.
>
>    Now we do the test
>    $$
>    H_0: \Theta = 2 \ \ \text{vs.} \ \ H_1: \Theta \neq 2\\
>    $$
>    with the given data:  $\hat{\Theta}_{MLE} = 2.45,\ \  n = 100$.​
>
>    $$
>    T_n'  =n {1\over \hat{\Theta}_{MLE}^2} (\hat{\Theta}_{MLE} - \Theta_0)^2 = 100  {1\over 2.45^2} (2.45-2)^2 = 3.37 < 3.84
>    $$
>    So we fail to reject the null hypothesis.



# Recitation 4. Distances between probability distribution

**Review:**

Let $P,Q,R$ be probability distributions over $E$, with PDF or PMF $p,q,r$.

Let event $A$ be $A \subset E,$ then $\mathbf{P}(A) = \int_Ap(x)dx$ or $\mathbf{P}(A) = \sum\limits_{x \in A}p(x)dx$, $(\mathbf{P}_\theta)_{\theta \in \Theta}$.

The distance is $d(\mathbf{P}_\theta, \mathbf{P}_{\theta'})$.

* Total Variation Distance (TV)
  $$
  \begin{aligned}
  \text{TV}(P,Q) &= \sup_{A \subset E} |\mathbf{P}(A) - \mathbf{Q}(A)|\\
  &= {1\over 2} \int_E |p(x) - q(x)| dx\\
  [&={1\over 2} \sum_{x \in E} |p(x) - q(x)|]
  \end{aligned}
  $$

* Kullback-Leibler Divergence (KL)
  $$
  \begin{aligned}
  \text{KL}(P,Q) &= \text{KL} (P || Q)\\
  &= \begin{cases} \int_E p(x) \log {p(x) \over q(x) } dx \qquad\text{or} \qquad  [\sum_{x \in E} p(x) \log {p(x) \over q(x)}] \\ +\infty \qquad \exist x: q(x) = 0, p(x) \neq 0 \end{cases}
  \end{aligned}
  $$

Properties:

|                                                              | TV       | KL       |
| ------------------------------------------------------------ | -------- | -------- |
| Non-negative: $d(P,Q)\geq 0$                                 | $\surd$  | $\surd$  |
| Definite: $d(P,Q) = 0 \implies P=Q$                          | $\surd$  | $\surd$  |
| Symmetric: $d(P,Q) = d(Q,P)$                                 | $\surd$  | $\times$ |
| Triangle inequality: $d(P,Q) \leq d(P,R) + d(R,Q)$           | $\surd$  | $\times$ |
| Amenable to estimation (replace $\mathbb{E}(\cdot)$ by ${1\over 2} \sum\limits^n_{i=1} [\cdot]$) | $\times$ | $\surd$  |

**Problem statement:**

1. Show that the Poisson distribution with parameter $1/n$ converges in total variation distance to the Dirac distribution at zero (i.e., the distribution of the random variable that is always equal to zero).
2. For $p,q \in (0,1)$ and $n \geq 1$, compute the Kullback-Leibler divergence between $\mathsf{B}(n,p)$ and $\mathsf{B}(n,q)$.
3. Compute the Kullback-Leibler divergence between two Gaussian distributions that have the same variance.

>  **Solution:**
>
> 1. Given $P_n = \mathsf{Poiss}({1\over n}), Q = \delta_o$, show $\text{TV}(P_n,Q) \xrightarrow[n\rightarrow \infty]{}0$.
>
>    Recall the PMF of $\mathsf{Poiss}(\lambda)$ is 
>    $$
>    p_\lambda(k) = {\lambda^k \over k!} e^{-\lambda}, k = 0,1,2,...;
>    $$
>    The PMF of Dirac distribution is 
>    $$
>     q(k) =\begin{cases} 1, & k=0\\ 0, &\text{o.w.} \end{cases}
>    $$
>    The TV is
>    $$
>    \begin{aligned}
>    \text{TV}(P_n, Q) &= {1\over 2} \sum^\infty_{k=0} |p_{1/2}(k) - q(k)| \\
>    &= {1\over 2} \sum^\infty_{k=0} |{ ({1\over n})^k \over k!} e^{-1/n} - \delta_o(k)|\\
>    &={1\over 2} |{({1\over n})^0 \over 0!} e^{-1/n}-1| - {1\over 2} \sum^\infty_{k\geq 1} |{ ({1\over n})^k \over k!} e^{-1/n} - \delta_o(k)|\\
>    &= {1\over 2} | e^{-1/n}-1| - {1\over 2} \sum^\infty_{k\geq 1} |{ ({1\over n})^k \over k!} e^{-1/n} - \delta_o(k)|\\
>    &= {1\over 2} | e^{-1/n}-1| - {1\over 2} \mathbf{P}_n(\{1,2,...\}) \\
>    &= {1\over 2} | e^{-1/n}-1| - {1\over 2} (1 -\mathbf{P}_n({\{0}\})) \\
>    &= {1\over 2} | e^{-1/n}-1| - {1\over 2} (1 - {(1/n)^0 \over 0!} e^{-1/n}) \\
>    & \xrightarrow[n \rightarrow \infty]{} {1\over 2}|1-1|  - {1\over 2} (1-1)\\
>    & \xrightarrow[n \rightarrow \infty]{} 0
>    \end{aligned}
>    $$
>
> 2. Given $\mathbf{P} = \mathsf{Bin}(n,p)$ and $\mathbf{Q} = \mathsf{Bin}(n,q), \quad p,q \in (0,1)$.
>
>    Recall the PDF of Binomial distribution is
>    $$
>    f(p,k) = {n \choose k} l^k(1-p)^{n-k}
>    $$
>    The KL is
>    $$
>    \begin{aligned}
>    \text{KL}(P||Q) &= \sum^n_{k=0} f(p,k) \cdot \log { f(p,k)\over f(q,k) }\\ 
>    &= \sum^n_{k=0} f(p,k) \cdot \log \left[ {{n\choose k}p^k (1-p)^{n-k} \over {n\choose k}q^k (1-q)^{n-k} } \right]\\
>    &=\sum^n_{k=0} f(p,k) \cdot \left[ \log(({p\over q})^k) + \log(({1-p \over 1-q })^{n-k})\right]\\
>    &= \sum^n_{k=0} f(p,k)\cdot \left[ k\log({p\over q}) + (n-k)\log({1-p \over 1-q })\right]\\
>    &\left(\text{Since } \sum^n_{k=0}k f(p,k) = \mathbb{E}[k] = np, \quad \text{where } k \sim \mathsf{Ber}(n,p)\right)\\
>    &= \log({p\over q}) \cdot np + \log ({1-p \over1-q }) \cdot (n - np)\\
>    &= 
>    \end{aligned}
>    $$
>    Note that when $q \rightarrow 0$, KL$(P,Q)\rightarrow \infty$.
>
> 3. Given $\mathbf{P} = \mathcal{N}(a,1), Q = \mathcal{N}(b,1), a,b \in \R; $
>
>    Recall the PDF of normal distribution is
>    $$
>    f_{a,\sigma^2} (x) = {1\over \sqrt{2\pi \sigma^2}} e^{-{1\over 2\sigma^2}(x-a)^2}
>    $$
>    The KL is
>    $$
>    \begin{aligned}
>    \text{KL} (P||Q) &= \int_\R f_{a,1}(x) \log \left[ {f_{a,1}(x) \over f_{b,1}(x) } \right]dx\\
>    &= \int_\R f_{a,1}(x) \log \left[ {1\over \sqrt{2\pi}} e^{-{1\over 2}(x-a)^2} \over {1\over \sqrt{2\pi }} e^{-{1\over 2}(x-b)^2}  \right]dx\\
>    &= \int_\R f_{a,1}(x) \log \left[  e^{-{1\over 2}(x-a)^2} \over  e^{-{1\over 2}(x-b)^2}  \right]dx\\
>    &= \int_\R f_{a,1}(x) \log e^{-{1\over 2} (x-a)^2 + {1\over 2} (x-b)^2} dx\\
>    &= \int_\R f_{a,1}(x) \left( -{1\over 2} (x-a)^2 + {1\over 2} (x-b)^2  \right)dx\\
>    &=\int_\R f_{a,1}(x) \left( x(a-b) -{1\over 2} a^2 + {1\over 2}^2 \right)dx\\
>    &= (a-b) \int_\R x f_{a,1}(x)dx + \left(-{1\over 2} a^2 + {1\over 2}b^2\right) \int_\R f_{a,1}(x)dx\\
>    & \left(\text{Since } \mathbb{E}[X] = \int_\R x f_{a,1}(x)dx = a, \text{ and }  \int_\R f_{a,1}(x)dx = 1\right)\\
>    &=  (a-b) a  + \left(-{1\over 2} a^2 + {1\over 2}b^2\right) 1\\
>    &= {1\over 2} (a^2 - 2ab + b^2)\\
>    &= {1\over 2}(a-b)^2
>    \end{aligned}
>    $$
>    

# Lecture 11. Fisher Information, Asymptotic Normality of MLE, and the Method of Moments

There are 7 topics and 7 exercises.

## 1. Fisher Information

**Definition 1:**

Define the log-likelihood for one observation as:
$$
\ell(\theta) = \log L_1(X,\theta), \quad \theta \in \Theta \subset \R^d
$$
Assume that $\ell$ is a.s. twice differentiable. Under some regularity conditions, the Fisher information of the statistical model is defined as
$$
I(\theta) =\mathsf{Cov}(\nabla \ell(\theta)) = \mathbb{E}[\nabla \ell(\theta) \nabla\ell(\theta)^T] - \mathbb{E}[\nabla \ell(\theta)]\mathbb{E}[\nabla \ell(\theta)]^T = - \mathbb{E}[\mathbf{H}\ell(\theta)]
$$
The log-likelihood is a concave function with a negative Hessian, so $ \mathbb{E}[\mathbf{H}\ell(\theta)]$ is negative and the fisher information is positive.

If $\Theta \subset \R$, we get 
$$
I(\theta) = \mathsf{var}[\ell'(\theta)] = -\mathbb{E}[l''(\theta)]
$$
**Definition 2:**

Let $(\mathbb {R}, \{ \mathbf{P}_\theta \} _{\theta \in \mathbb {R}})$ denote a continuous statistical model. Let $f_\theta (x)$ denote the pdf (probability density function) of the continuous distribution $\mathbf{P}_\theta$. Assume that $f_\theta (x)$ is twice-differentiable as a function of the parameter $\theta$.

Recall that
$$
\int _{-\infty }^\infty f_\theta (x) \,  dx = 1
$$
for all $\theta \in \R$.

Take the derivative,
$$
\frac{\partial }{\partial \theta }  \int _{-\infty }^\infty f_\theta (x) \,  dx  = 0\\
$$
Therefore,
$$
\int _{-\infty }^\infty \frac{\partial }{\partial \theta } f_\theta (x) \,  dx  = 0\\
\int _{-\infty }^\infty \frac{\partial ^2}{\partial \theta ^2} f_\theta (x) \,  dx =0
$$
The first derivative of the likelihood is
$$
\ell '(\theta ) = \frac{\partial }{\partial \theta } \ln f_\theta (X) = \frac{\frac{\partial }{\partial \theta } f_\theta (X)}{f_\theta (X)}.
$$
The expectation of it is
$$
\mathbb{E}[\ell'(\theta)]=\mathbb {E}\left[\frac{\frac{\partial }{\partial \theta } f_\theta (X)}{f_\theta (X)}\right] = \int _{-\infty }^\infty \left( \frac{\frac{\partial }{\partial \theta } f_\theta (x)}{f_\theta (x)} \right) f_\theta (x) \,  dx = \int _{-\infty }^\infty \frac{\partial }{\partial \theta } f_\theta (x) \,  dx = 0,
$$
The variance of it is by definition the fisher information
$$
\textsf{Var}(\ell '(\theta )) = \mathcal{I}(\theta)
$$
By definition of variance,
$$
\textsf{Var}(\ell '(\theta )) = \mathbb {E}[\ell '(\theta )^2] - \mathbb {E}[\ell '(\theta )]^2 = \mathbb {E}[\ell '(\theta )^2]  - 0 = \mathbb {E}[\ell '(\theta )^2] 
$$
Hence, the variance can be written as
$$
\begin{aligned}
\textsf{Var}\left(\ell '(\theta )\right) &=\mathbb {E}\left[(\ell '(\theta ))^2\right]\\
&= \mathbb {E}\left[\left( \frac{\frac{\partial }{\partial \theta }f_\theta (X)}{f_\theta (X)} \right)^2\right]\\
&= \int _{-\infty }^\infty \frac{(\frac{\partial }{\partial \theta } f_\theta (x))^2}{f_\theta (x)} \,  dx.
\end{aligned}
$$
Alternatively, the fisher information can be computed from 
$$
\mathcal{I}(\theta) = -\mathbb{E}[l''(\theta)]
$$
By computing $-\mathbb{E}[l''(\theta)]$ we can prove
$$
\mathsf{var}[\ell'(\theta)] = -\mathbb{E}[l''(\theta)]
$$
The second derivative of the likelihood is
$$
\ell''(\theta) = {{\partial^2 \over \partial \theta^2 }f_\theta(x) f_\theta(x) - ({\partial \over \partial \theta }f_\theta(x) )^2 \over (f_\theta(x))^2 }
$$
The negative expectation is
$$
\begin{aligned}
-\mathbb{E}[l''(\theta)] &= -\mathbb{E}\left[{{\partial^2 \over \partial \theta^2 }f_\theta(x) f_\theta(x) - ({\partial \over \partial \theta }f_\theta(x) )^2 \over (f_\theta(x))^2 }\right]\\
&= -\int {{\partial^2 \over \partial \theta^2 }f_\theta(x) f_\theta(x) - ({\partial \over \partial \theta }f_\theta(x) )^2 \over (f_\theta(x))^2 } f_\theta(x)\ dx\\
&= -\int {{\partial^2 \over \partial \theta^2 }f_\theta(x) f_\theta(x) - ({\partial \over \partial \theta }f_\theta(x) )^2 \over f_\theta(x) } \ dx\\
&= -\int  {\partial^2 \over \partial \theta^2 }f_\theta(x)\ dx + \mathsf{Var}(\ell'(\theta))\\
&= - 0 + \mathsf{Var}(\ell'(\theta))\\
&=\mathsf{Var}(\ell'(\theta))
\end{aligned}
$$
**Remark:** A convenient way to compute the Fisher information is to use 
$$
\mathcal{I}(\theta ) = \int _{-\infty }^\infty \frac{(\frac{\partial }{\partial \theta } f_\theta (x))^2}{f_\theta (x)} \,  dx.
$$

## 2. Examples of Fisher Information Computation

**Fisher Information of the Bernoulli Random Variable**

Let $X \sim \mathsf{Ber}(p)$, the PDF is 
$$
f_p(x) = p^x(1-p)^{1-x}
$$
The likelihood is
$$
\ell(p) = x\log p  + (1-x) \log(1-p)
$$
The first derivative of the likelihood is
$$
\ell'(\theta) = {x \over p} - {1-x\over 1-p}
$$
The second derivative of the likelihood is
$$
\ell''(\theta) = -{x \over p^2} - {1-x \over (1-p)^2}
$$
The fisher information can be computed from 
$$
\begin{aligned}
\mathsf{Var}(\ell'(\theta)) &= \mathsf{Var}(x({1\over p}+ {1\over 1-p}) - {1\over 1-p})\\
&= \mathsf{Var} (x({1\over p}+ {1\over 1-p}))\\ 
&= {p(1-p) \over p^2 (1-p)^2}\\ 
&= {1 \over {p(1-p)}}
\end{aligned}
$$
Alternatively,
$$
\begin{aligned}
\mathbb{E}[\ell''(\theta)] &= - {\mathbb{E}[X]\over p^2} - {1-\mathbb{E}[X] \over (1-p)^2}\\
&= -{1\over p} - {1\over 1-p}\\
&= -{1 \over p(1-p)}
\end{aligned}
$$
**Remark:** Computing by $\mathbb{E}[\ell''(\theta)]$ is actually the fast method in most cases.

> #### Exercise 60
>
> Let $\mathbf{X}$ be a Gaussian random vector with **independent** components $X^{(i)} \sim \mathcal{N}(\alpha + \beta t_ i,1)$ for $i = 1,...,d$, where $t_i$ are known constants and $\alpha$ and $\beta$ are unknown parameters.
>
> Compute the Fisher information matrix $\mathcal{I}(\theta )$ using the formula $\mathcal{I}(\theta ) = -\mathbb E\left[\mathbf{H}\ell (\theta )\right]$.
>
> **Solution:**
>
> Let $\theta = [\alpha ~ ~ \beta ]^ T$ denote the parameters of the statistical model. The **joint distribution** of the random vector $\mathbf {X}$ is equal to the product of the marginal distribution of its components $(2\pi ^{-1/2}e^{-(x^{(i)} - \alpha - \beta t_ i)/2}$, by independence. Therefore, the random vector $\mathbf X$ has the density
> $$
> f_\theta (\mathbf x) = (2\pi )^{-\frac{d}{2}} e^{-\frac{1}{2}\sum _{i=1}^ d \left(x^{(i)} - \alpha - \beta t_ i \right)^2}, \quad \text {where} \quad \mathbf x= \left[x^{(1)}~ ~ x^{(2)}~ ~ \cdots ~ ~ x^{(d)}\right]^ T \in \mathbb {R}^ d.
> $$
> Taking the log of the pdf yields (written as a random function)
> $$
>  \ell (\theta ) = -\frac{d}{2}\ln (2\pi ) - \frac{1}{2}\left[\sum _{i=1}^ d \left( (X^{(i)} - \beta t_ i)^2 - 2 \alpha (X^{(i)} - \beta t_ i) + \alpha ^2\right)\right]
> $$
> Therefore, the gradient of the likelihood with respect to $\theta$ is
> $$
> \nabla \ell (\theta ) = \begin{bmatrix}  \sum _{i=1}^ d \left(X^{(i)} - \beta t_ i - \alpha \right)\\ \sum _{i=1}^ d \left(t_ i X^{(i)} - \beta t_ i^2 - \alpha t_ i\right) \end{bmatrix},
> $$
> from which we can obtain the Hessian
> $$
> \mathbf{H}\ell (\theta ) = \begin{bmatrix}  \sum _{i=1}^ d (-1) &  \sum _{i=1}^ d (-t_ i)\\ \sum _{i=1}^ d (-t_ i) &  \sum _{i=1}^ d (-t_ i^2) \end{bmatrix}.
> $$
> Therefore,
> $$
> \mathcal{I}(\theta ) = -\mathbb E\left[\mathbf{H}\ell (\theta )\right] = \begin{bmatrix}  d &  \sum _{i=1}^ d t_ i\\ \sum _{i=1}^ d t_ i &  \sum _{i=1}^ d t_ i^2 \end{bmatrix},
> $$
> where the expectation is taken with respect to the pdf of the random vector $\mathbf X$. Since none of the entries of the hessian contained any $X^{(i)}$, the expectation was simply the Hessian matrix itself.

## 3. Fisher Information and the Asymptotic Normality of the ML Estimator

**Theorem**:

Let $\theta^* \in \Theta$ (the true parameter). Assume the following:

1. The parameter is identifiable.
2. For all $\theta \in \Theta$, the support of $\mathbb {P}_\theta$ does not depend on $\theta$.
3. $\theta^*$ Is not on the boundary of $\Theta$.
4. $\mathcal{I}(\theta)$ Is invertible in a neighborhood of $\theta^*$.
5. A few more technical conditions.

Then $\hat{\theta}_n^{MLE}$ satisfies

* $\hat{\theta}_n^{MLE} \xrightarrow[n\rightarrow \infty]{(d)} \theta^*$ w.r.t. $\mathbb{P}_{\theta^*}$
* $\sqrt{n}\left( \hat{\theta}_n^{MLE} - \theta^*\right) \xrightarrow[n\rightarrow \infty]{(d)}\mathcal{N}_d(0, \mathcal{I}(\theta^*)^{-1})$ w.r.t $\mathbb{P}_{\theta^*}$.

**Proof:**

The likelihood is
$$
l_i(\theta) = \log f_\theta(x_i)
$$
To find the MLE, we take the derivative and set it to zero
$$
{\partial\over \partial \theta} \sum^n_{i=1} l_1(\hat{\theta}) = \sum^n_{i=1} l_1'(\hat{\theta}) = 0
$$
We also know that $\mathbf{E}[l'(\theta^*)] = 0$.

By Taylor Series,
$$
\begin{aligned}
0 = \sum^n_{i=1}l_1'(\hat{\theta}) & \simeq \sum^n_{i=1}\left[ l_1'(\theta^*) + (\hat{\theta} - \theta^*) l''(\theta^*)\right]\\
&= {1\over \sqrt{n}} \sum^n_{i=1} \left[l_1'(\theta^*)-\mathbb{E}[l_1'(\theta^*)] + (\hat{\theta} - \theta^*) l''(\theta^*) \right]
\end{aligned}
$$
By CLT
$$
{1\over \sqrt{n}} \sum^n_{i=1} \left(l_1'(\theta^*)-\mathbb{E}[l_1'(\theta^*)]\right) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0, \mathsf{Var}(l'(\theta)))=\mathcal{N}(0, \mathcal{I}(\theta^*))
$$
Thus
$$
0 \simeq \mathcal{N}(0, \mathcal{I}(\theta^*)) + \sqrt{n}(\hat{\theta} - \theta^*) {1\over n} \sum^n_{i=1} l_1''(\theta^*)
$$
By LLN,
$$
{1\over n} \sum^n_{i=1} l_1''(\theta^*) \xrightarrow[n \rightarrow \infty]{(p)} \mathbb{E}[l_1''(\theta^*)] = - \mathcal{I}(\theta^*)
$$
Thus
$$
\begin{aligned}
0 &\simeq \mathcal{N}(0, \mathcal{I}(\theta^*)) - \sqrt{n}(\hat{\theta} - \theta^*) \mathcal{I}(\theta^*)\\
\implies& \sqrt{n}(\hat{\theta} - \theta^*) \sim \mathcal{N}(0, {\mathcal{I}(\theta^*)\over (\mathcal{I}(\theta^*))^2})\\
\implies& \sqrt{n}\left( \hat{\theta}_n^{MLE} - \theta^*\right) \xrightarrow[n\rightarrow \infty]{(d)}\mathcal{N}_d(0, \mathcal{I}(\theta^*)^{-1})
\end{aligned}
$$
**Conclusion:**

The **asymptotic normality of the ML estimator** depends upon the Fisher information. For a one-parameter model (like the exponential and Bernoulli), the asymptotic normality result will say something along the lines of following: that the asymptotic variance of the ML estimator is **inversely** proportional to the value of Fisher information at the true parameter of the statistical model. This means that if the value of Fisher information at is high, then the asymptotic variance of the ML estimator for the statistical model will be low.

> #### Exercise 61
>
> Let $(\mathbb {R}, \{ \mathbf{P}_\theta \} _{\theta \in \mathbb {R}})$ denote a statistical model. Recall that the MLE for one observation maximizes the log-likelihood for one observation, which is the random variable $\ell (\theta ) = \ln L_1(X, \theta )$ where $X \sim \mathbf{P}_\theta$. Suppose we observe $X_1 = x_1$, and now consider the graph of the function $\theta \mapsto \ln L_1(x_1, \theta )$.
>
> What does the Fisher information $\mathcal{I}(\theta )$ represent?
>
> **Solution:**
>
> It tells you, on average, how curved the function $\theta \mapsto \ln L(x_1, \theta )$ is.
>
> Recall $\mathcal{I}(\theta ) = -\mathbb {E}[\ell ^{\prime \prime }(\theta )]$. Since the second-derivative measures concavity/convexity (how curved a function is at a particular point), $\mathcal{I}(\theta )$ measures the *average* curvature of the function $\theta \mapsto \ell (\theta ) = \ln L_1(x_1, \theta )$.
>
> **Remark:**
>
> It turns out that the Fisher information tells how curved (on average) the log-likelihood $\ln L_ n(x_1, \ldots , x_ n, \theta )$ for several samples $X_1 = x_1, \ldots , X_ n = x_ n$ is. In particular, $\mathcal{I}(\theta ^*)$ tells how curved (on average) the log-likelihood is near the true parameter. As a rule of thumb, if the Fisher information $\mathcal{I}(\theta ^*)$ is large, then we expect the MLE to give a good estimate for $\theta^*$.

## 4. Method of Moments

Let $X_1, ..., X_n$ be an i.i.d. sample associated with a statistical model $(E, (\mathbb{P}_\theta)_{\theta \in \Theta})$.

* Assume that $E \subseteq \R$ and $\Theta \subseteq \R^d$, for some $d \geq 1$.

* Population moments: Let $m_k(\theta) = \mathbb{E}_\theta[X_1^k], 1 \leq k \leq d$.

* Empirical moments: Let $\hat{m}_k = \overline{X_n^k} = {1\over n} \sum\limits^n_{i=1} X_i^k, 1\leq k \leq d.$

* From LLN, they are consistent estimators
  $$
  \hat{m}_k \xrightarrow[n \rightarrow \infty]{\mathbb{P}/a.s} m_k(\theta)
  $$
  More compactly, we say that the whole vector converges
  $$
  (\hat{m}_1, ..., \hat{m}_d) \xrightarrow[n \rightarrow \infty]{\mathbb{P}/a.s} (m_1(\theta), ..., m_d(\theta))
  $$

**Moments estimator**

Let 
$$
\begin{aligned}
M : \Theta &\rightarrow \R^d\\
\theta &\mapsto M(\theta)= (m_1(\theta), ..., m_d(\theta))
\end{aligned}
$$
Assume $M$ is one to one:
$$
\theta = M^{-1}(m_1(\theta), ..., m_d(\theta))
$$
Definition: Moments estimator of $\theta$:
$$
\hat{\theta}_n^{MM} = M^{-1}(\hat{m}_1, ..., \hat{m}_d)
$$
provided it exists.

> #### Exercise 62
>
> Let $(\mathbb {R}, \{ N(\mu , \sigma ^2)\} _{\mu \in \mathbb {R}, \sigma > 0})$ be the statistical model of a normal random variable $X$. Let
> $$
> m_ k(\mu , \sigma ) = \mathbb {E}[X^ k]
> $$
> denote the $k$-th moment of $X$. Let $\psi : \mathbb {R} \times (0, \infty ) \to \mathbb {R}^2$ be defined by $\psi (\mu , \sigma ) = (m_1(\mu , \sigma ), m_2(\mu , \sigma ))$. (Since we have two parameters of interest, $\mu$ and $\sigma$, it makes sense to work with the first two moments. The hope is that the two moments will uniquely determine the parameters of interest $\mu$ and $\sigma$)
>
> Express $m_1(\mu,\sigma)$ and $m_2(\mu, \sigma)$ in terms of $\mu$ and $\sigma$.
>
> **Solution:**
>
> Note that
> $$
> \begin{aligned}
> m_1(\mu , \sigma ) &= \mathbb{E}[X] = \mu\\
> m_2(\mu, \sigma) &= \mathbb {E}[X^2] = (\mathbb {E}[X])^2 + \left(\mathbb {E}[X^2] - (\mathbb {E}[X])^2 \right) = \mu ^2 + \sigma ^2.
> \end{aligned}
> $$
> Hence,$\psi (\mu , \sigma ) = (\mu , \mu ^2 + \sigma ^2)$.

> #### Exercise 63
>
> Let 
> $$
> \begin{aligned}
> \psi : \mathbb {R} \times (0, \infty ) &\rightarrow \R^2\\
> (\mu , \sigma ) &\mapsto (m_1(\mu , \sigma ), m_2(\mu , \sigma )).
> \end{aligned}
> $$
> denote the moments map considered in the previous problem, where $m_k(\mu, \sigma)$ denotes the $k$-th moment of the distribution $\mathcal{N}(\mu,\sigma^2)$.
>
> 1. Is $\psi$ one-to-one on the domain $\R \times (0,\infty)$? (Equivalently, given the outputs $m_1$ and $m_2$, can we use them to uniquely reconstruct $\mu \in \R$ and $\sigma > 0$?)
> 2. If $\psi$ is one-to-one on the given domain and $\psi (\mu , \sigma ) = (m_1, m_2)$, what is $\mu$ and $\sigma$ expressed in terms of $m_1$ and $m_2$? 
>
> **Solution:**
>
> 1. From previous exercise we know $\psi (\mu , \sigma ) = (\mu , \mu ^2 + \sigma ^2)$. Hence, this function is one-to-one on the domain $\R \times (0,\infty)$. 
>
> 2. Since $m_1(\mu, \sigma) = \mu$, we can reconstruct the first parameter directly from the first moment: $\mu = m_1$.
>
>    Next since we know $m_2(\mu , \sigma ) = \sigma ^2 + \mu ^2$, we can back-solve for $\sigma$:
>    $$
>    \sigma = \sqrt{m_2 - \mu ^2} = \sqrt{m_2 - m_1^2}.
>    $$
>
> **Remark:** Assume $m_1$ and $m_2$ are one of the outputs of $\psi$, we have essentially shown how to construct $\psi ^{-1}(m_1, m_2)$.

> #### Exercise 64
>
> Let $(E, \{ \mathbf{P}_{\theta }\} _{\theta \in \Theta })$ denote a statistical model associated to a statistical experiment $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathbf{P}_{\theta ^*}$ where $\theta^* \in \Theta$ is the true parameter. Assume that $\Theta \subset \R^d$ for some $d \geq 1$. Let $m_k(\theta):=\mathbb{E}[X^k]$ where $X \sim \mathbf{P}_\theta$. $m_k(\theta)$ Is referred to as the $k$-th moment of $\mathbf{P}_\theta$. Also define the moments map:
> $$
> \begin{aligned}
> \psi : \Theta &\rightarrow \R^d\\
> \theta &\mapsto (m_1(\theta ), m_2(\theta ), \ldots , m_ d(\theta )).
> \end{aligned}
> $$
> Assume that $\psi$ is one-to-one (and hence, invertible).
>
> 1. What is $\theta^*$?
> 2. What is the method of moments estimator for $\theta^*$?
>
> **Solution:**
>
> 1. Observe that $\psi (\theta ^*) = (m_1(\theta ^*), m_2(\theta ^*), \ldots , m_ d(\theta ^*))$ by definition of $\psi$. Since $\psi$ is invertible, then we know that $\psi ^{-1}(m_1(\theta ^*), m_2(\theta ^*), \ldots , m_ d(\theta ^*)) =\theta ^*$. Hence, 
>    $$
>    \theta^* = \psi ^{-1}(m_1(\theta ^*), m_2(\theta ^*), \ldots , m_ d(\theta ^*)
>    $$
>
> 2. The method of moments estimator is given by
>    $$
>    \widehat{\theta }_ n^{\text {MM}} = \psi ^{-1}\left( \frac{1}{n} \sum _{i = 1}^ n X_ i, \frac{1}{n} \sum _{i = 1}^ n X_ i^2, \ldots , \frac{1}{n} \sum _{i = 1}^ n X_ i^ d \right),
>    $$

> #### Exercise 65
>
> Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } N(\mu ^*, (\sigma ^*)^2)$ and consider the associated statistical model $(\mathbb {R}, \{ N(\mu , \sigma ^2)\} _{\mu \in \mathbb {R}, \sigma > 0})$. Let
> $$
> \begin{aligned}
> \psi : \mathbb {R} \times (0,\infty) &\rightarrow \R^2\\
> (\mu , \sigma ) &\mapsto (m_1(\mu , \sigma ), m_2(\mu , \sigma )).
> \end{aligned}
> $$
> denote the moments map considered in the previous problem, where $m_k(\mu, \sigma)$ denotes the $k$-th moment of the distribution $\mathcal{N}(\mu, \sigma^2)$.
>
> Suppose we observe the dataset $X_1 = 0.5 , X_2 = 1.8 , X_3 = -2.3, X_4 = 0.9$.
>
> 1. What is the method of moments estimator $\widehat{\mu }^{\text {MM}}$ for $\mu^*$ evaluated on this dataset?
> 2. What is the method of moments $\widehat{\sigma }^{\text {MM}}$ estimator for $\sigma^*$ evaluated on this dataset?
>
> **Solution:**
>
> Since we computed $\psi^{-1}$ explicitly in the previous problem, we can apply the method of moments to estimate the true parameters $\mu^*$ and $\sigma^*$. Let
> $$
> \widehat{m}_1 = \frac{1}{n} \sum _{i = 1}^ n X_ i, \quad \widehat{m}_2 = \frac{1}{n} \sum _{i = 1}^ n X_ i^2
> $$
> denote the first and second sample moments, respectively. Then the **method of moments estimator** is defined to be
> $$
> (\widehat{\mu }_ n^{\text {MM}}, \widehat{\sigma }_ n^{\text {MM}}) := \psi ^{-1}(\widehat{m}_1, \widehat{m}_2) = (\widehat{m}_1, \sqrt{\widehat{m}_2 - \widehat{m}_1^2} ).
> $$
> Using a calculator to compute
> $$
> \widehat{m}_1(0.5, 1.8, -2.3, 0.9) = \frac{0.5 + 1.8 -2.3 + 0.9}{4} \approx 0.225
> $$
> and
> $$
> \widehat{m}_2(0.5, 1.8, -2.3, 0.9) = \frac{(0.5)^2 + (1.8)^2 + (-2.3)^2 + (0.9)^2}{4} \approx 2.3975.
> $$
> Applying the method of moments
> $$
> \widehat{\mu }_ n^{\text {MM}}(0.5, 1.8, -2.3, 0.9)= \widehat{m}_1(0.5, 1.8, -2.3, 0.9) \approx 0.225.
> $$
> and
> $$
> \widehat{\sigma }_ n^{\text {MM}}(0.5, 1.8, -2.3, 0.9) = \sqrt{\widehat{m_2} - (\widehat{m_1})^2} \approx \sqrt{2.3975 - (0.225)^2} \approx 1.532.
> $$

## 5. Generalized Method of Moments Estimator

#### Statistical analysis:

* Recall $M(\theta) = (m_1(\theta), ..., m_d(\theta))$;

* Let $\hat{M} = (\hat{m}_1, ...,\hat{m}_d)$.

* Let $\Sigma(\theta) = \mathsf{Cov}_\theta(X_1, X_1^2, ...,X_1^d)$ be the covariance matrix of the random vector $(X_1, X_1^2, ..., X_1^d)$, which we assume to exist.

  * Recall CLT
    $$
    \sqrt{n} \left( \overline{X_n^k} - m_k(\theta)\right) \xrightarrow[n\rightarrow \infty]{(d)} \mathcal{N}(0,\mathsf{Var}(X_1^k))
    $$
    The multivariate CLT is
    $$
    \sqrt{n} \left(\begin{pmatrix} \overline{X_n^k}\\ \vdots \\\overline{X_n^k} \end{pmatrix} - M(\theta)^T \right) \xrightarrow[n\rightarrow \infty]{(d)} \mathcal{N}(0,\mathsf{Cov}\begin{pmatrix} \overline{X_1^1}\\ \vdots \\\overline{X_1^d} \end{pmatrix})
    $$

* Assume $M^{-1}$ is continuously differentiable at $M(\theta)$.

#### Method of moments:

**Remark:** The method of moments can be extended to more general moments, even when $E \not \subset \R$.

* Let $g_1, ..., g_d: E \rightarrow \R$ be given functions, chosen by the practitioner.
* Previously, $g_k(x)=x^k, x \in E = \R$, for all $k=1,...,d$.
* Define $m_k(\theta) = \mathbb{E}_\theta [g_k(X)]$, for all $k = 1, ..., d$.
* Let $\Sigma(\theta) = \mathsf{Cov}_\theta(g_1(X_1), g_2(X_1), ...,g_d(X_1))$ be the covariance matrix of the random vector $g_1(X_1), g_2(X_1), ..., g_d(X_1)$, which we assume to exist.
* Assume $M$ is **one-to-one** and $M^{-1}$ is **continuously differentiable** at $M(\theta)$.

#### Generalized method of moments

$$
\sqrt{n} \left( \hat{m} - M(\theta) \right) \xrightarrow[n \rightarrow \infty]{ (d)} \mathcal{N}_d(0, \Sigma(\theta))
$$

Applying the **multivariate CLT** and **Delta method** yields:
$$
\sqrt{n} \left( M^{-1}(\hat{m}) -M^{-1}( M(\theta)) \right) \xrightarrow[n \rightarrow \infty]{ (d)} \mathcal{N}_d(0, \nabla (M^{-1})^T\ \Sigma(\theta)\ \nabla M^{-1})\\
\implies \sqrt{n}\left( \hat{\theta}^{MM}_n - \theta \right) \xrightarrow[n \rightarrow \infty]{ (d)} \mathcal{N}(0, \Gamma(\theta)) \quad \text{(w.r.t. } \mathbb{P}_\theta),\\
\text{where }\quad \Gamma(\theta) = \left[ { \partial M^{-1}\over \partial \theta }(M(\theta))\right]^T \Sigma(\theta) \left[ { \partial M^{-1}\over \partial \theta }(M(\theta))\right].
$$

## 6. Asymptotic Normality of the Method of Moments Estimator 

Let $(E, \{  \mathbf{P}_\theta \} _{\theta \in \Theta })$ denote a statistical model associated to a statistical experiment $ X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathbf{P}_{\theta ^*} $ for some unknown parameter $\theta^*$. Under some technical conditions, the method of moments estimator $\widehat{\theta }_ n^{\text {MM}}$ is **asymptotically normal**, which means that
$$
\sqrt{n}(\widehat{\theta }_ n^{\text {MM}} - \theta ^*) \xrightarrow [n \to \infty ]{(d)} \mathcal{N}(0, \sigma ^2).
$$
The quantity $\sigma^2$ above is referred to as the **asymptotic variance**.

> #### Exercise 66
>
> Let $X_1, \ldots , X_ n \sim \text {Exp}(\lambda ^*)$ denote a statistical experiment where $\lambda^*$ is the true, unknown parameter. You construct the associated statistical model $((0, \infty ), \{ \text {Exp}(\lambda )\} _{\lambda \in (0,\infty )})$. Since the parameter $\lambda$ is one-dimensional, we only consider the first moment with moment map:
> $$
> \begin{aligned}
> \psi : \mathbb {R} &\rightarrow \R\\
> \lambda &\mapsto m_1(\lambda) := \mathbb{E}[X], \quad (X \sim \rm{Exp}(\lambda))
> \end{aligned}
> $$
>
> 1. [**Step 1: Moments Map for an Exponential Statistical Model**] What is $\psi(\lambda)$ and $\psi^{-1}(m_1)$? 
>
> 2. [**Step 2a: Deriving the Method of Moments Estimator**] What is the method of moments estimator $\widehat{\lambda }_ n^{\text {MM}}$ for $\lambda^*$?
>
> 3. [**Step 2b: Deriving the Method of Moments Estimator**] By the CLT, 
>    $$
>    \sqrt{n}\left( \widehat{m}_1(\lambda ^*) - \frac{1}{\lambda ^*}\right)= \sqrt{n}\left(\frac{1}{n}\sum _{i = 1}^ n X_ i - \frac{1}{\lambda ^*}\right)
>    $$
>    Converges to a normal random variable $\mathcal{N}(0,\sigma^2)$, where $\sigma^2$ can be written in terms of $\lambda^*$. What is $\sigma^2$?
>
> 4. [**Step 3: Computing the Asymptotic Variance of the Method of Moments Estimator**] Suppose that
>    $$
>    \sqrt{n}(\widehat{m}_1 - m_1(\theta )) \xrightarrow [n \to \infty ]{(d)} \mathcal{N}(0, \sigma ^2).
>    $$
>    where $\widehat{m_1}$ and $m_1$ are first sample moment and first moment, respectively,
>
>     Recall that the **Delta method** states that if the above holds, then for any $g: \R \rightarrow \R$ that has a continuous first derivative,
>    $$
>    \sqrt{n}(g(\widehat{m}_1) - g(m_1(\theta ))) \xrightarrow [n \to \infty ]{(d)} \mathcal{N}(0, g'(m_1(\theta ))^2 \sigma ^2)
>    $$
>    By the CLT for the method of moments estimator, $\widehat{\lambda }_ n^{\text {MM}}$ is asymptotically normal, meaning that 
>    $$
>    \sqrt{n}(\widehat{\lambda }_ n^{\text {MM}} - \lambda ^*) \stackrel{(d)}{\to } \mathcal{N}(0, \tau ^2)
>    $$
>    Applying the last problem and the delta method, what is the asymptotic variance $\tau^2$ in terms of $\lambda^*$?
>
> **Solution:** 
>
> 1. The first moment is
>    $$
>    \begin{aligned}
>    m_1(\lambda) & = \mathbb{E}[X]\\
>    &= \int _{0}^\infty \lambda x e^{-\lambda x} \,  dx\\
>    &= - x e^{-\lambda x} \bigg|_{0}^\infty - \int _0^\infty - e^{-\lambda x}\,  dx = 1/\lambda .
>    \end{aligned}
>    $$
>    Hence, $\psi(\lambda) = 1/\lambda$. Since $\psi$ is its own inverse, we also have $\psi ^{-1}(m_1) = 1/m_1$.
>
> 2. Since $\psi(\lambda) = 1/\lambda$,
>    $$
>    \widehat{\lambda }_ n^{\text {MM}} = \psi ^{-1}\left(\frac{1}{n} \sum _{i = 1}^ n X_ i\right) = \frac{n}{\sum _{i = 1}^ n X_ i}.
>    $$
>
> 3. By the CLT, $\sigma ^2 = \textsf{Var}(X)$.
>
>    The second moment is
>    $$
>    \begin{aligned}
>    \mathbb{E}[X^2] &=\int _0^\infty x^2 \lambda ^* e^{-\lambda ^* x} \,  dx\\
>    &= - x^2 e^{-\lambda ^* x} \bigg|_{0}^\infty - \int _{0}^\infty - 2 x e^{-\lambda ^* x} \,  dx\\
>    &= 2/(\lambda ^*)^2
>    \end{aligned}
>    $$
>    Since we showed in the solution of the last question that $\int _0^\infty \lambda ^* x e^{-\lambda ^* x} \,  dx = 1/\lambda ^*$. Thus,
>    $$
>    \textsf{Var}(X) = 2/(\lambda ^*)^2 - 1/(\lambda ^*)^2 = 1/(\lambda ^*)^2.
>    $$
>    Therefore, $\sigma ^2 = 1/(\lambda ^*)^2$.
>
> 4. From (3) we have
>    $$
>    \sqrt{n}( \frac{1}{n} \sum _{i = 1}^ n X_ i - 1/\lambda ^*) \stackrel{(d)}{\to } \mathcal{N}(0, 1/(\lambda ^*)^2).
>    $$
>    Letting $g = \psi^{-1}$ in the statement of the delta method, and noting that 
>    $$
>    (\psi ^{-1})'(m) = -1/m^2, \ m_1(\lambda) = 1/\lambda \\
>    \implies (\psi ^{-1})'(m_1(\lambda )) = (\psi ^{-1})'(1/\lambda) = -1/(1/\lambda)^2 =-\lambda ^2
>    $$
>    Therefore
>    $$
>    \tau^2 = (\psi ^{-1})'(m_1(\lambda ))^2 \sigma^2 = (-\lambda^2)^2\ 1/\lambda^2 = (\lambda^*)^2
>    $$
>    we see that 
>    $$
>    \sqrt{n}(\widehat{\lambda }_ n^{\text {MM}} - \lambda ^*) \stackrel{(d)}{\to } \mathcal{N}(0, (\lambda ^*)^2).
>    $$

## 7. MLE versus Method of Moments

* Comparison of the quadratic risks: In general, the MLE is more accurate.
  * MLE is asymptotically unbiased.
  * **Cramer-Rao lower bound** states that no unbiased estimator may have an (asymptotic) variance that is smaller than the inverse Fisher information.
* MLE still gives good results if model is misspecified.
  * This is not going to be the case in general for the method of moments (MM).
  * The expression of the moments map $\psi$ in terms of the parameter $\theta$ can be quite complicated, so it may be difficult to deduce how many moments (or *degrees of freedom*) are needed to uniquely recover the true distribution from moments. MM requires you to find $d$ so that the first $d$ moments uniquely determine the distribution of interest. To compute the MLE, this step is not necessary.
* Computational issues: Sometimes, the MLE is intractable but MM is easier (polynomial equations)
  * Optimizing the likelihood function can be very inefficient if the likelihood function is complicated and has several local maxima which require testing.



# Lecture 8. Distance Measures Between Distribution

In previous examples, we intuitively use $\hat{p}=\bar{X}_n$, since by LLN, $p = \mathbb{E}[X]$. But what if $\theta \neq \mathbb{E}[X]$?

* Maximum likelihood estimation: a generic approach with every good properties
* Method of moments: a (fairly) generic and easy approach
* M-estimators: a flexible approach, close to machine learning.

There are 9 topics and 6 exercises.

## 1. Total Variation Distance

Let $(E, (\mathbf{P}_\theta)_{\theta \in \Theta})$ be a statistical model associated with a sample of i.i.d. r.v. $X_1, ..., X_n$. Assume that there exists $\theta^* \in \Theta$ such that $X_1 \sim \mathbf{P}_{\theta^*} : \theta^*$ is the **true** parameter.

**Statistical's goal**: 

Given $X_1, ..., X_n$, find an estimator $\hat{\theta} = \hat{\theta}(X_1, ..., X_n)$ such that $\mathbf{P}_{\hat{\theta}}$ is close to $\mathbf{P}_{\theta^*}$ for the true parameter $\theta^*$.

This means: $\left\vert\mathbf{P}_{\hat{\theta}}(A) - \mathbf{P}_{\theta^*}(A) \right\vert$ is small for all $A \subset E$.

**Definition of** **Total Variation Distance**:

The total variation distance between two probability measures $\mathbf{P}_{\theta }$ and $\mathbf{P}_{\theta'}$ is defined by
$$
\text {TV}(\mathbf{P}_{\theta }, \mathbf{P}_{\theta '})={\max _{A \subset E}}\, \big |\mathbf{P}_{\theta }(A)-\mathbf{P}_{\theta '}(A)\big |\,
$$
**Interpretation**:

By analyzing the data, we are able to produce an estimator $\hat{\theta}$ such that the distributions $\mathbf{P}_{\theta }$ and $\mathbf{P}_{\theta'}$ are close in total variation distance, More precisely,  
$$
\text {TV}(\mathbf{P}_{\hat{\theta }}, \mathbf{P}_{\theta ^*}) \leq \epsilon ,
$$
where $\epsilon$ is a very small positive number.

We conclude that

* Let $A$ be an event, then $|\mathbf{P}_{\theta ^*}(A) - \mathbf{P}_{\hat{\theta }}(A)| \leq \epsilon$.
* Let $X \sim \mathbf{P}_{\theta ^*}$, let $Y \sim \mathbf{P}_{\hat{\theta }}$, and suppose $a, b \in \R$ where $a \leq b$. Then $|\mathbf{P}_{\theta ^*}(a \leq X \leq b ) - \mathbf{P}_{\hat{\theta }}(a \leq Y \leq b)| \leq \epsilon$.

## 2. Total Variance Distance Between Discrete Measures

Assume that $E$ is discrete (i.e., finite or countable). This includes Bernoulli, Binomial, Poisson...

Therefore $X$ has a PMF: $\mathbf{P}_\theta(X = x) = p_\theta(x)$ for all $x \in E$,
$$
p_\theta(x) \geq 0, \quad \sum_{x \in E} p_\theta(x)=1.
$$
The total variation distance between $\mathbf{P}_\theta$ and $\mathbf{P}_{\theta'}$ is a simple function of the PMF's $p_\theta$ and $p_{\theta'}$:
$$
\text {TV}(\mathbf{P}_\theta, \mathbf{P}_{\theta'}) = {1\over 2} \sum_{x \in E}\left\vert p_{\theta}(x) -  p_{\theta'}(x) \right\vert.
$$

**Prove there exists $A$ such that**
$$
|\mathbf{P}_\theta(A) - \mathbf{P}_{\theta'}(A)| = {1\over 2} \sum\limits_{x \in E} |p_\theta(x) - p_{\theta'}(x)|
$$
Let $A = \{x \in E, \quad p_\theta(x) \geq p_{\theta'}(x)\}$, 
$$
\sum_{x:\,\, p_\theta(x) \geq p_{\theta'}(x)} |p_\theta(x) - p_{\theta'}(x)| = |\mathbf{P}_\theta(A) - \mathbf{P}_{\theta'}(A)|
$$
Let $A = \{x \in E, \quad p_\theta(x) < p_{\theta'}(x)\}$,
$$
\sum_{x:\,\, p_\theta(x) < p_{\theta'}(x)} |p_\theta(x) - p_{\theta'}(x)| = \mathbf{P}_\theta(A^c) - \mathbf{P}_{\theta'}(A^c) = \mathbf{P}_{\theta'}(A) - \mathbf{P}_{\theta}(A) = | \mathbf{P}_{\theta}(A) - \mathbf{P}_{\theta'}(A)|
$$
Combined together and we get
$$
\sum_{x \in E} |p_\theta(x) - p_{\theta'}(x)| = 2|\mathbf{P}_\theta(A) - \mathbf{P}_{\theta'}(A)|\\
{1\over 2}\sum_{x \in E} |p_\theta(x) - p_{\theta'}(x)| = |\mathbf{P}_\theta(A) - \mathbf{P}_{\theta'}(A)|\\
$$

> #### Exercise 44
>
> 1. Let $X \sim \mathbf{P} = \mathsf{Ber}(1/2)$ and $Y \sim \mathbf{Q} = \mathsf{Ber}(1/2)$. What is $\, \text {TV}(\mathbf{P},\mathbf{Q})\,$, the total variation distance between the distributions of the Bernoulli random variables $X$ and $Y$?
> 2. Let $X \sim \mathbf{P} = \mathsf{Ber}(1/2)$ and $Y \sim \mathbf{Q} = \mathsf{Ber}(1/3)$. What is $\, \text {TV}(\mathbf{P},\mathbf{Q})\,$, the total variation distance between the distributions of the Bernoulli random variables $X$ and $Y$?
>
> **Answer**: 
>
> $1. \quad 0;\\2. \quad 0.16667.$
>
> **Solution**:
>
> 1. Intuitively, since $X$ and $Y$ have the same distribution, we expect the (total variation) distance between their distributions to be $0$. Observe that for any event, $\mathbf{P}(A) = \mathbf{Q}(A)$ since $\mathbf{P}$ and $\mathbf{Q}$ are both $\mathsf{Ber}(1/2)$.
>    $$
>    \text {TV}(\mathbf{P}, \mathbf{Q}) = \max _{A \subset E} |\mathbf{P}(A) - \mathbf{Q}(A)| = 0.
>    $$
>    Note that the distance between two distributions only depends on the distributions themselves and *not* their relation to each other (the joint distribution). This is why assuming $X$ and $Y$ are independent (or not) does not affect the total variation distance.
>
> 2. The sample space of $X$ and $Y$ is $\{0,1\}$. Let $f$ be the PMF of $X$ and let $g$ be the PMF of $Y$. Note that $f(1) = f(0) = 1/2$ and  $g(1)=1/3, g(0)=2/3$. Hence, we can apply the given formula:
>    $$
>    \begin{aligned}
>    \text {TV}(\mathbf{P}, \mathbf{Q})&= \frac{1}{2} \sum _{x \in E} |f(x) - g(x)|\\
>    &=\frac{1}{2} (|f(0) - g(0)| + |f(1) - g(1)|)\\
>    &=\frac{1}{2} ( 1/6 + 1/6 ) = 1/6 \approx 0.16667.
>    \end{aligned}
>    $$
>    **Remark**: In general, we have the formula
>    $$
>    \text {TV}(\text {Ber}(p), \text {Ber}(q)) = |p - q|.
>    $$

## 3. Total Variance Distance for Continuous Distributions

Assume that $E$ is continuous. This includes Gaussian, Exponential, ....

Assume that $X$ has a density $\mathbb{P}_\theta(X \in A) = \int_A f_\theta(x)dx$ for all $A \subset E$.
$$
f_\theta(x) \geq 0, \quad \int_E f_\theta(x) dx = 1.
$$
The total variation distance between $\mathbb{P}_\theta$ and $\mathbb{P}_{\theta'}$ is a simple function of the densities $f_\theta$ and $f_{\theta'}$
$$
\text {TV}(\mathbb{P}_\theta, \mathbb{P}_{\theta'}) = {1\over 2} \int_{x \in E} |f_\theta(x) - f_{\theta'}(x)|dx
$$
**Graphical interpretation to total variation**

Let $X \sim \mathbf{P}$ and $Y \sim \mathbf{P}$ be Gaussian random variables with mean $0$. Let $f$ denote the probability density function of $X$ and $g$ denote the density of $Y$. The graphical interpretation of $2\text {TV}(\mathbf{P}, \mathbf{Q})$ is

![images_u3s1_areabetweencurves](../assets/images/images_u3s1_areabetweencurves.svg)

## 4. Properties of Total Variation Distance

* Symmetric: $\text{TV}(\mathbf{P}_\theta, \mathbf{P}_{\theta'})=\text{TV}(\mathbf{P}_{\theta'},\mathbf{P}_\theta)$
* Positive: $0 \leq \text{TV}(\mathbf{P}_\theta, \mathbf{P}_{\theta'}) \leq 1$
* Definite: If $\text{TV}(\mathbf{P}_\theta, \mathbf{P}_{\theta'}) = 0$ then $\mathbf{P}_\theta= \mathbf{P}_{\theta'}$ 

* Triangle inequality: $\text{TV}(\mathbf{P}_\theta, \mathbf{P}_{\theta'}) \leq \text{TV}(\mathbf{P}_{\theta}, \mathbf{P}_{\theta''}) + \text{TV}(\mathbf{P}_{\theta''}, \mathbf{P}_{\theta'})$

These imply that the **total variation is a distance between probability distributions**.

**Upper bound on TV:**

The smallest number $M$ such that $\text {TV}(\mathbf{P},\mathbf{Q})\le M$ for any probability measures $\mathbf{P,Q}$ is $1$.

Using the definition of total variation distance $\text {TV}(\mathbf{P},\mathbf{Q}) = {\max _{A \subset E}}| \mathbf{P}(A) - \mathbf{Q}(A)|,$ we can say that if the maximum is obtained using a set $A_1$ such that $\mathbf{P}(A_1) \ge \mathbf{Q}(A_1)$, then
$$
\text {TV}(\mathbf{P},\mathbf{Q}) = |\mathbf{P}(A_1) - \mathbf{Q}(A_1)| \le \mathbf{P}(A_1) \le 1.
$$

> #### Exercise 45
>
> Compute
>
> 1. $\text{TV}(\mathsf{Ber}(0.5), \mathsf{Ber}(0.1)) $
> 2. $\text{TV}(\mathsf{Exp}(1), \mathsf{Unif}(0,1)) $
> 3. $\text{TV}(X, X+a)$ For any $a\in (0,1)$, where $X \sim \mathsf{Ber}(0.5)$.
> 4. $\, \text {TV}(2\sqrt{n} (\bar X_ n-1/2),Z)\,$ , where $\, X_ i \stackrel{i.i.d}{\sim } \textsf{Ber}(0.5)$ and $Z \sim \mathcal{N}(0,1)$.
>
> **Answer**: 
>
> 1. $0.4$
> 2. $1/e$
> 3. $1$
>
> **Solution**:
>
> 1. $$
>    \begin{aligned} \text{TV}(\mathsf{Ber}(0.5), \mathsf{Ber}(0.1)) &= {1\over 2}\left[|p_{0.5}(0) - p_{0.1}(0)| + |p_{0.5}(1) - p_{0.1}(1)| \right] \\&= {1\over 2}[|0.5-0.9| + |0.5-0.1|]\\ &= 0.4 \end{aligned}
>    $$
>
> 2. Let $f$ and $g$ represent the density functions of $\mathsf{Exp}(1)$ and $\mathsf{Unif}[0,1]$, respectively. So $f(x) = e^{-x} \mathbf{1}_{( x \geq 0)}$ and $g(x) = \mathbf{1}_{(0 < x \leq 1)}$,
>    $$
>    \begin{aligned} 
>    \text{TV}(\mathsf{Exp}(1), \mathsf{Unif}(0,1)) &= {1\over 2} \int |e^{-x} \mathbf{1}_{( x \geq 0)} - \mathbf{1}_{(0 < x \leq 1)}| dx\\
>    &= {1\over 2} \int_0^1 |e^{-x}  - 1| dx + {1\over 2} \int_1^\infty |e^{-x}| dx\\
>    &= {1\over 2} \int_0^1 1- e^{-x} dx + {1\over 2} \int_1^\infty |e^{-x}| dx\\
>    &= {1\over 2} - {1\over 2} \int_0^1 e^{-x} dx + {1\over 2} \int_1^\infty e^{-x} dx\\
>    &= {1\over 2} + {1\over 2} \left. e^{-x}\right|^1_0 - {1\over 2} e^{-x}|^\infty_1\\
>    &= {1\over 2} + {1\over 2e} - {1\over 2} - (0-{1\over 2e})\\
>    &={1\over e}
>    \end{aligned}
>    $$
>    **Remark:** Even though the two distributions have different sample spaces, we can take the union of the two as the sample space for both, and integrate over it.
>
> 3. $$
>    \text{TV}(X, X+a) = |\mathbb{P}(X \in \{0,1\})  - \mathbb{P}(X + a \in \{0,1\})| = |1 - 0 | = 1
>    $$
>
> 4. By CLT, we know
>    $$
>    \sqrt n {\overline{X}_n - {1/2} \over \sqrt{1/2(1-1/2)}} = 2 \sqrt n (\overline{X} - 1/2) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
>    $$
>    However, when $\, X_ i \stackrel{i.i.d}{\sim } \textsf{Ber}(0.5)$, we have $S_n \triangleq \left\{ a_ i = 2 \sqrt{n} \left(\frac{i}{n} - \frac{1}{2}\right) \mid i = 0, 1, \dots , n\right\}$ , a set of $n+1$ point where the PMF of $2\sqrt{n} (\bar X_ n-1/2)$ is non-zero. So that $\mathbf{P}(Z \in S_n) = 0$ since a continuous random variable's probability to take values in a finite set is $0$ , and $\mathbf{P}(2\sqrt{n} (\bar X_ n-1/2) \in Sn) = 1$. 
>
>    Therefore, $|\mathbf{P}(A) - \mathbf{Q}(A)| = 1$ and $\text {TV}(2\sqrt{n} (\bar X_ n-1/2),Z)=1$.

## 5. Kullback-Leibler (KL) Divergence

There are many distances between probability measures to replace total variation. One is KL Divergence.

**Definition of Kullback-Leibler (KL) Divergence:**

Let consider $\mathbf{P}$ and $\mathbf{Q}$ be both **discrete** or **continuous** probability distributions with PMFs or PDFs $p$ and $q$ respectively. Let's also assume $\mathbf{P}$ and $\mathbf{Q}$  have a common sample space $E$ . Then the *KL divergence* (also known as *relative entropy* ) between $\mathbf{P}$ and $\mathbf{Q}$ is defined by
$$
KL(\mathbf{P},\mathbf{Q} ) = \begin{cases} \sum\limits _{x \in E} p(x) \ln \left( \frac{p(x)}{q(x)} \right), &\text{if E is discrete} \\{{\int }} _{x \in E} p(x) \ln \left( \frac{p(x)}{q(x)} \right) dx, & \text{if E is continuous} \end{cases}
$$
where the sum is only over the support of $\mathbf{P}$.  Note that $\mathbf{P}$ can be written as $\mathbb{P}_\theta$, $\mathbf{Q}$ can be written as $\mathbb{P}_{\theta'}$ 

> **Why do we sum only over the support of $\mathbf{P}$?**
>
> At any point $x \in E$ outside the support of $\mathbf{P}$ but where $q(x) \neq 0$:
> $$
> \begin{aligned}
> \lim _{p/q\to 0^+}q\left(\frac{p}{q}\right) \ln \left(\frac{p}{q}\right) &= q \lim _{p/q\to 0^+}\left(\frac{p}{q}\right) \ln \left(\frac{p}{q}\right)\\
> &= q \cdot (0) \quad  \text{(by L'hopital's rule)}
> \end{aligned}
> $$

> #### Exercise 46
>
> Let $X \sim \mathbf{P}_ X = \text {Ber}(1/2)$ and let $Y \sim \mathbf{P}_ Y = \text {Ber}(1/2)$. What is $\text {KL}(\mathbf{P}_ X, \mathbf{P}_ Y)$?
>
> **Answer**: $1$
>
> **Solution**:
>
> Let $p$ be the PMF of the distribution $\mathsf{Ber}(1/2)$. Note that the sample space is the discrete set $E = \{0,1\}$. Then
> $$
> \begin{aligned}
> \text {KL}(\mathbf{P}_ X, \mathbf{P}_ Y) &= p(1) \ln (p(1)/p(1)) + p(0) \ln (p(0)/p(0))\\
> &= (1/2) \ln (1) + (1/2) \ln (1) = 0.
> \end{aligned}
> $$
> Note that the result is consistent with the second property.

**Properties:**

* **In general:** $\text {KL}(\mathbf{P}, \mathbf{Q}) \neq \text {KL}(\mathbf{Q}, \mathbf{P}) $ [ **Asymmetric** ]
* **Non-negative**: $\text {KL}(\mathbf{P}, \mathbf{Q}) \geq 0$
* **Definite**: $\text {KL}(\mathbf{P}, \mathbf{Q}) = 0$ only if $\mathbf{P}$ and $\mathbf{Q}$ are the same distribution, i.e., $\mathbf{P}=\mathbf{Q}$.
* **In general:** $\text {KL}(\mathbf{P}, \mathbf{Q}) \nleq \text {KL}(\mathbf{P}, \mathbf{R}) + \text {KL}(\mathbf{R}, \mathbf{Q}) $

Not a distance.

This is called a divergence.

Asymmetry is the key to our ability to estimate it.

$\theta^*$ Is the unique minimizer of $\theta \mapsto \text{KL}(\mathbb{P}_{\theta^*}, \mathbb{P}_\theta)$.

> **Why does the KL divergence take only non-negative values?**
>
> Here is the proof of the positive semi-definiteness of the KL-divergence. For simplicity, we only prove the case when the two distributions are given by pdfs.
>
> $\text {KL}(\mathbf{P}_{X}, \mathbf{P}_{Y})\geq 0$ for all distributions $\mathbf{P}_ Y$ and $\mathbf{P}_ X$ (positive semi-definiteness).
>
> **Proof.** The main idea is to use **Jensen's inequality**.
>
> Let $p_X, p_Y$ (with suitable condition) be the PDFs defining the distribution $\mathbf{P}_X$ and $\mathbf{P}_Y$ respectively. Define another random variable $Z = {p _Y(X)\over  p_X(X)}$, which is a function of the random variable $X$. Observe that the function $-\ln$ is convex, then Jensen's inequality gives
> $$
> \begin{aligned}
> \text {KL}\left(\mathbf{P}_{X}, \mathbf{P}_{Y}\right)\, &=\, \mathbb E_ X\left[-\ln (Z)\right]  \geq -\ln \left(\mathbb E_ X[Z]\right)\qquad (\text {Jensen's Inequality})\\
> &= -\ln \left(\mathbb E_ X\left[\frac{p_{Y}(X)}{p_{X}(X)}\right]\right)\\
> &\geq -\ln(1) = 0
> \end{aligned}
> $$

## 6. Estimating KL Divergence

$$
\begin{aligned}
\text{KL}(\mathbb{P}_{\theta^*},\mathbb{P}_{\theta}) &= \mathbb{E}_{\theta^*}\left[ \log\left( {p_{\theta^*}(X) \over p_\theta(X)}\right) \right]\\
&= \mathbb{E}_{\theta^*}[\log p_{\theta^*}(X)] - \mathbb{E}_{\theta^*}[\log p_\theta(X)]
\end{aligned}
$$

So the function $\theta \mapsto \text{KL} (\mathbb{P}_{\theta^*},\mathbb{P}_{\theta})$ is of the form: (since the first term dose not depend on $\theta$.)
$$
\text{"constant"} - \mathbb{E}_{\theta^*}[\log p_\theta(X)]
$$
Can be estimated: $\mathbb{E}_{\theta^*}[h(X)] \rightarrow {1\over n} \sum^n_{i=1}h(X_i)$ (by LLN)
$$
\widehat{\text{KL}}(\mathbb{P}_{\theta^*},\mathbb{P}_{\theta}) = \text{"constant"} - {1\over n} \sum^n_{i=1} \log p_\theta(X_i)
$$
Therefore, as shown above, while we cannot find $\theta$ that minimizes $\text {KL}(P_{\theta ^*}, P_\theta )$, we can find $\theta$ that minimizes $\widehat{\text{KL}}(\mathbb{P}_{\theta^*},\mathbb{P}_{\theta}) $.

**Remark:**

Note that in general, it is **easier to build an estimator for the KL divergence than it is to build an estimator for the total variation distance**. The total variation distance is not an expectation with respect to either of the probability measures. Therefore there is no natural way to estimate it without requiring an estimate of the true parameter $\theta^*$. The KL divergence, by contrast, is an expectation of some function with respect to one of the probability measures. This means that it can be estimated naturally by replacing the expectation with a sample average. Note that this application of the law of large numbers does not require knowledge of the true parameter value, only a random sample generated from the true distribution.

## 7. Parameter Estimation via KL Divergence - Maximum Likelihood

$$
\widehat{\text{KL}}(\mathbb{P}_{\theta^*},\mathbb{P}_{\theta}) = \text{"constant"} - {1\over n} \sum^n_{i=1} \log p_\theta(X_i)\\
\begin{aligned}
\min_{\theta \in \Theta} \widehat{\text{KL}}(\mathbb{P}_{\theta^*},\mathbb{P}_{\theta}) &\iff \min_{\theta \in \Theta} - {1\over n} \sum^n_{i=1} \log p_\theta(X_i)\\
& \iff\max_{\theta \in \Theta} \log\left[ \prod_{i = 1}^n p_\theta (X_i)\right]\\
&\iff \max_{\theta \in \Theta} \prod^n_{i=1} p_\theta(X_i)
\end{aligned}
$$

This is the **maximum likelihood principle**.

The quantity 
$$
\hat{\theta }_ n := \text {maximizer of} \,  \,  \prod _{i = 1}^ n p_\theta (X_ i)
$$
is referred to as the **maximum likelihood estimator**. Note that this is the same as the estimator
$$
\hat{\theta }_ n := \text {minimizer of } \,  \,  - \displaystyle \frac{1}{n} \sum _{i = 1}^ n \ln (p_\theta (X_ i))
$$
Under certain technical conditions, the maximum likelihood estimator is guaranteed to **(weakly) converge** to the true parameter $\theta^*$.

> #### Exercise 47
>
> Consider the optimization problem in which we minimize the KL divergence between $P_{\theta ^*}$, the true distribution, and $P_{\theta}$. Formally, we want to solve
> $$
> \min _{\theta \in \mathbb {R}} \text {KL}(P_{\theta ^*}, P_{\theta }).
> $$
> We are not so much interested in the minimum value attained by the objective function $\text {KL}(P_{\theta ^*}, P_{\theta })$, but rather the value of $\theta$ where the minimum is attained. We refer to such a $\theta$ as a **minimizer**.
>
> Let's suppose that there is a unique minimizer for the above optimization problem - i.e., if $m$ is the minimum value of $\text {KL}(P_{\theta ^*}, P_{\theta })$, there is only one point $\theta_{\min}$ such that
> $$
> m = \text {KL}(P_{\theta ^*}, P_{\theta _{\min } }).
> $$
> For which $\theta$ is the minimum value of $\text {KL}(P_{\theta ^*}, P_{\theta })$ attained? (Equivalently, what is $\theta_\min$?)
>
> **Answer**: $\theta^*$
>
> **Solution**:
>
> The RHS is achieved if we set $\theta = \theta^*$: $\text {KL}(P_{\theta ^*}, P_{\theta ^*}) = 0$. Since the minimizer is unique by assumption, we conclude that the minimum value is attained at $\theta = \theta^*$.
>
> **Remark:** The assumption that there is a unique minimizer holds if we are given that the parameter $\theta$ is identified. Here is why: since KL divergence is definite, $\text {KL}(P_{\theta ^*}, P_\theta ) = 0$ if and only if $P_{\theta ^*}$ and $P_{\theta}$ are the same distribution. And if $\theta$ is identified, this implies that $\theta = \theta^*$.

## 8. Likelihood of a Discrete Distribution

Let $(E, (\mathbb{P}_{\theta \in \Theta}))$ be a statistical model associated with a sample of i.i.d. r.v. $X_1, ..., X_n$. Assume that $E$ is discrete (i.e. finite or countable).

**Definition**

The *likelihood* of the model is the map $L_n$ (or just $L$) defined as:
$$
\begin{aligned}
L_n : E^n \times \Theta &\rightarrow \R\\
(x_1,..., x_n, \theta) &\mapsto \mathbb{P}_\theta [X_1 = x_1, ..., X_n = x_n]
\end{aligned}
$$
**Example 1: Likelihood for the Bernoulli model**

If $X_1, ..., X_n \stackrel{iid}{\sim} \mathsf{Ber}(p)$ for some $p\in(0,1)$:

* $E = \{0,1\}$

* $\Theta = (0,1)$

* $\forall(x_1, ...,x_2) \in \{0,1\}^n, \quad \forall p \in (0,1)$,
  $$
  \begin{aligned}
  L(x_1,...,x_n,p) &= \prod^n_{i=1}\mathbb{P}_p[X_i = x_i]\\
  &= \prod^n_{i=1} p^{x_i} (1-p)^{1-x_i}\\
  &= p^{\sum\limits^n_{i=1}x_i} (1-p)^{n - \sum\limits^n_{i=1}x_i}
  \end{aligned}
  $$

* Alternatively, if we use the expression $f(x) = xp + (1 - x)(1 - p)$ for the PMF of $\text {Ber}(p)$, then
  $$
  L(x_1, \ldots , x_ n, p) = \prod _{i = 1}^ n \left( x_ i p + (1 - x_ i) (1 - p) \right)
  $$
  is, by definition, the likelihood. 

**Example 2: Likelihood of a Poisson Statistical Model**

If $X_1, ..., X_n \stackrel{iid}{\sim} \mathsf{Poiss}(\lambda)$ for some $\lambda > 0$:

* $E = \N$

* $\Theta = (0, \infty)$

* $\forall (x_1, ..., x_n) \in \N^n, \forall \lambda > 0$,
  $$
  L_ n(x_1, \ldots , x_ n, \lambda ) = \prod _{i = 1}^ n e^{-\lambda } \frac{\lambda ^{x_ i}}{{x_ i}!} = e^{-n \lambda } \frac{\lambda ^{\sum _{i = 1}^ n x_ i}}{x_1 ! \cdots x_ n !}.
  $$

> #### Exercise 48
>
> Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Poiss}(\lambda ^*)$ for some unknown $\lambda^* \in (0,\infty)$. You construct the associated statistical model $(E, \{ \text {Poiss}(\lambda )\} _{\lambda \in \Theta })$ where $E$ and $\Theta$ are defined as in the answers to the previous question.
>
> 1. Suppose you observe two samples $X_1 = 1, X_2 = 2.$ What is  $L_2(1, 2, \lambda )$? Express your answer in terms of $\lambda$.
> 2. Next, you observe a third sample $X_3 = 3$ that follows $X_1 = 1$ and $X_2 = 2$. What is $L_3(1,2,3,\lambda)$?
> 3. Suppose your data arrives in a different order: $X_1 = 2, X_2 = 3,X_3=1$. What is $L_3(2,3,1,\lambda)$?
>
> **Answer**:
>
> 1. $L_2(1,2,\lambda ) = e^{-2 \lambda } \frac{\lambda ^{3}}{2}.$
> 2. $L_3(1,2,3,\lambda )  = e^{-3 \lambda } \frac{\lambda ^6}{12}.$
> 3. $L_3(2,3,1, \lambda ) = e^{-3 \lambda } \frac{\lambda ^6}{12}.$
>
> **Solution:**
>
> 1. Plug in $n=2,x_1=1,x_2=2$:
>    $$
>    L_2(1,2,\lambda ) = e^{-2 \lambda } \frac{\lambda ^{1 + 2}}{2! 1!} = e^{-2 \lambda } \frac{\lambda ^{3}}{2}.
>    $$
>
> 2. We can simply evaluate the density of a Poisson at the observation:
>    $$
>    P(X_3 = 3) = e^{-\lambda } \frac{\lambda ^3}{3!}, \quad X \sim \text {Poiss}(\lambda )
>    $$
>    and multiply this by the previous response:
>    $$
>    L_3(1,2,3,\lambda ) = e^{-\lambda } \frac{\lambda ^3}{3!} L_2(1,2,\lambda ) = e^{-3 \lambda } \frac{\lambda ^6}{12}.
>    $$
>    **Remark 1:** Observe that we can compute the likelihood sequentially as the data arrives, updating it in the previous fashion after each new observation.
>
> 3. Similarly,
>    $$
>    L_3(2,3,1, \lambda ) = e^{-3 \lambda } \frac{\lambda ^6}{12}.
>    $$
>    **Remark 2**: Observe that the likelihood of observations $X_1 = x_1, \ldots , X_ n =x_ n$ is independent of the *order* in which these observations arrive.

> #### Exercise 49
>
> Consider a discrete statistical model $M_1 = (\mathbb {Z}, \{ \mathbf{P}_\theta \} _{\theta \in \mathbb {R}})$, Let $p_\theta$ denote the PMF of $\mathbf{P}_\theta$. Assume that $p_\theta$ vary continuously with the parameter $\theta$ for each fixed $x \in E$. Let $x_1, ..., x_n$ be fixed natural numbers. Let $(L_1)_ n$ denote the likelihood of the discrete model $M_1$. Keeping $x_1, ..., x_n$ fixed, let's think of $(L_1)_ n(x_1, \ldots , x_ n, \theta )$ as a function of $\theta$.
>
> True of False: The map $\theta \mapsto (L_1)_ n(x_1, \ldots , x_ n, \theta )$ is a continuous function of $\theta$.
>
> **Answer**: True.
>
> **Solution**:
>
> Observe that 
> $$
> (L_1)_ n(x_1, \ldots , x_ n, \theta ) = \prod _{i = 1}^ n p_\theta (x_ i)
> $$
> We are given that $p_\theta$ is a continuous function of the parameter $\theta \in \R$. Since products of continuous functions are continuous, this implies the map is a continuous function of the parameter $\theta \in \R$.

**Likelihood Properties**:

* Symmetric: we can take the product $\prod _{i = 1}^ n p_\theta (x_ i)$ in any order, and the result will still be the same.

* Can be updated sequentially as new samples are observed.

* The likelihood of a discrete statistical model can be continuous. Considering the likelihood of a Bernoulli:
  $$
  L(x_1, \ldots , x_ n, p) = \prod _{i = 1}^ n p^{x_ i}(1 - p)^{1 - x_ i} = p^{\sum _{i = 1}^ n x_ i} (1 -p)^{n - \sum _{i = 1}^ n x_ i}.
  $$
  we can clearly see that the above varies continuously as a function of the *parameter*. This is also true for a host of other discrete models (for example, the Poisson model).

## 9. Likelihood of a Continuous Distribution

Let $(E, (\mathbb{P}_{\theta \in \Theta}))$ be a statistical model associated with a sample of i.i.d. r.v. $X_1, ...,X_n$. Assume that all the $\mathbb{P}_\theta$ have density $f_\theta$.

**Definition:**

The likelihood of the model is the map $L$ defined as:
$$
\begin{aligned}
L : E^n \times \Theta &\rightarrow \R\\
(x_1,..., x_n, \theta) &\mapsto \prod^n_{i=1} f_\theta(x_i)
\end{aligned}
$$
**Example 3: Likelihood of a Gaussian Model**

If $X_1, ..., X_n \stackrel{iid}{\sim} \mathcal{N}(\mu, \sigma^2)$, for some $\mu \in \R,\sigma^2 > 0$,

* $E = \R$
* $\Theta = \R \times (0,\infty)$

* The PDF is
  $$
  f_{\mu, \sigma^2} (x) = {1\over \sigma \sqrt{2\pi}} \exp \left(-{1\over 2\sigma^2} (x-\mu)^2 \right)\\
  $$

* $\forall (x_1, ..,x_n) \in \R^n, \quad \forall(\mu,\sigma^2) \in \R \times (0,\infty)$, the likelihood is
  $$
  L(x_1, ...,x_n; \mu, \sigma^2)=\prod^n_{i=1} f_{\mu, \sigma^2} (x_i) = {1\over (\sigma \sqrt{2\pi})^{n}} \exp\left( -{1\over 2\sigma^2} \sum^n_{i=1} (x_i - \mu)^2 \right)
  $$

**Example 4: Likelihood of an Exponential Distribution**

Let $(E, (\mathbb{P}_\theta)_{\theta \in \Theta})$ be a statistical model associated with $X_1, ...,X_n \sim \mathsf{Exp}(\lambda), \lambda > 0$.

* $E \in (0,\infty)$

* $\Theta \in (0,\infty)$

* The PDF is
  $$
  f_\lambda(x) = \lambda e^{-\lambda x}, x> 0
  $$

* The likelihood is
  $$
  \prod^n_{i=1} f_\lambda(x_i) = \lambda^n e^{-\lambda \sum\limits^n_{i=1}x_i} \prod^n_{i=1} \mathbf{1}_{x_i > 0}
  $$
  Equivalently,
  $$
  \prod^n_{i=1} f_\lambda(x_i) = \lambda^n e^{-\lambda \sum\limits^n_{i=1}x_i} \mathbf{1}_{\min\limits_i x_i > 0}
  $$

* In general, the likelihood is usually written as
  $$
  \prod^n_{i=1} f_\lambda(x_i) = \lambda^n e^{-\lambda \sum\limits^n_{i=1}x_i}
  $$
  Note that it does not matter if this indicator does not depend on the parameter.

**Example 5: Likelihood of a Uniform Distribution**

Let $(E,(\mathbb{P}_\theta)_{\theta \in \Theta})$ be a statistical model associated with $X_1,. ..,X_n \stackrel{iid}{\sim}\mathsf{Unif}[0,b]$ for some $b > 0$.

* $E \in [0,\infty)$

* $\Theta \in [0,\infty)$

* The PDF is 
  $$
  f_b(x) = {1\over b} \mathbf{1}_{(0\leq x\leq b)}
  $$

* The likelihood is 
  $$
  \begin{aligned}
  \prod^n_{i=1}f_b(x) &= {1\over b^n} \prod^n_{i=1} \mathbf{1}_{(0\leq X_i \leq b)}\\
  &= {1\over b^n} \mathbf{1}_{(\min\limits_i X_i \geq 0)} \mathbf{1}_{(\max\limits_i X_i \leq b)}
  \end{aligned}
  $$

* In general, the likelihood is usually written as
  $$
  \prod^n_{i=1}f_b(x) 
  = {1\over b^n}  \mathbf{1}_{(\max\limits_i X_i \leq b)}
  $$


# Lecture 10. Consistency of MLE, Covariance Matrices, and Multivariate Statistics

There are 8 topics and 5 exercises.

## 1. Maximum Likelihood Estimator of Uniform Statistical Model

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Unif}[0, \theta ^*]$ where $\theta^*$ is an unknown parameter. We constructed the associated statistical model $(\mathbb {R}_{\geq 0}, \{  \text {Unif}[0, \theta ]\} _{\theta > 0})$ (where $\R_{\geq 0}$ denote the nonnegative reals).

For any $\theta > 0$, then density of $\mathsf{Unif}[0,\theta]$ is given by $f(x) = \frac{1}{\theta } \mathbf{1}(x \in [0, \theta ])$. 

Hence we can use the product formula and compute the likelihood to be
$$
L_ n(x_1, \ldots , x_ n, \theta ) = \prod _{i =1}^ n \left(\frac{1}{\theta } \mathbf{1}(x_ i \in [0, \theta ] )\right) = \frac{1}{\theta ^ n} \mathbf{1}(x_ i \in [0, \theta ]\,  \,  \forall \,  1 \leq i \leq n).
$$
For the fixed values $(1,3,2,2.5, 5,0.1)$ (think of these as observations of random variables $X_1, ...,X_6$), the MLE of $\theta$ is $5$.

Observe that
$$
L_6( 1,3,2,2.5, 5,0.1, \theta ) = \frac{1}{\theta ^6} \mathbf{1}(\{ 1,3,2,2.5, 5,0.1\}  \subset [0, \theta ]).
$$
If $\theta < \max \{ 1,3,2,2.5, 5,0.1\}$, then we have $\{ 1,3,2,2.5, 5,0.1\}  \not\subset [0, \theta ]$. By the definition of the indicator function, this means $L_6( 1,3,2,2.5, 5,0.1, \theta ) = 0$ for $\theta < \max \{ 1,3,2,2.5, 5,0.1\}  = 5$. Hence, when maximizing $L_6( 1,3,2,2.5, 5,0.1, \theta )$, we need to consider $\theta \in [5,\infty)$. Restricted to this interval, we observe that
$$
L_6( 1,3,2,2.5, 5,0.1, \theta ) = \frac{1}{\theta ^ n}.
$$
The above is a decreasing function on $[5,\infty)$, so the maximum is attained when $\theta = \max \{ 1,3,2,2.5, 5,0.1\}  = 5.$

**Remark 1:** In general, the MLE for $\theta^*$ in this uniform statistical model is
$$
\widehat{\theta _ n}^{MLE} = \max _{1 \leq i \leq n} X_ i.
$$
**Remark 2:** Sometimes the likelihood equals to zero, so we cannot take the log, or the likelihood is not differentiable, so we cannot take the derivative. But we can plot it and find the MLE.

## 2. Consistency of Maximum Likelihood Estimator

Given i.i.d. samples $X_1, \ldots , X_ n\sim \mathbf{P}_{\theta ^*}$ and an associated statistical model $\left(E,\{ \mathbf{P}_\theta \} _{\theta \in \Theta }\right),\,$ the maximum likelihood estimator $\widehat{\theta }_ n^{\text {MLE}}$ of $\theta^*$ is a consistent estimator under mild regularity conditions (e.g. continuity in $\theta$ of the pdf $p_\theta$ almost everywhere). i.e.
$$
\widehat{\theta }_ n^{\text {MLE}}\xrightarrow[n\rightarrow \infty]{\mathbb{P}}\theta ^*.
$$
This is because for all $\theta \in \Theta$
$$
{1\over n} \log L(X_1, ...,X_n, \theta) \xrightarrow[n\rightarrow \infty]{\mathbb{P}}\text{"constant" } - \text{KL}(\mathbf{P}_\theta^*, \mathbf{P}_\theta)
$$
Moreover, the minimizer of the RHS is $\theta^*$ if the parameter is **identifiable**.

The RHS can be plotted as a concave curve:

![images_u3s03_plot_KL](../assets/images/images_u3s03_plot_KL.png)

Technical conditions allow to transfer this convergence from $y$-axis to and $x$-axis convergence - a convergence of the minimizer. 

Note that this is true even if the parameter $\theta$ is a vector in a **higher dimensional parameter space** $\Theta$, and $\widehat{\theta }_ n^{\text {MLE}}$ is a **multivariate random variable**, e.g. if $\theta =\begin{pmatrix} \mu \\ \sigma ^2\end{pmatrix}\in \mathbb {R}^2$ for a Gaussian statistical model.

* **Multivariate Random Variables**

  A **multivariate random variable** , or a **random vector**, is a vector-valued function whose components are (scalar) random variables on the same underlying probability space. More specifically, a random vector $\mathbf X= \left(X^{(1)},\dots ,X^{(d)}\right)^ T$ of dimension $d \times 1$  is a vector-valued function from a probability space $\Omega$ to $\R^d$:
  $$
  \begin{aligned}
  \mathbf{X}\, \, :\,  \Omega &\rightarrow \R^d\\
  \omega &\mapsto \begin{pmatrix}  X^{(1)}(\omega ) \\ X^{(2)}(\omega )\\ \vdots \\ X^{(d)}(\omega )\end{pmatrix}
  
  \end{aligned}
  $$
  where each $X^{(k)}$ is a (scalar) random variable on $\Omega$. $k$ denotes the component of a random vector.

  The **probability distribution** of a random vector $X$ is the **joint distribution** of its components $X^{(1)},\, \ldots ,\, X^{(d)}$.

  The **cumulative distribution function (cdf)** of a random vector $X$ is defined as
  $$
  \begin{aligned}
   F: \mathbb {R}^ d &\rightarrow [0,1]\\
   \mathbf{x} &\mapsto \mathbf{P}(X^{(1)}\leq x^{(1)},\ldots ,\, X^{(d)}\leq x^{(d)})
  \end{aligned}
  $$

* **Convergence in Probability in Higher Dimension**

  To make sense of the consistency statement $\, \widehat{\theta }_ n^{\text {MLE}}\xrightarrow [(p)]{n\to \infty } \theta ^*\,$ where the MLE $\widehat{\theta }_ n^{\text {MLE}}$ is a random vector, we need to know that convergence in probability in higher dimensions means convergence in probability for **each** **component**.

  Let $X_1, X_2, ...$ be a sequence of random vectors of size $d \times 1$ i.e. $\, \mathbf{X}_ i\, =\,  \begin{pmatrix}  X_ i^{(1)} \\ \vdots \\ X_ i^{(d)}\end{pmatrix}$.

  Let $\mathbf{X}\, =\,  \begin{pmatrix}  X^{(1)} \\ \vdots \\ X^{(d)}\end{pmatrix}$ be another vector of size $d\times 1$.

  Then
  $$
  \mathbf{X}_ n\xrightarrow [n\to \infty ]{(p)} \mathbf{X} \iff X_ n^{(k)} \xrightarrow [n\to \infty ]{(p)} X^{(k)}\, \, \text {for all } 1\leq k\leq d.
  $$
  In other words, the sequence $X_1, X_2, ... $ converges in probability to $X$ if and only if each component sequence $\, X_1^{(k)},X_2^{(k)},\ldots \,$ converges in probability to $\, X^{(k)}$.

  Hence, for example, in the Gaussian model $\left((-\infty ,\infty ),\{ \mathcal{N}\left(\mu , \sigma ^2\right)\} _{(\mu , \sigma ^2)\in \mathbb {R}\times \mathbb {R}_{>0}}\right)$, consistency of the MLE $\widehat{\theta }_ n^{\text {MLE}}=\begin{pmatrix} \widehat{\mu }\\ \widehat{\sigma ^2}\end{pmatrix}$ means that $\widehat{\mu}$ and $\widehat{\sigma^2}$ are consistent estimators of $\mu^*$ and $(\sigma^2)^*$, respectively.

> #### Exercise 55
>
> Let $\, X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Unif}[0, \theta ^*]$ where $\theta^*$ is an unknown parameter. We construct the associated statistical model $(\mathbb {R}_{\geq 0}, \{  \text {Unif}[0, \theta ]\} _{\theta > 0})$. Consider the maximum likelihood estimator $\widehat{\theta }_ n^{\text {MLE}} = \max _{i=1,\dots ,n}X_ I$. What can we say about $\widehat{\theta }_ n^{\text {MLE}}$.
>
> **Answer**:
>
> $\max _{i=1,\dots ,n} X_ I$ is a consistent estimator.
>
> For any $0 < \epsilon \leq \theta^*$, $\mathbf{P}\left(|\max _{i=1,\dots ,n}X_ i - \theta ^*| \ge \epsilon \right) \rightarrow 0$ as $n \rightarrow \infty$.
>
> For any $0 < \epsilon \leq \theta^*$, $\mathbf{P}\left(|\max _{i=1,\dots ,n}X_ i - \theta ^*| \ge \epsilon \right) = \left(\frac{\theta ^*-\epsilon }{\theta ^*}\right)^ n$.
>
> **Solution:**
> $$
> \begin{aligned}
> \mathbf{P}\left(\left|\max _{i=1,\dots ,n}X_ i - \theta ^*\right| \ge \epsilon \right) &= \mathbf{P}\left(\theta ^*-\max _{i=1,\dots ,n}X_ i \ge \epsilon \right)\\
> &= \mathbf{P}\left(\max _{i=1,\dots ,n}X_ i \le \theta ^*-\epsilon \right)\\
> &= \left(\frac{\theta ^*-\epsilon }{\theta ^*}\right)^ n\, \longrightarrow \,  0 \text { as } n \rightarrow \infty .
> \end{aligned}
> $$

## 3. Covariance

We can say $\overline{X}_n$ is asymptotically normal and $\widehat{S}_n$ is asymptotically normal, how about asymptotic normality of a maximum likelihood vector $\hat{\theta} = \begin{pmatrix} \overline{X}_n\\ \widehat{S}_n \end{pmatrix}$?

In general, when $\theta \subset \R^d, d\geq2$, its coordinates are not necessarily **independent**.

The covariance between two random variables $X$ and $Y$ is
$$
\begin{aligned}
\mathsf{Cov}(X,Y) &= \mathbb{E}\left[(X -\mathbb{E}[X])(Y -\mathbb{E}[Y])\right]\\
&= \mathbb{E}\left[XY - X\mathbb{E}[Y] - \mathbb{E}[X]Y + \mathbb{E}[X]\mathbb{E}[Y]\right]\\
&=\mathbb{E}[X Y] - \mathbb{E}[X]\mathbb{E}[Y]\\
&= \mathbb{E}[X (Y-\mathbb{E}[Y])] \quad \text{or }\quad \mathbb{E}[(X-\mathbb{E}[X])Y]
\end{aligned}
$$
This means if each of the random variable is center, we only need to compute the expectation of the product.

**Bilinearity of Covariance**

Let $X,Y,Z$ be random variables and $a,b$ be constant, then $\textsf{Cov}(aX + bY, Z) = a\textsf{Cov}(X,Z) + b\textsf{Cov}(Y,Z)$.

This can be seen by using the trick of centering only $Z$ when computing covariance. First, note that $\textsf{Cov}(aX,Z) = \mathbb E[(aX)(Z-\mu _ Z)] = a\mathbb E[(X)(Z-\mu _ Z)] = a\textsf{Cov}(X,Z)$. Then
$$
\begin{aligned}
\textsf{Cov}(aX+ bY, Z) &=  \mathbb E[(aX+ bY)(Z - \mu _ Z)]\\
&=\mathbb E[(aX)(Z)] - \mu _ Z \mathbb E[aX] + \mathbb E[(bY)(Z)] - \mu _ Z \mathbb E[bY]\\
&= \mathbb E[(aX)(Z)] - \mu _ Z \mathbb E[aX] + \mathbb E[(bY)(Z)] - \mu _ Z \mathbb E[bY]\\
&= a\textsf{Cov}(X,Z) + b\textsf{Cov}(Y,Z)
\end{aligned}
$$

## 4. Covariance and Independence

Properties of covariance:

* $\mathsf{Cov}(X,X) = \mathsf{Var}(X)$
* $\mathsf{Cov}(X,Y) = \mathsf{Cov}(Y,X)$
* If $X$ and $Y$ are independent, $\mathsf{Cov}(X,Y)=0$, but **the converse is not true.** 

In general, the converse is not true, except if $(X,Y)^T$ is a **Gaussian vector**, i.e. $\alpha X + \beta Y$ is Gaussian for all $(\alpha, \beta)\in \R^2$ and $(\alpha, \beta) \notin \{(0,0)\}$.

* **Prove**: $(X,Y)^T$ is not a Gaussian, and the converse is not true:

  Take $X \sim \mathcal{N}(0,1), \quad B\sim \mathsf{Ber}(1/2), \quad R=2B-1 \sim \mathsf{Rad}(1/2)$. $X$ and $B$ are independent. Then
  $$
  Y =R \cdot X \sim \mathcal{N}(0,1)
  $$
  But taking $\alpha = \beta = 1$, we get
  $$
  X + Y = \begin{cases} 2X & \text{with prob. }1/2\\ 0 & \text{with prob. }1/2 \end{cases}
  $$
  Actually $\mathsf{Cov}(X,Y)=0$ but they are not independent: $|X| = |Y|$.
  $$
  \begin{aligned}
  \mathsf{Cov}(X,Y) &= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]\\
  &= \mathbb{E}[XY] - 0\\
  &= \mathbb{E}[XRX]\\
  &= \mathbb{E}[RX^2]\\
  &= \mathbb{E}[R] \mathbb{E}[X^2]\\
  &= 0
  \end{aligned}
  $$
  Note that $X$ is Gaussian, $Y$ is Gaussian, but the vector $(X,Y)$ is not Gaussian because a particular linear combination of the two $X+Y$ is even not continuous.

> #### Exercise 56
>
> Let $(X_1,Y_1), (X_2,Y_2),\ldots ,(X_ n,Y_ n) \stackrel{\text {iid}}{\sim } (X,Y)$ with $\mathbb E[X]=\mu _ X,\mathbb E[Y]=\mu _ Y$, and $\mathbb E[XY]=\mu _ {XY}$. That is, each random variable pair $(X_ i,Y_ i)$ has the same distribution as the random variable pair $(X,Y)$, and the pairs are independent of one another.
>
> Estimating the covariance of $X$ and $Y$ is a useful exercise because non-zero covariance implies statistical dependence of $X$ and $Y$. In this problem, we study one way to obtain an unbiased estimator for $\mathsf{Cov}(X,Y)$.
>
> Consider the following estimator of the covariance:
> $$
> \widetilde{S}_{XY} = \frac{1}{n}\left(\sum _{i=1}^ n (X_ i - \overline{X}_ n)(Y_ i - \overline{Y}_ n )\right)
> $$
> where $\overline{X}_n$ and $\overline{Y}_n$ denote the sample means estimators of $\mu_X$ and $\mu_Y$.
>
> 1. What is $\mathbb E\left[\frac{\left(\sum _{i=1}^ n X_ i\right) \left(\sum _{j=1}^ n Y_ j\right)}{n}\right]$? 
> 2. What is $\mathbb E[\widetilde{S}_{XY}]$?
> 3. Is $\widetilde{S}_{XY}$ an estimator of $\textsf{Cov}(X,Y)$?
>
> **Solution**:
>
> 1. Since $X_i$ And $Y_i$ are independent whenever $i \neq j$.
>    $$
>    \mathbb E\left[\frac{(\sum _{i=1}^ n X_ i)(\sum _{j=1}^ n Y_ j)}{n}\right] = \frac{1}{n}\mathbb E\left[\sum _{i=1}^ nX_ iY_ i + \sum _{i=1}^ n\sum _{j=1,j\neq i}^{n}X_ i Y_ j\right] = \mu _{XY}+(n-1)\mu _ X\mu _ Y
>    $$
>
> 2. $$
>    \begin{aligned}
>     \mathbb E[\widetilde{S}_{XY}] &= \frac{1}{n}\mathbb E\left[\sum _{i=1}^ n \left(X_ i - \overline{X}_ n\right)\left(Y_ i - \overline{Y}_ n \right)\right]\\
>     &=\frac{1}{n}\mathbb E\left[ \sum _{i=1}^ n X_ i Y_ i - \frac{\sum _{i=1}^ n X_ i}{n}\sum _{j=1}^ n Y_ j - \frac{\sum _{i=1}^ n Y_ i}{n}\sum _{j=1}^ n X_ j + \frac{\sum _{i=1}^ n X_ i \sum _{j=1}^ n Y_ j}{n} \right]\\
>     &= \frac{1}{n}\mathbb E\left[ \sum _{i=1}^ n X_ i Y_ i - \frac{\sum _{i=1}^ n X_ i \sum _{j=1}^ n Y_ j}{n} \right].
>    \end{aligned}
>    $$
>
>    Use the result in (1), we get
>    $$
>    \begin{aligned}
>     \mathbb E[\widetilde{S}_{XY}] &= \frac{1}{n}\left[n\mu _{XY} - \left( \mu _{XY}+(n-1)\mu _ X\mu _ Y\right)\right]\\
>     &=\frac{n-1}{n}\left[\mu _{XY} - \mu _ X \mu _ Y\right]\\
>     &= \frac{n-1}{n}\textsf{Cov}(X,Y).
>    \end{aligned}
>    $$
>
> 3. From the above, we can see that the estimator is **biased** because $\mathbb E[\widetilde{S}_{XY}] \ne \textsf{Cov}(X,Y)$.
>
>    However, the bias can be fixed by multiplying $\widetilde{S}_{XY}$ by $\frac{n}{n-1}$ to obtain the following unbiased estimator of $\mathsf{Cov}(X,Y)$:
>    $$
>    \widehat{S}_{XY} = \frac{1}{n-1}\left[\sum _{i=1}^ n \left(X_ i - \overline{X}_ n\right)\left(Y_ i - \overline{Y}_ n \right)\right].
>    $$

## 5. Covariance Matrices

Let $\mathbf X= \begin{pmatrix} X^{(1)}\\ \vdots \\ X^{(d)}\end{pmatrix}\,$ be a random vector of size $d \times 1$. Let $\mu \triangleq \mathbb E[\mathbf X]$ denote the **entry-wise** mean, i.e.
$$
\mathbb E[\mathbf X]=\begin{pmatrix} \mathbb E[X^{(1)}]\\ \vdots \\ \mathbb E[X^{(d)}]\end{pmatrix}.
$$
The covariance matrix $\Sigma$ can be written as
$$
\Sigma = \mathbf{Cov}(\mathbf{X}) = \mathbb{E}[(\mathbf{X} - \mu)(\mathbf{X}-\mu)^T]
$$
This is a matrix of size $d \times d$.

The term on the $i$-th row and $j$-th column is 
$$
\Sigma_{ij} = \mathbb{E}[(X^{(i)} - \mathbb{E}(X^{(i)}))(X^{(j)} - \mathbb{E}(X^{(j)}))] = \mathsf{Cov}(X^{(i)},X^{(j)})
$$
**Affine transformation**:

In particular, on the diagonal, we have
$$
\Sigma_{ij} = \mathsf{Cov}(X^{(i)},X^{(j)}) = \mathsf{Var}(X^{(i)})
$$
Recall that for $X \in \R, \mathsf{Var}(aX+b) = a^2 \mathsf{Var}(X).$ Actually, if $X \in \R^d$ and $A,B$ are matrices
$$
\mathsf{Cov}(AX+B) = \mathsf{Cov}(AX) = A \mathsf{Cov}(X)A^T=A \Sigma A^ T.
$$
**Properties:**

* Let $\mathbf{X}$ be a random vector and let $\mathbf Y= \mathbf X+B$, where $B$ is a constant vector. Let $\mu_{\mathbf{X}}$ be the mean vector of $\mathbf{X}$ and let $\Sigma_{\mathbf{X}}$ be the covariance matrix of $X$.

  * **$\Sigma_{\mathbf{Y}}$ and $\Sigma_{\mathbf{X}}$ are the same for all vector $B$, and they have the same size.**

  Note that $\mathbb E[\mathbf X+B] = \mu _{\mathbf X} + B$ for any vector $B$.
  $$
  \Sigma _{\mathbf Y} = \mathbb E\left[(\mathbf X+B - \mu _{\mathbf X} - B)(\mathbf X+B - \mu _{\mathbf X} - B)^ T\right] = \Sigma _{\mathbf X}
  $$

* Let $\mathbf X$ be a random vector of size $d \times 1$ and let $\mathbf Y= A\mathbf X+B$, where $A$ is a constant matrix of size $n \times d$ and $B$ is a constant vector of size $n \times 1$. Let $\mu_{\mathbf{X}}$ be the mean vector of $\mathbf{X}$ and let $\Sigma _{\mathbf X}$ be the covariance matrix of $\mathbf{X}$. Let $\mu _{\mathbf Y}$ be the mean vector of $\mathbf Y$ and let $\Sigma _{\mathbf Y}$ be the covariance matrix of $\mathbf Y$.

  * **$\Sigma _{\mathbf Y}$ is the same as covariance matrix of $A\mathbf X$.**

  * **$\Sigma _{\mathbf Y}$ Is of size $n \times n$.**

  * **$\Sigma _{\mathbf Y}=A \Sigma _{\mathbf X}A^ T$.**

  As $\mathbf Y$ is an $n \times 1$ random vector, $\Sigma _{\mathbf Y}$ is of size $n \times n$.

  From previous problem we know that $\Sigma _{\mathbf Y}$ is the same as the covariance matrix of $A \mathbf X$. Therefore, it suffices to find this matrix, which we denote $\Sigma _{A\mathbf X}$.
  $$
  \begin{aligned}
  \Sigma _{A\mathbf X} & =\mathbb E\left[\left(A\mathbf X- A \mu _ X\right)\left(A \mathbf X- A\mu _ X\right)^ T\right]\\
  &= \mathbb E\left[A\left(\mathbf X- \mu _ X\right)\left(\mathbf X^ T A^ T - \mu _ X^ T A^ T\right)\right]\\
  &= \mathbb E\left[A\left(\mathbf X- \mu _ X\right)\left(\mathbf X- \mu _ X\right)^ TA^ T\right]\\
  &= A\mathbb E\left[\left(\mathbf X- \mu _ X\right)\left(\mathbf X- \mu _ X\right)^ T\right]A^ T\\
  &= A \Sigma _{\mathbf X} A^ T.
  \end{aligned}
  $$

> #### Exercise 57
>
> Given random variables $X^{(1)}, X^{(2)}, \ldots , X^{(d)}$, one can write down the covariance matrix $\Sigma$, where $\Sigma _{i,j} = \textsf{Cov}(X^{(i)}, X^{(j)})$.
>
> Let $X,Y$ be random variables such that
>
> * $X$ takes the values $\pm 1$ each with probability $0.5$
> * (Conditioned on $X$) $Y$ is chosen uniformly from the set $\{  -3X-1, -3X, -3X+1 \}$.
>
> What is covariance matrix $\Sigma$?
>
> **Solution:**
>
> Observe that $\mathbb E[X]$ and $\mathbb E[Y]$ are both zero, since $X$ is uniformly distributed over $\{\pm 1\}$ and $Y$ is uniformly distributed over the set $\{ -4,-3,-2,2,3,4\}$.
>
> * $\textsf{Cov}(X,X)$ is the variance of $X$, which equals $\mathbb E[X^2] - \mathbb E[X]^2 = p + (1-p) = 1.$
> * $\textsf{Cov}(Y,Y)$ is the variance of $Y$, which equals $\mathbb E[Y^2] - \mathbb E[Y]^2 = \frac{16+9+4+4+9+16}{6} = \frac{29}{3} \approx 9.67.$
> * $\textsf{Cov}(X,Y) = \textsf{Cov}(Y,X)$. Observe the joint density of $(X,Y)$ is uniform over the pairs $(1,-4), (1,-3), (1,-2), (-1,2), (-1,3), (-1,4)$. Thus, either covariance can be computed as $\mathbb E[XY] - \mathbb E[X]\mathbb E[Y] = \frac{-4-3-2-2-3-4}{6}-0 = -3.$
>
> From above, we get 
> $$
> \begin{aligned}
> \Sigma _{1,1}  &= \textsf{Cov}(X,X)=1\\
> \Sigma _{1,2}  &= \textsf{Cov}(X,Y) = -3\\
> \Sigma _{2,1}  &= \textsf{Cov}(Y,X)=-3\\
> \Sigma _{2,2}  &= \textsf{Cov}(Y,Y)=9.67
> \end{aligned}
> $$
> Therefore, 
> $$
> \Sigma = \begin{pmatrix}  \Sigma _{1,1} &  \Sigma _{1,2} \\ \Sigma _{2,1} &  \Sigma _{2,2} \end{pmatrix} = \begin{pmatrix}  1 &  -3 \\ -3 &  \tfrac {29}{3} \end{pmatrix}
> $$

## 6. Multivariate Gaussian Distribution

A random vector $\mathbf{X}=(X^{(1)},\ldots ,X^{(d)})^ T\,$ is a **Gaussian vector**, or **multivariate Gaussian or normal variable**, if any linear combination of its components is a (univariate) Gaussian variable or a constant (a "Gaussian" variable with zero variance), i.e., if $\alpha^T\mathbf X$ is (univariate) Gaussian or constant for any constant non-zero vector $\alpha \in \R^d$.

The distribution of $\mathbf{X}$, the **$d$-dimensional Gaussian or normal distribution**, is completely specified by the vector mean $\mu =\mathbb E[\mathbf{X}]= (\mathbb E[X^{(1)}],\ldots ,\mathbb E[X^{(d)}])^ T$ and the $d\times d$ covariance matrix $\Sigma$.  We write
$$
X \sim \mathcal{N}_d(\mu,\Sigma)
$$
If $\Sigma$ is invertible, then the pdf of $\mathbf{X}$ is
$$
f_{\mathbf X}(\mathbf x) = \frac{1}{\sqrt{\left(2\pi \right)^ d \text {det}(\Sigma )}}\exp \left(-\frac{1}{2}(\mathbf x-\mu )^ T \Sigma ^{-1} (\mathbf x-\mu )\right), ~ ~ ~ \mathbf x\in \mathbb {R}^ d
$$
where $\text{det}(\Sigma)$ is the determinant of the $\Sigma$, which is positive when $\Sigma$ is invertible.

If $\mu =0 $ and $\Sigma$ is the identity matrix, then $\mathbf{X}$ is called a **standard normal random vector**.

Note that when the covariant matrix $\Sigma$ is diagonal, the pdf factors into pdfs of univariate Gaussians, and hence the components are independent.

> #### Exercise 58
>
> Consider the $2$-dimensional Gaussian $\mathbf X=\begin{pmatrix} X^{(1)}\\ X^{(2)}\end{pmatrix}$ with covariance matrix $\Sigma _ X = \begin{pmatrix}  1 &  2 \\ 2 &  5\end{pmatrix}$ and mean $\Sigma _ X = \begin{pmatrix}  1 &  2 \\ 2 &  5\end{pmatrix}$.
>
> Consider the vector $\alpha = \begin{pmatrix}  1 \\ -1 \end{pmatrix}$, so that $Y = \alpha ^ T \mathbf X$ is a 1-dimensional Gaussian.
>
> What is the variance $\textsf{Var}(Y)$ of $Y$?
>
> **Answer:** $\textsf{Var}(Y)=2$
>
> **Solution:**
>
> Method 1:
>
> Since $Y = X^{(1)} - X^{(2)}$, 
> $$
> \begin{aligned}
> \textsf{Var}(Y) &=\textsf{Cov}(Y,Y) \\
> &= \textsf{Cov}(X^{(1)} - X^{(2)},X^{(1)} - X^{(2)}) \\
> &= \textsf{Cov}(X^{(1)},X^{(1)}) +\textsf{Cov}(X^{(1)},X^{(1)})  -\textsf{Cov}(X^{(1)},X^{(2)})-\textsf{Cov}(X^{(2)},X^{(1)}) \\
> & = \textsf{Var}(X^{(1)}) + \textsf{Var}(X^{(2)}) - 2\textsf{Cov}(X^{(1)},X^{(2)})\\
> &= 1+5-4 = 2.
> \end{aligned}
> $$
> Method 2:
>
> Define the matrix $\, M\triangleq \alpha ^ T=\begin{pmatrix}  1 &  -1 \end{pmatrix},\,$ and apply the formula $\Sigma _ Y = M\Sigma _\mathbf XM^ T = 2.$

**Diagonalization of the Covariance Matrix**

Let $\Sigma$ be a covariance matrix of size $d \times d$. Note that its entries are all real numbers with diagonal elements being non-negative. $\Sigma$ has the following properties

* $\Sigma$ is symmetric. That is $\Sigma = \Sigma ^ T$.
* $\Sigma$ is diagonalizable to a diagonal matrix $D$ via a transformation $D = U\Sigma U^T$, where $U$ is an orthogonal matrix (recall that a square matrix $A$ is orthogonal if $AA^T = A^TA= I$, where $I$ is the identity matrix). This implies that $\Sigma = U^TDU$.
* $\Sigma $ is positive semidefinite. That is, the diagonal matrix $D$ has diagonal entries that are all non-negative.
* $\Sigma$ Has a unique positive semidefinite square root matrix. That is, there exists a positive semi-definite matrix $\Sigma^{1/2}$ that is unique such that $\Sigma^{1/2} \cdot \Sigma^{1/2} = \Sigma$.
* If $\Sigma $ is of size $d \times d$, then it has $d$ orthonormal eigenvectors (even if there are repeated eigenvalues). Furthermore, if $U$ is a matrix with rows corresponding to the orthonormal eigenvectors, then the diagonal matrix $D = U \Sigma U^T$ contains the eigenvalues of $\Sigma$ along its diagonal. Therefore, diagonalization of a symmetric matrix involves finding its eigenvalues and the orthonormal eigenvectors.
* If $\Sigma$ is positive definite, i.e. the diagonal matrix $D = U\Sigma U^T$ has diagonal entries that are all strictly positive, then it is invertible and the inverse $\Sigma^{-1}$ satisfies the following: $\Sigma ^{-\frac{1}{2}}\cdot \Sigma ^{-\frac{1}{2}} = \Sigma ^{-1}$, where $\Sigma ^{-\frac{1}{2}}$ is the inverse of the square root of $\Sigma$.

> #### Exercise 59
>
> Recall from an earlier part of this lecture that the covariance between two random variables being 0 does not necessarily imply that the random variables are independent. However, this is true if the random variables are multivariate Gaussian.
>
> Let $\mathbf X$ be a Gaussian random vector with mean $\mu$ and covariance $\Sigma$. Assume that $\Sigma$ is positive definite. How to prove that "there exists a vector $B$ and a matrix $A$ such that $A(\mathbf X + B)$ is a Gaussian random vector whose components are independent and each of mean 0".
>
> **Solution:**
>
> First, in order to remove the effect of $\mu$ we can set $B = -\mu$ to make the individual Gaussian random variables be of zero mean. Let $\widehat{\mathbf X} = \mathbf X- \mu$. From an earlier problem we know that the covariance matrix of $\widehat{\mathbf X}$ is the same as $\Sigma$.
>
> From the above note on covariance matrices we can see that there exists an orthogonal matrix $U$ such that $D = U \Sigma U^ T$. 
>
> Consider the following transformation: $\mathbf Y= U \widehat{\mathbf X}$.
>
> The covariance matrix of $\mathbf Y$ is 
> $$
> \mathsf{Cov}(\mathbf{Y}) = U\Sigma U^T
> $$
> which is precisely equal to the diagonal matrix $D$.
>
> Therefore, $\mathbf Y$ has component Gaussian random variables that are uncorrelated and hence independent.

## 7. Multivariate Central Limit Theorem

The CLT may be generalized to averages or random vectors (also vectors of averages). Let $X_1, ...,X_n \in \R^d$ be independent copies of a random vector $X$ such that $\mathbb{E}[X] =\mu, \quad \mathsf{Cov}(X)= \Sigma$,
$$
\sqrt{n} (\bar{X}_n - \mu) \xrightarrow[n\rightarrow \infty]{(d)} \mathcal{N}_d(0, \Sigma)
$$
Equivalently,
$$
\sqrt{n} \Sigma^{-1/2} (\bar{X}_n - \mu) \xrightarrow[n\rightarrow \infty]{(d)} \mathcal{N}_d(0,I_d)
$$
where $I_d$ is a $d \times d$ identity matrix.

**Convergence in Distribution in Higher Dimensions**

Convergence in distribution of a random vector is NOT implied by convergence in distribution of each of its components.

A sequence $\, \mathbf{T}_1,\mathbf{T}_2,\ldots$ of random vectors in $\R^d$ **converges in distribution** to a random vector $\mathbf{T}$ if
$$
\mathbf{v}^ T \mathbf{T}_ n \xrightarrow [(d)]{n\to \infty } \mathbf{v}^ T \mathbf{T} \qquad \text {for all }\, \mathbf{v}\in \mathbb {R}^ d \qquad (\text {multivariate convergence in distribution}).
$$
That is, the vector sequence $\, \left(\mathbf{T}_ n\right)_{n\geq 1}\,$ converges in distribution only if its dot product $\mathbf{v}^ T\mathbf{T}_ n$ with **any** constant vector $\mathbf{v}$, which is a scalar random variable, converges in distribution (or equivalently, if the projection of the vector sequence onto **any** line converges in distribution.)

**Univariate CLT Implies Multivariate CLT**

Let $\, \mathbf{X}_1, \ldots ,\, \mathbf{X}_ n\stackrel{i.i.d.}{\sim } \mathbf{X}\,$ be random vectors in $\R^d$ with (vector) mean $\mathbb{E}[\mathbf{X}] = \mu_\mathbf{X}$ and covariance matrix $\Sigma_{\mathbf{X}}$. Let $\mathbf{v} \in \R^d$ and define $Y_ i=\mathbf{v}^ T\mathbf{X}_ i$. Then

* $Y_i$ is a scalar random variable;
* Its mean and variance are $\mathbb E[Y_ i]= \mathbf{v}^ T\mathbb E[\mathbf{X}_ i]\,$ and $\, \sigma ^2_{Y_ i}=\mathbf{v}^ T \Sigma _{\mathbf{X}_ i} \mathbf{v}\,$

Hence $Y_i$ satisfies the univariate CLT:
$$
\sqrt{n}\left(\overline{Y_ n}-\mathbf{v}^ T\mu _\mathbf X\right) \xrightarrow [(d)]{n\to \infty }\mathcal{N}\left(0, \mathbf{v}^ T \Sigma _{\mathbf{X}} \mathbf{v}\right)
$$
On the other hand, consider a multivariate Gaussian variable $\, \mathbf{Z}\sim \mathcal{N}\left(\mathbf{0}, \Sigma _{\mathbf{X}}\right)$. For any constant vector $\, \mathbf{v}\in \mathbb {R}^ d,\, \, \mathbf{v}^ T \mathbf{Z}\,$ is a univariate Gaussian with variance $\, \mathbf{v}^ T\Sigma _{\mathbf{X}}\mathbf{v}.\,$ Hence, $\mathbf{v}^ T \mathbf{Z}\sim \mathcal{N}\left(0, \mathbf{v}^ T\Sigma _{\mathbf{X}}\mathbf{v}\right),\,$ which is the distribution on the RHS above. Therefore, $\overline{\mathbf{X}_ n}$ converges in distribution:
$$
\begin{aligned}
\sqrt{n}\left(\mathbf{v}^ T\overline{\mathbf{X}_ n}-\mathbf{v}^ T\mu _\mathbf X\right)\, =\, \sqrt{n}\left(\overline{Y_ n}-\mathbf{v}^ T\mu _\mathbf X\right) &\xrightarrow [(d)]{n\to \infty } \mathcal{N}\left(0, \mathbf{v}^ T \Sigma _{\mathbf{X}} \mathbf{v}\right)\, =\, \mathbf{v}^ T \mathcal{N}\left(0, \Sigma _{\mathbf{X}}\right)\\
\iff\sqrt{n}\left(\overline{\mathbf{X}}_ n-\mu _\mathbf X\right)&\xrightarrow [(d)]{n\to \infty }\mathcal{N}\left(0, \Sigma _{\mathbf{X}}\right).

\end{aligned}
$$

## 8. Multivariate Delta Method

Let $(T_n)_{n \geq 1}$ sequence of random vectors in $\R^d$ such that
$$
\sqrt{n} (\mathbf{T}_n - \theta) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}_d(0, \Sigma)
$$
for some $\theta \in \R^d$ and some covariance $\Sigma \in \R^{d \times d}.$

Let $\mathbf{g}: \R^d \rightarrow \R^k (k \geq 1)$ be continuously differentiable at $\theta$. The **gradient** or the **gradient matrix** of $\mathbf{g}$, denoted by $\nabla \mathbf{g}$, is the $d\times k$ matrix
$$
\nabla \mathbf{g} =  \begin{pmatrix} |& |& |& |\\ \nabla \mathbf{g}_1&  \nabla \mathbf{g}_2& \ldots & \nabla \mathbf{g}_ k\\ |& |& |& |\\ \end{pmatrix} = \begin{pmatrix} \frac{\partial \mathbf{g}_1}{\partial \theta_1}& \ldots & \frac{\partial \mathbf{g}_ k}{\partial \theta_1}\\ \vdots & \ldots & \vdots \\ \frac{\partial \mathbf{g}_1}{\partial \theta_ d}& \ldots & \frac{\partial \mathbf{g}_ k}{\partial \theta_ d} \end{pmatrix}.
$$
This is also the transpose of what is known as the **Jacobian matrix** $J_g$ of $g$.

Then,
$$
\sqrt{n}(\mathbf{g}(\mathbf{T}_n) - \mathbf{g}(\theta)) \xrightarrow[n\rightarrow \infty]{(d)} \mathcal{N}_k(0, \nabla \mathbf{g}(\theta)^T \Sigma \nabla \mathbf{g}(\theta))
$$
where $\nabla \mathbf{g}(\theta) = {\partial \mathbf{g}\over\partial \theta }(\theta) = \left({\partial \mathbf{g}_j \over \partial \theta_i } \right)_{1\leq i \leq d; 1\leq j\leq k} \in \R^{d \times k}$.


# Lecture 9. Introduction to Maximum Likelihood Estimation

There are 7 topics and 5 exercises.

## 1. Maximum Likelihood Estimator

**Review: Maximizing composite functions**:

The arguments of the minima (resp. Arguments of the maxima) of a function $f(x)$, denoted by $\text{argmin} f(x)$ (resp. $\text{argmin}f(x)$), is the value(s) of $x$ at which $f(x)$ is minimum (resp. maximum). We can also restrict to a subset $S$ of the domain $f$, and denote by $\text{argmin}_{x \in S} f(x)$ (resp. $\text{argmin}_{x\in S} f(x)$) the value(s) of $x \in S$ at which $f(x)$ is minimum (resp. maximum) over $S$.

**Definition of maximum likelihood estimator**:

Let $X_1, ..., X_n \stackrel{iid}{\sim} \mathbf{P}_{\theta^*}$ be discrete random variables. We construct a statistical model $(E,(\mathbf{P}_\theta)_{\theta \in \Theta})$ where $\mathbf{P}_\theta$ has PMF $p_\theta$. We observe our sample to be $X_1 = x_1, X_2 = x_2, ...., X_n = x_n$. Let $L$ be the corresponding likelihood.

The *maximum likelihood estimator* of $\theta^*$ is defined as
$$
\hat{\theta}_n^{MLE} = \text{argmax}_{\theta \in \Theta} L(X_1, ..., X_n, \theta)\\
\text{where } \quad L(X_1, ..., X_n, \theta) =  \left( \prod _{i = 1}^ n p_\theta (X_ i) \right)
$$
provided it exists.

**Remark (Log-likelihood estimator)**: In practice, we use the fact that
$$
\hat{\theta}_n^{MLE} = \text{argmax}_{\theta \in \Theta}\log L(X_1, ..., X_n, \theta)
$$
**Interpretation of MLE:**

* MLE is the value of $\theta$ that maximizes the probability $\mathbf{P}_\theta$ of observing the data set $(x_1,..., x_n)$. 

  Since the likelihood is the joint density of $n$ iid samples from $\mathbf{P}_\theta$
  $$
  \mathbf{P}_\theta [X_1 = x_1, \ldots , X_ n = x_ n] = L_ n(x_1, \ldots , x_ n, \theta ).
  $$
  Hence, the MLE finds $\hat{\theta}_n$ that maximizes the probability that $x_1, ..., x_n$ were sampled from $\mathbf{P}_{\hat{\theta}_n}$.

* MLE is the value of $\theta$ that minimizes an estimator of the KL divergence between $\mathbf{P}_\theta$ and the true distribution $\mathbf{P}_{\theta^*}$.

  This is how the MLE was derived from KL divergence.

**Remark:** Under some technical conditions the MLE is a **weakly consistent estimator** for $\theta^*$, meaning that the MLE will converge to $\theta^*$ in probability under these conditions. However, there are examples of statistical models where the maximum likelihood estimator will **not** converge to the true parameter.

## 2. Concavity in 1 Dimension

**Review: Maximizing/minimizing functions**

Optimization is a whole field that is concerned with maximizing or minimizing functions.

Note that
$$
\min_{\theta \in \Theta} -h(\theta) \iff \max_{\theta \in \Theta} h(\theta)
$$
We focus on maximization. Maximization of arbitrary functions can be difficult. For example, $\theta \mapsto \prod\limits^n_{i=1}(\theta - X_i)$.

**A analytical definition of concave and convex functions**:

A function twice differentiable function $h:\Theta \subset \R \rightarrow \R$ is said to be *concave* if its second derivative satisfies
$$
h''(\theta) \leq 0, \quad \forall \theta \in \Theta
$$
It is said to be strictly *concave* if the inequality is strict: $h''(\theta) \leq 0$.

Moreover, $h$ is said to be (strictly) *convex* if $-h$ is (strictly) concave, i.e. $h''(\theta) \geq 0 \,\, (h''(\theta)>0).$ 

Examples:

* $\Theta = \R, h(\theta) = -\theta^2$ is concave.
* $\Theta = (0,\infty), h(\theta) = \sqrt{\theta}$ is concave.
* $\Theta=(0,\infty),h(\theta) = \log \theta$ is concave.
* $\Theta = [0,\pi],h(\theta)=\sin(\theta)$ is concave.
* $\Theta = \R, h(\theta) = 2\theta - 3$ is neither concave nor convex.

**A general/synthetic definition of concave and convex functions:**

A function $g:I \rightarrow \R$ is **concave** (or concave down) on an interval $I$, if for all pairs of real numbers $x_1 < x_2 \in I$
$$
g(tx_1+(1-t)x_2)\geq tg(x_1)+(1-t)g(x_2)\qquad \text {for all } \, 0 < t < 1.
$$
Geometrically, a concave function is shown below

![images_u3s2_concave](../assets/images/images_u3s2_concave.svg)

At $x=x_2-t(x_2-x_1)=tx_1+(1-t)x_2$, the $y$ value of the graph of $g$ is $g(x)=g(tx_1+(1-t)x_2)$, while the $y$ value of the secant line is $tg(x_1)+(1-t)g(x_2)$.

If the inequality is strict, i.e. if 
$$
g(tx_1+(1-t)x_2)> tg(x_1)+(1-t)g(x_2)\qquad \text {for all } \, 0 < t < 1.
$$
then $g$ is **strictly concave**.

The definition for (strictly) **convex** is analogous.

If in addition $g$ is twice differentiable in the interval $I$, i.e. $g''(x)$ exists for all $x \in I$, then $g$ is

* **Concave** if and only if $g''(x)\leq 0$ for all $x \in I$;
* **Strictly concave** if $g''(x) < 0$ for all $x \in I$;
* **Convex** if and only if $g''(x) \geq 0$ for all $x \in I$;
* **Strictly convex** if $g''(x) > 0$ for all $x \in I$;

**Remark:** Note that the synthetic definition above does not require differentiability at every point. In contrast, the analytical definition is less general, using inequality conditions on the second derivative. For example, the function $x↦x^4$ is strictly convex according to the definition above, but has three vanishing derivatives at the origin $x=0$.

## 3. Concavity in Higher Dimensions: Gradients, Hessians, Semi-Definiteness

**Multivariate concave functions**:

More generally for a multivariate function: $h:\Theta \subset \R^d \rightarrow \R, d \geq 2$, define the 

* Gradient vector: 
  $$
  \nabla h(\theta) = \begin{pmatrix}{ \partial h\over \partial \theta_1}(\theta) \\ \vdots \\{ \partial h\over \partial \theta_1}(\theta)  \end{pmatrix} \in \R^d
  $$

* Hessian matrix:
  $$
  \mathbf{H}h(\theta) = \begin{pmatrix} {\partial^2 h \over  \partial \theta_1 \partial \theta_1} (\theta) \dots {\partial^2 h \over  \partial \theta_1 \partial \theta_d} (\theta)\\ \\ \\ {\partial^2 h \over  \partial \theta_d \partial \theta_1} (\theta) \dots {\partial^2 h \over  \partial \theta_d \partial \theta_d} (\theta)\ \end{pmatrix} \in \R^{d \times d}
  $$

$h$ is concave $\iff \quad x^T\mathbf{H}h(\theta)x \leq 0 \quad \forall x \in \R^d, \theta \in \Theta$.

$h$ is strictly concave $\iff \quad x^T\mathbf{H}h(\theta)x < 0 \quad \forall x \in \R^d, \theta \in \Theta$. 

Note that **singular value decomposition (SVD)** can be used to compute $x^T\mathbf{H}h(\theta)x$. $ x^T\mathbf{H}h(\theta)x \leq 0$ is equivalent to that all eigenvalues of the matrix $\mathbf{H}$ are nonpositive. So if $ x^T\mathbf{H}h(\theta)x \geq 0$, the Hessian matrix of a convex function is **positive semidefinite**.

> #### Exercise 50
>
> Suppose $\theta = \begin{pmatrix}\theta_1 \\ \theta_2 \end{pmatrix}, \quad -h(\theta) = -\theta^2_1 - 2 \theta_2^2$, is the function $f(\theta)$ concave or convex?
>
> **Answer:** Concave
>
> **Solution:**
>
> First compute the gradient vector
> $$
> \nabla f(\theta) = \begin{bmatrix}-2\theta_1 \\ -4\theta_2 \end{bmatrix}
> $$
> Then compute the Hessian matrix
> $$
> \mathbf{H}h(\theta) = \begin{bmatrix}-2 & 0 \\ 0 & -4 \end{bmatrix}
> $$
> Hessian matrix is a **diagonal** matrix, which is good, since we can directly determine whether the function is concave or convex by observing whether the diagonal entries are all negative or all positive.
>
> Suppose $x = \begin{bmatrix}x_1 \\ x_2 \end{bmatrix}$, we compute
> $$
> x^T\mathbf{H}h(\theta)x = \begin{bmatrix}x_1 & x_2 \end{bmatrix} \begin{bmatrix}-2 & 0 \\ 0 & -4 \end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \end{bmatrix} = -2x_1^2 - 4x_2^2 \leq 0
> $$

**Review: Compute the Hessian Matrix**:

Let
$$
f:\R^d \rightarrow \R \quad \theta = \theta =\begin{pmatrix} \theta _1\\ \theta _2\\ \vdots \\ \theta _ d\end{pmatrix} \mapsto f(\theta)
$$
denote a **twice-differentiable** function.

The Hessian of $f$ is the matrix
$$
\mathbf{H} f: \R^d \rightarrow \R^{d \times d}
$$
whose entry in the $i$-th row and $j$-th column is defined by
$$
\left(\mathbf{H}\, f\right)_{ij} :=\frac{\partial ^2}{\partial \theta _ i \partial \theta _ j} f, \quad 1 \leq i, j \leq d.
$$
The Hessian matrix of $f$ in this context is also denoted by $\nabla ^2 f$, the **second derivative** of $f$. This is not to be confused with the "Laplacian" of $f$, which is also denoted the same way.

> #### Exercise 51
>
> Consider the function $\, f(\theta )=-c_1 \theta _1^2-c_2 \theta _2^2-c_{3}\theta _3^2\,$ where $c_1, c_2, c_3 > 0$ as in the previous problem. Compute the Hessian matrix $\mathbf{H}f$.
>
> **Answer**:
>
> $\mathbf{H}f(\theta)=\begin{pmatrix}  -2c_1& 0& 0\\ 0& -2c_2& 0\\ 0& 0&  -2c_3\\ \end{pmatrix}.$
>
> **Solution**:
>
> First compute the gradient vector:
> $$
> \, f(\theta )=-c_1 \theta _1^2-c_2 \theta _2^2-c_{3}\theta _3^2\,\\
> \nabla f (\theta )=\left.\begin{pmatrix}  \frac{\partial f }{\partial \theta _1}\\ \frac{\partial f }{\partial \theta _2}\\ \frac{\partial f }{\partial \theta _3}\end{pmatrix}\right|_{\theta }\, =\, \begin{pmatrix}  -2c_1\theta _1\\ -2c_2 \theta _2\\ -2c_3\theta _3 \end{pmatrix}.
> $$
> Second compute the Hessian. One way to compute the Hessian is to start in $j$-th column of the Hessian matrix by the gradient of the $j$-th component of $\nabla f$. We obtain
> $$
> \, f(\theta )=-c_1 \theta _1^2-c_2 \theta _2^2-c_{3}\theta _3^2\,\\
> \begin{aligned}
> \mathbf{H}f(\theta ) &= \begin{pmatrix}  |& |& |\\ \nabla (-2c_1\theta _1)& \nabla (-2c_2 \theta _2)&  \nabla (-2c_3\theta _3)\\ |& |& |\\ \end{pmatrix}\\
> &= \begin{pmatrix}  -2c_1& 0& 0\\ 0& -2c_2& 0\\ 0& 0& -2c_3\\ \end{pmatrix}.
> \end{aligned}
> $$

**Semi-Definiteness**:

A symmetric (real-valued) $d\times d$ matrix $\mathbf{A}$ is **positive semi-definite** if 
$$
\mathbf{x}^ T \, \mathbf{A}\, \mathbf{x} \geq 0 \qquad \forall x \in \R^d
$$
If the inequality above is strict, i.e., if $\mathbf{x}^ T \, \mathbf{A}\, \mathbf{x} > 0$ for all non-zero vectors $\mathbb{x} \in \R^d$, then $\mathbf{A}$ is **positive definite**.

Analogously, a symmetric (real-valued) $d \times d$ matrix $\mathbf{A}$ is **negative semi-definite** (resp. **negative definite**) if $\mathbf{x}^T \mathbf{A}\mathbf{x}$ is **non-positive** (resp. negative) for all $\mathbf{x} \in \R^d - \{0\}$.

Note that by definition, positive (or negative) definiteness implies positive (or negative) semi-definiteness.

> #### Exercise 52
>
> Consider the same function
> $$
> \, f(\theta )=-c_1 \theta _1^2-c_2 \theta _2^2-c_{3}\theta _3^2\,
> $$
> Compute $\mathbf{x}^ T \, \left(\mathbf{H}f\right)\,  \mathbf{x}\,$ where $\mathbf{x}=\begin{pmatrix} x_1\\ x_2\\ x_3\end{pmatrix}$.
>
> **Answer**:
>
> $\mathbf{x}^ T \, \left(\mathbf{H}f\right)\,  \mathbf{x}\,= -2c_1x_1^2-2c_2x_2^2-2c_3x_3^2$
>
> **Solution**:
>
> Recall from the previous problems that
> $$
> \mathbf{H}f (\theta ) = \begin{pmatrix}  -2c_1& 0& 0\\ 0&  -2c_2& 0\\ 0& 0& -2c_3\\ \end{pmatrix}.
> $$
> Then, 
> $$
> \begin{aligned}
> \mathbf{x}^ T\, \left(\mathbf{H}f\right) \, \mathbf{x} &= \begin{pmatrix}  x_1& x_2& x_3 \end{pmatrix}\begin{pmatrix}  -2c_1 &  0 &  0\\ 0 &  -2c_2 &  0\\ 0 &  0& &  -2c_3\\ \end{pmatrix}\begin{pmatrix}  x_1\\ x_2\\ x_3 \end{pmatrix}\\
> &= -2c_1x_1^2-2c_2x_2^2-2c_3x_3^2\, <\, 0.
> \end{aligned}
> $$
> Since $c_1,c_2, c_3 > 0$, this means the $\mathbf{H}f$ is negative definite, (also negative semi-definite), and hence $f$ is strictly concave (also concave).

## 4. Worked Example: Concavity and Composition of Functions

Suppose $\Theta = (0,\infty), f(\theta) = \log(\theta_1 + \theta_2).$ Let's compute $\mathbf{x}^ T \, \left(\mathbf{H}f\right)\,  \mathbf{x}\,$.

First compute $\nabla f(\theta)$:
$$
\nabla f(\theta) = \begin{pmatrix} {1\over \theta_1 + \theta_2} \\{1\over \theta_1 + \theta_2} \end{pmatrix}
$$
Then compute $\mathbf{H}f(\theta)$
$$
\mathbf{H}f(\theta) =\begin{pmatrix}-{1\over (\theta_1 + \theta_2)^2} & -{1\over (\theta_1 + \theta_2)^2} \\-{1\over (\theta_1 + \theta_2)^2} & -{1\over (\theta_1 + \theta_2)^2} \end{pmatrix}
$$
Finally, compute $\mathbf{x}^ T \, \left(\mathbf{H}f\right)\,  \mathbf{x}\,$
$$
\begin{pmatrix}x_1 & x_2 \end{pmatrix} \left(\mathbf{H}f(\theta)\right) \begin{pmatrix}x_1\\x_2 \end{pmatrix} = \begin{pmatrix}x_1 & x_2 \end{pmatrix} \begin{pmatrix} -{x_1 + x_2 \over (\theta_1 + \theta_2)^2}\\ -{x_1 + x_2 \over (\theta_1 + \theta_2)^2}\end{pmatrix} = {x_1^2 + x_2x_1 \over (\theta_1 + \theta_2)^2} - {x_2^2 + x_1x_2 \over (\theta_1 + \theta_2)^2} = -{(x_1 + x_2)^2 \over (\theta_1 + \theta_2)^2}
$$
Therefore, it is a non-positive number,
$$
\mathbf{x}^ T \, \left(\mathbf{H}f\right)\,  \mathbf{x}\, = -{(x_1 + x_2)^2 \over (\theta_1 + \theta_2)^2} \leq 0
$$
which means the function $f (\theta)$ is concave.

**Remark:** If you have a linear function composed with a concave function, then you end up with a concave function.

> #### Exercise 53
>
> Let $f_1, f_2$ be convex functions on $\R$. Determine if the following functions are necessarily convex or concave.
>
> 1. $3f_1 + 2f_2$
>
> 2. $-10f_1$
>
> 3. $f_2f_1$
>
> **Answer**: 
>
> 1. Convex
>
> 2. Concave
>
> 3. Cannot be determined without more information
>
> **Solution**:
>
> 1. Given $f_1, f_2$ are convex we have
>    $$
>    f_1(tx_1+(1-t)x_2)\leq tf_1(x_1)+(1-t)f_1(x_2)\qquad \text {for all } \, 0\leq t\leq 1
>    $$
>    and the same holds for $f_2$.
>
>    The same inequality holds for $g=3f_1+2f_2$:
>    $$
>    \begin{aligned}
>    g(tx_1+(1-t)x_2) &= 3f_1(tx_1+(1-t)x_2)+2f_2(tx_1+(1-t)x_2)\\
>    &\leq 3\left(tf_1(x_1)+(1-t)f_1(x_2)\right)+2\left(tf_2(x_1)+(1-t)f_2(x_2)\right)\\
>    &=tg(x_1)+(1-t)g(x_2).
>    \end{aligned}
>    $$
>    Hence, $3f_1 + 2f_2$ is also convex.
>
>    **Remark:** In general, any function $c_1f_1 + c_2f_2$ where $c_1, c_2>0$ is convex of $f_1, f_2$ are.
>
> 2. $-10f_1$ is concave, because it is negative of a convex function.
>
> 3. For example, $f_1(x) = x$ and $f_2=x^2$, then $(f_1 f_2)(x) = x^3$ which is neither convex nor concave. Other examples of $f_1$ and $f_2$, e.g. $f_1 = f_2 = x^2$ will lead to $f_1 f_2$ being convex. 

## 5. Concavity in Higher Dimensions and Eigenvalues

**Concavity in 2 dimensions: Compute the Hessian**

Compute the Hessian $\mathbf{H}f$ of the function $f(x,y) = -2x^2 + \sqrt{2}xy - {5\over 2} y^2$? 

We compute that
$$
(\mathbf{H}f)_{11} = \frac{\partial ^2 f }{\partial x^2} = -4, \quad  (\mathbf{H}f)_{12} = \frac{\partial ^2 f }{\partial x \partial y} = \sqrt{2}\\
(\mathbf{H}f)_{21} = \frac{\partial ^2 f }{\partial x \partial y} = \sqrt{2}, \quad (\mathbf{H}f)_{22} = \frac{\partial ^2 f }{\partial y^2} = -5.
$$
Therefore,
$$
\mathbf{H}f = \begin{pmatrix} {\partial^2 f \over \partial x^2} & {\partial^2 f \over \partial x \partial y} \\ {\partial^2 f \over \partial y \partial x} & {\partial^2 f \over \partial y^2} \end{pmatrix}  = \begin{pmatrix}  -4 &  \sqrt{2} \\ \sqrt{2} &  -5 \end{pmatrix}.
$$

**Concavity in 2 dimensions: Positive Definiteness and Eigenvalues**

Consider $f(x,y) = -2x^2 + \sqrt{2}xy - {5\over 2} y^2$. What are the eigenvalues $\lambda_1, \lambda_2$ of $\mathbf{H}f$? Assume that $\lambda_1 < \lambda_2$. Is $f$ concave or convex?

Recall the Hessian calculated previously is
$$
\mathbf{H}f = \begin{pmatrix}  -4 &  \sqrt{2} \\ \sqrt{2} &  -5 \end{pmatrix}.
$$
To find the eigenvalues, we need to solve for $\lambda$ such that
$$
\mathrm{det}\left( \mathbf{H}f - \lambda I \right) = \mathrm{det}\left(\begin{pmatrix}  -4 - \lambda &  \sqrt{2} \\ \sqrt{2} &  -5 - \lambda \end{pmatrix} \right) = \lambda ^2 + 9\lambda + 18 = 0.
$$
Factoring the quadratic: $\lambda ^2 + 9\lambda + 18 = (\lambda + 6)(\lambda +3)$ shows that $\lambda_1 = -6$ and $\lambda_2 = -3$.

**Remark:** The function $f$ is twice-differentiable, so it is concave if $x^ T \mathbf{H}f x \leq 0$ for all $x \in \R^2$. By the remark in the problem statement, this is equivalent to all of the eigenvalues of $\mathbf{H}f$ being negative. Hence, $f$ is concave (in fact it is strictly concave).

## 6. Strictly Concave Functions and Unique Maximizer

**Optimality conditions**:

Strictly concave functions are easy to maximize: if they have a maximum, then it is **unique**. It is the unique solution to
$$
h'(\theta) = 0
$$
or, in the multivariate case
$$
\nabla h(\theta) = 0 \in \R^d
$$
There are many algorithms to find it numerically: this is the theory of "**convex optimization**". In this class, often a **closed form formula** for the maximum.

> #### Exercise 54
>
> Let $f:\R^2 \rightarrow \R$ be a twice-differentiable function, such that the top-left element of the Hessian matrix $\mathbf{H}f(0,0)_{1,1} > 0$ is positive. Is $f$ concave?
>
> **Answer**: No
>
> **Solution**:
>
> Recall that $f$ is concave at $(0,0)$ if for all $(x,y) \in \R^2$,
> $$
> (x, y) \mathbf{H}f(0,0) \begin{pmatrix}  x \\ y \end{pmatrix} < 0.
> $$
> By expanding and using the definition of the Hessian, we see that
> $$
> (x, y) \mathbf{H}f(0,0) \begin{pmatrix}  x \\ y \end{pmatrix} = x^2 \frac{\partial ^2}{\partial x^2} f(0,0) + xy \left( \frac{\partial ^2}{\partial x \partial y}f(0,0) + \frac{\partial ^2}{\partial y \partial x}f(0,0) \right) + y^2 \frac{\partial ^2}{\partial y^2}f(0,0).
> $$
> By assumption, we know that $\frac{\partial ^2}{\partial x^2} f(0,0) > 0$. Hence,
> $$
> (1, 0)^ T \mathbf{H}f(0,0) \begin{pmatrix}  1 \\ 0 \end{pmatrix} = \frac{\partial ^2}{\partial x^2} f(0,0) >0.
> $$
> This violates the definition of concavity.

## 7. Maximum Likelihood Estimator of Bernoulli, Poisson, Gaussian Trials

* Bernoulli trials: $\hat{p}_ n^{MLE} = \bar{X}_n$.

  Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Ber}(p^*)$ for some unknown $p^* \in (0,1)$. You construct the associated statistical model $(\{ 0,1\} , \{ \text {Ber}(p)\} _{p \in (0,1)})$. Let $L_n$ denote the likelihood of this statistical model. The **likelihood** of a Bernoulli statistical model has the formula 
  $$
  L_ n(x_1, \ldots , x_ n, p) = \prod _{i = 1}^ n p^{x_ i}(1 - p)^{1 - x_ i} = p^{\sum _{i = 1}^ n x_ i} (1 -p)^{n - \sum _{i = 1}^ n x_ i}.
  $$
  Oftentimes for computing the MLE it is more convenient to work with and optimize the **log-likelihood** 
  $$
  \ell (p) := \ln L_ n(x_1, \ldots , x_ n, p)
  $$
  Observe that
  $$
  \begin{aligned}
  \ln L_ n(x_1, \ldots , x_ n, p) &= \ln \left( p^{\sum _{i = 1} x_ i} (1 -p)^{n - \sum _{i = 1} x_ i} \right)\\
  &= \left( \sum _{i = 1}^ n x_ i \right) \ln p + \left(n - \sum _{i = 1}^ n x_ i \right) \ln (1 - p).
  \end{aligned}
  $$
  Taking the derivative with respect to $p$,
  $$
  \frac{\partial }{\partial p} \ln L_ n(x_1, \ldots , x_ n, p) = \frac{\sum _{i = 1}^ n x_ i}{p} - \frac{n - \sum _{i = 1}^ n x_ i}{1- p}.
  $$
  We set this to be $0$ and solve for $p$:
  $$
  \begin{aligned}
  \frac{\sum _{i = 1}^ n x_ i}{p} - \frac{n - \sum _{i = 1}^ n x_ i}{1- p} &= 0 \iff\\
  \frac{(1 - p) \sum _{i = 1}^ n x_ i - p\left(n - \sum _{i = 1}^ n x_ i\right) }{p(1 - p)} &= 0 \iff\\
  \frac{\sum _{i = 1}^ n x_ i - np}{p(1- p)} &= 0 .
  
  \end{aligned}
  $$
  Since the derivative blows up at $p=0,1$, we can assume $0<p < 1$ and ignore denominator for the purpose of solving for $p$. Hence $\hat{p} = {1 \over n} \sum^n_{i=1} x_i$ is the **unique critical point** of the log-likelihood.

  Take the second derivative
  $$
  \frac{\partial }{\partial p } \left( \frac{\sum _{i = 1}^ n X_ i}{p} - \frac{n - \sum _{i = 1}^ n X_ i}{1- p} \right) = - \frac{\sum _{i = 1}^ n x_ i}{p^2} - \frac{n - \sum _{i = 1}^ n x_ i}{(1 - p)^2}.
  $$
  Since the expression is always negative, this implies that the critical point $\hat{p}$ is a **local maximum**.

  Testing the endpoints we see
  $$
  L_ n(x_1, \ldots , x_ n, 0) = 0^{\sum _{i = 1}^ n x_ i} (1)^{n - \sum _{i = 1}^ n x_ i} = 0\\
  L_ n(x_1, \ldots , x_ n, 1) = 1^{\sum _{i = 1}^ n x_ i} (0)^{n - \sum _{i = 1}^ n x_ i} = 0
  $$
  Since the likelihood is non-negative, the endpoints are actually **global minima**.

  Hence, the global maximum is achieved at $\hat{p} = \displaystyle \frac{1}{n} \sum _{i = 1}^ n x_ i$. Plugging in the random variables $X_1, ...,X_n$, we derive the MLE true parameter $p^*$:
  $$
  \hat{p}_ n^{MLE} = \frac{1}{n } \sum _{i = 1}^ n X_ i
  $$
  which is the **sample mean**.

  **Remark**: **MLE** for a Bernoulli statistical model is the **sample mean**.

* Poisson model: $\hat{\lambda}_ n^{MLE} = \bar{X}_n$.

  Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Poiss}(\lambda^*)$ for some unknown $\lambda^* \in (0,\infty)$. You construct the associated statistical model $(\N \subset\{ 0\} , \{ \text {Poiss}(\lambda)\} _{\lambda \in (0,\infty)})$. Let $L_n$ denote the likelihood of this statistical model. The **likelihood** of a Poisson statistical model has the formula 
  $$
  L_ n(x_1, \ldots , x_ n, \lambda ) = \prod _{i = 1}^ n e^{-\lambda } \frac{\lambda ^{x_ i}}{{x_ i}!} = e^{-n \lambda } \frac{\lambda ^{\sum _{i = 1}^ n x_ i}}{x_1 ! \cdots x_ n !}
  $$
  Observe that
  $$
  \ln L_ n(x_1, \ldots , x_ n, \lambda ) = \ln \left(e^{-n \lambda } \frac{\lambda ^{\sum _{i = 1} x_ i}}{x_1! \cdots x_ n!} \right) = -n \lambda + (\sum _{i = 1}^ n x_ i) \ln \lambda - \ln \left(x_1! \cdots x_ n! \right)
  $$
  Taking the derivative with respect to $\lambda$,
  $$
  \frac{\partial }{\partial \lambda } \ln L_ n(x_1, \ldots , x_ n, \lambda ) = -n + \frac{\sum _{i = 1}^ n x_ i}{\lambda }.
  $$
  Setting this equal to $0$, we recover the **critical point**
  $$
  \hat{\lambda } = \frac{1}{n} \sum _{i =1 }^ n x_ i.
  $$
  Perform the second derivative test and verify that this critical point is indeed a **global maximum**. 
  $$
  \frac{\partial^2 }{\partial \lambda } \ln L_ n(x_1, \ldots , x_ n, \lambda ) = - \frac{\sum _{i = 1}^ n x_ i}{\lambda^2 }.
  $$
  which is non-positive, so that the likelihood the concave.

  Moreover, it is a global maximum since $\lim _{\lambda \to 0}L_ n(X_1, \ldots , X_ n, \lambda ) = -\infty$ and $\lim _{\lambda \to \infty}L_ n(X_1, \ldots , X_ n, \lambda ) = 0$.

  This verifies that the MLE is
  $$
  \hat{\lambda }_ n^{MLE} = \frac{1}{n} \sum _{i =1 }^ n x_ i,
  $$
  Which is the **sample mean**.

  **Remark**: **MLE** for a Poisson statistical model is the **sample mean**.

* Gaussian model: $(\hat{\mu}_n, \hat{\sigma}^2_n) = (\bar{X}_n, \hat{S}_n)$, where $\hat{S}_n = {1\over n}\sum\limits^n_{i=1}(X_i - \bar{X}_n)^2$

  Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathcal{N}(\mu,\sigma^2)$ for some unknown $\mu, \sigma^2$. You construct the associated statistical model $(\mathbb {R}, \{ N(0, \tau )\} _{\tau > 0})$. Let $L_n$ denote the likelihood of this statistical model. The **likelihood** of a Gaussian statistical model has the formula 
  $$
  L_ n(x_1, \ldots , x_ n, (\mu , \sigma ^2)) = \frac{1}{(\sigma \sqrt{2 \pi })^ n} \exp (-\frac{1}{2 \sigma ^2} \sum _{i = 1}^ n (x_ i - \mu )^2).
  $$
  Now compute the Hessian and verify $\mathbf{x^THx} \leq 0$.

  We first take the partial derivative and set it to $0$,
  $$
  \begin{aligned}
  \nabla L_n(\hat{\mu}, \hat{\sigma}^2) = \begin{pmatrix}{\partial \over \partial \mu} L(\hat{\mu}, \hat{\sigma}^2) \\ {\partial \over \partial \sigma^2} L(\hat{\mu}, \hat{\sigma}^2) \end{pmatrix} = 0
  \end{aligned}
  $$
  For the first entry, we get
  $$
  {\partial \over \partial \mu} L(\hat{\mu}, \hat{\sigma}^2) = {1\over 2 \hat{\sigma}^2} \sum^n_{i=1} 2(x_i -  \hat{\mu}) = 0\\
  \implies \hat{\mu}  = {1\over n} \sum^n_{i=1}x_i
  $$
  For the second entry, it is more convenient to work on log likelihood,
  $$
  \begin{aligned}
  \ln L_ n(x_1, \ldots , x_ n,\mu, \sigma^2 )=& \ln \left( \frac{1}{( \sqrt{2 \pi \sigma^2 })^ n} \exp (-\frac{1}{2 \sigma^2 } \sum _{i = 1}^ n (x_ i-\mu)^2) \right)\\
  =&- \frac{n}{2} \ln ( 2 \pi \sigma^2 ) - \frac{1}{2 \sigma^2 } \sum _{i = 1}^ n (x_ i-\mu)^2\\
  =&- \frac{n}{2} \ln (  \sigma^2 ) - n \ln(\sqrt {2\pi}) - \frac{1}{2 \sigma^2 } \sum _{i = 1}^ n (x_ i-\mu)^2
  \end{aligned}
  $$
  Taking derivatives with respect to $\sigma^2$ and we get
  $$
  {\partial \over \partial \mu}  \ln L_ n(x_1, \ldots , x_ n,\mu, \sigma^2 ) = -\frac{n}{2 \sigma^2 } + \frac{ \displaystyle \sum _{i = 1}^ n (x_ i-\mu)^2}{2 \sigma ^4} = 0\\
  \implies 
  $$
  Setting this equal to $0$, we get
  $$
  \widehat{S}_n = \sigma^2 = \frac{1}{n} \sum _{i = 1}^ n (X_ i-\hat{\mu})^2 = \frac{1}{n} \sum _{i = 1}^ n (X_ i-\bar{X}_n)^2
  $$


# Recitation 1: Mode of Convergence

## Example 1

Given $U \sim \mathsf{Unif}[0,1], X_n = U + U^n$, Claim: $X_n \xrightarrow[]{a.s.} U$.

**Proof**: 

We divide the sample space into two disjoint parts:

* $U < 1, X_n \xrightarrow[n \rightarrow \infty]{}U$
* $U = 1, X_n = 2$

$\mathbf{P}(X_n \xrightarrow[n \rightarrow \infty]{}U) = \mathbf{P}(X_n \rightarrow U | U < 1) \mathbf{P}(U < 1) + \mathbf{P}(X_n \rightarrow U|U=1) \mathbf{P}(U=1) = 1 + 0 = 1$

Since $U$ is a continuous random variable, $\mathbf{P}(U=1) = 0$.

## Example 2

Given $X_1, ...,X_n \stackrel{iid}{\sim} \mathsf{Unif[0,1]}, X_{(1)} = \min X_i$, Claim: $X_{(1)} \xrightarrow[]{P} 0$.

**Proof**:
Fix $\epsilon > 0$,
$$
\begin{aligned}
\mathbf{P}(|X_{(1)}-0| > \epsilon) &= \mathbf{P}(X_{(1)} > \epsilon)\\
&= \mathbf{P}(X_i > \epsilon, \forall i)\\
&= (\mathbf{P}(X_1 > \epsilon))^n\\
&= (\int^1_{\epsilon} 1 \, \mathsf{dx})^n\\
&= (1- \epsilon)^n \xrightarrow[n \rightarrow \infty]{} 0 \quad \text{ Since } 0<\epsilon < 1
\end{aligned}
$$

## Example 3

Given $X_1, ...,X_n \stackrel{iid}{\sim} \mathsf{Unif[0,1]}, X_{(1)} = \max X_i$, Claim: $n(1-X_{(n)}) \xrightarrow[]{(d)} \mathsf{Exp}(1)$.

**Proof**:
$$
\mathbf{P}(X_{(n)} < x) = \mathbf{P}(X_i < x, \forall i) = (x)^n \quad \text{by independence}
$$
Recall that $(1- {x \over n})^2 \rightarrow e^{-x}$, so we plug it in the equation above and get
$$
\mathbf{P}(X_{(n)} < 1-{x \over n}) = (1- { x \over n})^n\\
\mathbf{P}(x < n(1-X_{(n)})) = ( 1- {x \over n})^n\\
$$
Now we write it as CDF
$$
F_{n(1-X_{(n)})}(x) = 1 - \mathbf{P}(x < n(1- X_{(n)}) ) = 1 - (1- { x \over n})^n \xrightarrow[]{n \rightarrow \infty} 1 - e^{-x}
$$
which is the CDF of $\rm{Exp}(1)$.

## Example 4

Given $U \sim \rm{Unif}[0,1],$
$$
\begin{aligned}
X_1 &= U+ \mathbf{1}(U \in [0,1])\\
X_2 &= U+ \mathbf{1}(U \in [0,1/2])\\
X_3 &= U+ \mathbf{1}(U \in [1/2,1])\\
X_4 &= U+ \mathbf{1}(U \in [0,1/3])\\
X_5 &= U+ \mathbf{1}(U \in [1/3,2/3])\\
X_6 &= U+ \mathbf{1}(U \in [2/3,1])\\
\end{aligned}
$$
**Claim 1**: $X_n \xrightarrow[]{P} U$. **Proof**:

Fix $0 < \epsilon < 1$,
$$
\mathbf{P}(|X_n - U|> \epsilon) = \mathbf{P}(U \in \{ a_n, b_n\}) = b_n - a_n \xrightarrow[n \rightarrow \infty]{} 0
$$
Since when $U \in \{a_n ,b_n\}$, the value $X_i -U= \mathbf{1}(U \in [a_n, b_n])$ tends to be larger. When $n$ is large, the interval tends to be small.

**Claim 2**: $X_n \xrightarrow[]{a.s.} U$. **Disproof**:
$$
\mathbf{P}(X_n \rightarrow U) \neq 1
$$
Since $X_n$ does not converge to any number, i.e. $X_n$ has no limit.

## Summary

* Law of Large Numbers (LLN): 

  $X_1, ..., X_n, i.i.d, \,\, E[X_i] = \mu \implies \overline{X} = {1 \over n} \sum^n_{i=1 }X_i$

* **Central Limit Theorem (CLT)** is a typical example of convergence in distribution.

  $X_1, ..., X_n, i.i.d, \,\, \mathbb{E}[X] = \mu, \mathsf{Var}[X] = \sigma^2 < \infty \implies \sqrt{n}(\overline{X_n} - \mu) \xrightarrow[]{(d)} \mathcal{N}(0, \sigma^2)$



# Lecture 14. Wald's Test, Likelihood Ratio Test, and Implicit Hypothesis Test

There are 8 topics and 5 exercises.

## 1. Worked Example: Two-Sample T-test with Small Sample Sizes

#### Non-asymptotic Two-Sample Test using t-statistic

Assume

* $\, X_1,\dots ,X_ n\stackrel{iid}{\sim }\mathcal{N}(\mu _ X,\sigma _ X^2),\,$
* $\, Y_1,\dots ,Y_ m\stackrel{iid}{\sim }\mathcal{N}(\mu _ Y,\sigma _ Y^2),\,$
* $X_1,\dots ,X_ n,Y_1,\dots ,Y_ m$ are independent.

Then, for any $n$ and $m$, the distribution of the test statistic below is approximated by a $t$-distribution:
$$
\frac{\overline{X}_ n-\overline{Y}_ m-(\mu _ X-\mu _ Y)}{\sqrt{\hat{\sigma _ X^2}/n+\hat{\sigma _ Y^2}/m}}
$$
where the degrees of freedom $N$​​ Is given by the **Welch-Satterthwaite formula**:
$$
\min (n,m)\, \leq \, {{ N\, =\, \frac{\big (\hat\sigma _ X^2/n + \hat\sigma _ Y^2/m\big )^2}{\frac{\hat\sigma _ X^4}{n^2(n-1)}+\frac{\hat\sigma _ Y^4}{m^2(m-1)}}}} \, \leq \, n+m
$$

#### Example

Given $n=20, m=12, \bar{X}_n=156.4, \bar{Y}_m=132.7,\ hat{\sigma}_d^2 = 5198.4, \hat{\sigma}_c^2 = 3867.0$​​, the test statistic is
$$
{156.4-132.7\over \sqrt{{5198.4\over20 } + {3867.0\over12 }}} = 0.982
$$
We fail to reject the null hypothesis since $0.982$​ is less than $q_{5\%}$​ of a standard Gaussian which is $1.645$.

Since the sample size is small, this may not be a valid test. So this number is suppose to be the realization of a t-distribution. 

* Using the shorthand formula $N = \min(n,m) = 12$​​​, we get $q_{5\%} = 1.78$​​​​ so we fail to reject the null hypothesis as $1.78 > 0.982$. The p-value is
  $$
  \text{p-value} = \mathbf{P}[t_{12}  >0.982] = 17.27\%
  $$
  Note that the t-distribution with $12$ degrees of freedom looks like a Gaussian but with heavier tails.

* Using the W-S formula
  $$
  N = { ({5198.4\over 20} + {3867.0\over 12})^2\over {5198.4^2\over 20^2(20-1)} + {3867.0^2\over 12^2(12- = 26.07)}} = 26.07
  $$
  We round to $26$.

  We get the p-value
  $$
  \text{p-value} = \mathbf{P}[t_{26} > 0.982] = 16.76\%
  $$
  Note that $16.76\%$ is smaller than $17.27\%$ since intuitively we are working with more data and so we have more evidence on rejecting the null hypothesis.

#### Recap: Advantages and Limitations of T-test

* Advantage of Student's test: Non asymptotic / Can be run on small samples.
* Drawback of Student's test: It relies on the assumption that the sample is Gaussian.

## 2. Interlude: Square roots of a positive semi-definite matrices

Recall that a matrix $\mathbf{A}$​ of size $d \times d$​ is **positive semi-definite** if $\, \mathbf{x}^ T\mathbf{A}\mathbf{x}\geq 0\,$​ for all $\, \mathbf{x}\in \mathbb {R}^ d.\, \,$​ Two example classes of positive semi-definite matrices are

* Diagonal matrices with non-negative entries: $\, \mathbf{D}=\begin{pmatrix}  c_1 & 0& \ldots & 0\\ 0& c_2& & 0\\ \vdots & & \ddots & \vdots \\ 0& & \ldots & c_ d\end{pmatrix}\,$ where $c_i \geq 0$ For all $i$.

* Matrix products $\, \mathbf{P}^ T\mathbf{D}\mathbf{P}\,$ where $\mathbf{P}$ is an **invertible** (square) matrix and $\mathbf{D}$​ is a **diagonal** matrix with non-negative entries. Proof:
  $$
  \, \mathbf{x}^ T(\mathbf{P}^ T\mathbf{D}\mathbf{P})\mathbf{x}=(\mathbf{P}\mathbf{x})^ T \mathbf{D}(\mathbf{P}\mathbf{x})=\mathbf{y}^ T\mathbf{D}\mathbf{y}\geq 0 \qquad \text{for all vectors }\mathbf{x}
  $$

The **positive semi-definite square root** (or simply the square root) of a positive semi-definite matrix $\mathbf{A}$ is another positive semi-definite matrix, denoted by $\mathbf{A}^{1/2}$, satisfying $\, \mathbf{A}^{1/2}\mathbf{A}^{1/2}=\mathbf{A}.\, \,$​ It is the case that for any positive semi-definite matrix (positive definite matrix, respectively), the positive semi-definite square root (positive definite square root, respectively) is unique.

> #### Exercise 81
>
> Let 
> $$
> \begin{aligned}
> \mathbf{A} =\mathbf{P}^ T\mathbf{D}\mathbf{P}\qquad \text {where } \mathbf{D} &= \begin{pmatrix} 3& 0\\ 0& 0\end{pmatrix}\\
> \mathbf{P} &= \frac{1}{\sqrt{2}}\begin{pmatrix} 1& -1\\ 1& 1\end{pmatrix}.
> \end{aligned}
> $$
> Note that $\, \mathbf{P}^ T=\mathbf{P}^{-1}.$
>
> Find the square root $\, \mathbf{A}^{1/2}\,$ of the matrix $\mathbf{A}$.
>
> Hint: $\mathbf{P}^ T \mathbf{B}^2 \mathbf{P}=\mathbf{P}^ T \mathbf{B}(\mathbf{P}\mathbf{P}^ T)\mathbf{B}\mathbf{P}$
>
> **Solution:**
> $$
> \begin{aligned}
> (\mathbf{P}^ T\mathbf{D}^{1/2}\mathbf{P})(\mathbf{P}^{T} \mathbf{D}^{1/2}\mathbf{P}) &= \mathbf{P}^ T\mathbf{D}^{1/2}(\mathbf{P}\mathbf{P}^{T})\mathbf{D}^{1/2}\mathbf{P}
> \\
> &=\mathbf{P}^ T\mathbf{D}^{1/2}(\mathbf{P}\mathbf{P}^{-1})\mathbf{D}^{1/2}\mathbf{P}\qquad \text {since } \mathbf{P}^ T=\mathbf{P}^{-1}\\
> &= \mathbf{P}^ T\mathbf{D}^{1/2}\mathbf{D}^{1/2}\mathbf{P}\\
> &= \mathbf{P}^ T\mathbf{D}\mathbf{P}
> \end{aligned}
> $$
> Hence, $\, \mathbf{A}^{1/2}\, =\, \mathbf{P}^ T\mathbf{D}^{1/2}\mathbf{P}.\, \,$ Plugging in the values of $\mathbf{D}$ and $\mathbf{P}$, we get
> $$
> \begin{aligned}
> \mathbf{A}^{1/2}\, =\, \mathbf{P}^ T\mathbf{D}^{1/2}\mathbf{P}
> &= \left(\frac{1}{\sqrt{2}}\begin{pmatrix} 1& 1\\ -1& 1\end{pmatrix} \right) \begin{pmatrix} \sqrt{3}& 0\\ 0& 0\end{pmatrix} \left(\frac{1}{\sqrt{2}}\begin{pmatrix} 1& -1\\ 1& 1\end{pmatrix}\right)
> &= \frac{1}{2}\begin{pmatrix} \sqrt{3}& 0\\ -\sqrt{3}& 0\end{pmatrix}\begin{pmatrix} 1& -1\\ 1& 1\end{pmatrix}\\
> &= \frac{\sqrt{3}}{2}\begin{pmatrix} 1& -1\\ -1& 1\end{pmatrix}
> \end{aligned}
> $$

## 3. Wald's Test

#### A test based on the MLE

Consider an i.i.d. sample $X_1, ..., X_n$​​​​ with statistical model $(E, (\mathbf{P}_\theta)_{\theta \in \Theta})$​​​​, where $\Theta \in \R^d(d \geq 1)$​​ and let $\theta_0 \in \Theta$​ be fixed and given.

Consider the following hypotheses:
$$
\begin{cases} 
H_0: & \theta = \theta_0\\
H_1: & \theta \neq \theta_0.
\end{cases}
$$
Let $\hat{\theta}^{MLE}$ be the MLE. Assume the MLE technical conditions are satisfied.

If $H_0$ is true, then
$$
\sqrt{n}\ \mathcal{I}(\theta_0)^{1/2} \times (\hat{\theta}_n^{MLE} - \theta_0) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}_d(0,I_d)\\
\sqrt{n}\ \mathcal{I}(\theta^*)^{1/2} \times (\hat{\theta}_n^{MLE} - \theta_0) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}_d(0,I_d)\\
\sqrt{n}\ \mathcal{I}(\theta^{MLE})^{1/2} \times (\hat{\theta}_n^{MLE} - \theta_0) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}_d(0,I_d)
$$
are all true.

> #### Exercise 82
>
> Suppose 
> $$
> \mathbf{M} = \begin{pmatrix} \cos \phi & -\sin \phi \\ \sin \phi & \cos \phi \end{pmatrix}.
> $$
>
> 1. Is it true that $\, \mathbf{M}^ T\mathbf{M}=\mathbf{I}_{2}$​​? (Here $\mathbf{I}_{2}=\begin{pmatrix} 1& 0\\ 0& 1\end{pmatrix}$​​ is the identity matrix in 2 dimensions).
> 2. Now, let $\, \mathbf{Z}\sim \mathcal{N}_2(\mathbf{0},\mathbf{I}_{2}),\,$ i.e. $\mathbf{Z}$ is a standard Gaussian in $2$ dimensions. Is it true that $\, \mathbf{M}\mathbf{Z}\, \sim \mathcal{N}_2(\vec{\mu },\Sigma _{\mathbf{M}\mathbf{Z}})$, for some $\vec{\mu },\Sigma _{\mathbf{M}\mathbf{Z}}$?
> 3. Find the mean $\, \vec{\mu }=\mathbb E[\mathbf{M}\mathbf{Z}]\,$​ and covariance matrix $\, \Sigma _{\mathbf{M}\mathbf{Z}}\,$​ of $\mathbf{MZ}$​. 
>
> **Answer:**
>
> 1. True.
> 2. True.
> 3. $\begin{pmatrix} 0\\ 0\end{pmatrix}$ and $\begin{pmatrix} 1& 0\\ 0& 1\end{pmatrix}$.
>
> **Solution:**
>
> 1. Since
>    $$
>    \begin{aligned}
>    \mathbf{MM}^T &= \begin{pmatrix} \cos \phi & -\sin \phi \\ \sin \phi & \cos \phi \end{pmatrix}\begin{pmatrix} \cos \phi & \sin \phi \\ -\sin \phi & \cos \phi \end{pmatrix}\\
>    &= \begin{pmatrix} \cos ^2\phi +\sin ^2\phi & \cos \phi \sin \phi -\cos \phi \sin \phi \\ \cos \phi \sin \phi -\cos \phi \sin \phi & \sin ^2\phi +\cos ^2\phi \end{pmatrix}\, \\
>    &=\, \begin{pmatrix} 1& 0\\ 0& 1\end{pmatrix}.
>    \end{aligned}
>    $$
>    Hence $\, \mathbf{M}^{T}\mathbf{M}=\mathbf{I}_{2}\,$ or equivalently $\, \mathbf{M}^ T=\mathbf{M}^{-1}\,$
>
>    **Remark:** Geometrically, $\mathbf{M}$ rotates a vector $\mathbf{z}$ by an angle $\phi$ counterclockwise. Hence $\left\|  \mathbf{M}\mathbf{z} \right\| =\left\|  \mathbf{z} \right\| \,$ for any nonzero $\mathbf{z}$.
>
> 2. Since a main property of (multivariate) Gaussian variables is that any linear transformation of them remain (multivariate) Gaussian.
>
> 3. Compute the mean and covariance of $\mathbf{MZ}$.
>    $$
>    \begin{aligned}
>    E[\mathbf{M}\mathbf{Z}] &= \mathbf{M0} = 0\\
>    \Sigma _{\mathbf{M}\mathbf{Z}} &= \mathbb{E}[(\mathbf{MZ})(\mathbf{MZ})^T]= \mathbb{E}[\mathbf{MZZ^T M^T}] = \mathbf{M} \cdot \mathbb{E}[\mathbf{ZZ^T}]\mathbf{M}^T\\
>    &=  \mathbf{M}\Sigma _{\mathbf{Z}}\mathbf{M}^ T\, =\, \mathbf{M}\mathbf{I}_{2} \mathbf{M}^ T\, =\, \mathbf{M}\mathbf{M}^{-1}\, =\, \mathbf{I}_{2}.
>          
>    \end{aligned}
>    $$
>    Hence, $\, \mathbf{M}\mathbf{Z}\, \sim \mathcal{N}_ d\left(\mathbf{0},\mathbf{I}_{2}\right),\,$​i.e. a **standard** Gaussian vector.
>
>    **Remark:** Real matrices satisfying $\, \mathbf{M}^{T}=\mathbf{M}^{-1}\,$​​ (Or equivalently $\mathbf{M}\mathbf{M}^ T=\mathbf{M}^ T\mathbf{M}=\mathbf{I}_{d}$​​) are called **orthogonal** matrices. In general, in $d$​ dimensions and for any orthogonal matrix $\mathbf{M}$​, $\mathbf{MZ}$ is also a **standard** multivariate Gaussian vector if $\mathbf{Z}$ is a standard multivariate Gaussian.

#### Review: Asymptotic Normality of the MLE

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathbf{P}_{\theta ^*}$​ for some true parameter $\theta ^* \in \mathbb {R}^ d$​. We construct the associated statistical model $(\mathbb {R}, \{  \mathbf{P}_\theta \} _{\theta \in \mathbb {R}^ d } )$​ and the MLE $\widehat{\theta }_ n^{MLE}$​ for $\theta^*$.

Recall that, under some technical conditions,
$$
\sqrt{n}(\widehat{\theta }_ n^{MLE} - \theta ^*) \xrightarrow [n \to \infty ]{(d)} \mathcal{N}(0, \mathcal{I}(\theta ^*)^{-1})
$$
where $\mathcal{I}(\theta ^*)$​ denotes the Fisher information. That is, the MLE $\widehat{\theta }_ n^{MLE}$​ is asymptotically normal with asymptotic covariance matrix $\mathcal{I}(\theta ^*)^{-1}$​.

Standardize the statement of asymptotic normality above, we find that
$$
\sqrt{n}\,  \mathcal{I}(\theta ^*)^{1/2} (\widehat{\theta }_ n^{MLE} - \theta ^*) \xrightarrow [n \to \infty ]{(d)} \mathcal{I}(\theta ^*)^{1/2} \mathbf{N}(\mathbf{0}, \mathcal{I}(\theta ^*)^{-1}) = \mathbf{N}(0, \mathbf{I}_{d}).
$$
**Proof:**

By the result of *Exercise 82*, if $\mathbf{X}\sim \mathcal{N}(\mathbf{0}, \mathcal{I}(\theta ^*)^{-1})$​​, the $\mathcal{I}(\theta ^*)^{1/2} \mathbf{X}$​​ is mean $0$​​ and has covariance matrix
$$
\mathcal{I}(\theta ^*)^{1/2} \mathcal{I}(\theta ^*)^{-1} \left(\mathcal{I}(\theta ^*)^{1/2}\right)^ T = \mathcal{I}(\theta ^*)^{1/2} \mathcal{I}(\theta ^*)^{-1} \mathcal{I}(\theta ^*)^{1/2} = \mathbf{I}_{d}.
$$
Indeed, $\mathcal{I}(\theta ^*)^{1/2} \mathbf{X}\sim \mathcal{N}(\mathbf{0}, \mathbf{I}_{d})$.

By the asymptotic normality of the MLE,
$$
\sqrt{n}(\widehat{\theta }_ n^{MLE} - \theta ^*) \xrightarrow [n \to \infty ]{(d)} \mathcal{N}(\mathbf{0}, \mathcal{I}(\theta ^*)^{-1})
$$
So that by continuity,
$$
\sqrt{n}\,  \mathcal{I}(\theta ^*)^{1/2} (\widehat{\theta }_ n^{MLE} - \theta ^*) \xrightarrow [n \to \infty ]{(d)} \mathcal{I}(\theta ^*)^{1/2} \mathcal{N}(\mathbf{0}, \mathcal{I}(\theta ^*)^{-1}) = \mathcal{N}(0, \mathbf{I}_{d}).
$$

#### Review: Chi-Squared Distribution

The $\chi^2$​ distribution with $d$​ degrees of freedom is by definition the distribution of
$$
Z_1^2+Z_2^2\ldots +Z_ d^2\qquad \text {where}\, Z_ i\stackrel{iid}{\sim }\mathcal{N}(0,1)
$$
or equivalently the distribution of
$$
\left\|  \mathbf{Z} \right\| ^2 \qquad \text {where }\, \mathbf{Z}\sim \mathcal{N}_ d(\mathbf{0},\mathbf{I}1),
$$
whose components are independent because the off-diagonal elements of the covariance matrix $\mathbf{1}$​ are all $0$.

**Remark:** The vector $\mathbf{MZ}$​​​​, where $\, \mathbf{M}^{T}=\mathbf{M}^{-1}\,$​​​​ (or equivalently $\, \mathbf{M}\mathbf{M}^ T=\mathbf{M}^ T\mathbf{M}=\mathbf{1}_{d\times d},\,$​​​​) is also a **standard multivariate Gaussian vector**. Hence, $||\mathbf{MZ}||^2$​​​ also follows a $\chi^2_d$ distribution.

#### Review: Norm Squared

Given a squared norm $\, \left\|  \mathbf{A}\mathbf{x} \right\| ^2\,$​​​​​​ of the vector $\mathbf{Ax}$​​​​​, where $\mathbf{A}$​​​ is a symmetric $d\times d$​ matrix and $\mathbf{x}$ is a vector in $\R^d$, we have
$$
\begin{aligned}
\left\|  \mathbf{A}\mathbf{x} \right\| ^2 &= (\mathbf{A}\mathbf{x})^ T(\mathbf{A}\mathbf{x})\, =\, \mathbf{x}^ T\mathbf{A}^ T\mathbf{A}\mathbf{x}\\
&=\mathbf{x}^ T\mathbf{A}^ T\mathbf{A}\mathbf{x}\qquad (\text {since } \mathbf{A}^ T=\mathbf{A})\, =\, \mathbf{x}^ T\mathbf{A}^2\mathbf{x}
\end{aligned}
$$

#### Wald's test

Hence, 
$$
T_n = n\left(\hat{\theta}_n^{MLE} - \theta_0\right)^T \mathcal{I}(\hat{\theta}^{MLE}) \left(\hat{\theta}_n^{MLE} - \theta_0\right) \xrightarrow[n \rightarrow \infty]{(d)}\chi_d^2
$$
Wald's test with asymptotic level $\alpha \in (0,1)$:
$$
\psi = \mathbb{1}\{T_n > q_\alpha\}
$$
where $q_\alpha$​ is the $(1-\alpha)$​-quantile of $\chi_d^2$​.

**Proof:**

Knowing that
$$
\sqrt{n}\,  \mathcal{I}(\hat{\theta})^{1/2} (\widehat{\theta }_ n - \theta_0) \xrightarrow [n \to \infty ]{(d)} = \mathcal{N}(0, \mathbf{I}_{d\times d}).
$$
Let $\mathbf{V} = \mathcal{I}(\hat{\theta})^{1/2}(\hat{\theta} - \theta_0)$​​, which is a standard Gaussian random variable. 

By definition of the $\chi^2$ distribution with $d$ degrees of freedom.  $||V||^2= V^T V$ is $\chi^2_d$​​​ distributed. Thus by continuity, we have
$$
||V||^2= V^TV = (\mathcal{I}(\theta_0)^{1/2}(\hat{\theta} - \theta_0))^T(\mathcal{I}(\theta_0)^{1/2}(\hat{\theta} - \theta_0)) = (\hat{\theta} - \theta_0)\mathcal{I}(\theta_0) (\hat{\theta}- \theta_0)
$$
is $\chi^2_d$​ distributed.

Equivalently,
$$
\left\|  \sqrt{n} \mathcal{I}({\theta_0})^{1/2} (\widehat{\theta }_ n^{MLE} - \theta_0) \right\| _2^2 \xrightarrow [n \to \infty ]{(d)} \chi _ d^2.
$$
**Remark:** Wald's test is also valid if $H_1$​​​ has the form "$\theta > \theta_0$​​​" or "$\theta < \theta_0$​​​" or "$\theta = \theta_0$​​​"...

> #### Exercise 83
>
> Let $\, X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathbf{P}_{\theta ^*}$ for some true parameter $\theta ^* \in \mathbb {R}^ d$. We construct the associated statistical model $(\mathbb {R}, \{  \mathbf{P}_\theta \} _{\theta \in \mathbb {R}^ d } )$ and the maximum likelihood estimator $\hat{\theta}^{MLE}_n$ for $\theta^*$.
>
> We have hypotheses:
> $$
> H_0: \theta^* = 0\\
> H_1: \theta^* \neq 0
> $$
> Assuming that the null hypothesis is true, the asymptotic normality of the MLE $\widehat{\theta }_ n^{MLE}\,$ implies that the following random variable
> $$
> \left\|  \sqrt{n}\, \mathcal{I}(\mathbf{0})^{1/2}(\widehat{\theta }_ n^{MLE}- \mathbf{0})  \right\| ^2
> $$
> converges to a $\chi^2_k$ distribution.
>
> Derive the Wald's Test. 
>
> **Solution:**
>
> Under the assumption $X_1, \ldots , X_ n \stackrel{iid}{\sim } P_{\mathbf{0}}$,
> $$
> \sqrt{n} \mathcal{I}(\mathbf{0})^{1/2} (\widehat{\theta }_ n^{MLE} - \mathbf{0}) \xrightarrow [n \to \infty ]{(d)} = \mathcal{N}(\mathbf{0}, I_{d \times d}).
> $$
> Next, by definition of the $\chi^2$​​​​​ distribution and  by continuity, we have
> $$
> \left\|  \sqrt{n} \mathcal{I}(\mathbf{0})^{1/2} (\widehat{\theta }_ n^{MLE} - \mathbf{0}) \right\| _2^2 \xrightarrow [n \to \infty ]{(d)} \chi _ d^2.
> $$
> To derive the Wald's test for the given hypotheses, we define the **test statistic** $W_n$
> $$
> W_ n := \left\|   \sqrt{n}\mathcal{I}(\mathbf{0})^{1/2}(\widehat{\theta }_ n^{MLE}- \mathbf{0})  \right\| ^2 = n (\widehat{\theta }_ n^{MLE}- \mathbf{0})^ T \mathcal{I}(\mathbf{0}) (\widehat{\theta }_ n^{MLE}- \mathbf{0}).
> $$
> Then, Wald's test of level $\alpha$ is the test
> $$
> \psi _\alpha = \mathbf{1}(W_ n > q_\alpha (\chi ^2_ d)),
> $$
> where $\, q_\alpha (\chi ^2_ d)\,$​is the $1-\alpha$-quantile of the (pivotal) distribution $\chi^2_d$. 

#### Comparing quantiles

Let $\, Z\sim \mathcal{N}(0,1).\, \,$Then $\, Z^2\sim \chi ^2_1$.

The quantile $\, q_{\alpha }(\chi ^2_1)$ of the $\chi_1^2$-distribution is then umber such that
$$
\mathbf{P}\left(Z^2>q_{\alpha }(\chi ^2_1)\right)=\alpha .
$$
The quantiles of the $\chi_1^2$​​​ distribution in terms of the quantiles of the normal distribution is $q_{\alpha/2}^2$.

**Proof:** 

Since $Z^2 > q$​ for​ any $q > 0$​ if and only if $|Z| > \sqrt q$​, we have
$$
P(Z^2> q_{\alpha }(\chi ^2_1))\, =\, P(\left| Z \right|> \sqrt{q_{\alpha }(\chi ^2_1)})\, =\, \alpha .
$$
Since $\, Z\sim \mathcal{N}(0,1),\, P(\left| Z \right|> \sqrt{q_{\alpha }(\chi ^2_1)})\, =\, \alpha \,$​if and only if
$$
\sqrt{q_{\alpha }(\chi ^2_1)}=q_{\alpha /2}(\mathcal{N}(0,1))
$$
Hence $\, q_{\alpha }(\chi ^2_1)=q_{\alpha /2}(\mathcal{N}(0,1))^2$.

For example, for $\alpha = 5\%$, using a table or software, we have
$$
q_{0.05}(\chi ^2_1) \approx 3.84\\
q_{0.025}(\mathcal{N}(0,1))^2 \approx (1.96)^2  \approx 3.84
$$

## 4. Wald's Test in 1 dimension

**In 1 dimension, Wald's Test coincides with the two-sided test based on on the asymptotic normality of the MLE.**

Given the hypotheses
$$
H_0: \theta^* = \theta_0\\
H_1: \theta^* \neq \theta_0\\
$$
a two-sided test of level $\alpha$, based on the asymptotic normality of the MLE, is
$$
\psi _\alpha =\mathbf{1}\left(\sqrt{nI(\theta _0)} \left| \widehat{\theta }^{\text {MLE}} -\theta _0 \right|>q_{\alpha /2}(\mathcal{N}(0,1))\right)
$$
where the Fisher information $\, I(\theta _0)^{-1}\,$​ is the asymptotic variance $\hat{\theta}^{MLE}$​ under the null hypothesis. 

On the other hand, a Wald's test of level $\alpha$ is
$$
\begin{aligned}
\psi_\alpha^{Wald} &= \mathbf{1}\left(nI(\theta _0) \left(\widehat{\theta }^{\text {MLE}} -\theta _0\right)^2\, >\, q_{\alpha }(\chi ^2_1)\right)\\
&= \mathbf{1}\left(\sqrt{nI(\theta _0)} \, \left| \widehat{\theta }^{\text {MLE}} -\theta _0 \right|\, >\, \sqrt{q_{\alpha }(\chi ^2_1)}\right).
\end{aligned}
$$
Using the result from the problem above, we see that the two-sided test of level $\alpha$​​ is the same as Wald's test at level $\alpha$.

## 5. Review: Power of a test for different alternative hypotheses

Recall that the **power** $\pi_\psi$ of a test $\psi$​ for the hypotheses
$$
H_0: \theta^* \in \Theta_0\\
H_1: \theta^* \in \Theta_1
$$
is defined to be
$$
\pi_\psi = \inf_{\theta\in\theta_1} (1- \beta_\psi(\theta))
$$
where $\beta_\psi(\theta) = \mathbf{P}_\theta(\psi = 0)$, defined for $\theta \in \Theta_1$, is the **type 2 error rate** of test $\psi$.

Suppose $X_1, ..., X_n$ are i.i.d. random variable (in 1 dimension). Assume the theorem of MLE applies so that $\hat{\theta }^{\text {MLE}}$ is asymptotically normal. You use the test
$$
\psi =\mathbf{1}\left(\sqrt{nI}\, \left| \hat{\theta }^{\text {MLE}}-0 \right|>C_\alpha \right),
$$
which has level $\alpha$​ for some threshold $C_\alpha$​ to test the hypotheses
$$
H_0: \theta^* = 0\\
H_1: \theta^* \neq 0
$$
As
$$
\begin{aligned}
\pi_\psi &= \inf _{\theta \neq 0} \left(1-\beta _\psi (\theta )\right)\\
&=\inf _{\theta \neq 0} \mathbf{P}_\theta (\psi =1)\, =\, \inf _{\theta \neq 0} \mathbf{P}_\theta \left(\sqrt{nI}\, \left| \hat{\theta }^{\text {MLE}}-0 \right|>C_\alpha \right)
\end{aligned}
$$
$\, \sqrt{nI}\, \left(\hat{\theta }^{\text {MLE}}-\theta \right)\sim \mathcal{N}(0,1)\,$ (Asymptotically if $\theta^* = \theta$), $\, \mathbf{P}_\theta \left(\sqrt{nI}\, \left| \hat{\theta }^{\text {MLE}}-0 \right|>C_\alpha \right)\,$ decrease as $\theta \rightarrow 0$ and approaches $\, \mathbf{P}_0\left(\sqrt{nI}\, \left| \hat{\theta }^{\text {MLE}}-0 \right|>C_\alpha \right)\, =\alpha \,$​. 

Hence, the asymptotic power $\pi_\psi$​​ with $H_1: \theta^* \neq0$​​ is $\alpha$​​. 

If we use the same test $\, \psi =\mathbf{1}\left(\sqrt{nI}\, \left| \hat{\theta }^{\text {MLE}}-0 \right|>C_\alpha \right)\,$​​ to test a different alternative hypothesis against the same null hypothesis:
$$
H_0: \theta^* = 0\\
H_1: \theta^* = 1
$$
then
$$
\pi_\psi = \mathbf{P}_{\theta =1}\left(\sqrt{nI}\, \left| \hat{\theta }^{\text {MLE}}-0 \right|>C_\alpha \right)
$$
which is greater than $\, \mathbf{P}_{\theta =0}\left(\sqrt{nI}\, \left| \hat{\theta }^{\text {MLE}}-0 \right|>C_\alpha \right)=\alpha .\,$ 

The (smallest) asymptotic level of $\psi$​ stays the same (since the alternative hypothesis has no effect on the level of the test once the test has been fixed), and the asymptotic power of $\psi$​ increases.

## 6. Performing Wald's Test on a Gaussian Data Set

Suppose $X_1, \ldots , X_ n \stackrel{iid}{\sim } N(\mu , \sigma ^2)$. Your goal is to hypothesis test between
$$
H_0: (\mu, \sigma^2) = (0,1)\\
H_1: (\mu, \sigma^2) \neq (0,1)
$$
Recall Wald's test from a previous problem, which, under the above hypothesis, takes the form
$$
\psi _\alpha := \mathbf{1}\left(W_ n > q_\alpha (\chi _2^2) \right) = \mathbf{1}\left( n \left(\widehat{\theta }_ n^ T-\begin{pmatrix} 0& 1\end{pmatrix}\right)\mathcal{I}((0,1))\left(\widehat{\theta }_ n- \begin{pmatrix} 0\\ 1\end{pmatrix}\right) > q_\alpha (\chi _2^2) \right)
$$
where $q_\alpha(\chi_2^2)$​​ is the (1-$\alpha$​)-quantile of $\chi_2^2$. You are given that the technical conditions required for the MLE to be asymptotically normal are satisfied for a Gaussian statistical model with unknown mean and variance.

Tthe smallest value of $q_\alpha(\chi_2^2 )$​​ so that $\psi_\alpha$​ is a test with asymptotic level $5\%$​ : $q_\alpha(\chi_2^2 ) \geq 5.991$​. 

Since we have assumed that the MLE is asymptotically normal, we have
$$
W_ n \xrightarrow [n \to \infty ]{(d)} \chi _2^2.
$$
There are precisely two degrees of freedom since we have two unknowns. The test $\psi_\alpha$​​​​​ has asymptotic level $5\%$​​​ if $\alpha = 5\%$​​​. Consulting a table, we see that the $0.05$​-quantile for $\chi_2^2$ is $q_\alpha = 5.991$.

Suppose you observe the data set
$$
0.2, -0.1, -1.9, -0.4, -1.8
$$
Recall that the MLE of a Gaussian $\mathcal{N}(\mu,\sigma^2)$ is given by 
$$
\begin{pmatrix} \widehat{\mu }_ n^{MLE}\\ (\widehat{\sigma ^2})_ n^{MLE}\end{pmatrix} = \begin{pmatrix}  \overline{X}_ n\\ \frac{1}{n} \sum _{i = 1}^ n ( X_ i - \overline{X}_ n )^2 \end{pmatrix}
$$
and the Fisher information is given by
$$
\mathcal{I}(\mu , \sigma ^2) = \begin{pmatrix}  \frac{1}{\sigma ^2} &  0 \\ 0 &  \frac{1}{2 \sigma ^4} \end{pmatrix}.
$$
To compute the test statistic $W_5$​​​ for the given data set, we first compute 
$$
\begin{pmatrix} \widehat{\mu }_ n^{MLE}\\ (\widehat{\sigma ^2})_ n^{MLE}\end{pmatrix} = \begin{pmatrix}  \overline{X}_ n\\ \frac{1}{n} \sum _{i = 1}^ n ( X_ i - \overline{X}_ n )^2 \end{pmatrix} = \begin{pmatrix} -0.8\\ 0.772\end{pmatrix} 
$$
The Fisher information, under the null hypothesis $(\mu, \sigma^2) = (0,1)$​, is
$$
\begin{pmatrix}  1 &  0 \\ 0 &  \frac{1}{2} \end{pmatrix}.
$$
Therefore,
$$
W_5 = 5 \cdot ( (-0.8, 0.772 ) - (0,1)) \begin{pmatrix}  1 &  0 \\ 0 &  \frac{1}{2} \end{pmatrix} \left( \begin{pmatrix} -0.8\\ 0.772\end{pmatrix} - \begin{pmatrix} 0\\ 1\end{pmatrix}\right)^ T \approx 3.33.
$$
Since $q_{0.05}=5.991 > 3.33$, we would fail to reject the null hypothesis for the given sample.​

## 7. Likelihood Ratio Test

#### Basic Form

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathbf{P}_{\theta ^*}$​​, and consider the associated statistical model $(E, \{  \mathbf{P}_{\theta } \} _{\theta \in \mathbb {R}^ d} )$​​. Suppose that $\mathbf{P}_\theta$​​ is a discrete probability distribution with PMF given by $p_\theta$​.

In its most basic form, the likelihood ratio test can be used to decide between two hypotheses of the following form:
$$
H_0: \theta^* = \theta_0\\
H_1: \theta^* = \theta_1
$$
Recall the likelihood function
$$
L_n: \R^n \times \R^d \rightarrow \R\\
(x_1, ..., x_n; \theta) \mapsto \prod _{i =1}^ n p_\theta (x_ i).
$$
The **likelihood ratio test** in this set-up is of the form
$$
\psi _ C = \mathbf{1}\left( \frac{L_ n(x_1, \ldots , x_ n; \theta _1 )}{L_ n(x_1, \ldots , x_ n; \theta _0 )} > C \right).
$$
where $C$ is a threshold to be specified.

> #### Exercise 84
>
> In this problem, you have an unfair coin. It comes up heads with probability either $25\%$​ or $75\%$​, but you don't know which is true. You design a test to decide between the two possibilities. First, you model the results of $n$​ coin flips as $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Ber}(p^*)$​ where Heads is $+1$​ and Tails is $0$​. The associated model is $(\{ 0,1 \} , \{ \text {Ber}(p)\} _{p \in (0,1)})$​.
>
> You formulate the hypotheses
> $$
> H_0: p^* = 0.25\\
> H_1: p^* = 0.75.
> $$
> You decide to use the likelihood-ratio test described above with threshold $C=1$. Suppose you observe a single coin-flip: $X_1 = 1$.
>
> 1. Should you reject or fail to reject the null hypothesis?
>
> 2. Now suppose you observe the data set
>    $$
>    X = \{1,0,0,0,1,1,1,0,0,1,0,1,0,0,0,0\}
>    $$
>    Using the specified likelihood ratio test, would you reject or fail to reject the null hypothesis?
>
> **Answer:**
>
> 1. Reject.
> 2. Fail to reject.
>
> **Solution:**
>
> 1. If we observe $X_1 = 1$, then
>    $$
>    L_1( 1; 0.25) = 0.25, \quad L_1(1; 0.75) = 0.75.
>    $$
>    Hence,
>    $$
>    \frac{L_1(1; 0.75)}{L_1( 1; 0.25)} > 1,
>    $$
>    and we would reject the null hypothesis.
>
> 2. Now consider the given data set, which consists of $6$​​​​ heads, $10$​​​ tails, and $16$​​ total coin flips. Intuitively, since there are more tails than heads, it seems more likely that the parameter $p^*$​​ is closer to $0$​​ than $1$​. And indeed, the likelihood ratio test we designed confirms this heuristic​
>    $$
>    L_{16}( \mathbf{X}; 0.25) = \left( \frac{1}{4} \right)^{6} \left( \frac{3}{4} \right)^{10} , \quad L_{16}( \mathbf{X}; 0.75) = \left( \frac{3}{4} \right)^{6} \left( \frac{1}{4} \right)^{10}.
>    $$
>    Since $L_{16}( \mathbf{X}; 0.25) > L_{16}( \mathbf{X}; 0.75)$​, we see that $\psi = 0$. For this data set, we fail to reject the null hypothesis.

#### A test based on the log-likelihood

Consider an i.i.d. sample $X_1, ..., X_n$​​ with statistical model $(E, (\mathbf{P}_\theta )_{\theta \in \Theta})$​​, where $\Theta \subseteq \R^d (d\geq 1)$​.

Suppose the null hypothesis has the form
$$
H_0: (\theta_{r+1}, ..., \theta_{d}) = (\theta_{r+1}^{(0)}, ..., \theta_{d}^{(0)})
$$
for some fixed and given numbers $\theta_{r+1}^{(0)}, ..., \theta_{d}^{(0)}$​.

Let
$$
\hat{\theta}_n = \text{argmax}_{\theta \in \Theta} \ \  \ell_n(\theta) \quad \text{(MLE)}
$$
and
$$
\hat{\theta}_n^c = \text{argmax}_{\theta \in \Theta_0} \ \ell_n(\theta) \quad \text{("constrained MLE")}
$$
where $\Theta_0 = \{\theta \in \Theta: (\theta_{n+1}, ..., \theta_d) =  (\theta^{(0)}_{n+1}, ..., \theta^{(0)}_{d})\}$​.

#### Likelihood ratio test

**Test statistic:**
$$
T_n = 2\left(\ell_n (\hat{\theta}_n)-\ell_n (\hat{\theta}_n^c)\right).
$$

*  $\ell_n (\hat{\theta}_n)$​ is the completely unconstrained MLE and is defined by the optimization problem
  $$
  \widehat{\theta _ n^{MLE}} = \text {argmax}_{\theta \in \Theta } \ell _ n(X_1, \ldots , X_ n; \theta )
  $$
   In particular, we find the maximizer over the entire parameter space $\Theta$. 

* $\ell_n (\hat{\theta}_n^c)$​ is the **constrained MLE** and is defined to be
  $$
  \widehat{\theta _ n^{c}} = \text {argmax}_{\theta \in \Theta _0} \ell _ n(X_1, \ldots , X_ n ; \theta ).
  $$

* The difference is non-negative, that is
  $$
  \ell_n (\hat{\theta}_n) \geq \ell_n (\hat{\theta}_n^c)
  $$

* **Remark:** The likelihood ratio test is a natural test in a situation where we only care about some (e.g. the last $d-r$ coordinates) of the unknowns involved in the parameter $\theta^* \in \R^d$.

**Wilk's Theorem:**

Assume $H_0$​​ is true and the MLE technical conditions are satisfied.

Then, 
$$
T_n \xrightarrow[n \rightarrow \infty]{(d)} \chi^2_{d-r}
$$

* Note that first $r$​​​​​ coordinates are not fixed. While the parameter space corresponding to $H_0$​ is $\Theta_0 = \R^r$ which, intuitively, has $r$ free variables, the test statistic $T_n$ converges to a $\chi^2$ distribution with $d-r$ degrees of freedom. 
* $T_n$​​ is a pivotal statistic; i.e., it converges to a pivotal distribution, since the distribution $\chi_{d-r}^2$​​ does not depend on the specific value of the true parameter $\theta^*$​​.

Likelihood ratio test with asymptotic level $\alpha \in (0,1)$:
$$
\psi = \mathbb{1}\{T_n > q_\alpha\},
$$
where $q_\alpha$​ is the $(1-\alpha)$​-quantile of $\chi^2_{d-r}$​.

## 8. Testing Implicit Hypotheses

Let $X_1, .., X_n$​ be i.i.d. random variables and let $\theta \in \R^d$​ be an **unknown** parameter associated with the distribution of $X_1$​ (e.g. a moment, the parameter of a statistical model, etc...)

Let $g : \R^d \rightarrow \R^k$​​ be **continuously differentiable** (with $k < d$​​).

Consider the following hypotheses:
$$
\begin{cases}H_0 : g(\theta) = 0\\ H_1 : g(\theta) \neq 0\end{cases}
$$
E.g. $g(\theta) = g(\theta_1, \theta_2)\ (k=2), \text{or } \ g(\theta) = \theta_1 - \theta_2  (k=1), \ \text{or } ...$

#### Delta Method

Suppose an **asymptotic normal estimator** $\hat{\theta}_n$​ is available:
$$
\sqrt{n} \left( \hat{\theta}_n - \theta \right) \xrightarrow[n\rightarrow \infty]{ (d)} \mathcal{N}_d(0, \Sigma(\theta)), \quad \Sigma(\theta) \in \R^{d \times d}
$$
By **delta method**, that $g(\hat{\theta}_n)$​ is also asymptotically normal, i.e., 
$$
\sqrt{n}\left(g(\hat{\theta}_n)- g(\theta))\right) \xrightarrow[n\rightarrow \infty]{(d)} \mathcal{N}_k(0, \Gamma(\theta))
$$
where $\Gamma(\theta) = \nabla g(\theta)^T \Sigma (\theta) \nabla g(\theta) \in \R^{k \times k}$​​​.

By the properties of **multivariate Gaussians**, if $\mathbf{Z} \sim \mathcal{N}(\mathbf{0}, \Gamma (\theta ^*))$, then
$$
\Gamma (\theta ^*)^{-1/2} \mathbf{Z} \sim \mathcal{N}(\mathbf{0}, I_k)
$$
provided that $\Gamma(\theta)^{1/2}$​​​ exists. 

Assume $\Sigma(\theta)$​​​​ is invertible and $\nabla g(\theta)$​​ has rank $k$. So, $\Gamma (\theta)$ is invertible and
$$
\sqrt{n}\ \Gamma(\theta)^{-1/2}\left( g(\hat{\theta}_n) - g(\theta)\right) \xrightarrow[n\rightarrow \infty]{ (d)} \mathcal{N}_k(0,{I}_k)
$$
**Remark:** For a square matrix $M$​​, we are guaranteed that $M^{1/2}$​​ exists if $M$​​ is **positive-definite**. In particular, since $\Gamma (\theta ^*)$​​ is a covariance matrix, it is guaranteed to be **positive semidefinite**. So then $\Gamma (\theta ^*)^{-1/2}$​​ exists if and only if $\Gamma (\theta ^*)$​​ is invertible. 

#### Renormalizing 

**Wald's test for implicit hypotheses**

By **Slutsky's theorem**, if $\Gamma(\theta)$​ Is continuous in $\theta$​,
$$
\sqrt{n}\ \Gamma(\hat{\theta}_n)^{-1/2} \left(g(\hat{\theta}_n) - g(\theta)\right) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}_k(0, I_k)
$$
To test between the null and alternative hypotheses, we consider the test statistic (if $H_0$ is true, i.e., $g(\theta) = 0$)
$$
T_ n := \left| \sqrt{n} \Gamma (\widehat{\theta }_ n)^{-1/2} (g(\widehat{\theta }_ n)) \right|_2^2 = n g(\widehat{\theta }_ n)^ T \Gamma (\widehat{\theta }_ n)^{-1} g(\widehat{\theta }_ n)
$$
Under the null hypothesis, the test statistic converges to 
$$
n g(\hat{\theta}_n)^T \Gamma^{-1} (\hat{\theta}_n) g(\hat{\theta}_n) \xrightarrow[n \rightarrow \infty]{(d)} \chi_k^2
$$
Test with asymptotic level $\alpha$:
$$
\psi = \mathbb{1}\{T_n > q_\alpha\},
$$
where $q_\alpha$​ is the $(1-\alpha)$​-quantile of $\chi^2_{k}$​.

> #### Exercise 85
>
> Supposing that $d=6$ and $k=3$, what value of $C$ should be chosen so that $\psi$ is a test of asymptotic level $5\%$?
>
> **Answer:** 7.815
>
> **Solution:** 
>
> When $k=3$​, then $T_ n \xrightarrow [n \to \infty ]{(d)} \chi _3^2$​. The test $\psi = \mathbf{1}(T_ n > C)$​ will have asymptotic level $5\%$​ precisely when $C$​ is the $5\%$​-quantile $q_{0.05}$ of $\chi_3^2$. Consulting a table, we have that $q_{0.05} = 7.815$.



# Recitation 12 T-test

**Review: Orthogonal matrices:** $\begin{bmatrix} | & & | \\ v_1 & ... & v_n \\ | & & | \end{bmatrix} = V \in \R^{n \times n}$

$v_i^T v_j = 0,\ i \neq j , \ v_i^T v_i = ||v||_2^2 = 1 \implies \{v_i\}_{i=1,...,n}$​​​​​​​​​​ are **orthonormal**, $V$​​ is **orthonormal**.

Then:

$$
V^TV = \begin{bmatrix} - & v_1^T & - \\ & \vdots & \\ - & v_n^T & - \end{bmatrix} \begin{bmatrix} | & & | \\ v_1 & ... & v_n \\ | & & | \end{bmatrix}= \begin{bmatrix} 1& &0\\ & ... & \\ 0 & & 1 \end{bmatrix}= I_n = VV^T
$$


$$
x \in \R^n, \  ||Vx||^2_2 = (Vx)^T(Vx) = x^T V^TVx = x^Tx = ||x||^2_2
$$
$W = \begin{bmatrix} | & & | \\ w_1 & ... & w_l \\ | & & | \end{bmatrix} \in \R^{n \times k}, \{w_i\}_{i = 1,..., l}$ are orthonormal.
$$
||WW^TX||^2_2 = (WW^TX)^T(WW^TX) = X^TWW^TWW^TX  = ||W^TX||^2_2
$$
Let $X_1,\ldots ,X_ n\stackrel{i.i.d.}{\sim }\mathcal N(\mu ,\sigma ^2)$​, for some **unknown** parameter $(\mu ,\sigma ^2)\in \mathbb {R}\times (0,\infty )$​. We want to test the following hypotheses at non-asymptotic level $\alpha$​ (for some fixed $\alpha \in (0,1)$​):
$$
H_0: \mu \ge 0 \hspace{2mm} \text{vs.} \hspace{2mm} H_1: \mu < 0.
$$

1. Recall the maximum likelihood estimator $(\hat\mu ,\hat\sigma ^2)$​ or $(\mu ,\hat\sigma ^2)$​.
2. Let ${S=\sqrt{n-1}\frac{\hat\mu -\mu }{\sqrt{\hat\sigma ^2}}}$. Prove that $S$ is a Student random variable with $n-1$ degrees of freedom.
3. Propose a test with non-asymptotic level $\alpha$. Prove your answer.

> **Solution:**
>
> 1. The likelihood is
>    $$
>    f(\mu, \sigma^2, X_1, ..., X_n) =  \prod^n_{i=1} {1\over \sqrt{2 \pi \sigma^2}} e ^{-{1\over 2 \sigma^2}(X_i - \mu)^2}
>    $$
>    The log likelihood is
>    $$
>    \begin{aligned}
>    \ell(\mu, \sigma^2) = \log f &= \sum^n_{i=1}\left[ -{1\over 2}\log 2 \pi - {1\over 2} \log \sigma^2 - {1\over 2\sigma^2} (X_i - \mu)^2 \right]\\
>    &= -{n \over 2} \log 2\pi - {n \over 2} \log \theta^2 - {1\over 2\sigma^2} \sum^n_{i=1}(X_i - \mu)^2
>    \end{aligned}
>    $$
>    Take the derivative and set it to zero, we get
>    $$
>    {\partial \ell(\mu, \sigma^2)\over \partial \mu} = -{1\over 2\sigma^2} \sum^n_{i=1} 2 \cdot (X_i - \mu) (-1) = 0\\
>    \implies \hat{\mu} = {1\over n} \sum^n_{i=1} X_i\\
>    {\partial \ell(\mu, \sigma^2) \over \sigma^2} = - {n \over 2\sigma^2} + {1\over 2 \sigma^4} \sum^n_{i=1} (X_i - \mu)^2 = 0\\
>    \implies \hat{\sigma}^2 = {1\over n}\sum^n_{i=1} (X_i - \hat{\mu})^2
>    $$
>
> 2. The variance is $\hat{\sigma}^2 = {1\over n}\sum\limits^n_{i=1} (X_i-\hat{\mu})^2$​​. 
>
>    Since $\hat{\mu} = {1\over n} \mathbb{1}^T X, \  \begin{pmatrix} \hat{\mu} \\ \vdots \\ \hat{\mu} \end{pmatrix} = {1\over n} \mathbb{1}\mathbb{1}^TX = {1\over \sqrt{n}}\mathbb{1} \left({1\over \sqrt{n}}\mathbb{1}\right)^T X = v_1 v_1^T X$​​​, we can write 
>    $$
>    \begin{aligned}
>    n \hat{\sigma}^2 &= \left|\left| \begin{pmatrix} X_1 \\ \vdots \\ X_n \end{pmatrix}-\begin{pmatrix} \hat{\mu} \\ \vdots \\ \hat{\mu} \end{pmatrix} \right|\right|^2_2\\
>    &= || X - v_1v_1^TX ||^2_2\\
>    &= || X-\mu \cdot \mathbb{1} - (v_1v_1^TX - \mu \mathbb{1}) ||^2_2\\
>    &= || X-\mu \cdot \mathbb{1} - v_1v_1^T(X - \mu \mathbb{1})||^2_2\\
>    \end{aligned}
>    $$
>    Since $Y = {X - \mu \mathbb{1} \over \sigma} = \begin{pmatrix}{X - \mu \over \sigma}\\\vdots \\{X - \mu \over \sigma} \end{pmatrix} \sim \mathcal{N}(0, I_n)$, and for orthogonal matrix $V = \begin{bmatrix} | & & | \\ v_1 & ... & v_n \\ | & & | \end{bmatrix} \in \R^{n \times n}$.
>    $$
>    \begin{aligned}
>    I_n &= VV^T =  \begin{bmatrix} - & v_1^T & - \\ & \vdots & \\ - & v_n^T & - \end{bmatrix} \begin{bmatrix} | & & | \\ v_1 & ... & v_n \\ | & & | \end{bmatrix}\\
>    &= \left(\sum^n_{i=1} v_ie_i^T\right)\left(\sum^n_{j=1} v_je_j^T\right)^T\\
>    &= \sum^n_{i,j=1} v_ie_i^Te_j v_j^T\\
>    &= \sum^n_{i=1} v_i v_i^T \qquad \text{where }e_i^Te_j = \begin{cases}1, &i=j\\0,&i \neq j \end{cases}
>    \end{aligned}
>    $$
>    We can write
>    $$
>    \begin{aligned}
>    {n\hat{\sigma}^2\over \sigma^2} & = ||Y - v_1v_1^T Y||^2_2\\
>    &= ||\sum^n_{i=1} v_iv_i^TY - v_1v_1^T Y||^2_2\\
>    &= ||\sum^n_{i=2} v_iv_i^TY ||^2_2\\
>    \end{aligned}
>    $$
>    Let $w = \begin{bmatrix} | & & | \\ v_2 & ... & v_n \\ | & & | \end{bmatrix}, w \in \R^{n \times (n-1)}$​​​​, so $\sum\limits^n_{i=2}= v_iv_i^T = WW^T$​​. Then
>    $$
>    \begin{aligned}
>    {n\hat{\sigma}^2\over \sigma^2} 
>    &= ||\sum^n_{i=2} v_iv_i^TY ||^2_2\\
>    &= ||WW^TY ||^2_2\\
>    &= ||W^TY ||^2_2, \qquad W^TY \in \R^{n-1}\\
>    \end{aligned}
>    $$
>    We now try to understand the distribution of $W^TY$​. The covariance of $W^TY$ is
>    $$
>    \text{Cov}(W^TY) = W^T \text{Cov}(Y) (W^T)^T = W^T I_nW = I_{n-1}\\
>    \implies {n \hat{\sigma}^2 \over (n-1)\sigma^2} \sim \chi^2_{n-1}
>    $$
>    We know that $T_n^{(1)} = {(\mu - \hat{\mu})\sqrt{n}\over \sqrt{\sigma^2}}\sim\mathcal{N}(0,1)$​​​​, where the $\hat{\mu}$​​​ can be written as
>    $$
>    \hat{\mu} = {1\over \sqrt{n} }v_1^T X = \mu + \sigma {1\over \sqrt{n} }v_1^T Y
>    $$
>    The covariance $\text{Cov}(W^TY, v_1^TY)$ is
>    $$
>    \begin{aligned}
>    \text{Cov}(W^TY, v_1^TY) &= \mathbb{E}[W^TYY^Tv_1] \qquad \text{where } W^TY \in \R^{n-1}, v_1^TY \in \R\\
>    &= W^T\mathbb{E}[YY^T]v_1\\
>    &= W^Tv_1\\
>    &= \begin{pmatrix}0 \\ \vdots \\ 0 \end{pmatrix} \in \R^{n-1}
>    \end{aligned}
>    $$
>    Thus $W^TY$​ and $v_1^TY$​​ are uncorrelated. And since they are jointly Gaussian, they are independent. So 
>    $$
>    W^TY \perp v_1^TY \implies \hat{\sigma}^2 \perp \hat{\mu}
>    $$
>    Therefore
>    $$
>    \tilde{T}_n^{(2)} = {{(\mu - \hat{\mu})\sqrt{n} \over \sqrt{\sigma^2}}\over \sqrt{{n \hat{\sigma}^2\over (n-1)\sigma^2}} } = {(\mu - \hat{\mu})\sqrt{n-1} \over \sqrt{\hat{\sigma}^2}} \sim t_{n-1}\\
>    $$
>    For $H_0$, we plug in $\mu = 0$, the true pivot is
>    $$
>    T_n = { -\sqrt{n-1} \hat{\mu} \over \sqrt{\hat{\sigma}^2}}
>    $$
>
> 3. For $\mu = 0$​, 
>    $$
>    \mathbf{P}_{(0,\sigma^2)}(\psi = 1) = \mathbf{P}_{(0,\sigma^2)}(T_n > S) = \mathbf{P}(Z > S) = \alpha ,\quad \text{where } Z \sim t_{n-1}\\
>    \implies S = q_{1-\alpha}(t_{n-1})
>    $$
>    For $\mu > 0$,
>    $$
>    \begin{aligned}
>    \mathbf{P}_{(\mu, \sigma^2)} (\psi = 1) &= \mathbf{P}_{(\mu, \sigma^2)} (T_n > S)\\
>    & = \mathbf{P}_{(\mu, \sigma^2)}({-\hat{\mu} \sqrt{n-1} \over \sqrt{\hat{\sigma}^2}} > S)\\
>    &=  \mathbf{P}_{(\mu, \sigma^2)}({(\mu-\hat{\mu}) \sqrt{n-1} \over \sqrt{\hat{\sigma}^2}} > S+{\mu \sqrt{n-1}\over \sqrt{\hat{\sigma}^2}})\\
>    &=  \mathbf{P}_{(\mu, \sigma^2)}(Z > S+{\mu \sqrt{n-1}\over \sqrt{\hat{\sigma}^2}}) \quad \text{where } Z \sim t_{n-1}\\
>    & \leq \mathbf{P}_{(\mu, \sigma^2)}(Z > S) = \alpha
>    \end{aligned}
>    $$
>    

# Recitation 5. Inference for the variance of a Gaussian distribution

Consider a sample of $n$ i.i.d. Gaussian random variables $X_1,\ldots ,X_ n$ with known mean $\mu = 0$ and unknown variance $\sigma^2 > 0$.

1. Let $\hat\sigma ^2$ be the sample variance of $X_1,\ldots ,X_ n$. Recall an expression of $\hat\sigma ^2$.
2. Prove that $\hat\sigma ^2$ is a consistent estimator of $\sigma^2$.
3. Using the CLT, prove that ${\left(\frac{1}{n}\sum _{i=1}^ n X_ i,\frac{1}{n}\sum _{i=1}^ n X_ i^2\right)'}$ is asymptotically normal.
4. Conclude that $\hat\sigma ^2$ is asymptotically normal and compute its asymptotic variance.
5. Using the previous questions, find a C.I. for $\sigma^2$ with asymptotic level $95\%$ (use two methods).

> **Solution:**
>
> To find asymptotic C.I. for $\sigma^2$, we go through the following three steps:
>
> 1. Find estimator $\widehat{\sigma^2}$ for $\sigma^2$.
> 2. Determine the asymptotic distribution of $\widehat{\sigma^2}$
> 3. Construct C.I. based on $\widehat{\sigma^2}$.
>
> ---
>
> 1. Since $\mathsf{Var}(X_1) = \mathbb{E}[(X_1 - \mathbb{E}[X_1])^2]= \mathbb{E}[X_1^2] - (\mathbb{E}[X_1])^2$, the estimator can be written as
>    $$
>    \widehat{\sigma^2} = {1\over n} \sum_{i=1}^n X_i^2 - \left( {1 \over n} \sum^n_{i=1} X_i\right)^2
>    $$
>     $\widehat{\sigma^2}$ is **consistent** since by LLN, 
>    $$
>    {1\over n} \sum_{i=1}^n X_i^2  \xrightarrow[n\rightarrow \infty]{\mathbb P} \mathbb{E}[X_1^2]\\
>    {1\over n} \sum_{i=1}^n X_i  \xrightarrow[n\rightarrow \infty]{\mathbb P} \mathbb{E}[X_1]\\
>    \implies \widehat{\sigma^2} \xrightarrow[n\rightarrow \infty]{\mathbb P} \mathsf{Var}(X_1)
>    $$
>
> 2. Let $Y_1, ..., Y_n$ be i.i.d random variables, then by CLT.
>    $$
>    \sqrt{n} \left( {1\over n} \sum^n_{i=1} Y_i - \mathbb{E}[Y_1] \right) \xrightarrow[n \rightarrow \infty]{ (d)} \mathcal{N}(0, \mathsf{Var}(Y_1))
>    $$
>    Let $\begin{pmatrix}Y_1\\W_1 \end{pmatrix}, ..., \begin{pmatrix}Y_n\\W_n \end{pmatrix}$ be i.i.d random variables, then by 2D CLT.
>    $$
>    \sqrt{n} \left( \begin{pmatrix}{1\over n} \sum^n_{i=1} Y_i\\{1\over n} \sum^n_{i=1} W_i \end{pmatrix} - \begin{pmatrix} \mathbb{E}[Y_1]\\\mathbb{E}[W_1]\end{pmatrix} \right) \xrightarrow[n \rightarrow \infty]{ (d)} \mathcal{N}\left(\begin{pmatrix}0\\0 \end{pmatrix},\begin{pmatrix} \mathsf{Var}(Y_1)& \mathsf{Cov}(Y_1,W_1)\\\mathsf{Cov}(Y_1, W_1) &\mathsf{Var}(W_1) \end{pmatrix}\right)
>    $$
>    Let $Y_i = X_i^2, \quad W_i = X_i$, we now compute the covariance matrix.
>
>    $\mathsf{Var}(W_i) = \sigma^2\\\mathsf{Var}(Y_1) = \mathbb{E[(X_1^2)^2]} - (\mathbb{E}[X_1^2])^2  =3 \sigma^4 - (\sigma^2)^2 = 2 \sigma^4\\ \mathsf{Cov}(Y_1, W_1) = \mathbb{E}[X_1^2 X_1] - \mathbb{E}[X_1^2]\mathbb{E}[X_1] = 0 - \sigma^2 0 = 0$
>
>    To obtain the asymptotic distribution of $\widehat{\sigma^2} = {1\over n} \sum_{i=1}^n X_i^2 - \left( {1 \over n} \sum^n_{i=1} X_i\right)^2$ , we use **multivariate delta method**
>
>    Let $T_n = {1\over n} \sum^n_{i=1} \begin{pmatrix}X_i^2 \\ X_i\end{pmatrix},\quad \widehat{\sigma^2}=g(T_n),$ we can write
>    $$
>    \widehat{\sigma^2}=g(y,w) = y - w^2
>    $$
>    
>
>    Calculate the gradient
>    $$
>    \nabla g(y,w) = \begin{pmatrix} 1 \\ -2w \end{pmatrix}\\
>    $$
>    Calculate the mean
>    $$
>    \mathbb{E}[T_n] = \begin{pmatrix}\mathbb{E}[Y_1] \\ \mathbb{E}[W_1] \end{pmatrix} = \begin{pmatrix}\sigma^2 \\ 0\end{pmatrix}
>    $$
>    Therefore, applying **multivariate delta method**, we have
>    $$
>    \sqrt{n} (\widehat{\sigma^2}-\sigma^2 )\xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}\left(0, \begin{pmatrix}1\\0 \end{pmatrix}^T \begin{pmatrix}2\sigma^2 & 0\\0& \sigma^2 \end{pmatrix}\begin{pmatrix}1\\0 \end{pmatrix} \right) = \mathcal{N}(0,2\sigma^4)
>    $$
>
> 3. Write the **two-sided** C.I. as $\mathcal{I} = [\hat{\sigma}^2 - s, \hat{\sigma}^2 + s]$.
>    $$
>    \begin{aligned}
>    &\mathbf{P}\left(\sigma^2 \in \mathcal{I}\right)\\
>    \iff& \mathbf{P}\left(\sigma^2 \in \left[\widehat{\sigma^2}-s, \widehat{\sigma^2} + s\right]\right)\\
>    \iff& \mathbf{P}\left(\sigma^2- \widehat{\sigma^2} \in [-s, s]\right)\\
>    \iff& \mathbf{P}\left(\widehat{\sigma^2}-\sigma^2 \in [-s, s]\right)\\
>    \iff& \mathbf{P}\left(\sqrt{{n \over 2}} {\widehat{\sigma^2} - \sigma^2 \over  \sigma^2} \in \left[-\sqrt{{n\over 2}} \cdot {s\over\sigma^2 },\sqrt{{n\over 2}} \cdot {s\over\sigma^2 }\right]\right)
>    \end{aligned}
>    $$
>    Let $\sqrt{{n\over 2}} \cdot {s\over\sigma^2 } = q$, we can write
>    $$
>    \mathbf{P}\left(\sigma^2 \in \mathcal{I}\right) \xrightarrow[n\rightarrow \infty]{} \mathbf{P}(Z \in [-q,q])
>    $$
>    where $Z \sim \mathcal{N}(0,1)$.
>
>    With confidence level $\alpha$, we write
>    $$
>    \mathbf{P}(Z \in [-q,q]) = 1-\alpha
>    $$
>    We want $q = q_{\alpha/2}$, $1 - \alpha/2$ quantile of $\mathcal{N}(0,1)$.
>
>    Since $\sqrt{{n\over 2}} \cdot {s\over\sigma^2 } = q$, we can write $s $ as $s = q_{\alpha/2} {\sigma^2 \sqrt{2}\over \sqrt{n} }$.
>
>    Therefore the C.I. is
>    $$
>    \mathcal{I} = \widehat{\sigma^2} + \left[ -\sqrt{{2\over n}} q_{\alpha/2} \sigma^2,\sqrt{{2\over n}} q_{\alpha/2} \sigma^2 \right]
>    $$
>    By Slutsky's theorem, since
>    $$
>    \sqrt{n} {\widehat{\sigma^2} - \sigma^2 \over \sqrt{2} \widehat{\sigma^2}} \xrightarrow[x\rightarrow \infty]{(d)} \mathcal{N}(0,1)
>    $$
>    We can write the asymptotic C.I. as
>    $$
>    \mathcal{I} = \widehat{\sigma^2} + \left[ -\sqrt{{2\over n}} q_{\alpha/2} \widehat{\sigma^2},\sqrt{{2\over n}} q_{\alpha/2} \widehat{\sigma^2} \right]
>    $$



# Lecture 13. Chi Squared Distribution, T-Test

There are 9 topics and 6 exercises.

## 1. Hypothesis Testing (Review)

#### Review:

We have seen the basic notions of hypothesis testing:

* Hypotheses $H_0 / H_1$​,
* Type 1/ Type 2 error, level and power,
* Test statistics and rejection region,
* p-value.

Our tests were based on CLT (and sometimes Slutsky)...

* What if data is Gaussian, $\sigma^2$​​​​ is unknown and Slutsky does not apply? **T-test**.
* Can we use asymptotic normality of MLE? **Wald's test**.
* Tests about multivariate parameter $\theta=(\theta_1, ..., \theta_d)$​​​​ (e.g.: $\theta_1 = \theta_2$​​​)? **Testing implicit hypotheses**.
* More complex tests: "Does my data follow a Gaussian distribution"? **Goodness of fit**.

**Problem Set-up:**

The National Assessment of Educational Progress tested a simple random sample of $n$ thirteen year old students in both 2004 and 2008 and recorded each student's score. The standard deviation in 2004 was 39. In 2008, the standard deviation was 38.

Your goal as a statistician is to assess whether or not there were statistically significant changes in the average test scores of students from 2004 to 2008. To do so, you make the following modeling assumptions regarding the test scores:

- $X_1, ..., X_n$ represent the scores in 2004.
- $X_1, ..., X_n$​ are iid Gaussians with standard deviation $39$.
- $\mathbb{E}[X_1] = \mu_1$, which is an unknown parameter.
- $Y_1, ..., Y_n$ represent the scores in 2008.
- $Y_1, ..., Y_n$ are iid Gaussians with standard deviation $38$.
- $\mathbb{E}[Y_1] = \mu_2$, which is an unknown parameter.
- $X_1,..., X_n$ are independent of $Y_1, ..., Y_n$.

You define your hypothesis test in terms of the null $H_0: \mu_1 = \mu_2$ (Signifiying that there were not significant changes in test scores) and $H_1: \mu_1 \neq \mu_2$​. We can conclude that

* The test given above is a **two-sided, two-sample test.**

* Assuming that the null hypothesis holds, the distribution of the statistic $\sqrt{n} \frac{\overline{X}_ n - \overline{Y}_ n}{\sqrt{38^2 + 39^2}}$​​​ is **standard Gaussian.**
  $$
  \sqrt{n} \frac{\overline{X}_ n - \overline{Y}_ n}{\sqrt{38^2 + 39^2}} \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
  $$
  Since by **linearity of expectation**,
  $$
  \mathbb {E}\left[ \sqrt{n} \frac{\overline{X}_ n - \overline{Y}_ n}{\sqrt{38^2 + 39^2}} \right] = \sqrt{\frac{n}{38^2 + 39^2}} ( \mathbb {E}[\overline{X}_ n] - \mathbb {E}[\overline{Y}_ n] ) = 0.
  $$
  By **independence**, the variance is additive
  $$
  \text {Var}\left( \sqrt{n} \frac{\overline{X}_ n - \overline{Y}_ n}{\sqrt{38^2 + 39^2}} \right) = \frac{1}{n(38^2 + 39^2)} \left( \sum _{i = 1}^ n \text {Var}(X_ i) + \sum _{i = 1}^ n \text {Var}(Y_ i) \right) = 1.
  $$

* Suppose now that the variance $\sigma_X^2$​ of $X_1, ..., X_n$​ and the variance $\sigma_Y^2$​ of $Y_1, ..., Y_n$​ are unknown. Let $\widehat{\sigma _ X}^2$​ denote the sample variance of $X_1, ..., X_n$​, and let $\widehat{\sigma _ Y}^2$​ denote the sample variance of $Y_1, ..., Y_n$​​. Assuming the null hypothesis holds, the statistic converges to standard Gaussian by **Slutsky's theorem**.

  Since $\widehat{\sigma _ X}$​ and $\widehat{\sigma _ Y}$​ are random variables, the distribution of the above cannot be standard Gaussian for any fixed $n$. However, we know that
  $$
  \widehat{\sigma _ X} \xrightarrow {n \to \infty } \sigma _ X, \quad \widehat{\sigma _ Y} \xrightarrow {n \to \infty } \sigma _ Y
  $$
  Therefore, **Slutsky's theorem** applies because
  $$
  \sqrt{n} \frac{\overline{X}_ n - \overline{Y}_ n}{\sqrt{\sigma _ X^2 + \sigma _ Y^2}} \sim \mathcal{N}(0,1)
  $$
  Therefore we conclude that
  $$
  \sqrt{n} \frac{\overline{X}_ n - \overline{Y}_ n}{\sqrt{\widehat{\sigma _ X}^2 + \widehat{\sigma _ Y}^2}} \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
  $$

* Assuming true variances are known: $\sigma_X^2 = 39^2, \ \sigma_Y^2 = 38^2$​. Accordingly, you design the test,
  $$
  \psi = \mathbf{1}\left( \sqrt{n}\bigg| \frac{\overline{X}_ n - \overline{Y}_ n}{\sqrt{38^2 + 39^2}} \bigg| \geq q_{\eta /2} \right)
  $$
  where $q_{\eta}$​ represents the $1- \eta$​ quantile of a standard Gaussian. 

  The level of this test is $\eta$​, which can be either **non-asymptotic** or **asymptotic**.

  The level is given by
  $$
  P( |Z| > q_{\eta /2} ) = \eta
  $$
  where $Z \sim \mathcal{N}(0,1)$.

## 2. Parametric Hypothesis Testing - Clinical Trials

#### Notation and modeling

* Let $\Delta_d > 0$ denote the expected decrease of LDL level (in mg/dL) for a patient that has used the drug.
* Let $\Delta_c \geq 0$ denote the expected decrease of LDL level (in mg/dL) for a patient that has used the placebo.
* We want to know if $\Delta_d > \Delta_c$.
* We observe two independent samples:
  * $X_1, ..., X_n \stackrel{iid}{\sim} \mathcal{N}(\Delta_d ,\sigma_d^2)$​ from the test group and
  * $Y_1, ..., Y_m \stackrel{iid}{\sim}\mathcal{N}(\Delta_c, \sigma_c^2)$ from the control group.

Consequence of Gaussian Samples

* $\overline{X}_ n=\frac{X_1 + X_2 + \dots + X_ n}{n}$ is a Gaussian random variable
* $\overline{Y}_ m=\frac{Y_1 + Y_2 + \dots + Y_ m}{m}$ is a Gaussian random variable
* $\overline{X}_ n-\overline{Y}_ m=\frac{X_1 + X_2 + \dots + X_ n}{n} - \frac{Y_1 + Y_2 + \dots + Y_ m}{m}$ is a Gaussian random variable
* $X_ i + Y_ j$​ is​ a Gaussian random variable for any $i=1, ..., n$​ and $j=1,...,m$.
* The variance of $\overline{X}_ n - \overline{Y}_ m$ is $\frac{\sigma _ d^2}{n} + \frac{\sigma _ c^2}{m}$.

#### Hypothesis Testing

* Hypotheses:
  $$
  H_0: \Delta_c = \Delta_d \ \  vs. \ \ H_1: \Delta_d > \Delta_c
  $$

* Since the data is Gaussian by assumption we don't need the CLT.

* We have
  $$
  \overline{X}_n \sim \mathcal{N}(\Delta_d, {\sigma_d^2\over n})\ \ \text{and} \ \ \overline{Y}_m \sim \mathcal{N}(\Delta_c, {\sigma_c^2\over m})
  $$

* Therefore,
  $$
  {\overline{X}_n - \overline{Y}_n - (\Delta_d - \Delta_c) \over \sqrt{{\sigma_d^2 \over n} + {\sigma^2_c \over m}}} \sim \mathcal{N}(0,1)
  $$
  But there is an issue that we do not know $\sigma^2_d$​ and $\sigma_c^2$​.

## 3. Parametric Hypothesis Testing - Asymptotic Test with Level Alpha

#### Asymptotic test

* Assume that $m=cn$​ and $n \rightarrow \infty$​.

* Using **Slutsky's lemma**, we also have
  $$
  {\overline{X}_n - \overline{Y}_n - (\Delta_d - \Delta_c) \over \sqrt{{\widehat{\sigma_d}^2 \over n} + {\widehat{\sigma_c}^2 \over m}}} \xrightarrow[n\rightarrow \infty]{(d)} \mathcal{N}(0,1)
  $$
  where 
  $$
  \hat{\sigma}_d^2 = {1\over n}\sum^n_{i=1} (X_i - \bar{X}_n^2)^2 \ \ \text{and} \ \ \hat{\sigma}_d^2 = {1\over n}\sum^n_{i=1} (Y_i - \bar{Y}_m^2)^2
  $$
  Scaling by ${1\over n-1}$​ leads to an unbiased estimator for the covariance between two random variables, thus
  $$
  \hat{\sigma}_d^2 = {1\over n-1}\sum^n_{i=1} (X_i - \bar{X}_n^2)^2 \ \ \text{and} \ \ \hat{\sigma}_d^2 = {1\over n-1}\sum^n_{i=1} (Y_i - \bar{Y}_m^2)^2
  $$
  Since by **Continuous Mapping Theorem (CMT)**,
  $$
  {\sqrt{{\widehat{\sigma_d}^2 \over n} + {\widehat{\sigma_c}^2 \over m}}\over \sqrt{{{\sigma_d}^2 \over n} + {{\sigma_c}^2 \over m}}} \xrightarrow[n \rightarrow \infty]{\mathbf{P}}1
  $$
  Thus, by **Slutsky's theorem**,
  $$
  {\overline{X}_n - \overline{Y}_n - (\Delta_d - \Delta_c) \over \sqrt{{\widehat{\sigma_d}^2 \over n} + {\widehat{\sigma_c}^2 \over m}}} \times {\sqrt{{\widehat{\sigma_d}^2 \over n} + {\widehat{\sigma_c}^2 \over m}}\over \sqrt{{{\sigma_d}^2 \over n} + {{\sigma_c}^2 \over m}}} = {\overline{X}_n - \overline{Y}_n - (\Delta_d - \Delta_c) \over \sqrt{{{\sigma_d}^2 \over n} + {{\sigma_c}^2 \over m}}} \xrightarrow[n\rightarrow \infty]{(d)} \mathcal{N}(0,1)
  $$
  
* We get the following test at asymptotic level $\alpha$.
  $$
  R_\psi = \Bigg\{ {\overline{X}_n \cdot \overline{Y}_m \over \sqrt{{\widehat{\sigma_d}^2 \over n} + {\widehat{\sigma_c}^2 \over m}}} > q_\alpha \Bigg\}
  $$

* This is a **one-sided, two-sample** test.

Example:

* $n=70, m=50, \bar{X}_n = 156.4, \bar{X}_m=132.7, \hat{\sigma}_d^2 = 5198.4, \hat{\sigma}_c^2 = 3867.0$,
  $$
  {156.4 - 132.7 \over \sqrt{{5198.4 \over 70} + {3867.0 \over 50}}} = 1.57
  $$
  Since $q_{5\%}=1.645$, we fail to reject the $H_0$.

* We can also compute the p-value = $\mathbf{P}(\mathcal{N}(0,1) > 1.57) = 0.0582$.

## 4. Hypothesis Testing in the Regime of Small Sample Sizes

* What if $n=20, m=12$​​? We cannot realistically apply Slutsky's lemma.

  * Because the calculation presented on the given slides was an asymptotic analysis (*i.e.*, we assumed $n \rightarrow \infty$​). Slutsky's theorem only gives a good approximation when the sample size is very large.

* We needed it to find the (asymptotic) distribution of quantities of the form
  $$
  {\bar{X}_n - \mu \over \sqrt{\hat{\sigma}^2}}
  $$
  when $X_1, ..., X_n \stackrel{iid}{\sim}\mathcal{N}(\mu, \sigma^2)$​​.

* It turns out that this distribution does not depend on $\mu$ or $\sigma$​ so we can compute its **quantiles**.

## 5. The Chi-Squared Distribution and its Properties

* The definition of $\chi^2$​ distribution:

  For a positive integer $d$, the $\chi^2$ distribution with $d$ degrees of freedom is the law of the random variable $Z_1^2 + Z_2^2 + ... + Z_d^2$, where $Z_1, ..., Z_d \stackrel{iid}{\sim}\mathcal{N}(0,1)$​.

  

  Example:

  * If $Z \sim \mathcal{N}(0, I_d)$​, then $||Z||_2^2 \sim \chi_d^2$​.

    The PDF of $X \sim X_k^2 = \Gamma ({k \over 2}, {1\over 2})$​​ is
    $$
    {1\over 2^{k/2} \Gamma({k \over 2})} x^{k/2-1} e^{-x/2}, \ \ x > 0
    $$

  * $\chi_2^2 = \text{Exp}(1/2)$​.

  We should see the expectation and variance increase with the increase of the degree of freedom.

* Properties of $\chi^2$ distribution:

  For a positive integer $d$​​​, the $\chi^2$​​​ Distribution with $d$​​​ Degrees of freedom is the law of the random variable $Z_1^2 + ... + Z_d^2$​​, where $Z_1, ..., Z_d \stackrel{iid}{\sim} \mathcal{N}(0,1)$.

  If $V\sim \chi_k^2$​, then

  * $\mathbb{E}[V]=\mathbb{E}[Z^2_1] + ... + \mathbb{E}[Z_d^2] = d$.

  * $\text{Var}[V] = \text{Var}[Z_1^2]+ ... + \text{Var}[Z_d^2] = 2d$

    Since $\text{Var}[Z_1^2] = \mathbb{E}[Z_1^4] -1 = 3-1 =2$.

> #### Exercise 75
>
> 1. What is the smallest possible sample space of $\chi_d^2$?
>
> 2. If $X \sim \chi_d^2$, what is $\mathbb{E}[X]$?
>
> 3. Let $\mathbf{Z} \sim \mathcal{N}(0, I_{d \times d})$ denote a random vector whose components are standard Gaussians: $Z^{(1)}, \ldots , Z^{(d)} \sim \mathcal{N}(0,1)$. Which one of the following random variables has a $\chi$-squared distribution with $d$ degrees of freedom.
>
>    a. $\max (Z^{(1)}, \ldots , Z^{(d)})$
>
>    b. $|Z^{(1)}| + |Z^{(2)}| + \cdots |Z^{(d)}|$
>
>    c. $\left\|  \mathbf{Z} \right\| _2$
>
>    d. $\left\|  \mathbf{Z} \right\| _2^2$
>
> **Solution:**
>
> 1. The smallest possible sample space of $Z^2$​ is $\R_{\geq 0}$​, since the smallest sample space of a Gaussian random variable $Z$​ is $\R$​.
>
> 2. By linearity of expectation,
>    $$
>    \mathbb {E}[X] = \mathbb {E}[Z_1^2 + Z_2^2 + \cdots + Z_ d^2] = d \cdot 1 = d,
>    $$
>    Because $Z_1, \ldots , Z_ d \stackrel{iid}{\sim } \mathcal{N}(0,1)$.
>
> 3. The $\ell_2$ norm $||\cdot||_2$ measures the Euclidean distance from the origin. Hence, if $\mathbf{Z} \sim \mathcal{N}(0, I_{d \times d})$, then
>    $$
>    \left\|  \mathbf{Z} \right\| _2^2 = \left(Z^{(1)}\right)^2 + \left(Z^{(2)}\right)^2 + \cdots + \left(Z^{(d)}\right)^2 \sim \chi _ d^2.
>    $$

> #### Exercise 76
>
> You are playing darts on a dart-board that is represented by the entire plane, $\R^2$. You get a 'bullseye' if the dart lands inside of the unit disc $D^1 := \{  (x, y): x^2 + y^2 \leq 1 \}$. You dart throws are modeled by a Gaussian random vector $\mathbf{Z}$, where $Z^{(1)}, Z^{(2)} \stackrel{iid}{\sim } \mathcal{N}(0,1)$.
>
> Let $f_d$​ represent the density of the $\chi_d^2$​ distribution. What is the probability of getting a bullseye?
>
> **Answer:** $\int _0^1 f_2(x) \,  dx$​
>
> **Solution:** 
>
> A bullseye is given by the event $\left(Z^{(1)}\right)^2 + \left(Z^{(2)}\right)^2 \leq 1$. Since $\left(Z^{(1)}\right)^2 + \left(Z^{(2)}\right)^2 \sim \chi _2^2$, it follows that
> $$
> P(\text {bullseye}) = \int _{0}^1 f_2(x) \,  dx.
> $$

> #### Exercise 77
>
> Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathcal{N}(0, \sigma ^2)$​ and let
> $$
> V_ n = \frac{1}{n} \sum _{i = 1}^ n X_ i^2
> $$
> denote the sample second moment. For an appropriate deterministic constant $a$​, that depends on $n$​ and $\sigma^2$​, and an integer $k$, we have that $a \cdot V_n \sim \chi_k^2$. 
>
> 1. What is $a$​​?
> 2. How many degrees of freedom does the above $\chi$-squared random variable have? 
>
> **Answer:**
>
> 1. $a = n/\sigma^2$​
> 2. $k=n$​
>
> **Solution:**
>
> Observe that
> $$
> \frac{n}{\sigma ^2} V_ n = \sum _{i = 1}^ n \frac{X_ i^2}{\sigma ^2}= \sum _{i = 1}^ n \left(\frac{X_ i}{\sigma }\right)^2,
> $$
> and $X_ i/\sigma \sim \mathcal{N}(0,1)$ because $X_ i \sim \mathcal{N}(0, \sigma ^2)$. Hence, $\frac{n}{\sigma ^2} V_ n$ is a $\chi_n^2$ random variable.

## 6. Sample Variance and Sample Mean of IID Gaussians: Cochran's Theorem

The sample variance:

* **Cochran's theorem** states that for $X_1, ..., X_n \stackrel{iid}{\sim}\mathcal{N}(\mu,\sigma^2)$, $S_n$​ is the sample variance defined as 
  $$
  S_n = {1\over n}\sum^n_{i=1}(X_i - \bar{X}_n)^2 = {1\over n} \sum^n_{i=1}X_i^2 - (\bar{X}_n)^2
  $$

* For $X_1, ..., X_n \stackrel{iid}{\sim}\mathcal{N}(\mu,\sigma^2)$​, if $S_n$​ is the sample variance, then it satisfies

  * $\bar{X}_n \perp S_n$​​ for all $n$​​, i.e., $\overline{X}_n$​ is independent of  $S_n$,​

  * ${n S_n\over \sigma^2} \sim \chi_{n-1}^2$​​.

  * We often prefer the unbiased estimator of $\sigma^2$.

    $\tilde{S}_n = {1\over n-1}\sum^n_{i=1} (X_i - \bar{X}_n)^2 = {n \over n-1} S_n$​.

    $\mathbb{E}[\tilde{S}_n] = {n \over n-1}\mathbb{E}[{\sigma^2 \over n} \chi^2_{n-1}] = {n \sigma^2 \over n-1} (n-1) = \sigma^2$​​​​​.

* For unbiased sample variance $\tilde{S}_n$​ of $X_1, ..., X_n$, which is
  $$
    \widetilde{S}_ n = \frac{1}{n-1} \sum _{i=1}^ n \left(X_ i - \overline{X}_ n\right)^2
  $$
  $\frac{(n-1)\widetilde{S}_ n}{\sigma ^2}$​​​ is a $\chi_{n-1}^2$​​​ random variable, thereby the distribution of $\frac{\widetilde{S}_ n}{\sigma ^2}$​​​ is the distribution of a $\chi _{n-1}^2$​​​​ random variable scaled by ${1\over n -1}$.

> #### Exercise 78
>
> Verify that Cochran's theorem holds when $n=2$. Let $X_1, X_2 \stackrel{iid}{\sim } \mathcal{N}(\mu , \sigma ^2)$.
>
> The expression $S_2$​​​ can be written in the form $A^2$​​ where $A$​​ is a polynomial in $X_1$​ and $X_2$​.
>
> 1. What is $A^2$​​?​
>
> 2. The expression $A$​​​​​ is a random variable, and is distributed as $\mathcal{N}(\mu^*, (\sigma^*)^2)$​​​​​ for some $\mu^*$​​​​ and $\sigma^*$​​​​ that can be expressed in terms of the original parameters $\mu$ and $\sigma$.
>
>    What is $\mu^*$​ and $(\sigma^*)^2$?
>
> **Solution:**
>
> 1. Observe that
>    $$
>    S_ n = \frac{X_1^2 + X_2^2}{2} - \left( \frac{X_1 + X_2}{2} \right)^2 = \frac{X_1^2}{4} + \frac{X_2^2}{4} - \frac{1}{2} X_1 X_2 = \left( \frac{X_1 - X_2}{2} \right)^2.
>    $$
>
> 2. We can take $A =\pm \frac{X_1 - X_2}{2}$. Then,
>    $$
>    \mathbb {E}[A] = \frac{1}{2}\mathbb {E}[X_1 - X_2] = \frac{1}{2}(\mu - \mu ) = 0,
>    $$
>    and
>    $$
>    \text {Var}(A) = \text {Var}\left(\frac{X_1 - X_2}{2}\right) = \frac{1}{4} (\text {Var}(X_1) + \text {Var}(X_2)) = \frac{\sigma ^2}{2}.
>    $$

## 7. Student's T Distribution

Definition of student's t distribution:

For a positive integer $d$, the Student's T distribution with $d$ degrees of freedom (denoted by $t_d$) is the law of the random variable ${Z \over \sqrt{V/d}}$, where $Z \sim \mathcal{N}(0,1), V \sim \chi_d^2$ and $Z \perp V$ ($Z$ is independent of $V$).

> #### Exercise 79
>
> 1. Consider the distribution $\chi_n^2$​​​​. Let $f_n:\R \rightarrow \R$​​​​ denote the PDF of $\chi_n^2$​​​​, and let $A_n$​​​​ denote the maximizer of $f_n$​​​. (i.e., the peak of the PDF of the distribution $\chi_n^2$​​​ is located at $A_n$​​​). What is $\lim _{n \to \infty } A_ n$​​​​?
> 2. Consider the Student's T Distribution, which is defined to be the distribution of $T_ n := \frac{Z}{\sqrt{V/n}}$, where $Z \sim \mathcal{N}(0,1),V \sim \chi_n^2$, and $Z$ and $V$ are independent. Let $g_n$ denote the PDF of $T_n$, and let $B_n$ denote the maximizer of $g_n$ (i.e., the peak of the PDF of the distribution $T_n$ Is located at $B_n$). What is $\lim _{n \to \infty } B_ n$?
>
> **Answer:**
>
> 1. $\lim _{n \to \infty } A_ n = \infty$​.
> 2. $\lim _{n \to \infty } B_ n= 0$​​
>
> **Solution:**
>
> 1. The graph of the PDF of $\chi _ n^2$​ in the slides shows that the peak of the distribution moves to the right as $n \rightarrow \infty$. Hence
>    $$
>    \lim _{n \to \infty } A_ n = \infty
>    $$
>    This is intuitive since 
>    $$
>    \mathbb {E}[X] = n, \ \ \text{if} \ X \sim \chi _ n^2
>    $$
>
> 2. As $n \rightarrow \infty$​, the random variable $V/n$​ converges to $1$​ In probability. Hence, as $n \rightarrow \infty$.
>    $$
>    T_ n \xrightarrow [n \to \infty ]{(d)} \mathcal{N}(0,1).
>    $$
>    which implies 
>    $$
>    \lim _{n \to \infty } B_ n= 0
>    $$

## 8. The Student's T Test (T Test)

* One sample, two-sided:

  Let $X_1, ..., X_n \stackrel{i.i.d}{\sim} \mathcal{N}(\mu,\sigma^2)$​​​ where​​ both $\mu$​​​ and $\sigma^2$​​​ are unknown.

  We want to test:
  $$
  H_0: \mu = 0, \ \ vs. \ \ H_1: \mu \neq 0
  $$
  Test statistic:
  $$
  T_n = {\overline{X}_n \over \sqrt{\tilde{S}_n/n}} = {\sqrt{n}{\overline{X_n} - \mu \over \sigma}  \over \sqrt{\tilde{S_n}/\sigma^2}} = {\sqrt{n}{\overline{X_n} \over \sigma}  \over \sqrt{\tilde{S_n}/\sigma^2}} \sim \mathcal{N}(0,1)
  $$
  Since $\overline{X}_n \sim \mathcal{N}(\mu, {\sigma^2 \over n})$​,
  $$
  { \overline{X}_n - \mu \over \sqrt{\widehat{\sigma^2}/n}} = {\sqrt{n}{\overline{X_n} - \mu \over \sigma}  \over \sqrt{\tilde{S_n}/\sigma^2}} \sim \mathcal{N}(0,1)
  $$
   $\sqrt{n} \overline{X}_n / \sigma \sim \mathcal{N}(0,1)$​​ (Under )  and $\tilde{S}_n/\sigma^2 \sim {\chi^2_{n-1}\over n-1}$​​​ are independent by **Cochran's Theorem**, we have
  $$
  T_n \sim t_{n-1}
  $$
  Student's test with (non-asymptotic) level $\alpha \in (0,1)$:
  $$
  \psi_\alpha = \mathbb{1} \{|T_n| > q_{\alpha/2}\}
  $$
  where $q_{\alpha/2}$ is the $(1-\alpha/2)$-quantile of $t_{n-1}$.

* One sample, one-sided:

  We want to test:
  $$
  H_0: \mu \leq \mu_0, \ \ vs. \ \ H_1: \mu > \mu_0
  $$
  Test statistic:
  $$
  T_n = {\overline{X}_n - \mu_0 \over \sqrt{\tilde{S}_n}} \sim t_{n-1}, \ \ \text{under } H_0
  $$
  Student's test with (non asymptotic) level $\alpha \in (0,1)$:
  $$
  \psi_\alpha = \mathbb{1}\{T_n > q_\alpha\}
  $$
  where $q_\alpha$​​ is the $(1-\alpha)$​​-quantile of $t_{n-1}$​​.

**Remark:**

For any fixed $n$, we may find the quantiles of the student's T distribution in tables. Since the distribution does not depend on the value of the true parameter, the test statistic $T_n$​ is indeed **pivotal**. Therefore, the test is **non-asymptotic**.

Assuming the data is Gaussian, the student's T test is useful in situations where the sample size is not very large, since the level may be precisely quantified even for small $n$.

> #### Exercise 80
>
> Consider the statistic
> $$
> T_{n} := \sqrt{n} \left( \frac{\overline{X}_ n - \mu }{\sqrt{\frac{1}{n- 1} \sum _{i = 1}^ n (X_ i - \overline{X}_ n)^2} } \right),
> $$
> where $\overline{X}_n$ is the sample mean of i.i.d Gaussian observations with mean $\mu$ and variance $\sigma^2$.
>
> 1. For all $n \geq 2$​​, is the distribution of $T_n$​​ a standard Gaussian $\mathcal{N}(0,1)$​​?
>
> 2. As $n \rightarrow \infty$, what does
>    $$
>    \frac{1}{n- 1} \sum _{i = 1}^ n (X_ i - \overline{X}_ n)^2
>    $$
>    converge to?
>
> 3. As $n \rightarrow \infty$​​​, what does the statistic $T_n$​​​ converges in distribution to?
>
> **Solution:**
>
> 1. No. The definition of the student's T distribution with $n-1$​​ degrees of freedom is that it is given by the distribution of $\frac{Z}{\sqrt{V/(n-1)}}$​​ where $Z \sim \mathcal{N}(0,1), V\sim \chi_{n-1}^2$​​ and $Z$​​ and $V$​ are​ independent. Since we are dividing by $V$​, a $\chi^2$​ random variable, then $T_n$​ will not have the same distribution as $\mathcal{N}(0,1)$ for all $n \geq 2$.
>
> 2. By the **LLN** and **Slutsky's lemma**,
>    $$
>    \frac{1}{n- 1} \sum _{i = 1}^ n (X_ i - \overline{X}_ n)^2 = \frac{n}{n-1} \left[\left(\frac{1}{n}\sum _{i = 1}^ n X_ i^2\right) - (\overline{X}_ n)^2\right] \to \sigma ^2
>    $$
>    in probability.
>
> 3. By the **CLT**, 
>    $$
>    \sqrt{n} \left( \frac{\overline{X}_ n - \mu }{\sigma } \right) \to \mathcal{N}(0,1).
>    $$
>    Hence, by the **LLN** and **Slutsky's lemma**,
>    $$
>    \sqrt{n} \left( \frac{\overline{X}_ n - \mu }{\sqrt{\frac{1}{n- 1} \sum _{i = 1}^ n (X_ i - \overline{X}_ n)^2} } \right) \xrightarrow [n \to \infty ]{(d)} \mathcal{N}(0,1).
>    $$

## 9. Back to Clinical Trials: Two Sample T-Test and the Welch-Satterthwaite Formula

Two Sample T-Test:

* Back to our cholesterol example. What happens for small sample size?

* We want to know the distribution of 
  $$
  {\overline{X}_n - \overline{Y}_n - (\Delta_d - \Delta_c) \over \sqrt{{\widehat{\sigma_d}^2 \over n} + {\widehat{\sigma_c}^2 \over m}}}
  $$

* We have approximately
  $$
  {\overline{X}_n - \overline{Y}_n - (\Delta_d - \Delta_c) \over \sqrt{{\widehat{\sigma_d}^2 \over n} + {\widehat{\sigma_c}^2 \over m}}} \sim t_N
  $$
  where
  $$
  N = { \left(\widehat{\sigma_d^2}/n + \widehat{\sigma_c^2}/m\right)^2\over {\widehat{\sigma_d^4} \over n^2(n-1)} + {\widehat{\sigma_c^4}\over m^2(m-1) }} \geq \min(n,m)
  $$
  (Welch-Satterthwaite Formula)

Non-asymptotic test:

* Example $n=70, m=50, \bar{X}_n = 156.4, \bar{Y}_m = 132.7,\hat{\sigma}_d^2 = 5198.4, \hat{\sigma}_c^2 = 3867.0$.
  $$
  \frac{156.4 - 132.7}{\sqrt{\frac{5198.4 }{70} + \frac{3867}{50}}} \approx 1.9248.
  $$

* Using the shorthand formula $N = \min (n,m) = 50$, we get $q_{5\%} = 1.68$ and
  $$
  \text{p-value } = \mathbf{P}(t_{50} > 1.9248) = 0.0614
  $$

* Using the W-S formula
  $$
  N =  { ({5198.4 \over 70}+ {3867.0\over 50 })^2 \over {5198.4^2\over 70^2(70-1) } + {3867.0^2\over50^2(50-1) }} = 113.78 \approx 113
  $$

* We get
  $$
  \text{p-value }= \mathbf{P}(t_{113} > 1.9248) = 0.0596
  $$



# Recitation 2: Confidence Intervals of the shift of shifted exponential random variables

Consider a sample of $n$ i.i.d. continuous random variables $X_1,…,X_n$ with density
$$
f(x)=e^{-(x-a)} \mathbf{1}_{x\geq a}
$$
where $a∈\R$ is an unknown parameter.

1. Find an estimator $\hat{a}$ for the unknown shift parameter $a$.
2. Determine the (asymptotic) distribution of $\hat{a}$.
3. Compute a two-sided C.I. for a based on $\hat{a}$.
4. Compare (Q3) results with a different estimator of the same parameter $a$.
5. Compute an one-sided C.I. for a based on $\hat{a}$.

> 1) Consider $\overline{X}_n = {1\over n} \sum^n_{i=1} X_i$, and LLN $\overline{X}_n \xrightarrow[n\rightarrow \infty]{\mathbb{P}} \mathbb[X_1]$, we calculate
> $$
> \begin{aligned}
> \mathbb{E}[X_1] &= \int^\infty_{-\infty} x e^{-(x-a)}\mathbf{1}_{\{x \geq a\}}\\
> &= \int^\infty_{a} xe^{-(x-a)} dx\\
> &= \left[ -e^{-(x-a)}x\right]^\infty_a + \int^\infty_ae^{-(x-a)} dx\\
> &= a - \left[e^{-(x-a)}\right]^\infty_a\\
> &= a + 1\\
> \implies & \hat{a}_1 = \overline{X}_n - 1 \xrightarrow[x\rightarrow \infty]{\mathbb{P}} a
> \end{aligned}
> $$
> 2) Use **CLT** to analyze the asymptotic distribution of $\overline{X}_n$.
>
> By CLT, 
> $$
> {\sqrt{n} \over \sqrt{\mathsf{Var}(X_1)}} \left( \overline{X}_n - \mathbb{E}[X_1] \right) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
> $$
> First we calculate the variance by the formula $\mathsf{Var}(X_1) = \mathbb{E}[X_1^2] - (\mathbb{E}[X_1])^2$.
> $$
> \begin{aligned}
> \mathbb{E}[X_1] &= \int^\infty_{-\infty}x^2 e^{-(x-a)} \mathbf{1}_{\{x \geq a\}} dx \\
> &= \int^\infty_{a}x^2 e^{-(x-a)}dx\\
> &= \left[ -e^{-(x-a)}x^2 \right]^\infty_a + \int 2xe^{-(x-a)} dx\\
> &= a^2 + 2(a+1)
> \end{aligned}
> $$
> Thus
> $$
> \mathsf{Var}(X_1) = a^2 + 2(a+1) - (a+1)^2 = 1
> $$
> Plug it back in and we have the distribution
> $$
> {\sqrt{n}} \left( \hat{a}_1 - a \right) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
> $$
> 3) Consider a two-sided asymptotic C.I., $\mathcal{I}_1 = \hat{a}_1 + [-s, s], s > 0$
> $$
> \begin{aligned}
> \mathbb{P}(a\in\mathcal{I}_1) &= \mathbb{P}\left(a\in \left[\hat{a}_1 -s,\hat{a}_1 +s \right]\right)\\
> &= \mathbb{P}(\hat{a}_1 - s \leq a \leq \hat{a}_1 +s)\\
> &= \mathbb{P} (-s \leq a-\hat{a}_1 \leq s)\\
> &= \mathbb{P} (-s \leq \hat{a}_1 -a \leq s)\\
> &= \mathbb{P}(-\sqrt{n}s \leq \sqrt{n}(\hat{a}_1 - a) \leq \sqrt{n} s)
> \end{aligned}
> $$
> Let $q = \sqrt{n} s$. Thus,
> $$
> \mathbb{P}\left(a\in \left[\hat{a}_1 -s,\hat{a}_1 +s \right]\right) \xrightarrow[n \rightarrow \infty]{(d)} \mathbb{P}(Z \in [-q, q])
> $$
> where $Z \sim \mathcal{N}(0,1)$.
>
> We want 
> $$
> \mathbb{P}\left(a\in \left[\hat{a}_1 -s,\hat{a}_1 +s \right]\right) \xrightarrow[n \rightarrow \infty]{(d)} \mathbb{P}(Z \in [-q, q]) = 1 -\alpha = 2 \Phi(q) -1
> $$
> where $\Phi(\cdot)$ is the cumulative distribution function of a standard Gaussian, and $ q = q_{\alpha/2}$ is the $1-{\alpha/2}$ quantile. Hence, the asymptotic C.I. is 
> $$
> \mathcal{I}_1 = \hat{a}_1 + \left[-{q_{\alpha/2}\over\sqrt{n}}, {q_{\alpha/2}\over\sqrt{n}}\right]
> $$
> 4) Let $\hat{a}_2 = \min\limits_{1\leq i \leq n} X_i$ be an estimator. 
>
> Now calculate the asymptotic distribution of it. Since it is enough to give CDF of a r.v. to characterize the whole distribution. i.e. give $\mathbb{P}(Y \leq t), \forall t \in \R$ to determine distribution of $Y$. In order to have CDF, we apply
> $$
> \mathbb{P}(\hat{a}_i \leq t) = 1 - \mathbb{P}(\hat{a}_i > t)
> $$
> Let $t < a$: $\mathbb{P}(\hat{a}_2 > t) = 1$;
>
> Let $t > a$: 
> $$
> \begin{aligned}
> \mathbb{P}(\hat{a}_2 > t) &= \mathbb{P}(X_i > t, \forall i) \\
> &= \mathbb{P}(\bigcap^n_{i=1} \{X_i > t\}) \\
> &= \prod^n_{i=1} \mathbb{P}(X_i > t)\\
> &= \prod^n_{i=1} \int^\infty_t e^{-(x-a)} dx\\
> &= \prod^n_{i=1}\left[ -e^{-(x-a)} \right]^\infty_t\\
> &= \prod^n_{i=1}e^{-(t-a)}\\
> &= e^{-n(t-a)}
> \end{aligned}
> $$
> Since this is a scaled and shifted exponential distribution, we can scale and shift it back by using $\stackrel{\sim}{t}= (1/n) t + a, \stackrel{\sim}{t} > 0$. Then we have
> $$
> \mathbb{P}(\hat{a}_2 >  {1\over n} t + a) = e^{-\stackrel{\sim}{t}}
> $$
> Therefore, we have
> $$
> n(\hat{a}_2 - a) \sim \mathsf{Exp}(1)
> $$
> Notice that if "1- CDF" looks like $e^{-x}$, it corresponds to an exponential distribution.
>
> 5) Consider one-sided C.I. $\mathcal{I}_2 = [\hat{a}_2 -s , \hat{a}_2]$ 
> $$
> \begin{aligned}
> \mathbb{P}(a \in \mathcal{I}_2) &= 1 - \alpha\\
> &= \mathbb{P}\left(a \in [\hat{a}_2 - s, \hat{a}_2]\right) \\
> &= \mathbb{P}(\hat{a}_2 -s \leq a \leq \hat{a}_2)\\
> &= \mathbb{P} (\hat{a}_2 -a \leq s)\\
> &= \mathbb{P}(n(\hat{a}_2 - a) \leq ns)
> \end{aligned}
> $$
> Let $q = ns$, we have
> $$
> \begin{aligned}
> \mathbb{P}(a \in \mathcal{I}_2) &= \mathbb{P}(Y \leq q), \quad \text{ where } Y \sim \mathsf{Exp}(1)\\
> &= 1- e^{-q} \\
> &= 1 - \alpha\\
> \implies & e^{-q} = \alpha \\
> \implies & q = - \log(\alpha) = \log(1/\alpha)
> \end{aligned}
> $$
> Therefore, we have the one-sided C.I.
> $$
> \mathcal{I}_2 = \left[ \hat{\alpha}_2 - {\log({1\over \alpha})\over n}, \hat{a}_2 \right]
> $$

#### Remark: Comparing two estimators

| $\hat{a}_1 = {1\over n}\sum^n_{i=1}X_i-1; \quad \mathcal{I}_1$ | $\hat{a}_2 = \min\limits_{1\leq i \leq n}X_i; \quad \mathcal{I}_2$ |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Generic approach.                                            | Specifically tailored to the model.                          |
| $\vert\mathcal{I}_1\vert \sim {1\over \sqrt{n}}$             | $\vert \mathcal{I_2} \vert \sim {1\over n}$                  |
| Asymptotic.                                                  | Non-asymptotic.                                              |
| Some robustness properties.                                  | Highly non-robust; sensitive to outliers.                    |





# Recitation 11. Review: Comparisons of Two Proportions

You are interested in comparing the proportions of people in their 20's that smoke in France and in the US. After you sample randomly and independently $n$​ people in their 20's in both countries, you observe that $N_{US}$​ sampled US Americans and $N_F$ sampled French are smokers. Based on such an experiment, how would you test whether there is a significant difference between the proportions of smokers in both countries ?

The formal definition of a **pivotal quantity** (or a **pivot**) is as follows. Let $X_1, ..., X_n$ be random samples and let $T_n$ be a function of $X$ and a parameter vector $\theta$. That is, $T_n$ is a function of $X_1, ...., X_n, \theta$. Let $g(T_n)$ be a random variable whose distribution is the same for all $\theta$. Then, $g$ is called a pivotal quantity or a pivot.

Let $X$​ be a random variable with mean $\mu$ and variance $\sigma^2$. Let $X_1, ..., X_n$ be i.i.d sample of $X$. Then,
$$
g_ n \triangleq \frac{\overline{X_ n} - \mu }{\sigma }
$$
is a **pivot** with $\theta = [\mu \ \ \sigma^2]^T$​​​ being the parameter vector. The notion of a parameter vector here is not to be confused with the set of parameters that we use to define a statistical model.

> **Solution:**
>
> According to the scenario, let $X_1, ..., X_n \stackrel{iid}{\sim} \text{Be}(p_x)$ and $Y_1, ..., Y_n  \stackrel{iid}{\sim} \text{Be}(p_y)$.  We have the hypothesis:
> $$
> H_0: p_X= p_Y, \ \ H_1: p_X \neq p_Y
> $$
>
> 1. First find the statistic $T_n(X_1, ...,X_n, Y_1, ..., Y_n)$​​​, and define hypothesis test $\psi = \mathbb{1}\{T_n > s\}$​​.
>
>    By LLN,
>    $$
>    \hat{p}_X = {1\over n} \sum^n_{i=1} X_i \xrightarrow[n \rightarrow \infty]{\mathbf{P}} p_X\\
>    \hat{p}_Y = {1\over n} \sum^n_{i=1} Y_i \xrightarrow[n \rightarrow \infty]{\mathbf{P}} p_Y\\
>    $$
>    Consider $\hat{p}_X-\hat{p}_Y = g(\hat{p}_X,\hat{p}_Y)$​,  $g(x,y) = x-y$​. By CLT,
>    $$
>    \sqrt{n} \left(\begin{pmatrix}\hat{p}_X \\\hat{p}_Y  \end{pmatrix} - \begin{pmatrix}\hat{p}_X \\\hat{p}_Y  \end{pmatrix} \right) \xrightarrow[n\rightarrow \infty]{D} \mathcal{N}(0,\Sigma)
>    $$
>    where $\begin{pmatrix}\hat{p}_X \\\hat{p}_Y  \end{pmatrix}  = {1\over n} \sum\limits^n_{i=1} \begin{pmatrix}X_i \\Y_i  \end{pmatrix}$​,  $\Sigma = \begin{pmatrix}p_X(1-p_X) & 0\\ 0 & p_Y(1-p_Y) \end{pmatrix}$​.​
>
>    **Delta** Method:
>    $$
>    \sqrt{n}\left(g(\hat{p}_X,\hat{p}_Y) - g({p}_X,{p}_Y)\right) \rightarrow \mathcal{N}(0, \nabla g({p}_X,{p}_Y)^T\Sigma \  g({p}_X,{p}_Y) )
>    $$
>    Let $\sigma^2_g = \nabla g({p}_X,{p}_Y)^T\Sigma \  g({p}_X,{p}_Y)$​, we know
>    $$
>    \nabla g(x,y) = \begin{pmatrix}1 \\ -1 \end{pmatrix}
>    $$
>    Then the covariance is
>    $$
>    \hat{\sigma}^2_g = (1 \ -1)\begin{pmatrix}p_X(1-p_X) & 0\\ 0 & p_Y(1-p_Y) \end{pmatrix} \begin{pmatrix}1 \\ -1 \end{pmatrix} = p_X(1-p_X) + p_Y(1-p_Y)
>    $$
>    Therefore, we have
>    $$
>    \sqrt{n} {(\hat{p}_X) - \hat{p}_Y) - (p_X - p_Y) \over \sqrt{p_X(1-p_X) + p_Y(1-p_Y)}} \xrightarrow[n \rightarrow \infty]{D} \mathcal{N}(0,1)
>    $$
>    For $H_0$​​, set $p_X = p_Y = p \in (0,1) $​​, thus $ p_X(1-p_X) + p_Y(1-p_Y) =  2p(1-p)$​​​.
>    $$
>    \hat{p} = {1\over 2} (\hat{p}_X + \hat{p}_Y) \xrightarrow[n \rightarrow \infty]{\mathbf{P}} P
>    $$
>    By **Slutsky's method**, 
>    $$
>    \sqrt{n} {\hat{p}_X-\hat{p}_Y \over \sqrt{2 \hat{p} (1-\hat{p})}} \xrightarrow[n\rightarrow \infty]{D} \mathcal{N}(0,1)
>    $$
>    Therefore,
>    $$
>    T_n =\left|\sqrt{n} {\hat{p}_X-\hat{p}_Y \over \sqrt{2 \hat{p} (1-\hat{p})}}\right| \xrightarrow[n\rightarrow \infty]{D} \mathcal{N}(0,1) \ \ \text{under } H_0: p_X = p_Y
>    $$
>
> 2. Second adjust $s$​ to guarantee asymptotic level $\alpha$​​.
>
>    Since we have to guarantee that when we cut off at level $s$, when we decide to reject the null hypothesis, that this, under the null hypothesis itself, has asymptotic probability $\alpha$. 
>    $$
>    \mathbf{P}(T_n > s) = \mathbf{P}(\sqrt{n} {\hat{p}_X-\hat{p}_Y \over \sqrt{2 \hat{p} (1-\hat{p})}}  >s) \xrightarrow[n\rightarrow \infty]{} \mathbf{P}(|Z| >s) = 2\cdot (1- \Phi(s))
>    $$
>    where $Z \sim \mathcal{N}(0,1)$​, $\Phi(s)$​ is the CDF of a standard normal random variable.​​
>
>    So $s = q_{\alpha/2}$, which is $1-\alpha/2$ quantile of $\mathcal{N}(0,1)$.
>
> 3. Then find the type II error.
>
>    We have consistency of estimators
>    $$
>    \hat{p}_X \xrightarrow[n \rightarrow \infty]{\mathbf{P}} p_X,\quad  \hat{p}_Y \xrightarrow[n \rightarrow \infty]{\mathbf{P}} p_Y,\quad \hat{p} = {1\over 2} (\hat{p}_X + \hat{p}_Y) \xrightarrow[n \rightarrow \infty]{\mathbf{P}} {1\over 2}(p_X + p_Y) =: \tilde{p}
>    $$
>    which means 
>    $$
>    T_n =\left|\sqrt{n} {\hat{p}_X-\hat{p}_Y \over \sqrt{2 \hat{p} (1-\hat{p})}}\right| =\left|\sqrt{n} {{p}_X-{p}_Y \over \sqrt{2 \tilde{p} (1-\tilde{p})}}\right| \xrightarrow[n \rightarrow \infty]{\mathbf{P}} + \infty\\
>    \implies \text{type II error} \xrightarrow[n\rightarrow \infty]{\mathbf{P}} 0
>    $$





# Recitation 6. Maximum Likelihood Estimator for Multinomial

Consider a finite space $E=\{ a_1,a_2,\ldots ,a_ r\}$ of size $r \geq 2$ and let $X$ be a random variable taking values in $E$. For $j = 1,..., r$, let $p_ j^*=\mathbf{P}[X=a_ j]$. Consider a sample of $n$ i.i.d. copies $X_1, ..., X_n$ of $X$. Based on this example, we would like to estimate the multivariate parameter $p^*=(p_1^*,\ldots ,p_ r^*)$.

1. What is the parameter space ?
2. Write the likelihood associated with the model described above.
3. Compute the maximum likelihood estimator $\hat{p}$ or $p^*$,
4. Using the CLT, show that $\hat{p}$ is asymptotically normal. Compute the asymptotic covariance matrix. Denote it by $\Sigma$.
5. Prove that $\Sigma$ is not invertible. Conclude that the theorem for the MLE could not have been applied here. What condition is not satisfied?

> **Solution:**
>
> 1. Parameter space $\{p_j \geq 0,\ \ \sum\limits^r_{j=1} p_j = 1\} = P$.
>
> 2. The likelihood is
>    $$
>    \begin{aligned}
>    L = \mathbf{P}(X_1 = x_1, ..., X_n = x_n) &=\prod_{i=1}^n \mathbb{P}(X_i = x_i)\\
>    &= \prod_{i=1}^n \prod_{j=1}^r p_j^{\mathbf{1}\{x_i = j\}}\\
>    &= \prod_{j=1}^r \prod_{i=1}^n  p_j^{\mathbf{1}\{x_i = j\}}\\
>    &=\prod_{j=1}^r  p_j^{\sum\limits_{i=1}^n\mathbf{1}\{x_i = j\}}\\
>    & = \prod_{j=1}^r  p_j^{T_j} 
>    \end{aligned}
>    $$
>    where $T_j =  \sum\limits_{i=1}^n\mathbf{1}\{x_i = j\}$.
>
>    The log likelihood is
>    $$
>    \log L = \log \mathbf{P}(X_1 = x_1, ..., X_n = x_n) = \log \prod^{r}_{j=1} p_j^{T_j} = \sum^r_{j=1} T_j \log p_j
>    $$
>
> 3. Let $f(p) = \log \prod^{r}_{j=1} p_j^{T_j} = \sum^r_{j=1} T_j \log p_j$. 
>
>    Calculating MLE:  $\max\limits_{p\in P} f(p) \ \iff \ \max f(p) \ s.t. \ h(p) = \sum\limits^r_{j=1}p_j - 1 = 0$.
>
>    Assume $T_j > 0, \ \ \forall j$.
>
>    Necessary conditions: $0 = \nabla f(\hat{p}) + \lambda \cdot \nabla h(\hat{p}), \ \lambda \in \R$.
>
>    If we set the necessary condition to be $0 = \nabla f(\hat{p})$, the partial derivative is 
>    $$
>    \partial _{p_j} f(p) = {T_j\over p_j } = 0, \ \text{when }p_j \rightarrow \infty
>    $$
>    Which is not correct since we have parameter space $\{p_j \geq 0,\ \ \sum\limits^r_{j=1} p_j = 1\} = P$.
>
>    So we set the necessary condition to be $0 = \nabla f(\hat{p}) + \lambda \cdot \nabla h(\hat{p}), \ \lambda \in \R$, applying **Lagrange multiplier**.
>    $$
>    \begin{aligned}
>    &\partial _{p_j} h(p) = 1\\
>    \implies&  0 = {T_j \over \hat{p}_j} + \lambda\\
>    \implies& \lambda \neq 0\\
>    \implies& \hat{p}_j = -{T_j \over \lambda}
>    \end{aligned}
>    $$
>    Plug it into the parameter space
>    $$
>    \begin{aligned}
>    1 = \sum^r_{j=1}\hat{p}_j &= \sum^r_{j=1}\left (-{T_j \over \lambda}\right) = - {1\over \lambda} \sum^r_{j=1} T_j = - {n \over \lambda}\\
>    &\implies  \lambda = -n\\
>    &\implies \hat{p}_j = {T_j \over n}
>    \end{aligned}
>    $$
>    To show that it is the global maximum, we show that the likelihood is concave. The second derivative of the likelihood is
>    $$
>    \partial_{p_k} \partial_{p_j} f(p) = \partial_{p_k} {T_j \over p_j} = \begin{cases}- { T_j\over p_j^2} & j = k \\ 0 & j \neq k \end{cases} \implies  \nabla^2 f(p) < 0  \implies f \ \text{is concave.}
>    $$
>    To handle the condition where $T_j = 0$. We use  **Karush-Kuhn-Tucker conditions**, a generalization of the Lagrange multiplier.
>    $$
>    \mathbf{P}(X_1 = x_1, ..., X_n = x_n) = \prod_{j=1}^r  p_j^{T_j} \implies  \hat{p}_j = {T_j \over n}
>    $$
>    which is the global maximum.
>
> 4. (1) The MLE is
>    $$
>    \hat{p}_j = {T_j \over n} = {1\over n} \sum^n_{i=1} \mathbf{1} \{x_i = j\}
>    $$
>    Let  $(Y_i)_j = \mathbf{1} \{x_i = j\}$, by CLT,
>    $$
>    \sqrt{n} (\hat{p} - \mathbb{E}[Y_1]) \xrightarrow[n \rightarrow \infty]{ (d)} \mathcal{N}(0, \mathsf{Cov}(Y_1))
>    $$
>    where $\mathbb{E}[(Y_i)_j] = \mathbb{E}[\mathbf{1}\{x_i = j\}] = \mathbf{P}(x_i = j) = p_j$, since $\mathbf{1} \{x_i = j\} \sim \mathsf{Ber}(p_j)$. Let $\Sigma = \mathsf{Cov}(Y_1)$.
>
>    Recall that the covariance matrix is
>    $$
>    \Sigma_{j,k} = \begin{cases}\mathsf{Var}(Y_1)_j, & j = k\\\mathsf{Cov}((Y_1)_j,(Y_1)_k), & j \neq k \end{cases}
>    $$
>    The variance of Bernoulli is
>    $$
>    \mathsf{Var}((Y_1)_j) = p_j(1-p_j)
>    $$
>    The expectation is
>    $$
>    \mathbb{E}[(Y_1)_j(Y_1)_k] = \mathbb{E}[\mathbf{1}\{x_1 = j\}\mathbf{1}\{x_1 = k\}] = 0
>    $$
>    The covariance is
>    $$
>    \mathsf{Cov}\left((Y_1)_j, (Y_1)_k\right ) = \mathbb{E}[(Y_1)_j, (Y_1)_k] - \mathbb{E}[(Y_1)_j]\mathbb{E}[(Y_1)_k] = 0 - p_j p_k, \ \ j \neq k
>    $$
>    Therefore, the covariance matrix is
>    $$
>    \Sigma_{j,k} = \begin{cases}p_j(1-p_j), & j = k\\- p_j p_k, & j \neq k \end{cases}
>    $$
>    (2) Another more disciplined way of calculating asymptotic covariance matrices for MLE.
>
>    Recall that
>    $$
>    \sqrt{n} (\hat{\theta} - \theta^*) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0, \mathcal{I}(\theta^*)^{-1})\\
>    \text{where }\ \mathcal{I}(\theta) = - \mathbb{E}[\nabla^2 f(\theta)]
>    $$
>    The Fisher information is
>    $$
>    \mathcal{I}(p)_{jk} = -\mathbb{E}[(\nabla^2\ f(p))_{jk}] = \begin{cases} {p_j \over p_j^2} = {1 \over p_j}, & j = k \\ 0, & j \neq k \end{cases}
>    $$
>    So the inverse is
>    $$
>    \mathcal{I}(p)_{jk}^{-1} = \begin{cases} {p_j} , & j = k \\ 0, & j \neq k \end{cases}
>    $$
>    This is NOTs equal to the asymptotic covariance matrix calculated previously
>    $$
>    \Sigma_{j,k} \neq \mathcal{I}(p)_{jk}^{-1}
>    $$
>
> 5. Now we have to check the assumptions.
>
>    (1) The model need to be identified. (Satisfied)
>
>    (2) The support of the probability distribution does depend on theta. (Satisfied)
>
>    (3) $\theta^*$ does not lie on the boundary of the parameter set $\Theta$. (Not satisfied)
>
>    (4) $\mathcal{I}(\theta)$ is invertible in a neighborhood of $\Theta^*$. (Not satisfied)
>
>    (5) Technical assumptions - continuous differential. 
>
>    To solve this problem, try
>    $$
>    \tilde{P} = \{p \in \R^{r-1}: p_j > 0, 1- \sum^{r-1}_{j=1}p_j > 0 \}\\
>    \tilde{\mathcal{I}}(p)^{-1}  = (\Sigma)_{j=1, ..., r-1; k = 1, ..., r-1}
>    $$
>    where $p_j > 0$ satisfies (4) and $1- \sum^{r-1}_{j=1}p_j > 0$ satisfies (3).
>
>    **Remark:** **Explanation of why $1- \sum^{r-1}_{j=1}p_j > 0$ satisfies (3):** 
>
>    If we consider the case where there are three options that occur with probabilities $p_1$, $p_2$, and $p_3$, the parameter space will be some subset of 3D space. A few examples of subsets of 3D space are volumes, such as spheres and cubes and pyramids. These examples of volumes all have interiors as well as boundaries (the boundaries are the surfaces that enclose the volumes; i.e., the surface of the sphere or the faces of the cube or pyramid). However, the surfaces themselves (just the boundaries of the volumes, excluding the interiors of the volumes) are also subsets of 3D space. Similarly, you could have a circle or a square floating in 3D space; these would certainly be subsets of 3D space, but would be surfaces rather than volumes.
>
>    The positivity constraints ($p_1, p_2, p_3 > 0$) lead to a parameter space that is a giant cube with its boundaries at $0$ and $\infty$ on all three axes. If these were our only constraints, then the requirement for our MLE theorem would be that none of the parameters are $0$ or $\infty$, i.e., that the true parameter $(p_1, p_2, p_3)$ is not on the boundary of our parameter space.
>
>    However, once we add the constraint $p_1 + p_2 + p_3 = 1$ , we say that the parameter space is the intersection of this plane and the cube described above. We get a triangle lying in the plane defined by $p_1 + p_2 + p_3 = 1$ with its three edges on the planes $p_1 = 0$, $p_2 = 0$, and $p_3 = 0$. (See it drawn here.)
>
>    ![reci6-boundary](../assets/images/reci6-boundary.png)
>
>    Since the parameter space is a triangle, it is a surface and it is "all boundary." Our theorem for MLE requires the true parameter to be a point on the interior of the parameter space volume, which is impossible when there is no real interior, as in our case.
>
>    Our real problem is that the equality constraint brings our parameter space down a dimension. We have $r$ variables, but only $r-1$ degrees of freedom, so our parameter space is a surface instead of a volume. If we could represent our problem in terms of only $r-1$ variables, we'd have a parameter space that was volume in $\R^{r-1}$. We can do that by inverting our equality constraint to solve for one of the variables in terms of the other $r-1$, then substituting that in to eliminate that variable and get down to only $r-1$.
>
>    Since our equality constraint is linear, the inversion is very easy; it's $p_r = 1 - \sum_{i=1}^{r-1}p_i$. Now, we need to express all our inequality constraints (we've already taken care of the equality constraint by the substitution) in terms of only $p_1, \ldots, p_{r-1}$. The first $r-1$ are simple: $p_i > 0 \forall i \in \{1,\ldots,r-1\}$. The last one is also not too bad; we're just substituting in our new expression for $p_r$ to get $1 - \sum_{i=1}^{r-1}p_i > 0$. With this substitution, our parameter space is a volume again and we've avoided the trouble of the parameter space being all boundary, since now the parameter space is a volume in $\R^{r-1}$ instead of a surface in $\R^r$.



# Lecture 16. Goodness of Fit Tests Continued: Kolmogorov-Smirnov test, Kolmogorov-Lilliefors test, QQ-Plots

There are 6 topics and 2 exercises.

## 1. CDF and Empirical CDF

Let $X$ be a random variable with distribution $\mathbf{P}$. Recall the CDF of $\mathbf{P}$ is given by the function
$$
\begin{aligned}
F: \mathbb {R} &\rightarrow [0,1]\\
t &\mapsto \mathbf{P}(X \leq t)
\end{aligned}
$$
Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } X$​. The **empirical cumulative distribution function**, also called the **empirical CDF**, is the random function
$$
\begin{aligned}
F_n : \R &\rightarrow [0,1]\\
t &\mapsto \frac{1}{n} \sum _{i = 1}^ n \mathbf{1}(X_ i \leq t).
\end{aligned}
$$
The empirical CDF depends on $n$​ and the observed data $X_i, \ i=1,...,n$​.

Note that for any $t$​​​​ (in the domain of $F_n$​​​​ i.e. $t \in \R$​​​​), the empirical CDF $F_n(t)$​​​​ is **random**, since $F_n(t)$​​​ is a function of the random variables $X_1, ..., X_n$​​​. For any $t$​​​ (in the domain of $F_n$​​​), the true CDF $F(t) = P(X \leq t)$​​​ is **deterministic**.

#### Pointwise and Uniform Convergence of Functions

A sequence of functions $g_n(x)$ **converges pointwise** to a function $g(x)$ if for each $x$, $\displaystyle \lim _{n\to \infty } g_ n(x)=g(x).$

**Example:** In the region $x> 1$, $g_n(x) = {1\over x^n}$ converges pointwise to $g(x)= 0$. For any fixed $x>1$ ${1\over x^n} \xrightarrow[n \rightarrow \infty]{} 0$.

A sequence of functions $g_n(x)$​ **converges uniformly** to a function $g(x)$​ if $\lim _{n\to \infty } \sup _{x\in \mathbb {R}} |g_ n(x)-g(x)|=0.\, $​ That is, for every $M > 0$​, there exists an $n_M$​ such that $\sup _ x |g_ n(x) - g(x)| < M$​ for all $n \geq n_M$.

**Example:** In the region $x > 2$, $g_n(x) = {1\over x^n}$ converges uniformly to $g(x)=0$, since $\sup _{x>2} g_ n(x)=\sup _{x>2} \frac{1}{x^ n}= \frac{1}{2^ n}\xrightarrow [n\to \infty ]{} 0$​.

**Example of pointwise but not uniform convergence:** (The 1st one does not imply the 2nd one)

The sequence of functions $\, g_ n(x)=\frac{1}{x^ n}\,$ does not converge uniformly to $g(x) =0$ in the region $x > 1$, since $\sup _{x>1} g_ n(x)=\sup _{x>1} \frac{1}{x^ n}= 1,\,$ which does not converge to 0 as $n \rightarrow \infty$.

#### Consistency

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } X$ be i.i.d. random variables with CDF $F(t)$ and empirical CDF $F_n(t)$.

By the LLN, for all $t \in \R$​,
$$
F_n(t) \xrightarrow[n \rightarrow \infty]{a.s.} F(t)
$$
**Glivenko-Cantelli Theorem: (aka. The Fundamental Theorem of Statistics)** [LLN uniformly]
$$
\sup _ {t \in \R} |F_ n(x) - F(x)| \xrightarrow[n \rightarrow \infty]{a.s.} 0.
$$
(Note that the 1st convergence does not imply the 2nd convergence.)

This is a **stronger** result than the one happens **uniformly** over $t$​​. This means for all large enough $n$​​ and for any $\delta > 0$​​, the difference $|F_n(t) - F(t)|$​​ is bounded above by $\delta$​​ for all $t$​​. Almost sure convergence means that for all $\delta > 0$​​ and $\epsilon > 0$​​, there exists $N = N(\delta, \epsilon)$​​ such that the event $\sup _{t} |F_ n(t) - F(t)| < \delta$​​ occurs with probability at least $1-\epsilon$​​ for all $n > N$​​. In other words, with probability approaching $1$​​, the function $F_n$​​ is a close $L_{\infty}$​​ (the sup-norm) approximation of $F$.

## 2. Asymptotic Normality of the Empirical CDF

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathbf{P}$​ for some distribution $\mathbb{\mathbf{P}}$​, and let $F$​ denote its CDF. Let $F_n$​ denote the empirical CDF. 

By the **CLT**, it holds for all $t\in \R$​,
$$
\sqrt{n} (F_n(t)-F(t)) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0, F(t)(1-F(t))).
$$
Note that when $t\rightarrow 0$​​​, the variance goes to $0$​​, and when $t \rightarrow \infty$​​​, the variance goes to $1$.

**Donsker's Theorem: (aka. Uniform Central Limit Theorem)** [CLT uniformly]

A stronger result than the one held by the CLT.

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathbf{P}$ for some distribution $\mathbb{\mathbf{P}}$, and let $F$ denote its CDF. Let $F_n$ denote the empirical CDF. 

Donsker's Theorem states that if $F$ is continuous, then
$$
\sqrt{n} \sup_{t \in \R} |F_n(t)-F(t)| \xrightarrow[n \rightarrow \infty]{(d)} \sup_{0 \leq t \leq 1} |\mathbb{B}(t)|
$$
where $\mathbb{B}$​ is a random curve called **Brownian bridge** on $[0,1]$​.

**Remark:** $\sup _{0 \leq x \leq 1} |\mathbb {B}(x)|$​ is a **pivotal distribution**, i.e. it does not depend on the unknown distribution of the data, and hence we can look up its quantiles in tables or by using software. This will be important as we develop goodness of fit tests for continuous distributions.

## 3. Goodness of Fit Test of Continuous Distributions: Kolmogorov-Smirnov Test

#### Goodness of fit test of continuous distributions:

Let $X_1, ..., X_n$ be i.i.d. real random variables with unknown CDF $F$ and let $F^0$ be a continuous CDF.

Consider the two hypotheses:
$$
H_0 : F=F^0 \quad v.s. \quad H_1 : F \neq F^0
$$
Let $F_n$ be the empirical CDF of the sample $X_1, ..., X_n$.

If $F=F^0$, then $F_n(t) \approx F^0(t)$, for all $t \in [0,1]$.

#### Kolmogorov-Smirnov Test

Let $T_n = \sqrt{n} \sup_{t \in \R} |F_n(t)-F^0(t)|$​.

By Donsker's theorem, if $H_0$​​​ is true, then $T_n \xrightarrow[n\rightarrow \infty]{(d)} Z$​, where​​ $Z$​ has a known distribution (supremum of a | Brownian bridge |)

**KS test with asymptotic level $\alpha$:**
$$
\delta_\alpha^{KS} = \mathbb{1}\{T_n > q_\alpha\}
$$
where $q_\alpha$​ is the $(1-\alpha)$​-quantile of $Z$​ (obtained in tables).

P-value of KS test: $\mathbf{P}[Z > T_n | T_n]$​, which is the probability that the supremum of the | Brownian bridge| is larger than the data that we observed for $T_n$.

**Remark:** This is an **asymptotic** test and this test will have asymptotic level alpha. We used an asymptotic statement which is **Donsker's theorem**.

#### Computational Issues

In practice, how to compute $T_n$?

$F^0$​​ is non decreasing, $F_n$​​ is piecewise constant, with jumps at $t_i = X_i,\  i = 1, ..., n$​​.

Let $X_{(1)} \leq X_{(2)} \leq ... \leq X_{(n)}$​​​​​ be the reordered sample, so $X_{(i)}$​​​ is the **order statistic**.

The expression for **Kolmogorov-Smirnov test statistic** $T_n$​ reduces to the following practical formula:
$$
\begin{aligned}
T_n &= \sup_{t \in \R}\sqrt{n}\ |F_n(t) - F^0(t)|\\
&=\sqrt{n} \max_{1 \leq i \leq n} \Big\{\max\left(\left|F^0(X_{(i)}) - F_n(X_{(i)})\right|, \left|F^0(X_{(i)})-F_n(X_{(i)})\right|\right)\Big\}\\
&=\sqrt{n} \max_{i = 1,...,n} \left\{\max\left(\left|{i-1 \over n} - F^0(X_{(i)})\right|, \left|{i \over n} - F^0(X_{(i)})\right|\right)\right\}
\end{aligned}
$$
The **Kolmogorov-Smirnov test** is
$$
\displaystyle  \displaystyle \mathbf{1}(T_ n>q_\alpha )\qquad \text {where } q_\alpha =q_\alpha (\sup _{t \in [0,1]}\left| \mathbb {B}(t) \right|).
$$
Here, $q_\alpha =q_\alpha (\sup _{t \in [0,1]}\left| \mathbb {B}(t) \right|)\,$​ is the $(1-\alpha)$​-quantile of the supremum $\sup _{t \in [0,1]}\left| \mathbb {B}(t) \right|$​ of the Brownian bridge as in Donsker's Theorem.

**Note:**

* Under $H_0$​, $T_n$​ converges to the **supremum** of a Brownian bridge. A Brownian motion is a **random curve** while its supremum over the interval $[0,1]$​ is a **random variable**. 

* Under $H_0$​, $T_n$​ converges to a **pivotal distribution**, since the limiting distribution is **independent** of the distribution of the $X_1, ..., X_n$​ (as long as $F$​ is continuous). Recall we have
  $$
  \sqrt{n}\sup _{t \in \mathbb {R}} |F_ n(t) - F(t)| \xrightarrow [n \to \infty ]{(d)} \sup _{x \in [0,1]} |\mathbb {B}(x)|.
  $$

* Under $H_0$, $T_n$ converges to a distribution whose quantiles we can either look up in tables or estimate very well using simulations. In general, **pivotal distributions can be understood by consulting a table of quantiles**. Using computational tools, Brownian motions (and their suprema) can be simulated, so this is another approach to computing the quantiles.

**Example:**

![images_u4s6_KSstat](../assets/images/images_u4s6_KSstat.svg)

An example of the empirical cdf $\, F_ n(x_{(1)},x_{(2)},x_{(3)},x_{(4)})\,$​​ for a specific data set $x_{(1)},x_{(2)},x_{(3)},x_{(4)}$​​ of sample size $4$​​, and the cdf $F_X(x)$​​ under the null hypothesis. We see that because $F^0(t)$​​ is increasing, and $F_n(t)$​​ is piecewise constant, $\bigg| F_ n(t) - F^0(t) \bigg|$​​ can only possibly achieve its maximum at $t=x_{(i)}$​.

#### Pivotal distribution

$T_n$​​​ is called a **pivotal statistic**: If $H_0$​​​ is true, the distribution of $T_n$​​​ does not depend on the distribution of the $X_i$​​​'s and it is easy to reproduce it in simulations. 

Indeed, let $U_i=F^0(X_i), i=1, ... n$​ and let $G_n$ be the empirical CDF of $U_1, ..., U_n$.

If $H_0$​​​​​​​​ is true, then $U_1, ..., U_n \stackrel{iid}{\sim} \text{Unif}([0,1])$​​ and the test statistic is calculated by letting $x = F(t),\ F = F^0$:
$$
\begin{aligned}
T_n 
&= \sup\limits_{0 \leq x \leq 1} \sqrt{n} \ |F_n(F^{-1}(x)) -x|\\
&= \sup\limits_{0 \leq x \leq 1} \sqrt{n} \ \left|{1\over n}\sum^n_{i=1} \mathbb{1}(X_i \leq F^{-1}(x)) -x\right|\\
&= \sup\limits_{0 \leq x \leq 1} \sqrt{n} \ \left|{1\over n}\sum^n_{i=1} \mathbb{1}(F(X_i) \leq x) -x\right|\\
&= \sup\limits_{0 \leq x \leq 1} \sqrt{n} \ \left|{1\over n}\sum^n_{i=1} \mathbb{1}(U_i \leq x) -x\right|\\
&= \sup\limits_{0 \leq x \leq 1} \sqrt{n} \ |G_n(x) -x|

\end{aligned}
$$
**Note:** 

* Pivotal means it's a universal distribution. We can compute and reproduce in simulation, and it's always the same.

* Let's generate data $X$​​​ with a given CDF $F$​​​. We start with $U \sim \text{Unif}\ ([0,1])$​​​ and apply $X = F^{-1}(U)$.
  $$
  \begin{aligned}
  \mathbf{P}(X \leq t) &= \mathbf{P}(F^{-1}(U) \leq t)\\
  &= \mathbf{P}(U \leq F(t))\\
  &= F(t)\\
  \end{aligned}
  $$
  The distribution of $F(X)$ is​ uniform $F(X) \sim \text{Unif}\ ([0,1])$.

> #### Exercise 89
>
> Let $X$ be a random variable with invertible CDF $F_X$. Define another random variable $Y = F_X(X)$. Find the CDF $F_Y$ of $Y$.
>
> 1. For $t < 0$
> 2. For $t \geq 0$
> 3. For $ 0 \leq t < 1$
>
> **Solution:**
>
> Given $\, Y=F_ X(X)\,$ where $F_X$ is a CDF, $Y$ only takes values between $0$ and $1$. This means that $F_Y(t) = 0$ for all $t \leq 0$ and $F_Y(t) = 1$ for all $t > 1$.
>
> In the region $0 \leq t < 1$
> $$
> F_ Y(t)\, =\, P(F_ X(X)\leq t)\, =\, P(X\leq F^{-1}(t))\, =\, F(F^{-1}(t))\, =\,  t.
> $$
> We see that the CDF of $Y$ is that of a uniform distribution with support in $[0,1]$, i.e. $Y \sim \text{Unif}(0,1)$.
>
> **Remark 1:** Note that $Y=F_ X(X)\sim \textsf{Unif}(0,1)$ regardless of the distribution of $X$ as long as $F_X$ is invertible.
>
> **Remark 2:** Inverting the result gives $X\sim F_ X^{-1}(Y)$​​​​​ where $Y\sim \textsf{Unif}(0,1)$​​​​. This is useful for simulating data from a given distribution with CDF​ $F_X$​​. Start by sampling from $\text{Unif}(0,1)$​​, and apply $F_X^{-1}$​​ to the sample. The resulting sample will be from a distribution with CDF $F_X$.

#### Quantiles of the pivotal distribution and p-values

For same large integer $M$:

* Simulate $M$​​ i.i.d. copies $T_n^1,...,T_n^M$​​ of $T_n$​​​ (capable since we can just pretend that $F^0 \sim \text{Unif}([0,1])$​​);
* Estimate the $(1-\alpha)$​-quantile $q_\alpha^{(n)}$​ of $T_n$​ by taking the sample $(1-\alpha)$​-quantile $\hat{q}_\alpha^{(n,M)}$​ of $T_n^1,...,T_n^M$​.

Test with approximate level $\alpha$:
$$
\delta_\alpha = \mathbb{1}\{T_n > \hat{q}_\alpha^{(n,M)}\}
$$
Approximate p-value of this test
$$
\text{p-value} \approx {\# \{j = 1, ..., M: T_n^j > T_n\}\over M}
$$

> #### Exercise 90
>
> Let $X_1, ..., X_n$​ be i.i.d samples with CDF $F$​, and let $F^0$​ denote the CDF of $\text{Unif}\ (0,1)$​. Recall that
> $$
> F^0(t) = t \cdot \, \mathbf{1}(t \in [0,1]) + 1 \cdot \mathbf{1}(t > 1) .
> $$
> We want to use goodness of fit testing to determine whether or not $X_1, \ldots , X_ n \stackrel{iid}{\sim } \textsf{Unif}(0,1)$. To do so, we will test between the hypotheses
> $$
> H_0: F(t) = F^0\\
> H_1: F(t) \neq F^0
> $$
> To make computation of the test statistic easier, let us first reorder the samples from smallest to largest, so that
> $$
> X_{(1)} \leq X_{(2)} \leq \ldots \leq X_{(n)}
> $$
> is the reordered sample. In this set-up, the Kolmogorov-Smirnov test statistic is given by the formula
> $$
> T_ n = \sqrt{n} \max _{i = 1, \ldots , n} \left\{  \max \left(\bigg| \frac{i -1}{n} - X_{(i)} \mathbf{1}\left( X_{(i)} \in [0,1]\right) \bigg|, \bigg| \frac{i }{n} - X_{(i)} \mathbf{1}\left( X_{(i)} \in [0,1]\right) \bigg|\right) \right\} .
> $$
> You observe the data set $\mathbf{x}$​​ consisting of $5$​ samples:
> $$
> \mathbf{x}= 0.8, 0.7, 0.4, 0.7, 0.2
> $$
>
> 1. Using the formula above, what is the value of $T_n$​​​ for this data set?
>
> 2. You will use the KS test
>    $$
>    \psi _5 = \mathbf{1}( T_5 > C ).
>    $$
>    What value of $C$​​​​ should be chosen so that $\psi_5$​​​ has (non-asymptotic) level $5\%$​​​​​? Would you reject or fail to reject the null hypothesis on the above data set? Note that the number $x$​ in the $n$-th row of the column labeled by the level $\alpha$ table in the slide "K-S table" is such that
>    $$
>    P_ n^{KS}\left(\frac{T_ n}{\sqrt{n}} > x \right) = \alpha .
>    $$
>
> **Solution:**
>
> 1. First we reorder the given data set to get $0.2, 0.4, 0.7, 0.7, 0.8.$​
>
>    Now $X_{(i)}$​​ is defined to be the $i$​​-th element in the list above. To compute $T_n$​​ for $n=5$​ and plug in the above reordered data set, we compute the maximum of the following list of numbers
>    $$
>     \max (|0 - 0.2|,|0.2 - 0.2|) = 0.2\\
>     \max (|0.2 - 0.4|,|0.4 - 0.4|) = 0.2\\
>     \max (|0.4 - 0.7|, |0.6 - 0.7|) = 0.3\\
>     \max (|0.6 - 0.7|,|0.8 - 0.7|) = 0.1\\
>     \max (|0.8 - 0.8|,|1 - 0.8|) ) = 0.2
>    $$
>    which comes out to be $0.3$. Therefore, $T_5 = \sqrt{5} \cdot 0.3 \approx 0.6708$.
>
> 2. The number in the $5$'S row and column labeled by $0.05$ is $0.56328$. Therefore, we need to set $C = \sqrt{5} \cdot 0.56328 \approx 1.2595.$ 
>
>    Since $0.6708 < 1.2596$, the test $\psi_5$ will fail to reject the null hypothesis that $X_1, \ldots , X_5 \stackrel{iid}{\sim } U([0,1])$ on the given data.

## 4. Other Goodness of Fit Tests

We want to measure the distance between two functions: $F_n(t)$​​​ and $F(t)$​​​. There are other ways, leading to other tests. What's important is that we find a distance such that the distance between $F_n$​​ and $F$​​ has a distribution that's **pivotal**.

* Kolmogorov-Smirnov: [$L_\infty$ distance]
  $$
  d(F_n, F) = \sup_{t \in \R} |F_n(t) -F(t)|
  $$

* Cramer-Von Mises: [$L_2$​ distance]
  $$
  d^2(F_n,F) = \int_{\R}[F_n(t) - F(t)]^2 dt
  $$
  or
  $$
  d^2(F_n,F) = \int_{\R}[F_n(t) - F(t)]^2 dF(t) := \mathbb{E}_{X \sim F}[|F_n(t) - F(t)|^2]
  $$

* Anderson-Darling:
  $$
  d^2(F_n,F) = \int_\R {[F_n(t) - F(t)]^2\over F(t)(1-F(t))} dt
  $$
  or
  $$
  d^2(F_n,F) = \int_\R {[F_n(t) - F(t)]^2\over F(t)(1-F(t))}dF(t)
  $$

## 5. Kolmogorov-Lilliefors Test

#### Motivation: Goodness of Fit Testing for a Gaussian Distribution

Let $X_1, \ldots , X_ n$ be iid random variables with continuous CDF $F$. Let $\{  \mathcal{N}(\mu , \sigma ^2) \} _{\mu \in \mathbb {R}, \sigma ^2 > 0}$ denote the family of all **Gaussian** distribution. We want to test whether or not $F \in \{  \mathcal{N}(\mu , \sigma ^2) \} _{\mu \in \mathbb {R}, \sigma ^2 > 0}$.

Let $\Phi _{\mu , \sigma ^2}$ denote the CDF of $\mathcal{N}(\mu , \sigma ^2)$. We formulate the null and alternative hypotheses
$$
  H_0 :  F = \Phi _{\mu , \sigma ^2} \,  \text {for some} \,  \mu \in \mathbb {R}, \sigma ^2 > 0\\
  H_1 :  F \neq \Phi _{\mu , \sigma ^2} \,  \text {for some} \,  \mu \in \mathbb {R}, \sigma ^2 > 0
$$
Motivated by the Kolmogorov-Smirnov test, you define a test-statistic using the sample mean $\hat{\mu} = \overline{X}_n$​ and sample variance $\hat{\sigma}^2 = S_n^2$​​​ :
$$
\widetilde{T}_ n = \sup _{t \in \mathbb {R}} \sqrt{n} |F_ n(t) - \Phi _{\hat{\mu }, \hat{\sigma ^2}}|.
$$
Assume that the null hypothesis is true. Is it true that
$$
\widetilde{T}_ n \xrightarrow [n \to \infty ]{(d)} \sup _{x \in [0,1]} |\mathbb {B}(x)|
$$
where $\mathbb {B}(x)$​​​ is NOT a **Brownian bridge**.

Moreover, the statistic $\widetilde{T}_ n$ converges in distribution as $n \rightarrow \infty$. The quantiles of $\widetilde{T}_ n$ can be found in tables, and the test based on $\widetilde{T}_ n$ is known as the **Kolmogorov-Lilliefors test**.

**Explanation of not being a Brownian bridge:**

It is true that for any fixed $\mu , \sigma ^2$ that
$$
T_ n = \sup _{t \in \mathbb {R}}\sqrt{n} |F_ n(t) - \Phi _{\mu , \sigma ^2}| \xrightarrow [n \to \infty ]{(d)} \sup _{x \in [0,1]} |\mathbb {B}(x)|.
$$
This result follows by Donsker's theorem as the Gaussian CDF is continuous over the real time.

But if we plug in **estimators** for $\mu$​ and $\sigma^2$​ (and not their true values), then this convergence result no longer holds.

#### Kolmogorov-Lilliefors Test

The quantiles for the test statistic
$$
\widetilde{T}_ n = \sup _{t \in \mathbb {R}} \sqrt{n} |F_ n(t) - \Phi _{\hat{\mu }, \hat{\sigma ^2}}(t)|.
$$
do not depend on true unknown parameters, so the test statistic is **pivotal** (so one can compute its quantiles using a table or computational software). 

This is the **Kolmogorov-Lilliefors Test**.
$$
\psi _ n = \mathbf{1}( T_ n > q_\eta )
$$
where $q_\eta$ denotes the $1-\eta$ quantile of the distribution $T_n$​.

#### Kolmogorov-Smirnov vs. Kolmogorov-Lilliefors Test

Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathcal{N}(\mu , \sigma ^2)$, the empirical distribution $F_ n(t) = \frac{1}{n} \sum _{i =1}^ n \mathbf{1}(X_ i \leq t)$, and the CDF of the Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$ which is denoted by $\Phi _{\mu , \sigma ^2}$.

The test statistic in the Kolmogorov-Smirnov test:
$$
T_ n = \sqrt{n}\sup _{t \in \mathbb {R}} |F_ n(t) - \Phi _{\mu , \sigma ^2}|
$$
The test statistic in the Kolmogorov-Lilliefors test:
$$
\widetilde{T}_ n = \sqrt{n}\sup _{t \in \mathbb {R}} |F_ n(t) - \Phi _{\widehat{\mu }, \widehat{\sigma }^2}|
$$
**Remark 1:** Note that $T_n$ and $\widetilde{T}_ n $ will have different distribution for all $n$, since we plug in the actual values of parameters for $T_n$, while we plug in estimators $\hat{\mu}$ and $\hat{\sigma}^2$ for the true parameter.

**Remark 2:** The **Kolmogorov-Smirnov** test was designed to test **if the data has a specific distribution**; it is not useful for deciding whether or not the true distribution $\mathbf{P}$​ lies in a given family of distributions, in which scenario we should apply **Kolmogorov-Lilliefors** test. 

**Remark 3:** The **student's T test** is only valid if we know that the data has a Gaussian distribution. If the **Kolmogorov-Lilliefors** test fail to reject the null hypothesis that the data has a Gaussian distribution, then we know our data is likely to be Gaussian, then the student's T test can be applied to test if the true mean of our data is $\mu=0$. In practice, many of the methods for statistical inference, such as the student's T test, rely on the assumption the data is Gaussian. Hence, before performing such a test, we need to evaluate whether or not the data is Gaussian.

Let $q_\eta$ denote the $1-\eta$ quantile of $T_n$ (i.e. $P(T_n \geq q_\eta) = \eta$) and let $q_\eta'$ denote the $1-\eta$ quantile of $\widetilde{T}_ n$ (i.e., $P(\widetilde{T}_ n \geq q_\eta ') = \eta$), we expect
$$
q_\eta > q_\eta '
$$
**Explanation:**

For $\widetilde{T}_ n$, the mean and variance of the empirical distribution are $\widehat{\mu}$ and $\widehat{\sigma}^2$, respectively. Hence, it is natural to expect $\mathcal{N}( \widehat{\mu }, \widehat{\sigma }^2)$ to be a good approximation to $F_n(t)$, at least amongst all Gaussian distributions.

For $T_n$, the mean and variance of the empirical distribution do NOT match the mean and variance of $\mathcal{N}(\mu , \sigma ^2)$. This again leads reason to believe that the CDF $\Phi _{\widehat{\mu }, \widehat{\sigma }^2}$ will approximate the empirical distribution $F_n(t)$ better than $\Phi _{\mu , \sigma ^2}$.

Therefore, on average, we expect $\widetilde{T}_ n$ to be smaller than $T_n$. 

Hence, the CDF of $\widetilde{T}_ n$ will be more shifted to the left, and the CDF of $T_n$ will be more shifted to the right. This means that the quantiles of $T_n$ will be larger than the quantiles of $\widetilde{T}_ n$. That is, we are more likely to fail to reject the null hypothesis when applying **Kolmogorov-Lilliefors** test.

## 6. Quantile-Quantile (QQ) Plots

Provide a visual way to perform GoF tests. Not formal test but quick and easy check to see if a distribution is plausible.

**Main idea**: we want to check visually if the plot of $F_n$ is close to that of $F$ or equivalently if the plot of $F_n^{-1}$ is close to that of $F^{-1}$.

More convenient to check if the points
$$
(F^{-1}({1\over n}), F^{-1}_n({1\over n})),(F^{-1}({2\over n}), F^{-1}_n({2\over n})),...,(F^{-1}({n-1\over n}), F^{-1}_n({n-1\over n}))
$$
are near the line $y=x$.

$F_n$ is not technically invertible but we define
$$
F_n^{-1}(i/n) = X_{(i)}
$$
the $i$​​​​​th largest observation. 

The **quantile-quantile (QQ) plot** is constructed in the following way from a data set:

1. Reorder the samples to be in increasing order. Denote the reordered sample by $X_{(1)}, X_{(2)}, \ldots , X_{(n)}$.

2. Plot the points
   $$
   \bigg(F^{-1}\left(\frac{1}{n}\right), X_{(1)}\bigg), \,  \,  \bigg(F^{-1}\left(\frac{2}{n}\right), X_{(2)}\bigg), \,  \,  \ldots , \,  \,  \bigg(F^{-1}\left(\frac{i}{n}\right), X_{(i)}\bigg), \,  \,  \ldots , \,  \,  \bigg(F^{-1}\left(\frac{n-1}{n}\right), X_{(n-1)}\bigg).
   $$
   Note that above we omit plotting the $n$​th point because $F^{-1}(n/n) = F^{-1}(1) = \infty$​. 

The x-values above $(\Phi = F)$.
$$
\Phi ^{-1}(1/n), \Phi ^{-1}(2/n), \ldots , \Phi ^{-1}(i/n), \ldots , \Phi ^{-1}((n-1)/n)
$$
are referred to as the **theoretical quantiles**, and the y-values above given by the reordered sample
$$
X_{(1)}, X_{(2)}, \ldots , X_{(n)}
$$
are referred to as the **empirical quantiles**.

#### The Four Patterns of Quantiles-Quantile (QQ) Plots

A distribution $\mathbf{P}$​​​​ has a **heavier right tail** if
$$
P(X \geq t) \geq P(Y \geq t) \quad \text {for} \,  \,  t> 0 \,  \,  \text {sufficiently large},
$$
where $X \sim P$​​​ and $Y \sim Q$​​​. (Otherwise, $\mathbf{P}$ is said to have a lighter right tail than $Q$.)

Similarly, $\mathbf{P}$​​ has a **heavier left tail** if
$$
P(X \leq -t) \geq P(Y \leq -t) \quad \text {for} \,  \,  t> 0 \,  \,  \text {sufficiently large},
$$
where $X \sim P$​ and $Y \sim Q$​. (Otherwise, $\mathbf{P}$​ is said to have a lighter left tail than $Q$​.)

Four patterns of QQ plots:

* Heavier tails
* Lighter tails
* Right skewed
* Left skewed

![How to interpret a QQ plot - Cross Validated](../assets/images/qqplot4p.png)



# Recitation 10. Gamma Method of Moments

Let $X_1, ..., X_n \stackrel{i.i.d}{\sim} \text{Gamma}(\alpha, \beta)$​​, the PDF is
$$
f(x;\alpha, \beta) = {\beta^{\alpha} \over \Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x}, \quad x > 0
$$
The **moment generating function**:
$$
M_x(t) = (1- {t \over \beta})^{-\alpha}, \quad (t < \beta)
$$

1. Find first $4$​ moments of $X \sim \text{Gamma}(\alpha, \beta)$​.
2. Use Method of Moments to find estimators for $\alpha, \beta$.
3. Find the asymptotic distribution of $\hat{a}$.

> **Solution:**
>
> 1. Recall that the first moment is $M_x^{(1)}(0) = \mathbb{E}[X^1]$​​​​, the $k$​​​th moment is $M_x^{(k)}(0) = \mathbb{E}[X^k]$​​​​.
>
>    First, we compute the 1st moment:
>    $$
>    \begin{aligned}
>    M_x'(t) &= {d\over dt} (1- {t \over \beta})^{-\alpha} = -\alpha \cdot \left( 1- {t \over \beta} \right)^{-\alpha - 1}\left(- {1\over \beta}\right)\\
>    \mathbb{E}[X] &= M_x'(0) = {\alpha \over \beta}
>    \end{aligned}
>    $$
>    Then, we compute the 2nd moment:
>    $$
>    \begin{aligned}
>    M_x''(t) &= {d^2\over dt^2} (1- {t \over \beta})^{-\alpha} ={d\over dt} {\alpha \over \beta} \left(1- {t \over \beta}\right)^{-\alpha - 1} = {\alpha(\alpha + 1) \over \beta^2} \left( 1- {t \over \beta} \right)^{-\alpha - 2}\\
>    \mathbb{E}[X^2] &= M_x''(0) = {\alpha(\alpha + 1) \over \beta^2}
>    \end{aligned}
>    $$
>    So the 3rd moment:
>    $$
>    \mathbb{E}[X^3] = M_x'''(0) = {\alpha(\alpha + 1)(\alpha + 2) \over \beta^3}
>    $$
>    And the 4th moment:
>    $$
>    \mathbb{E}[X^4] = M_x''''(0) = {\alpha(\alpha + 1)(\alpha + 2)(\alpha + 3) \over \beta^4}
>    $$
>
> 2. From (1) we have
>    $$
>    m_1 = {\alpha \over \beta}, \quad m_2 = {\alpha^2 + \alpha \over \beta^2}
>    $$
>    and
>    $$
>    \overline{X} = {1\over n} \sum^n_{i=1}X_i, \ \overline{X^2} = {1\over n} \sum^n_{i=1}X_i^2
>    $$
>    Let $\alpha = g_1(m_1, m_2), \ \beta = g_2(m_1, m_2)$​​​, since $m_1^2 = {\alpha^2 \over \beta^2}, \ m_2-  m_1^2 = {\alpha \over \beta^2}$​​,
>    $$
>    g_1(m_1, m_2) = {m_1^2 \over m_2 - m_1^2} = {\alpha^2/\beta^2 \over \alpha/ \beta^2} = \alpha\\
>    g_2(m_1, m_2) = {m_1 \over m_2 - m_1^2} = {\alpha/ \beta \over \alpha/ \beta^2} = \beta
>    $$
>    Then,
>    $$
>    \hat{\alpha} = g_1(\overline{X}, \overline{X^2}) = {\overline{X^1}^2 \over \overline{X^2} - \overline{X^1}^2}\\
>    \hat{\beta} = g_2(\overline{X}, \overline{X^2}) = {\overline{X^1} \over \overline{X^2} - \overline{X^1}^2}
>    $$
>
> 3. We first apply the **multivariate CLT**
>    $$
>    \sqrt{n} \left(\begin{pmatrix}\overline{X}\\ \overline{X^2} \end{pmatrix} - \begin{pmatrix} \mathbb{E}[X_1]\\ \mathbb{E}[X_1^2] \end{pmatrix} \right) \xrightarrow[]{(d)} \mathcal{N}\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix} , \begin{pmatrix}\text{Var}(X_1) & \text{Cov}(X_1, X_1^2) \\ \text{Cov}(X_1, X_2^2) & \text{Var}(X_1^2) \end{pmatrix} \right)
>    $$
>    where $\begin{pmatrix}\overline{X}\\ \overline{X^2} \end{pmatrix}  = {1\over 2}\sum^n_{i=1}\begin{pmatrix}{X_1}\\ {X_1^2} \end{pmatrix} $​.
>
>    Since $m_k = \mathbb{E}[X_1^k]$​​, $\text{Cov}(X_1, X_1^2) = \mathbb{E}[X^3] - \mathbb{E}[X_1] \mathbb{E}[X_1^2]$, we can rewrite the multivariate CLT to
>    $$
>    \sqrt{n} \left(\begin{pmatrix}\overline{X}\\ \overline{X^2} \end{pmatrix} - \begin{pmatrix} m_1\\ m_2 \end{pmatrix} \right) \xrightarrow[]{(d)} \mathcal{N}\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix} , \Sigma \right)
>    $$
>    where $\Sigma = \begin{pmatrix} m_2 - m_1^2 & m_3 - m_1m_2\\ m_3 - m_1m_2 & m_4 - m_2^2  \end{pmatrix} $.
>
>    Let $g \begin{pmatrix}\overline{X} \\ \overline{X^2} \end{pmatrix} = {\overline{X}^2 \over \overline{X^2} - \overline{X}^2}$​​​​, we use **multivariate delta method**.
>    $$
>    \sqrt{n} \left(g \begin{pmatrix}\overline{X}\\ \overline{X^2} \end{pmatrix} - g\begin{pmatrix} m_1\\ m_2 \end{pmatrix} \right) \xrightarrow[]{(d)} \mathcal{N}\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \nabla g^T\begin{pmatrix}m_1 \\ m_2 \end{pmatrix}\Sigma \nabla g\begin{pmatrix}m_1 \\ m_2 \end{pmatrix} \right)
>    $$
>    Let calculate $\nabla g\begin{pmatrix}m_1 \\ m_2 \end{pmatrix}$​​,
>    $$
>    \nabla g\begin{pmatrix}m_1 \\ m_2 \end{pmatrix} =\nabla \left({m_1^2 \over m_2 - m_1^2}\right) = \begin{pmatrix}{m_1(m_2 - m_1^2)+ m_1^2(-2m_1)\over (m_2 - m_1^2)^2} \\ {m_1^2\over (m_2 - m_1^2)^2} \end{pmatrix}
>    $$
>    Therefore, we can compute the asymptotic variance
>    $$
>    \sqrt{n} \left(g \begin{pmatrix}\overline{X}\\ \overline{X^2} \end{pmatrix} - g\begin{pmatrix} m_1\\ m_2 \end{pmatrix} \right) \xrightarrow[]{(d)} \mathcal{N}\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \nabla g^T\begin{pmatrix}m_1 \\ m_2 \end{pmatrix}\Sigma \nabla g\begin{pmatrix}m_1 \\ m_2 \end{pmatrix} \right)\\
>    \iff \sqrt{n} \left(\hat{\alpha} - \alpha \right) \xrightarrow[]{(d)} \mathcal{N}\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \begin{pmatrix}{m_1(m_2 - m_1^2)+ m_1^2(-2m_1)\over (m_2 - m_1^2)^2} \\ {m_1^2\over (m_2 - m_1^2)^2} \end{pmatrix}^T \begin{pmatrix} m_2 - m_1^2 & m_3 - m_1m_2\\ m_3 - m_1m_2 & m_4 - m_2^2  \end{pmatrix} \begin{pmatrix}{m_1(m_2 - m_1^2)+ m_1^2(-2m_1)\over (m_2 - m_1^2)^2} \\ {m_1^2\over (m_2 - m_1^2)^2} \end{pmatrix} \right)\\
>    $$





# Recitation 7. M-Estimation

1. Derive the asymptotic variance of the sample median.
2. Compare the asymptotic variance for sample mean and median under Laplace $(\mu,1)$​​​.
3. Compare the asymptotic variance for sample mean, median under Cauchy ($\mu,1$​​).
4. Compare the asymptotic variance for those in (3) and Huber estimator under Cauchy ($\mu,1$).

> **Solution:**
>
> 1. Let $m_n$​ be the sample median and suppose that we observe data $X_1, ..., X_n \sim F$​, where $F$​ is a continuous distribution, and let $\mu$​ be the population median $F(\mu)=1/2$​​. For the following, assume that $n$​ is odd, so that the sample median is the unique point $m_n = X_{(n+1)/2}$​. Then,
>
>    We will show that
>    $$
>    P(\sqrt{n}(m_n - \mu) \leq a) \rightarrow \mathcal{N}(0,\sigma^2)
>    $$
>    For some asymptotic variance $\sigma^2$. To this end, notice that
>    $$
>    \{m_n \leq \mu + a/\sqrt{n}\} = \{\#(X_i \leq \mu + a/ \sqrt{n}) \geq {n+1 \over 2}\} = \{\overline{Y_n} \geq {n+1 \over 2}\}
>    $$
>    where the random variables $Y_i$ are defined by $Y_i = \mathbb{1}(X_i \leq \mu + a/ \sqrt{n})$​. Then we have
>    $$
>    \begin{aligned}
>    P(\sqrt{n} (m_n - \mu )\leq a) &= P\left(\overline{Y_n} \geq {n+1 \over 2} \right)\\
>    &= P\left(\overline{Y_n} - p_n n \geq {n+1 \over 2} - p_n n\right)\\
>    &= P\left( {\overline{Y_n} - p_n n \over \sqrt{n p_n (1-p_n)}} \geq {{n+1 \over 2} - p_n n \over \sqrt{n p_n (1 - p_n)}} \right)
>    \end{aligned}
>    $$
>    where $p_n = F(\mu + 1/ \sqrt{n})$​​ is the probability that $Y_i = 1$​ (Notice that $Y_i$​ is a Bernoulli ($p_n$) random variable).
>
>    Let $p = F(\mu) = 1/2$. Notice that
>    $$
>    {\overline{Y_n} - p_n n \over \sqrt{n p_n (1-p_n)}} -  {\overline{Y_n} - p n \over \sqrt{n p (1-p)}}  \xrightarrow[n \rightarrow \infty]{p}0
>    $$
>    which then yields
>    $$
>    {\overline{Y_n} - p_n n \over \sqrt{n p_n (1-p_n)}}\xrightarrow[n \rightarrow \infty]{d}\mathcal{N}(0,1)
>    $$
>    Let ${\overline{Y_n} - p_n n \over \sqrt{n p_n (1-p_n)}} = Z$​, we have
>    $$
>    P(\sqrt{n} (m_n - \mu )\leq a)= P(m_n \leq \mu + a/\sqrt{n}) = P\left( Z \geq {{n+1 \over 2} - p_n n \over \sqrt{n p_n (1 - p_n)}} \right)
>    $$
>    Finally, we notice that
>    $$
>    \begin{aligned}
>    {{n+1 \over 2} - p_n n \over \sqrt{n p_n (1 - p_n)}} &= {{n \over 2} + {1\over 2} - F(\mu + a/\sqrt{n}) n \over \sqrt{n p_n (1- p_n)}} \\
>    &= {{n \over 2} - F(\mu + a/\sqrt{n}) n \over \sqrt{n p_n (1- p_n)}} + {{1\over 2}\over \sqrt{n p_n (1- p_n)} }\\
>    &= {F(\mu) - F(\mu + a/\sqrt{n}) \over a/\sqrt{n}} \cdot {a \over \sqrt{p_n(1-p_n)}} + {{1\over 2} \over \sqrt{n p_n (1- p_n)}}\\
>    &\xrightarrow[]{d} -2a F'(\mu) 
>    \end{aligned}
>    $$
>    Therefore, 
>    $$
>    P(Z \geq -2a F'(\mu) ) = P(-Z \leq 2a F'(\mu)) = P(Z \leq 2a F'(\mu)) = P({Z\over 2 f(\mu)} \leq a) \sim \mathcal{N}\left(0,{1\over 4f(\mu)^2}\right)
>    $$
>    Equivalently,
>    $$
>    \sqrt{n}(m_n - \mu) \xrightarrow[]{d}\mathcal{N}\left(0,{1\over 4f(\mu)^2}\right)
>    $$
>
> 2. For the Laplace distribution, the likelihood of $X_1, ..., X_n$ is
>    $$
>    L(X_1, ..., X_n; \mu) = \prod^n_{i=1} {1\over 2} \exp\left(-|x_i - \mu|\right)
>    $$
>    The log-likelihood is
>    $$
>    \ell(X_1, ..., X_n;\mu) = -n \log(2) - \sum^n_{i=1}|x_i - \mu|
>    $$
>    Notice that maximizing $\ell$​ is equivalent to minimizing $-\ell$, and so
>    $$
>    \hat{\mu}_{MLE} = \min_\mu \sum^n_{i=1}|x_i - \mu|
>    $$
>    This is an M-estimator that corresponds to the sample median.
>
>    The asymptotic variance of sample mean is
>    $$
>    \mathsf{avar}(\overline{X_n})=\mathsf{Var}(X_1) = 2
>    $$
>    Since Laplace$(\mu, 1)$​ and Laplace$(0, 1)$​ must have the same variance, we can use integration by parts twice on the Laplace$(0, 1)$​ to obtain the variance of the Laplace$(\mu, 1)$​​ distribution.
>    $$
>    \begin{aligned}
>    \mathsf{Var}(X) &= \mathbb{E}[X^2] = \int^\infty_{-\infty} {x^2 \over 2} \exp(-|x|)dx\\
>    &= 2 \int^{\infty}_0 {x^2 \over 2} \exp(-x) dx\\
>    &= 2 \left( -{x^2 \over 2} \exp (-x) |^{\infty}_0 - \int^{\infty}_0 x \exp (-x)dx \right)\\
>    &= 2 \left( -\int^{\infty}_0 x \exp(-x) dx \right)\\
>    &= 2 \left( x \exp (-x)|^{\infty}_0 + \int^{\infty}_0  \exp(-x) dx \right)\\
>    &= 2 \left( -\exp(-x)|^{\infty}_0 \right)\\
>    &= 2
>    \end{aligned}
>    $$
>    On the other hand, we derive the asymptotic variance of the sample median in (1), which is
>    $$
>    \text{avar}(m_n) = {1\over 4 f(\mu)^2} = {1\over 4} \cdot {1 \over \left[ {1\over 2} \exp\left(-|\mu - \mu|\right) \right]^2} = 1
>    $$
>    Therefore,
>    $$
>    \text{avar}(m_n) < \text{avar}(\overline{X})
>    $$
>    This gives some evidence that using the sample median at least for the Laplace distribution, would give you better performance, at least asymptotically.
>
> 3. For $X \sim \text{Cauchy}(\mu, 1)$​, we have PDF
>    $$
>    f_X(x) = {1\over \pi (1 + (x - \mu)^2)}
>    $$
>    and CDF
>    $$
>    \begin{aligned}
>    F_X(x) &= \int^x_{-\infty} f(t)dt\\
>    &=\int^x_{-\infty}{1\over \pi(1 + (t-\mu)^2)} dt\\
>    &={\text{arctan}(t-\mu)|^x_{-\infty} \over\pi } \\
>    &=  {1\over 2} + {1\over \pi} \text{arctan}(x-\mu)
>    \end{aligned}
>    $$
>    Note that $F(\mu) = {1\over2}$.
>
>    And its inverse
>    $$
>    F_X^{-1}(t)= \text{tan}\left(\pi \left(t - {1\over2}\right)\right) + \mu
>    $$
>    Notice that these are both continuous on $\R$.
>
>    The asymptotic variance of the sample mean $\overline{X}$​​​ diverges.
>    $$
>    \text{avar}(\overline{X_n}) = n \text{Var}(\overline{X_n}) = \text{Var}(X_i) = \infty
>    $$
>    On the other hand, the asymptotic variance of the sample median is
>    $$
>    \text{avar}(\tilde{X_n}) = {1/4 \over f_X(F_X^{-1}(1/2))^2} = {1/4 \over f_X(\mu)^2} = {\pi^2 \over 4}
>    $$
>    Therefore, we see that the median gives us a better estimator than the mean is the case of the Cauchy distribution. It has a finite asymptotic variance, while 
>
> 4. The Huber loss function is
>    $$
>    \rho(x) = \begin{cases} {x^2 \over 2}, & |x| < \delta\\ \delta|x| - {\delta^2 \over 2}, &|x| > \delta \end{cases}
>    $$
>    The first derivative
>    $$
>    \rho'(x) = \begin{cases}x, & |x| < \delta \\ \delta \ \text{sign}(x), & |x| \geq \delta \end{cases}
>    $$
>    The second derivative
>    $$
>    \rho''(x) = \begin{cases}1, & |x| < \delta \\ 0, & |x| \geq \delta \end{cases}
>    $$
>    The Huber estimator
>    $$
>    \hat{\mu}_{huber} = \text{argmin}_{b \in \R}\sum^n_{i=1} \rho(X_i - b)
>    $$
>    The asymptotic normal distribution is
>    $$
>    \sqrt{n}(\hat{\mu}_{huber} - \mu) \xrightarrow[]{(d)} \mathcal{N}\left(0, {\rm{Var}(\rho'(x))\over [\mathbb{E}\rho''(x)]^2 }\right)
>    $$
>    To compute $\mathbb{E}\rho''(x)$​​​​,
>    $$
>    \begin{aligned}
>    \mathbb{E}\rho''(x) &= \int^{\delta}_{-\delta} 1 \cdot f(x) dx\\
>    &= \int^{\delta}_{-\delta}{1\over \pi(1 + x^2)}dx\\
>    &= {1\over \pi}(\text{arctan}\ \delta - \text{arctan}\ (-\delta))\\
>    &= {2\over \pi} \text{arctan} \ \delta
>    
>    \end{aligned}
>    $$
>    To compute $\rm{Var}(\rho'(x))$​,​
>    $$
>    \begin{aligned}
>     \mathbb{E}(\rho'(x))^2
>    &= \int^{-\delta}_{-\infty} \delta^2 {1\over \pi(1 + x^2)}dx + \int^{\delta}_{-\delta} \delta^2 {1\over \pi(1 + x^2)}dx + \int^{\infty}_{\delta} \delta^2 {1\over \pi(1 + x^2)}dx \\
>    &= {\delta^2 \over \pi} \left(\text{arctan}(-\delta) - \left(-{\pi \over 2}\right)\right) + {\delta^2 \over \pi}\left({\pi \over 2} - \text{arctan}\ \delta \right) + {2\delta \over \pi} - {2 \arctan \delta \over \pi}
>    \end{aligned}
>    $$
>    Since $1+ \tan^2 x = \sec^2 x$​, let $x = \tan u$, then $\sec^2 u\ du = dx$.
>    $$
>    \begin{aligned}
>    \int{x^2 \over \pi (1 + x^2)} dx &= \int{\tan^2u \ \sec^2u du\over \pi (\sec^2u)}\\
>    &={1\over \pi} \int (\sec^2 u - 1 )du\\
>    &={1\over \pi} (\tan u - u)\\
>    \implies\int^{\delta}_{-\delta} {x^2 \over \pi (1 + x^2)} dx &= {1\over \pi}(x - \arctan x)|^\delta_{-\delta}\\
>    &= {1\over \pi} (\delta - \arctan \ \delta - (-\delta - \arctan(-\delta)))\\
>    &= {2\delta \over \pi} - {2 \arctan \delta \over \pi}
>    \end{aligned}
>    $$
>    So the asymptotic variance is
>    $$
>    \begin{aligned}
>    \text{avar}(\hat{\mu}_{huber}) &= {\rm{Var}(\rho'(x))\over [\mathbb{E}\rho''(x)]^2 } = {\mathbb{E}(\rho'(x)^2) \over [\mathbb{E}\rho''(x)]^2}\\
>    &={{\delta^2 \over \pi} \left(\text{arctan}(-\delta) - \left(-{\pi \over 2}\right)\right) + {\delta^2 \over \pi}\left({\pi \over 2} - \text{arctan}\ \delta \right) + {2\delta \over \pi} - {2 \arctan \delta \over \pi} \over \left[{2\over \pi} \text{arctan}\ (\delta)\right]^2}\\
>    &= {\delta^2 + {2\delta \over \pi} - {2 \arctan \delta \over \pi} - 2{\delta^2 \over \pi} \arctan(\delta) \over \left[{2\over \pi} \text{arctan}\ (\delta)\right]^2}
>    \end{aligned}
>    $$
>    By **L'Hopital's rule**,
>    $$
>    \begin{aligned}
>    \text{avar}(\hat{\mu}_{huber}) &\xrightarrow[s\rightarrow 0]{} {\pi^2 \over 4} = \text{avar}(m_n)\\
>    \text{avar}(\hat{\mu}_{huber}) &\xrightarrow[s\rightarrow \infty]{} \text{diverges}
>    \end{aligned}
>    $$



# Lecture 12. M-Estimation

There are 6 topics and 8 exercises.

## 1. Introduction of M-Estimation

* Let $X_1, ..., X_n$ be i.i.d. with some unknown distribution $\mathbf{P}$ and an associated parameter $\mu^*$ in some sample space $E(E \subseteq \R^d$ for some $d \geq 1)$.

* No statistical model / family of distribution needs to be assumed (similar to ML). (Unlike maximum likelihood estimation and the method of moments.)

* **Goal**: estimate some parameter $\mu^*$ associated with $ \mathbf{P}$ , e.g. its mean, variance, median, other quantiles, the true parameter in some statistical model...

  Find a **loss function** $\rho(X,\mu): E \times \mathcal{M} \rightarrow \R$, where $\mathcal{M}$ is the set of all possible values for the unknown $\mu^*$, such that
  $$
  \mathcal{Q}(\mu) := \mathbb{E}[\rho(X_1, \mu)]
  $$
  achieves its minimum at $\mu = \mu^*$. 

  Note that the function $\rho(X,\mu)$ is in particular a function of the random variable $X$, and the expectation in $\mathbb{E}[\rho(X,\mu)]$ is to be taken against the **true distribution** $\mathbf{P}$ of $X$, with associated parameter value $\mu^*$.

  Proved by taking derivative and set it to zero to solve for the minimal parameter: 
  $$
  {\partial \over \partial\mu} \mathbb{E}[(X-\mu)^2] = \mathbb{E}[-2X + 2\mu] = -2 \mu^* + 2\mu = 0 \implies \mu = \mu^*
  $$

* **Formal definition**: An **M-estimator** $\widehat{\mu}$ of the parameter $\mu^*$ is the **argmin of an estimator of a function** $\mathcal{Q}(\mu)$ of the parameter which satisfies the following:

  * $\mathcal{Q}(\mu )\, =\, \mathbb E\left[\rho (X,\mu ) \right] \,$ for some function $\rho:E \times \mathcal{M} \rightarrow \R$, where $\mathcal{M}$ is the set of all possible values of the unknown true parameter $\mu^*$.
  * $\mathcal{Q}(\mu)$ attains a unique minimum at $\mu = \mu^*$, in $\mathcal{M}$. That is, $\text {argmin}_{\mu \in \mathcal{M}}\mathcal{Q}(\mu ) \, =\, \mu ^*$.

* Notes: 

  * Because $\mathcal{Q}(\mu)$ is an expectation, we can construct a (consistent) estimator of $\mathcal{Q}(\mu)$ by **replacing the expectation in its definition by the** **sample mean**.

    * Define $\hat{\mu}_n$ as a minimizer of 
      $$
      \mathcal{Q}_n(\mu) := {1\over n}\sum^n_{i=1} \rho(X_i, \mu)
      $$
      So we have the equation
      $$
      \widehat{\mu } = \text {argmin}_{\mu \in \mathbb {R}} \frac{1}{n} \sum _{i = 1}^ n [\rho (X_ i, \mu )]
      $$
      instead of
      $$
      \mu ^* = \text {argmin}_{\mu \in \mathbb {R}} \mathbb E_{X \sim \mathbf{P}}[ \rho (X, \mu )]
      $$
      Examples: Empirical mean, empirical median, empirical quantiles, MLE, etc.

  * Maximum likelihood estimation is a special case of M-estimation. If we set the loss function to be the **negative log-likelihood**, then the same optimization problem defining the MLE is the one considered for the M-estimator associated to this loss function.

  * M-estimation can be used in both a **parametric** and **non-parametric** context.

#### MLE is an M-estimator

Assume that $(E, \mathbf\{P_{\theta}\}_{\theta \in \Theta})$ is a statistical model associated with the data.

**Theorem**:

Let $\mathcal{M} \in \Theta$ and $\rho(x,\theta) = - \log L(x, \theta)$, provided the likelihood is positive everywhere. Then
$$
\mu^* = \theta^*
$$
Where $\mathbf{P} = \mathbf{P}_{\theta^*}$ (i.e. $\theta^*$ is the true value of the parameter).

**Proof**:

Recall that the MLE is defined by
$$
\widehat{\theta }_ n^{\text {MLE}} = \text {argmax}_{\theta \in \Theta } \frac{1}{n} \sum _{i = 1}^ n \ln p_\theta (X_ i).
$$
By symmetry, we also have
$$
\widehat{\theta }_ n^{\text {MLE}} = \text {argmin}_{\theta \in \Theta } \frac{1}{n} \sum _{i = 1}^ n - \ln p_\theta (X_ i).
$$
Indeed, setting $\rho (x, \theta ) = - \ln p_\theta (x)$, we recover the MLE.

#### Examples:

* If $E = \mathcal{M} = \R$ and $\rho(x,\mu) = (x-\mu)^2$, for all $x \in \R, \mu \in \R$: $\qquad \mu^* = \mathbb{E}[X]$.
* If $E = \mathcal{M} = \R^d$ and $\rho(x,\mu) = ||x - \mu||^2_2$, for all $x \in \R^d, \mu \in \R^d:\qquad \mu^* = \mathbb{E}[X] \in \R^d$.
* If $E = \mathcal{M} = \R$ and $\rho(x, \mu) = |x- \mu|$, for all $x \in \R, \mu \in \R: \quad \mu^*$ is a median of $\mathbf{P}$.

#### Example: multivariate mean as minimizer

Let $\, \mathbf{X}=\begin{pmatrix}  X^{(1)}\\ X^{(2)} \end{pmatrix}\,$ be a continuous random vector with density $\, f: \mathbb {R^2} \to \mathbb {R}.\, \,$ Recall the mean of $X$ is
$$
\mathbb E[\mathbf{X}] = \begin{pmatrix}  \mathbb E[X^{(1)}]\\ \mathbb E[X^{(2)}]\end{pmatrix}
$$
Recall the square of the **Euclidean** norm function on $\R^2$:
$$
\left\|  \cdot  \right\| ^2: \mathbb {R}^2 \rightarrow \R\\
\mathbf{y}\, \, =\, \begin{pmatrix}  y_1\\ y_2 \end{pmatrix} \mapsto \left(y_1\right)^2+\left(y_2\right)^2.
$$
We now show that the (multivariate) mean of $\mathbf{X}$ satisfies
$$
\mathbb E[\mathbf{X}] =  \text {argmin}_{\vec{\mu } \in \mathbb {R^2}} \mathbb E\left[\left\|  \mathbf{X}- \vec{\mu } \right\| ^2\right].
$$
First, expand $ \mathcal{Q}(\vec{\mu })=\mathbb E\left[\left\|  \mathbf{X}- \vec{\mu } \right\| ^2\right]\,$ as an integral expression, and write down both partial derivatives $\frac{\partial \mathcal{Q}}{\partial \mu _1}(\vec{\mu })\,$ and $\frac{\partial \mathcal{Q}}{\partial \mu _2}(\vec{\mu })\,$:
$$
\begin{aligned}
\mathbb E\left[\left\|  \mathbf{X}- \vec{\mu } \right\| ^2\right] & =  \int _{-\infty }^{\infty } \int _{-\infty }^{\infty } \left(\left(x_1-\mu _1\right)^2+\left(x_2-\mu _2\right)^2\right) f(x_1,x_2) dx_1 dx_2 \\
\implies \frac{\partial }{\partial \mu _1}\mathbb E\left[\left\|  \mathbf{X}- \vec{\mu } \right\| ^2\right]& = -2 \int _{-\infty }^{\infty }\int _{-\infty }^{\infty } \left(x_1-\mu _1\right)f(x_1,x_2) dx_1 dx_2\\
\frac{\partial }{\partial \mu _2}\mathbb E\left[\left\|  \mathbf{X}- \vec{\mu } \right\| ^2\right] &=  -2 \int _{-\infty }^{\infty }\int _{-\infty }^{\infty } \left(x_2-\mu _2\right)f(x_1,x_2) dx_1 dx_2.

\end{aligned}
$$
To find the argmin of $\mathbb E\left[\left\|  \mathbf{X}- \vec{\mu } \right\| ^2\right],\,$ we set both partial derivatives to $0$, and obtain
$$
\text {argmin}_{\vec{\mu } \in \mathbb {R^2}} \mathbb E\left[\left\|  \mathbf{X}- \vec{\mu } \right\| ^2\right] = \begin{pmatrix} \displaystyle \int _{-\infty }^{\infty }\int _{-\infty }^{\infty } x_1f(x_1,x_2) dx_1 dx_2 \\ \displaystyle \int _{-\infty }^{\infty }\int _{-\infty }^{\infty } x_2f(x_1,x_2) dx_1 dx_2 \end{pmatrix}\, =\, \begin{pmatrix}  \mathbb E[X^{(1)}]\\ \mathbb E[X^{(2)}]\end{pmatrix}.
$$

#### Median as a minimizer

Assume that $X$ is a continuous random variable with density $f: \mathbb {R} \to \mathbb {R}$. Then a **median** of $X$ is defined to be any point $\text {med}(X) \in \mathbb {R}$ such that
$$
P(X > \text {med}(X)) = P(X < \text {med}(X)) = \frac{1}{2}.
$$
(Recall that for a continuous distribution, $P(X > \text {med}(X)) = P(X \geq \text {med}(X))$ Note: A median of a distribution is not necessarily unique.)

Here, we show that any median satisfies
$$
\text {med}(X) = \text {argmin}_{\mu \in \mathbb {R}} \mathbb E\left[|X - \mu |\right].
$$
First, calculate $\mathbb E\left[|X - \mu |\right]$ in terms of the density $f(x)$.
$$
\begin{aligned}
\mathbb E\left[|X - \mu |\right] & = \int _{-\infty }^\infty |x - \mu | f(x) \,  dx\\
&= \int _{\mu }^\infty (x - \mu ) f(x) \,  dx + \displaystyle \int _{-\infty }^\mu (-x + \mu ) f(x) \,  dx\\
&= \int _{\mu }^\infty x f(x) \,  dx - \displaystyle \int _{-\infty }^\mu x f(x) \,  dx - \mu \left( \displaystyle \int _\mu ^\infty f(x) \,  dx - \displaystyle \int _{-\infty }^\mu f(x) \,  dx \right)
\end{aligned}
$$
Second, let $\, \mathcal{Q}(\mu )=\displaystyle \mathbb E\left[|X - \mu |\right]\,$ denote the expression obtained in the previous question. Then $\, \mathcal{Q}(\mu )$ consists of a sum of terms, each of which can be differentiated with respect to $\mu$. 

We compute $\mathcal{Q}'(\mu )=\frac{d}{d\mu } \mathcal{Q}(\mu )$ by differentiating it term by term. We have, by the fundamental theorem of calculus and the product rule that
$$
\begin{aligned}
\frac{d}{d \mu } \left( \displaystyle \int _{\mu }^\infty x f(x) \,  dx \right) &=-\mu f(\mu )\\
 \frac{d}{d \mu } \left(- \displaystyle \int _{-\infty }^\mu x f(x) \,  dx \right) &= - \mu f(\mu )\\
 \frac{d}{d \mu } \left( -\mu \left( \displaystyle \int _\mu ^\infty f(x) \,  dx - \displaystyle \int _{-\infty }^\mu f(x) \,  dx \right) \right) &= -\displaystyle \int _\mu ^\infty f(x) \,  dx + \displaystyle \int _{-\infty }^\mu f(x) \,  dx + 2\mu f(\mu ).
\end{aligned}
$$
Adding these terms, we have cancellations, yielding
$$
\frac{d}{d \mu } \mathcal{Q}(\mu ) = - \displaystyle \int _\mu ^\infty f(x) \,  dx + \displaystyle \int _{-\infty }^\mu f(x) \,  dx.
$$
Third, compute $\, \mathcal{Q}'(\text {med}(X))\,$. 
$$
\mathcal{Q}'( \text {med}(X) ) = \int _{-\infty }^{\text {med}(X)} f(x) \,  dx - \int _{\text {med}(X)}^\infty f(x) \,  dx = P( X < \text {med}(X)) - P( X > \text {med}(X)) = 0.
$$

#### Quantile as a minimizer

Recall from the lecture that the **check function** is defined as
$$
C_\alpha (x) = \begin{cases} -(1-\alpha)x & \text{if }x < 0\\ \alpha x & \text{if } x \geq 0 \end{cases}
$$
![checkFunction](../assets/images/checkFunction.png)

Assume that $X$ is a continuous random variable with density $f: \R \rightarrow \R$. Define the $\alpha$-**quantile** of $X$ to be $Q_X{\alpha} \in \R$ such that
$$
\mathbf{P}\left(X \leq Q_ X(\alpha )\right) = \alpha
$$
(Here we have used a different convention of the definition of the quantile function from before, where for a standard normal distribution, $q_\alpha$ is such that $P(X > q_\alpha) = \alpha$).

Just like for the median, whether $Q_\alpha$ is unique depends on the distribution.

Here we show that any $\alpha$-quantile of $X$ satisfies
$$
Q_{X}(\alpha ) = \text {argmin}_{\mu \in \mathbb {R}} \displaystyle \mathbb E\left[C_\alpha (X - \mu )\right].
$$
First, compute $F(\mu) = \mathbb E\left[C_\alpha (X - \mu )\right]$ with the form as below. $A,B,C,D$ are coefficients.
$$
\begin{aligned}
F(\mu) =  \mathbb E\left[C_\alpha (X - \mu )\right] =& -\int _{-\infty }^{\mu } (1-\alpha ) (x-\mu ) f(x)\, dx+ \int _{\mu }^{\infty } \alpha (x-\mu ) f(x)\, dx\\
=& -(1-\alpha )\int _{-\infty }^{\mu } x f(x)\, dx+ \alpha \int _{\mu }^{\infty } x f(x)\, dx \\
&+(1-\alpha ) \mu \int _{-\infty }^{\mu } f(x)\, dx-\alpha \mu \int _{\mu }^{\infty } f(x)\, dx.
\end{aligned}
$$
Second, the derivative of $F$ with respect to $\mu$ is
$$
\begin{aligned}
F'(\mu )\, =\, \frac{d}{d\mu }F(\mu ) =& -(1-\alpha )\frac{d}{d\mu }\int _{-\infty }^{\mu } x f(x)\, dx+ \alpha \frac{d}{d\mu }\int _{\mu }^{\infty } x f(x)\, dx\\ &+(1-\alpha ) \frac{d}{d\mu } \left(\mu \int _{-\infty }^{\mu } f(x)\, dx\right)-\alpha \frac{d}{d\mu }\left(\mu \int _{\mu }^{\infty } f(x)\, dx\right)\\
=& -(1-\alpha )\left(\mu f(\mu )\right)+\alpha \left(-\mu )f(\mu )\right) \\
& +(1-\alpha ) \left(\int _{-\infty }^{\mu } f(x)\, dx+ \mu f(\mu )\right) -\alpha \left(\int _{\mu }^{\infty } f(x)\, dx-\mu f(\mu )\right)\\
&(\text {by fundamental theorem of calculus 2})\\
=& (1-\alpha ) \int _{-\infty }^{\mu } f(x)\, dx -\alpha \int _{\mu }^{\infty } f(x)\, dx\\
=& (1-\alpha )\left(\int _{-\infty }^{\mu } f(x)\, dx\right) -\alpha \left(1-\int _{-\infty }^{\mu } f(x)\, dx\right)\\
=& \left(\int _{-\infty }^{\mu } f(x)\, dx\right)-\alpha.
\end{aligned}
$$
Setting $F'(\mu) = 0$ yields
$$
\int _{-\infty }^{\mu } f(x)\, dx = \alpha
$$
Hence, $\text {argmin}_{\mu \in \mathbb {R}} F(\mu )$ is an $\alpha$-quantile of $X$.

#### Convexity of the Expectation of the Loss Function

**Strict convexity** of $\, \mathcal{Q}(\mu )\, =\, \mathbb E\left[\rho (X,\mu ) \right] \,$ ensures that it has a unique **minimum**, and this is guaranteed by strict convexity of $\, \rho (X,\mu )\,$ in $\mu$. **Expectation of a convex function is convex**.

Let $X$ be a random variable with some unknown distribution $\mathbf{P}$ with some associated parameter $\mu^*$ on some sample space $E.$ Let $\rho:E \times \mathcal{M} \rightarrow \R$, where $\mathcal{M}$ is the set of all possible values of the unknown true parameter $\mu^*$ and let $\, \mathcal{Q}(\mu )\, =\, \mathbb E\left[\rho (X,\mu ) \right].\, \,$
$$
\rho (X,\mu )\, \, \text { strictly convex in }\mu \implies \mathbb E\left[\rho (X,\mu ) \right]\, \, \text { strictly convex in }\mu .
$$
**Proof:**

Recall that $\rho$ being strictly convex in $\mu$ means that
$$
t\rho (x,\mu _1)+(1-t)\rho (x,\mu _2)- \rho (x,t\mu _1+(1-t)\mu _2)>0 \quad \text {for all } x.
$$
Taking the expectation of the above inequality gives:
$$
 \mathbb E[t\rho (X,\mu _1)+(1-t)\rho (X,\mu _2)- \rho (X,t\mu _1+(1-t)\mu _2)]\\
= t\mathbb E[\rho (X,\mu _1)]+(1-t)\mathbb E[\rho (X,\mu _2)]- \mathbb E[\rho (X,t\mu _1+(1-t)\mu _2)]>0
$$
For any $\mu_1 \neq \mu_2 \in \mathcal{M}$, and $t \in (0,1)$. This is because $\mathbb E[f(X)]>0$ if $f(X) > 0$. The above inequality exactly implies strict convexity of $\, \mathbb E\left[\rho (X,\mu ) \right]$.

#### Summary of MLE and Moment of Method strategies

* MLE:
  * $\theta \mapsto \text{KL}(\mathbf{P}_{\theta^*}, \mathbf{P}_{\theta})$ is minimized at $\theta = \theta^*$
  * $\text{KL} = - \mathbb{E}[\text{log likelihood}] + $ constant
  * Max log likelihood by ${1\over n} \sum\limits^n_{i=1} \log f_\theta(x_i)$
* Moment of Methods
  * $\mu \mapsto \mathbb{E}[\rho(X,\mu)] = \mathcal{Q}(\mu)$ is minimized at $\mu^*$.
  * Estimate $\mathbb{E}[\rho(X,\mu)]$ by ${1\over n} \sum\limits^n_{i=1} \rho(X,\mu)$
  * Minimize the estimator in $\mu$.

## 2. Asymptotic Normality of M-estimators

#### Preparation

Let $\, \mathbf{X}_1,\, \ldots ,\, \mathbf{X}_ n\,$ be i.i.d. random vector in $\R^d$ with some unknown distribution $\mathbf{P}$ with some associated parameter $\, \vec{\mu }^*\in \mathbb {R}^ d\,$ on some sample space $E$. Let $\mathcal{Q}(\vec{\mu })\, =\, \mathbb E\left[\rho (\mathbf{X},\vec{\mu }) \right] \,$ for some function $\, \rho :E\times \mathcal{M}\to \mathbb {R},\,$ where $\mathcal{M}$ is the set of all possible values of the unknown true parameter $\vec{\mu }^*.\,$

* Let the covariance matrix of the loss be $K(\mu) = \mathsf{Cov}\left[{\partial \rho\over\partial \mu }(X_1, \mu)\right]$. 

  Formally,
  $$
  \mathbf{K}\, =\, \textsf{Cov}\left[\nabla \rho (\mathbf{X}_1,\vec{\mu })\right] = \textsf{Cov}\left[\begin{pmatrix} \frac{\partial \rho }{\partial \mu _1 } (\mathbf{X}_1, \vec{\mu })\\ \vdots \\ \frac{\partial \rho }{\partial \mu _ d } (\mathbf{X}_1, \vec{\mu })\end{pmatrix}\right]\qquad (d\times d).
  $$
  If we have one parameter, the covariance is just the variance 
  $$
  K(\mu) = \mathsf{Var}\left[{\partial \rho\over\partial \mu }(X_1, \mu)\right]
  $$

* Let the curvature be $J(\mu) = {\partial^2 \mathcal{Q} \over\partial \mu \partial \mu^T }(\mu)$ ($= \mathbb{E}\left[ {\partial^2\rho \over\partial \mu \partial \mu^T }(X_1, \mu)\right]$ under some regularity conditions).

  Formally,
  $$
  \mathbf{J}\, =\, \mathbb E[\mathbf{H}\rho ] =\mathbb E\left[\begin{pmatrix} \frac{\partial ^2 \rho }{\partial \mu _1 \partial \mu _1} (\mathbf{X}_1, \vec{\mu })& \ldots &  \frac{\partial ^2 \rho }{\partial \mu _1 \partial \mu _ d} (\mathbf{X}_1, \vec{\mu })\\ \vdots & \ddots & \vdots \\ \frac{\partial ^2 \rho }{\partial \mu _ d \partial \mu _1} (\mathbf{X}_1, \vec{\mu })& \ldots &  \frac{\partial ^2 \rho }{\partial \mu _ d \partial \mu _ d} (\mathbf{X}_1, \vec{\mu })\end{pmatrix}\right]\qquad (d\times d)
  $$
  In one dimension (i.e. $d = 1$), the matrices reduce to 
  $$
  J(\mu )= \mathbb E\left[ \frac{\partial ^2 \rho }{\partial \mu ^2} (X_1, \mu ) \right]
  $$

**Remark 1**: In the log-likelihood case (write $\mu = 0$).

$J(\theta) = K(\theta)= \mathcal{I}(\theta) \qquad \text{ Fisher Information}$

Note that in general the functions $J(\mu)$ and $K(\mu)$ will not equal to each other. In the special case where $\rho(x,\mu)$ is defined to be the negative log-likelihood of the statistical model, then it is true that $J(\mu) = K(\mu)$.

**Remark 2**: Under some technical conditions, the functions $J(\mu)$ and $K(\mu)$ determine the asymptotic variance of the M-estimator $\widehat{\mu}$. The asymptotic variance of $\widehat{\mu}_n$ is given by $\, J(\mu ^*)^{-1} K(\mu ^*) J(\mu ^*)^{-1},$  assuming some hypotheses.

**Remark 3 on signs: ** Let's match the signs in the definition of $\mathbf{J}$ and $\mathbf{K}$ with those in the definition of Fisher information. For maximum likelihood estimation,
$$
\rho _ n(\theta )\, :=\, \rho (\mathbf{X}_1,\, \ldots ,\, \mathbf{X}_ n,\, \theta ) = {-} \ell _ n(\theta )\qquad \text {where }\, \ell _ n(\theta )\, =\,  \ln L_ n(\mathbf{X}_1,\, \ldots ,\, \mathbf{X}_ n,\theta ).
$$
For this particular loss function $\rho$, the $\mathbf{J}$ and $\mathbf{K}$ matrices are
$$
\begin{aligned}
\mathbf{J} =& \mathbb E[\mathbf{H}\rho _1(\theta )]\, =\, -\mathbb E[\mathbf{H}\ell _1(\theta )]\\
\mathbf{K} =&  \textsf{Cov}[\nabla \rho _1(\theta )]\, =\, \textsf{Cov}[-\nabla \ell _1(\theta )]\, =\, \textsf{Cov}[\nabla \ell _1(\theta )]\\ 
& (\textsf{Cov}[\mathbf{Y}]=\textsf{Cov}[-\mathbf{Y}]\, \text {for any random vector }\, \mathbf{Y}.
\end{aligned}
$$
Both of these matrices equals the Fisher information matrix.

#### Asymptotic Normality

Let $\mu^* \in \mathcal{M}$ (the true parameter). Assume the following:

1. $\mu^*$ is the only minimizer of the function $\mathcal{Q}$.
2. $J(\mu)$ is invertible for all $\mu \in \mathcal{M}$.
3. A few more technical conditions.

Then, $\hat{\mu}_n$ satisfies:

* $\hat{\mu}_n \xrightarrow[n \rightarrow \infty]{ \mathbf{P}} \mu^*$.
* $\sqrt{n} (\hat{\mu}_n - \mu^*) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N} (0, J(\mu^*)^{-1} K(\mu^*) J(\mu^*)^{-1})$.

> #### Exercise 67
>
> Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathbf{P}$. Let $\rho(x,\mu)$ denote a loss function satisfying 
> $$
> \mu^* = \text{argmin}_{\mu \in R} \mathbb{E}[\rho(X_1, \mu)]
> $$
> Where $\mu^* \in \R$ is some unknown one-dimensional parameter associated with $\mathbf P$ that we would like to estimate. Let 
> $$
> J(\mu ) =\mathbb E\left[ \frac{\partial ^2 \rho }{\partial \mu ^2} (X_1, \mu ) \right]\\
> K(\mu ) = \text {Var}\left[ \frac{\partial \rho }{\partial \mu }(X_1, \mu ) \right]
> $$
> You construct the M-estimator $\hat{\mu}_n$ associated $\rho$.
>
> 1. Assuming that the conditions for the asymptotic normality of this M-estimator hold, we have
>    $$
>    \sqrt{n} \frac{\widehat{\mu }_ n - \mu ^*}{\sqrt{ J(\mu ^*)^{-2} K(\mu ^*)} } \xrightarrow [n \to \infty ]{(d)} Q
>    $$
>    For some distribution $Q$. What is $Q$?
>
> 2. Let $q_\alpha$ denote the $(1-\alpha)$-quantile of the distribution $Q$. For what value of $q_\alpha$ is it true that
>    $$
>    \mu ^* \in \left[ \widehat{\mu }_ n - q_{\alpha } \sqrt{\frac{J(\mu ^*)^{-2} K(\mu ^*)}{n}} , \widehat{\mu }_ n + q_{\alpha } \sqrt{\frac{J(\mu ^*)^{-2} K(\mu ^*)}{n}} \right]
>    $$
>    with probability $95\%$ as $n \rightarrow \infty$?
>
> 3. Let 
>    $$
>    \mathcal{I} := \left[ \widehat{\mu }_ n - q_{\alpha } \sqrt{\frac{J(\mu ^*)^{-2} K(\mu ^*)}{n}} , \widehat{\mu }_ n + q_{\alpha } \sqrt{\frac{J(\mu ^*)^{-2} K(\mu ^*)}{n}} \right]
>    $$
>    denote the interval in the previous questions.
>
>    Is $\mathcal{I}$ and asymptotic confidence interval for $\mu^*$ of confidence level $95\%$?
>
> **Answer:**
>
> 1. Standard normal
> 2. 1.96
> 3. No, because the endpoints of $\mathcal{I}$ depend on the true parameter.
>
> **Solution:**
>
> 1. $Q$ is a standard normal distribution. Referring to the theorem regarding the asymptotic normality of the M-estimators, we see that the asymptotic variance of $\widehat{\mu}_n$ is $J(\mu ^*)^{-2} K(\mu ^*)$. Hence,
>    $$
>    \sqrt{n} \frac{\widehat{\mu _ n} - \mu ^*}{\sqrt{J(\mu ^*)^{-2} K(\mu ^*)}} \xrightarrow [(d)]{n \to \infty } \mathcal{N}(0,1).
>    $$
>
> 2. $q_\alpha = 1.96$.
>    $$
>    \begin{aligned}
>    P\left( \sqrt{n} \bigg| \frac{\widehat{\mu }_ n - \mu ^*}{\sigma } \bigg| \geq q_{0.025} \right) &= 1 - P \left( \mu ^* \in \left[ \widehat{\mu }_ n - q_{0.025} \sqrt{\frac{J(\mu ^*)^{-2} K(\mu ^*)}{n}} , \widehat{\mu }_ n + q_{0.025} \sqrt{\frac{J(\mu ^*)^{-2} K(\mu ^*)}{n}} \right] \right)\\ &= 0.05
>    \end{aligned}
>    $$
>    where $q_{0.025}=1.96$ is the $95\%$-quantile of a standard Gaussian.
>
> 3. By definition, the endpoints of a confidence interval should be estimators, and this is not the case for $\mathcal{I}$ because $K^{-1}(\mu ^*)$ and $J(\mu ^*)$ depend on the true parameter.

## 3. Robust Statistics and Cauchy's Distribution

Example: **Location parameter**

If $X_1, ..., X_n$ are i.i.d. with density $f(\cdot -m)$, where

* $f$ is an unknown, positive, even function (e.g. the Cauchy density);

  * **Cauchy distribution** is a continuous distribution with a parameter $m$, the PDF is: 
    $$
    f_m(x) = {1\over \pi} {1 \over 1 + (x-m)^2}
    $$
    Set $m=0$ and compute the mean,
    $$
    \int _{-\infty }^{\infty } \frac{1}{\pi } \cdot \frac{x}{1 + x^2} \,  dx  =  \frac{1}{2\pi } \ln (1 + x^2)
    $$
    which is unbounded as $|x|\rightarrow \infty$.

    Hence, $\mathbb{E}[X]$ is not defined.
    $$
    \int \vert x \vert f_m(x)dx= \infty
    $$

* $m$ is a real number of interest, a **location parameter**.

How to estimate $m$?

* M-estimators: empirical mean, empirical median,...
* Compare their risks or asymptotic variances;
* The empirical median is more robust.

Some estimators are more resilient to corruptions or mistakes in the data than others. They are referred to as **robust**. 

* For example, **median** is more robust than mean in describing the average of the data especially when there is a typo in one data point.

> #### Exercise 68
>
> Recall that the **median** of a continuous distribution is any number $M$ such that $P(X > M) = P(X < M) = 1/2$. For the Cauchy distribution, it turns out that the median is unique.
>
> 1. If the location parameter is set to be $m=1/2$, what is med$(X)$?
>
> 2. As in the previous problem, set $X$ denote a random variable distributed as the Cauchy distribution with location parameter $m$. Which of the following are true about the random variable $X-m$?
>
>    a. The expectation (first moment) of $X-m$ is not defined.
>
>    b. $X-m$ Is distributed as a Cauchy random variable with location parameter set to be $0$.
>
>    c. $X-m$ Is symmetric in the sense that $X-m$ and $m-X$ both have the same distribution.
>
>    d. The method of moments can be used to estimate the location parameter $m$.
>
> **Answer:** 
>
> 1. 0.5
> 2. abc
>
> **Solution:**
>
> 1. The answer is $1/2$ since
>    $$
>    P(X > 1/2) = \displaystyle \int _{1/2}^{\infty } \frac{1}{\pi } \cdot \frac{1}{1 + (x-1/2)^2} \,  dx = - \displaystyle \int _{1/2}^{-\infty } \frac{1}{\pi } \cdot \frac{1}{1 + (-y + 1/2)^2} \,  dy = P(X < 1/2).
>    $$
>    It follows from making the substitution $x = -y + 1$.
>
> 2. a. The improper integral (to compute the mean)
>    $$
>    \int _{-\infty }^\infty \frac{1}{\pi } \cdot \frac{x}{1 + (x - m)^2} \,  dx
>    $$
>    does not converge, so the expectation of $X$ is not defined.
>
>    b. $X-m$ has the same CDF as a Cauchy random variable $Y$ with location parameter set to be $0$.
>    $$
>    P( X - m < t) = \displaystyle \int _{-\infty }^{t + m} \frac{1}{\pi } \cdot \frac{1}{1 + (x - m)^2} \,  dx = \displaystyle \int _{-\infty }^{t} \frac{1}{\pi } \cdot \frac{1}{1 + y^2} \,  dy = P(Y < t).
>    $$
>    Here we made the substitution $y=x-m$.
>
>    c. $X-m$ has a density given by $f(x) = \frac{1}{\pi } \frac{1}{1 + x^2}$. This is an even function, so it follows that $X-m$ and $m-X$ have the same distribution.
>
>    d. Since the moments of a Cauchy random variable do not exit, the method of moments cannot be used for parameter estimation for this family of distribution.

## 4. Robust Statistics and Huber's Loss

Huber's loss:
$$
h_\delta(x) = \begin{cases} {x^2 \over 2} & \text{if }|x| \leq \delta\\ \delta(|x| - {\delta \over 2}) & \text{if } |x| > \delta \end{cases}
$$
![images_u3s5_huberloss](../assets/images/images_u3s5_huberloss.svg)

The first derivative of Huber's loss is the **clip function**:
$$
h_\delta'(x) = \text{clip}_\delta(x) = \begin{cases} -\delta & \text{if }x < -\delta\\ x& \text{if }|x| \leq \delta \\ \delta & \text{if }x > \delta \end{cases}
$$
The second derivative is
$$
h_\delta''(x) = \mathbf{1}_{(|x| \leq \delta)}
$$

M-estimation tries to estimate a parameter by minimizing some loss function.

In the univariate case with data $X_i$, this procedure takes the form
$$
\hat{\mu} = \text{argmin}_{b \in \R}\sum^n_{i=1}\rho(x_i - m)
$$
for some choice of function $\rho$​. Some examples include $\rho(x) = x^2$​, which yields the sample mean; $\rho(x)=|x|$, which yields the sample median, and 
$$
\rho(x) = \begin{cases}|x-m|, & |x-m| \geq \delta \\ {(x-m)^2 \over 2\delta} + {\delta \over 2}, & |x-m| < \delta \end{cases}
$$
for some parameter $\delta$​, which is called the **Huber loss function**.

## 5. Applying Huber's loss to the Laplace distribution

#### The Laplace Distribution

The **Laplace distribution** (also known as the **double-exponential distribution**) is a continuous distribution with location parameter $m \in \R$ and density given by
$$
f_m(x)={1\over 2} \exp(-|x-m|)
$$
The likelihood for $n$ observations is given by
$$
\prod _{i = 1}^ n f_ m(X_ i) = \frac{1}{2^ n} \prod _{i = 1}^ n e^{-|x - m|}.
$$
Therefore, the log likelihood is
$$
\log L(X_1, ...,X_n;m) = \sum^n_{i=1} \log \left( {1\over 2} \exp(-|x-m|) \right) = - n \log 2 - \sum^n_{i=1} |X_i - m|
$$
The MLE estimator is
$$
\hat{m}^{MLE} = \text{argmin}_{m}\sum^n_{i=1}|X_i - m|
$$
where $|X_i - m|$ is called **empirical median**.

> #### Exercise 69
>
> Let $X$ denote a Laplace variable with location parameter set to be $m=0$.
>
> 1. What is $\mathbb{E}[X]$?
>
> 2. Does the variance $\sigma ^2 = \mathbb E[(X - \mathbb E[X])^2]$ exist?
>
> 3. Which of the following are true about $X$? (Hint: The function $x^ k e^{-|x|}$ is integrable, i.e. $\int _{-\infty }^{\infty }x^ k e^{-|x|} dx$ is finite for all $k$.)
>
>    a. The distribution of $X$ is symmetric in the sense that $X$ and $-X$ have the same distribution.
>
>    b. The function $\ln f_m(x)$ has a continuous first derivative
>
>    c. For any integer $k>0$, the $k$-th moment $\mathbb{E}[X^k]$ exists.
>
> **Answer:**
>
> 1. $\mathbb{E}[X]=0$
> 2. Yes.
> 3. ac.
>
> **Solution:**
>
> 1. We observe that the function $x e^{-|x|}$ is odd and also integrable. Therefore,
>    $$
>    \mathbb E[X] = \int _{-\infty }^\infty \frac{1}{2} x e^{-|x|} \,  dx = 0.
>    $$
>
> 2. The function $x^2 e^{-|x|}$ is integrable. Hence,
>    $$
>    \mathbb E[(X - \mathbb E[X])^2] = \mathbb E[X^2] = \displaystyle \int _{-\infty }^\infty \frac{1}{2} x^2 e^{-|x|} \,  dx
>    $$
>
> 3. a. Yes because the density $\frac{1}{2} e^{-|x|}$ is an even function.
>
>    b. No because $\ln f_ m(x) = -|x - m|$ is not differentiable at $x=m$.
>
>    c. Yes because the function $x^ k e^{-|x|}$ is integrable on $\R$, so the $k$-th moment $x^ k e^{-|x|}$ exists for all $k > 0$.

> #### Exercise 70
>
> Let $\widehat{m}_ n^{\text {MLE}}$ denote the MLE for an unknown parameter $m^*$ of a Laplace distribution.
>
> Can we apply the theorem for the asymptotic normality of the MLE to $\widehat{m}_ n^{\text {MLE}}$?
>
> **Answer:** No because the log-likelihood is not twice-differentiable, so the Fisher information does not exist.
>
> **Solution:** This is because $\ell _ n(X_1, \ldots , X_ n; m)$ has discontinuities in its first derivative with respect to $m$ at $m=X_i$ for $i=1,...,n.$

#### Asymptotic Normality

In the framework of M-estimation, our loss function is not Huber's loss itself, but rather,  
$$
\rho(X,m) = h_\delta(X-m)
$$
Since $\rho(x,m)$ is **twice-differentiable**, functions $K$ and $J$ exist for a Laplace statistical model.

$J(m)$ is
$$
\begin{aligned}
J(m) &= \mathbb{E}[h_\delta''(X)] = \mathbb{E}[\mathbf{1}(|X-m| \leq \delta)] = \mathbf{P}(|X-m|\leq \delta)\\
&= 2 \int^\delta_0 f_0(x)dx = \int^\delta_0 e^{-x} dx = 1-e^{-\delta}
\end{aligned}
$$
$K(m)$ is
$$
\begin{aligned}
K(m) &= \mathsf{Var}(\text{Clip}_\delta(X-m))\\
&= \mathbb{E}[\text{clip}_\delta^2(X-m)] - \mathbb{E}[\text{clip}_\delta(X-m)]^2\\

\end{aligned}
$$
We know that
$$
\begin{aligned}
\mathbb{E}[\text{clip}_\delta(X-m)] &= 0\\
\mathbb{E}\left[\text{clip}_\delta^2(X-m)\right] &= 2 \left[\int^\delta_0 x^2 f_0(x)dx + \int^\infty_\delta \delta^2 f_0(x)dx\right]\\
&= 2\int^\delta_0 x^2 {e^{-x}\over 2} dx + \delta^2 \int^{\infty}_\delta e^{-x} dx\\
&= \int^\delta_0 x^2 e^{-x} dx + \delta^2 \int^{\infty}_\delta e^{-x} dx\\
\end{aligned}
$$
Apply the formula of **integration by parts**.
$$
\begin{aligned}
\int^\delta_0 x^2 e^{-x} dx &= - x^2 e^{-x} |^{\delta}_0 + 2 \int^{\delta}_0 x e^{-x}dx\\
&= \delta^2 e^{-\delta} + 2\left[ -x e^{x}|^\delta_0 + \int^\delta_0 e^{-x}dx \right]\\
&= -\delta^2 e^{-\delta} - 2 \delta e^{-\delta} + 2 - 2e^{-\delta}
\end{aligned}
$$
We have $K(m)$,
$$
K(m) =-\delta^2 e^{-\delta} - 2 \delta e^{-\delta} + 2 - 2e^{-\delta} + \delta^2 e^{-\delta} = - 2 \delta e^{-\delta} + 2 - 2e^{-\delta}
$$
Therefore, for $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Lap}(m^*)$, the M estimator is
$$
\widehat{m}(\delta) = \text{argmin}_{m\in \R} {1\over n} \sum^n_{i=1} h_{\delta} (X_i - m)
$$
where we emphasize the dependence on the parameter $\delta \in (0,\infty)$.

$\widehat{m}$ is **asymptotic normal** since $m^*$ is the **unique minimizer** of the function $m \mapsto \mathbb E_{X \sim P_{m^*}}[\rho (X, m)]$, and $J(m)$ is **invertible**, given $J(m) = 1 - e^{-\delta }$.
$$
\sqrt{n}( \widehat{ m}(\delta ) - m^*) \xrightarrow [n \to \infty ]{(d)} N\left( 0 \,  , \,  g(\delta ) \right).
$$
where 
$$
g(\delta ) = \frac{2(1 - \delta e^{-\delta } - e^{-\delta })}{(1 - e^{-\delta })^2}.
$$
We can extend $g$ to be a continuous function with domain $[0,\infty]$ by setting $g(0)=1$ and $g(\infty) =2$.

By graphing, $g(\delta)$ is an **increasing** function on $[0,\infty]$. The minimum asymptotic variance is for $\delta \rightarrow 0$, where $\hat{m}_0 = 1$, implying that $\hat{m}_0 = \hat{m}^{MLE}$. While the maximum asymptotic variance is for $\delta \rightarrow \infty$, where $\hat{m}_\infty = \overline{X}_n$. 

> #### Exercise 71
>
> Find the value of 
> $$
> \frac{\partial }{\partial m} \mathbb E_{X \sim P_{m^*}}[ h_{\delta }(X - m) ] \bigg|_{m = m^*}.
> $$
> where $h_\delta(X-m)$ is the Huber's loss.
>
> **Answer:** 0
>
> **Solution:**
>
> Observe that
> $$
> \begin{aligned}
> \frac{\partial }{\partial m} \mathbb E_{X \sim P^*_ m}[h_\delta (X-m)] &= \mathbb E_{X \sim P^*_ m}\left[ \frac{\partial }{\partial m} h_\delta (X-m) \right] \\
> &= \frac{1}{2} \displaystyle \int _{-\infty }^\infty \text {clip}_\delta (x - m) e^{-|x - m^*|} \,  dx\\
> &=  \frac{1}{2} \left( -\delta \displaystyle \int _{m + \delta }^\infty e^{-|x - m^*|} \, dx + \delta \displaystyle \int _{-\infty }^{-\delta + m} e^{-|x - m^*|} \, dx + \displaystyle \int _{-\delta + m}^{\delta + m} \,  (x-m) e^{-|x-m^*|} dx \right).
> \end{aligned}
> $$
> Applying the change of variables $y = x-m$, we have
> $$
> = \frac{1}{2} \left( -\delta \displaystyle \int _\delta ^\infty e^{-|y + m - m^*|} \, dy + \delta \displaystyle \int _{-\infty }^{-\delta } e^{-|y + m - m^*|} \, dy + \displaystyle \int _{-\delta }^\delta y e^{-|y + m - m^*|} \,  dy \right).
> $$
> Setting $m=m^*$, we have
> $$
> \frac{\partial }{\partial m} \mathbb E_{X \sim P^*_ m}[h_\delta (X)] \bigg|_{m = m^*} = \frac{1}{2} \left( -\delta \displaystyle \int _\delta ^\infty e^{-|y|} \, dy + \delta \displaystyle \int _{-\infty }^{-\delta } e^{-|y|} \, dy + \int _{-\delta }^\delta y e^{-|y|} \,  dy \right) = 0.
> $$
> **Remark:** The function $m \mapsto \mathbb E_{X \sim P^*_ m}[h_\delta (X)]$ is strictly convex, so this means the loss function has a unique critical point, and this is where the minimum is attained. The above calculation guarantees that the minimum is at $m=m^*$, the value of the true parameter.

> #### Exercise 72
>
> If $\delta = \infty$, it makes sense to extend the definition of Huber's loss to be
> $$
> h_\infty (x) = \frac{x^2}{2}.
> $$
> Setting $\delta = \infty$, we have
> $$
> \widehat{m}(\infty ) = \text {argmin}_{m \in \mathbb {R}} \frac{1}{2n} \sum _{i = 1}^ n (X_ i - m)^2.
> $$
> What is another name for $\widehat{m}(\infty )$?
>
> **Answer:** The sample average.
>
> **Solution:** 
>
> Let differentiate and find the value of $m$ that is a critical point of the function
> $$
> F(m) := \frac{1}{2n} \sum _{i = 1}^ n (X_ i - m)^2.
> $$
> Observe that
> $$
> F'(m) = -\frac{1}{n} \sum _{i = 1}^ n (X_ i - m).
> $$
> Setting $m = \frac{1}{n} \sum _{i = 1}^ n X_ i$, we see that $F'(m) = 0$. By strict convexity, this implies that the sample average is the unique global minimizer of $F(m)$.

#### The Sample Median

Let $S=x_1 < x_2 < ... < x_n$ denote a sorted list of numbers. We define the elementary median med$_e(S)$ to be 
$$
\text {med}_ e(S) := \begin{cases}  x_{\lceil n/2 \rceil } & \text {if} \,  \,  \,  n \,  \text {is odd} \\ \frac{1}{2}(x_{n/2} + x_{n/2 + 1}) & \text {if} \,  \,  \,  n \,  \text {is even} \end{cases}
$$
A more advanced definition, useful for statistical purposes, is to define the sample median med$_s(S)$ of a sample $S:= X_1, X_2, ..., X_n$ to be
$$
\text {med}(S) := \text {argmin}_ m \sum _{i =1 }^ n \left| X_ i - m \right|.
$$
While the elementary median is unique, this is not always the case for the sample median.

## 6. Review of Methods of Estimation

Recap

* Three principled methods for estimation: maximum likelihood, Method of moments, M-estimators.
* Maximum likelihood is an example of M-estimation.
* Method of moments inverts the function that maps parameters to moments.
* All methods yield to asymptotic normality under regularity conditions.
* Asymptotic covariance matrix can be computed using multivariate delta-method.
* For MLE, asymptotic covariance matrix is the inverse Fisher information matrix.

> #### Exercise 73
>
> Which of the following estimators are defined in terms of an optimization problem?
>
> a. Maximum likelihood estimator
>
> b. Method of moments estimator
>
> c. M-estimator
>
> **Answer:** ac
>
> **Solution:**
>
> The MLE is defined by maximizing the log-likelihood, and an M-estimator is defined by minimizing a loss function. However, the method of moments estimator is constructed by solving a system of equations.

> #### Exercise 74
>
> All three method of estimation studied in this unit: maximum likelihood estimation, the method of moments, and M-estimation, lead to asymptotically normal estimators if certain technical conditions are satisfied.
>
> In general, an asymptotically normal estimator $\widehat{\theta }_ n$ can be used to construct a confidence interval for an unknown parameter.
>
> What quantity related to the estimator $\hat{\theta}$ determines the length of an asymptotic confidence interval at level $95\%$ (Assume that you use the plug-in method and that $n$ is very large.)
>
> a. The asymptotic variance of $\hat{\theta}_n$.
>
> b. The rate of convergence of $\hat{\theta}_n$ to the normal distribution $\mathcal{N}(0,1)$.
>
> c. The mean of $\hat{\theta}_n$.
>
> **Answer:** a
>
> **Solution:**
>
> Consider an asymptotically normal estimator $\widehat{\theta_n}$, which satisfies
> $$
> \sqrt{n} (\widehat{\theta _ n} - \theta ) \xrightarrow [n \to \infty ]{(d)} \mathcal{N}(0, \sigma ^2)
> $$
> for some asymptotic variance $\sigma^2 > 0$. Let $q_{\alpha/2}$ denote the $\alpha/2$-quantile of a standard Gaussian. Then we have that
> $$
> P\left( \sqrt{n} \frac{\bigg| \widehat{\theta _ n} - \theta \bigg|}{\sigma } \geq q_{\alpha /2} \right) \xrightarrow {n \to \infty } \alpha
> $$
> which implies that 
> $$
> P\left( \theta \notin \left[ \widehat{\theta _ n} - q_{\alpha /2} \frac{\sigma }{\sqrt{n}}, \widehat{\theta _ n} + q_{\alpha /2}\frac{\sigma }{\sqrt{n}} \right] \right) \xrightarrow {n \to \infty } \alpha .
> $$
> Therefore, using the plug-in method, we have that
> $$
> P\left( \theta \notin \left[ \widehat{\theta _ n} - q_{\alpha /2}\frac{\widehat{\sigma }}{\sqrt{n}}, \widehat{\theta _ n} + q_{\alpha /2}\frac{\widehat{\sigma }}{\sqrt{n}} \right] \right) \xrightarrow {n \to \infty } \alpha ,
> $$
> Setting $\alpha=0.05$, we have that
> $$
> \mathcal{I} := \left[ \widehat{\theta _ n} - q_{\alpha /2}\frac{\widehat{\sigma }}{\sqrt{n}}, \widehat{\theta _ n} + q_{\alpha /2}\frac{\widehat{\sigma }}{\sqrt{n}} \right]
> $$
> If $n$ is very large, we have that $\widehat{\sigma }_ n \approx \sigma$, so the length of $\mathcal{I}$ is approximately $2 q_{0.025} \sigma /\sqrt{n}$. That is, the length depends only on the $\alpha/2$ quantile, the sample size, and the asymptotic variance. 



# Recitation 14. Chi Squared Goodness of Fit Test

Suppose we have $X_1, \dots , X_ n$ iid with a discrete and finite sample space. We wish to perform hypothesis tests to:

* **Task 1:** Understand whether the data was generated with a distribution with a particular PMF $p^0$.
* **Task 2:** Understand whether the data was generated with a distribution in a family of distributions $\{ \mathbf {P}_\theta \}$​​, where $\theta \in \R^d$​​ models the family of distribution $\{ \mathbf {P}_\theta \}$​​.

In this recitation, we derive the chi-squared goodness of fit test statistic to perform **Task 1** and generalize this test statistic to perform **Task 2**.

1. What is the categorical statistical model and Goodness of fit hypothesis test?
2. What is the reparameterization, Wald's test, and Fisher information matrix?
3. What is the Chi-squared goodness of fit test if ${p}^0$​​​​ equals to a particular PMF?
4. What is the Chi-squared goodness of fit test if ${p}^0$​ belongs to a family of PMFs?

> **Solution:**
>
> 1. Recall *Categorical Statistical Model* is written as $\left(\{a_1, ..., a_k\}, \{\mathbf{P}_{\mathbf{p}}\}_{p \in \Delta_K}\right)$​​​​​​​​​​​​​​ where $\mathbf{p} = [p_1, ...p_K]^T$​​​​​​​​ is the parameter vector, $\Delta_K$​​​​​ is the probability simplex in $\R^K$​​​, $\mathbf{p} \in \R^K$​​​, $\sum\limits^{K-1}_{i=1} p_i =1,\  p_i \geq 0$​.
>
>    Let $X$​​ be a categorical random variable with sample space $\{a_1...a_K\}$​​, then $\mathbf{P}[X=a_i] = p_i$​​, $\mathbf{p}=\begin{bmatrix} p_1 \\ \vdots \\ p_K \end{bmatrix}$​​.
>
>    Recall the *Goodness of fit test*: observe $X_1...X_n$​ be i.i.d. Let $p^0$ be the distribution that generated $X_1 ... X_n$
>    $$
>    H_0: p = p^0\\
>    H_1: p \neq p^0
>    $$
>    
>    Recall the *Wald's Test*: If $\hat{\Theta} =$ MLE and $\Theta \in \R^K$, asymptotic normality is (under the null hypothesis $H_0$)
>    $$
>    n(\theta^0 - \hat{\theta})^T I(\theta^0)(\theta^0 - \hat{\theta}) \xrightarrow[n \rightarrow \infty]{(d)} \chi_k^2
>    $$
>    
>    * $\hat{\mathbf{p}} = [{N_1 \over n} ... {N_K \over n}]^T$​​, where $N_i = \# $​ observations of $a_i$​.
>    
>    * The Fisher information is not invertible since
>      $$
>      (p^0 - \hat{p})^T \mathbb{1} = \sum^{K-1}_{i=1} (p_i^0 - \hat{p}_i) = 0
>      $$
>    
> 2. 
>
> 2. **Reparameterization** of the categorical statistical model:
>
>    Let $\left(\{a_1... a_K\}, \mathbf{P}_{\overline{\mathbf{p}}}\right)$​​​ where $\overline{\mathbf{p}}$​​​ is $K-1$​​​ dimensional $\mathbb{P}[X = a_K] = 1 - \sum_{i=1}^{K-1} \overline{p}_i$​​​. Our test is
>    $$
>    H_0: \overline{p} = \overline{p}^0\\
>    H_1: \overline{p} \neq \overline{p}^0
>    $$
>    where $\overline{\mathbf{p}}^0 = [p_1^0 ... p_{K-1}^0]^T$​​​.
>
>    **Wald's Test**:
>
>    Observe $X_1, ..., X_n$, we need $\overline{\mathbf{p}}_{MLE}, \ \overline{\mathbf{p}}^0, \ I(\overline{\mathbf{p}}^0)$, to write 
>    $$
>    n (\overline{\mathbf{p}}^0 - \overline{\mathbf{p}}_{MLE})^T I(\overline{\mathbf{p}}^0) (\overline{\mathbf{p}}^0 - \overline{\mathbf{p}}_{MLE})
>    $$
>    The log-likelihood of $X_1, ..., X_n$:
>    $$
>    \ell (X_1,..., X_n, \overline{\mathbf{p}}) = N_1 \log(\overline{\mathbf{p}_1}) + ... + N_{K-1} \log(\overline{\mathbf{p}}_{K-1})+ N_{K} \log(1- \sum^{K-1}_{i=1}\overline{\mathbf{p}}_{i})
>    $$
>    No matter whether we leave the last coordinate $N_{K} \log(1- \sum^{K-1}_{i=1}\overline{\mathbf{p}}_{i})$​​​​ or not, we have the same  $\overline{\mathbf{p}}_{MLE}$​​​, which is
>    $$
>    \overline{\mathbf{p}}_{MLE} = \left[{N_1 \over n} ... {N_{K-1} \over n}\right]^T
>    $$
>    So
>    $$
>    \hat{p}[X = a_K] = {N_K \over n}
>    $$
>    The Fisher information matrix is
>    $$
>    I(\overline{\mathbf{p}}) = \begin{bmatrix}{1\over \overline{p_1}} + {1 \over \overline{p_K}} & &  & {1\over \overline{p}_K}\\ &{1\over \overline{p_2}} + {1 \over \overline{p_K}} & &  \\ & & ... & \\ {1\over \overline{p}_K} & & & {1\over \overline{p_{K-1}}} + {1 \over \overline{p_K}} \end{bmatrix}_{(K-1) \times (K-1)}
>    $$
>    where $\overline{p}_K = 1 - \sum\limits^{K-1}_{i=1} \overline{p}_i$. Except for the diagonal line, all values equal to ${1\over \overline{p}_K}$​ .
>
>    Equivalently, the Fisher information matrix can be rewritten as
>    $$
>    I(\overline{\mathbf{p}}) = \text{diag} \left(\left[{1\over \overline{p}_1} ... {1\over \overline{p}_{K-1}}\right]^T\right) + {1\over \overline{p}_K} \mathbb{1}_{K-1}\mathbb{1}^T_{K-1}
>    $$
>
> 3. Plugging in the Fisher information, the Chi-squared Goodness of fit is calculated as
>    $$
>    \begin{aligned}
>    n \ \left(\overline{\mathbf{p}}^0 - \overline{\mathbf{p}}_{MLE}\right)^T I(\overline{\mathbf{p}}^0) \left(\overline{\mathbf{p}}^0 - \overline{\mathbf{p}}_{MLE}\right) &= n \ \left(\overline{\mathbf{p}}^0 - \overline{\mathbf{p}}_{MLE}\right)^T \text{diag}\left(\left[{1\over \overline{\mathbf{p}}_1^0} ... {1\over \overline{\mathbf{p}}_{K-1}^0}\right]^T\right)  \left(\overline{\mathbf{p}}^0 - \overline{\mathbf{p}}_{MLE}\right)\\
>    &+ {n \over \overline{\mathbf{p}}_K^0} \ \left(\overline{\mathbf{p}}^0 - \overline{\mathbf{p}}_{MLE}\right)^T \left(\mathbb{1}_{K-1} \cdot \mathbb{1}_{K-1}^T \right)  \left(\overline{\mathbf{p}}^0 - \overline{\mathbf{p}}_{MLE}\right)\\
>    &= n \sum^{K-1}_{i=1} { (\overline{\mathbf{p}}^0_i - \overline{\mathbf{p}}^0_{i,MLE})^2 \over \overline{\mathbf{p}_i}^0} + {n \over \overline{\mathbf{p}}_K^0} \ \left[\overline{\mathbf{p}}_{K, MLE} - \overline{\mathbf{p}}_{K}^0 ... \overline{\mathbf{p}}_{K, MLE} - \overline{\mathbf{p}}_{K}^0 \right]^T \cdot \left(\overline{\mathbf{p}}^0 - \overline{\mathbf{p}}^0_{MLE} \right)\\
>    &= n \sum^{K-1}_{i=1} {({\mathbf{p}}^0_i - \hat{\mathbf{p}}^0_{i})^2  \over {\mathbf{p}_i}^0} + n  {({\mathbf{p}}^0_K - \hat{\mathbf{p}}^0_{K})^2  \over {\mathbf{p}_K}^0}
>    \end{aligned}
>    $$
>    So the test statistic of Chi-squared test is
>    $$
>    T_n = n \sum^{K-1}_{i=1} {({\mathbf{p}}^0_i - \hat{\mathbf{p}}^0_{i})^2  \over {\mathbf{p}_i}^0} + n  {({\mathbf{p}}^0_K - \hat{\mathbf{p}}^0_{K})^2  \over {\mathbf{p}_K}^0}\rightarrow \chi_{K-1}^2
>    $$
>
> 4. We test the hypothesis
>    $$
>    H_0: p \in \{\mathbf{P}_{\Theta}\}_{\Theta \in \R^d}\\
>    H_1: p \notin \{\mathbf{P}_{\Theta}\}_{\Theta \in \R^d}
>    $$
>    Suppose $p$ is a binomial distribution with parameter $(k, p)$. The PMF is
>    $$
>    \mathbf{P}(X=k)= {K \choose k}p^k (1-p)^{K-k}
>    $$
>    Let $\{a_0...a_K\}, f_\Theta(a_i) = \mathbf{P}[X=a_i]$, where $\hat{\Theta}$ is MLE of $\Theta$. Observe $X_1, ..., X_n$, MLE = $\hat{p} = \left[ {N_0\over n} ... {N_K\over n} \right]^T$. The test statistic is (under $H_0$)
>    $$
>    T_n = n \sum^K_{i=1} {\left({N_i \over n} - f_{\hat{\Theta}}(a_i) \right)^2\over f_{\hat{\Theta}}(a_i)} \xrightarrow[n \rightarrow \infty]{(d)} \chi^2_{(k+1)-1-d}
>    $$



# Recitation 3: Hypothesis Testing

Let $X_1,\ldots ,X_ n\stackrel{i.i.d.}{\sim }\text {Poiss}(\lambda )$, for some unknown $\lambda > 0$ and let $\lambda_0$ be a fixed (known) positive number.

1. Consider the following hypotheses: $H_0: \lambda =\lambda _0 \quad \text {vs.} \quad H_1: \lambda \neq \lambda _0.$ Give a test with asymptotic level $5\%$.
2. Consider the following hypotheses: $H_0: \lambda \leq \lambda _0 \quad \text {vs.} \quad H_1: \lambda >\lambda _0 .$ Give a test with asymptotic level (at most) $5\%$.
3. Consider the following hypotheses: $H_0: \lambda \geq \lambda _0 \quad \text {vs.} \quad H_1: \lambda <\lambda _0 .$ Give a test with asymptotic level (at most) $5\%$.
4. Consider the following hypotheses: $H_0: |\lambda -2|\leq 1 \quad \text {vs.} \quad H_1: |\lambda -2|> 1 .$ Give a test with asymptotic level (at most) $5\%$.

> **Solution**:
>
> 1. Let $\lambda_0 = 2$. This is a two-sided test.
>
>    The mean and variance of Poisson r.v. is $\mathbb{E}[X_i] = \lambda, \quad \mathsf{Var}(X) = \lambda$. 
>
>    According to LLN, 
>    $$
>    \hat{\lambda} = {1\over n}\sum^n_{i=1}X_i \xrightarrow[n \rightarrow \infty]{R_\lambda} \lambda
>    $$
>    According to CLT,
>    $$
>    \sqrt{n} {\hat{\lambda} - \lambda \over\sqrt{\lambda} }\xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
>    $$
>    The **test statistic** for the test is
>    $$
>    T_n = \left\vert \sqrt{n}{\hat{\lambda} - 2 \over \sqrt2} \right\vert
>    $$
>    If we define $\psi = \mathbf{1}\{T_n > s\}$, the **type 1 error** with test statistic plugged in is
>    $$
>    \begin{aligned}
>    \alpha_\psi(2) = \mathbb{P}_2(\psi(X_1, ..., X_n) = 1) &= \mathbb{P}_2(T_n > s) \\
>    &= \mathbb{P}_2\left(\left\vert \sqrt{n}{\hat{\lambda} - 2 \over \sqrt2} \right\vert > s\right) \xrightarrow[n \rightarrow \infty]{} \mathbb{P}(|Z| > s) \\ 
>    & \left(\text{By LLN: } \quad \left( {\hat{\lambda} -2  \over \sqrt2} \right) \xrightarrow[n \rightarrow \infty]{R_\lambda} \left({\lambda -2  \over \sqrt2} \right)\neq 0\right)\\
>    &= 2(1-\Phi(s)) = \alpha\\
>    &\implies \Phi(s) = 1 - {\alpha \over 2}\\
>    &\implies s = q_{\alpha/2}, \quad 1-{\alpha \over 2 } \text{ quantile of }\mathcal{N}(0,1)
>    \end{aligned}
>    $$
>    where $Z \sim \mathcal{N}(0,1)$.
>
>    Type 2 error: if $\lambda \neq 2$, 
>    $$
>    \mathbb{P}_\lambda(T_n \leq s) = \mathbb{P}_\lambda\left( \left\vert \sqrt{n}{\hat{\lambda} - 2 \over \sqrt2} \right\vert \leq s\right) \xrightarrow[n \rightarrow \infty]{} 0
>    $$
>
> 2. /
>
> 3. This is a one-sided test.
>
>    According to CLT,
>    $$
>    \sqrt{n} {\hat{\lambda} - \lambda \over\sqrt{\lambda} }\xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
>    $$
>    If we define $\psi = \mathbf{1}\{T_n < -s\}$, for $\lambda > 2$, the **type 1 error** is 
>    $$
>    \begin{aligned}
>    \alpha_\psi(\lambda) &= \mathbb{P}_\lambda(T_n < -s) = \mathbb{P}_\lambda\left(\left\vert \sqrt{n}{\hat{\lambda} - 2 \over \sqrt2} \right\vert < -s\right) \xrightarrow[n \rightarrow \infty]{} 0\\
>    & \left(\text{By LLN: } \quad \left( {\hat{\lambda} -2  \over \sqrt2} \right) \xrightarrow[n \rightarrow \infty]{R_\lambda} \left({\lambda -2  \over \sqrt2}\right) < 0\right)
>    \end{aligned}
>    $$
>    When $\lambda = 2$, the type 1 error is 
>    $$
>    \begin{aligned}
>    \alpha_\psi(2) &= \mathbb{P}_2(T_n < -s) = \mathbb{P}_2\left(\left\vert \sqrt{n}{\hat{\lambda} - 2 \over \sqrt2} \right\vert < -s\right) \xrightarrow[n \rightarrow \infty]{} \mathbb{P}(Z < -s)\\
>    &= 1 - \Phi(s) = \alpha\\
>    &\implies \Phi(s) = 1 - {\alpha }\\
>    &\implies s = q_{\alpha}, \quad 1-{\alpha} \text{ quantile of }\mathcal{N}(0,1)
>    \end{aligned}
>    $$
>    Note that it is easier to reject the null hypothesis in one-sided test than in two-sided test.
>
> 4. **Composite test**
>
>    The **test** is defined as 
>    $$
>    \psi = \mathbf{1}\{ T_n^l < -s_l \quad \text{ or } \quad T_n^r > s_r\}
>    $$
>    The **test statistics** are
>    $$
>    T_n^l = \sqrt{n} {\hat{\lambda} - 1 \over \sqrt1} = \sqrt{n} \left(\hat{\lambda} - 1\right); \quad T_n^r = \sqrt n{ \hat{\lambda} - 3 \over\sqrt{3} }
>    $$
>    For $\lambda = 1$, the **type 1 error** is
>    $$
>    \begin{aligned}
>    \alpha_\psi(1) &= \mathbb{P}_1(T_n^l < -s_l \quad \text{or} \quad T_n^r > s_r)\\
>    &= \mathbb{P}_1(T_n^l < -s_l) + \mathbb{P}_1(T_n^r > s_r)
>    \end{aligned}
>    $$
>    where
>    $$
>    \begin{aligned}
>    &\mathbb{P}_1(T_n^l < -s_l) = \mathbb{P}_1\left(\sqrt n (\hat{\lambda} -1)< -s_l\right) \xrightarrow[n \rightarrow \infty]{} \mathbb{P}(Z < -s_l) = \alpha\\
>    & \left(\text{By LLN: } \quad \left({\hat{\lambda} -1} \right)\xrightarrow[n \rightarrow \infty]{R_\lambda} (\lambda -1 ) \right)\\
>    &\mathbb{P}_1(T_n^r > s_r) = \mathbb{P}_1\left(\sqrt{n} {\hat{\lambda}-3 \over \sqrt 3} > s_r\right) \xrightarrow [n \rightarrow \infty]{} 0 \quad \text{ by LLN}\\
>    & \left(\text{By LLN: } \quad \left( {\hat{\lambda} -3  \over \sqrt3} \right) \xrightarrow[n \rightarrow \infty]{R_\lambda} \left({\lambda -3  \over \sqrt3}\right) < 0\right)
>    \end{aligned}
>    $$
>    So we only need to adjust the level at the **left** boundary.
>
>    For $\lambda = 3$, similarly, we only need to adjust the level at the **right** boundary
>    $$
>    \mathbb{P}_3\left(T_n^r > s_r\right) = \mathbb{P}_3\left(\sqrt n {\hat{\lambda} - 3 \over \sqrt 3} > s_r\right) \xrightarrow [n \rightarrow \infty]{} \mathbb{P}(Z > s_r) = \alpha\\
>    \left(\text{By LLN: } \quad \left( {\hat{\lambda} -3  \over \sqrt3} \right) \xrightarrow[n \rightarrow \infty]{R_\lambda}\left( {\lambda -3  \over \sqrt3} \right)\right)
>    $$
>    For $\lambda \in (1,3)$: 
>    $$
>    \begin{aligned}
>    &\mathbb{P}_\lambda\left(T_n^l < -s_l\right) = \mathbb{P}_\lambda\left(\sqrt n (\hat{\lambda} - 1) < -s_l\right) \xrightarrow[n\rightarrow \infty]{} 0\\
>    &\left(\text{By LLN: } \quad \left({\hat{\lambda} -1} \right)\xrightarrow[n \rightarrow \infty]{R_\lambda} \left(\lambda -1 \right) > 0 \right)\\
>    &\mathbb{P}_\lambda\left(T_n^r > s_r\right) = \mathbb{P}_\lambda\left(\sqrt{n} {\hat{\lambda}-3 \over \sqrt 3} > s_r\right) \xrightarrow [n \rightarrow \infty]{} 0 \\
>    &\left(\text{By LLN: } \quad \left( {\hat{\lambda} -3  \over \sqrt3} \right) \xrightarrow[n \rightarrow \infty]{R_\lambda} \left({\lambda -3  \over \sqrt3}\right) < 0\right)
>    \end{aligned}
>    $$
>    In this case, the type 1 error will vanish.



# Recitation 8. (Review) Mean Squared Error

The MSE of an estimator is given by
$$
\text{mse}(\hat{\theta}) = \mathbb{E}(\hat{\theta} - \theta_0)^2
$$
where $\theta_0$​ is the true parameter in our model.

The **bias-variance decomposition** of the MSE is
$$
\begin{aligned}
\text{mse}(\hat{\theta}) &= \mathbb{E}(\hat{\theta} - \theta_0)^2\\
&= \mathbb{E}\hat{\theta}^2 + \mathbb{E}\theta_0^2 - 2 \mathbb{E}\hat{\theta} \theta_0\\
&= \text{var}(\hat{\theta}) + (\mathbb{E}\hat{\theta})^2 + 2 \mathbb{E}\hat{\theta}\theta_0\\
&= \text{var}(\hat{\theta}) + (\mathbb{E}\hat{\theta})^2 + \theta_0^2 - 2 \theta_0 \mathbb{E}\hat{\theta}\\
&= \text{var}(\hat{\theta}) + (-\theta + \mathbb{E \hat{\theta}})^2.\\
&= \text{var}(\hat{\theta}) + (\text{bias}(\hat{\theta}))^2.
 \end{aligned}
$$
For two unbiased estimators of $\theta$​, $\hat{\theta}_1$​ and $\hat{\theta}_2$​, the **relative efficiency** of $\hat{\theta}_1$​ versus $\hat{\theta}_2$ is
$$
\text{eff}(\hat{\theta}_1,\hat{\theta}_2) = {\text{var}(\hat{\theta}_1) \over \text{var}(\hat{\theta}_2)}.
$$
Small efficiency means the $\hat{\theta}_1$ is a much better estimator than $\hat{\theta}_2$.

1. Calculate the bias, variance and MSE for the following distributions and estimators

   a. $X_1, ..., X_n \stackrel{iid}{\sim} \mathsf{Poisson}(\lambda), \hat{\lambda}_1 = \overline{X_n}$.

   b. $X_1, ..., X_n \stackrel{iid}{\sim} \mathsf{Poisson}(\lambda), \hat{\lambda}_2 = X_1$.

   c. $X_1, ..., X_n \stackrel{iid}{\sim} \mathsf{Unif}([0,\theta]), \hat{\theta} = \max_i X_i$​​​.

   d. $X_1, ..., X_n \stackrel{iid}{\sim} \mathsf{Unif}([0,\theta]), \hat{\theta} = 2 \overline{X_n}$​​​.

   e. $X_1, ..., X_n \stackrel{iid}{\sim} f(x;\sigma) = {1\over 2 \sigma} \exp \left(- {|x| \over \sigma} \right), \hat{\sigma} = {\sum_i|X_i| \over n }$​​​.

2. What is the relative efficiency of (a) vs (b)? What about the unbiased estimator based on (c) vs (d)?

3. Find the minimum MSE shrinkage estimators of the mean and variance for the $\mathcal{N}(\mu, \sigma^2)$ distribution.

> **Solution:**
>
> 1. a.
>    $$
>    \mathbb{E}[\overline{X}_n] = {1 \over n } \sum^n_{i=1} \mathbb{E}(X_i) = \lambda\\
>    \text{var}(\overline{X_n}) = {1\over n^2} \sum^n_{i=1} \text{var}(X_i) = {\lambda \over n}\\
>    \text{mse}(\overline{X_n}) = {\lambda \over n} + 0 = {\lambda \over n}
>    $$
>    b. 
>    $$
>    \mathbb{E}[X_1] = \lambda\\
>    \text{var}(X_1) = \lambda\\
>    \text{mse}(\overline{X_n}) = \lambda + 0 = \lambda
>    $$
>    We see that $\hat{\lambda}_1$ will probably be a much better estimator of the parameter $\lambda$​​​ in this model than just using one of the $X_i$'s to estimate the $\lambda$​.
>
>    c. 
>
>    The CDF of $\hat{\theta}$ is given by
>    $$
>    \begin{aligned}
>    F(x) &= \mathbf{P}(\hat{\theta} \leq x)\\
>    &= \mathbf{P}(\max_i X_i \leq x)\\
>    &= (\mathbf{P}(X_i \leq x))^n\\
>    &= \left({x \over \theta}\right)^n
>    \end{aligned}
>    $$
>    where $x \in [0,\theta]$​​. The PDF is
>    $$
>    f(x) = F'(x) = {n x^{n-1} \over \theta^n}, \ x \in [0,\theta].
>    $$
>    We find
>    $$
>    \mathbb{E}[\hat{\theta}] = \int^{\theta}_0 {n x^n\over \theta^n} dx = {n \over n+1}{x^{n+1} \over \theta^n} |^\theta_0 = {n \over n+1} \theta
>    $$
>    and
>    $$
>    \mathbb{E}[\hat{\theta}^2] = \int^{\theta}_0 {n x^{n+1} \over \theta^n} dx = {n \over n+2} {x^{n+2} \over \theta^n} |^\theta_0 = {n \over n+2} \theta^2
>    $$
>    The MSE is
>    $$
>    \begin{aligned}
>    \text{mse}(\hat{\theta}) &= \text{var}(\hat{\theta}) + (\text{bias}(\hat{\theta}))^2\\
>    &= \left({n \over n+2} \theta^2 - \left( {n \over n+1} \theta \right)^2\right) + \left({1\over n+1} \theta\right)^2\\
>    &= \theta^2 \left[ {n \over n+2} - {n^2 \over (n+1)^2} + {1^2 \over (n+1)^2} \right] \\
>    &= \theta^2 \left[{n(n+1)^2 - n^2 (n+2) + (n+2) \over (n+2)^2(n+1)}\right]\\
>    &= \theta^2 {n+3 \over (n+2)^2(n+1)}
>    \end{aligned}
>    $$
>    d.
>    $$
>    \mathbb{E}[\hat{\theta}]= {2\over n}\sum^n_{i=1}\mathbb{E}[X_i] = 2 {\theta \over 2} = \theta\\
>    \text{var}(\hat{\theta})= {4 \over n^2} \sum^n_{i=1}\text{var}(X_i) = {4\theta^2 \over 12n} = {\theta^2 \over 3n}\\
>    \text{mse}(\hat{\theta}) = \text{var}(\hat{\theta}) + (\text{bias}(\hat{\theta}))^2 = {\theta^2 \over 3n} + 0 = {\theta^2 \over 3n}
>    $$
>    Although the $\hat{\theta} = \max_i X_i$​ is a biased estimator, the MSE of it is actually smaller than the MSE of $2\overline{X}_n$.
>
>    e. 
>    $$
>    \mathbb{E}[\hat{\sigma}] = {1\over n}\sum^n_{i=1} \mathbb{E}|X_i|
>    $$
>    We have
>    $$
>    \mathbb{E}|X_i| = \int^\infty_{-\infty} {|x| \over 2 \sigma} \exp(-|x|/\sigma)dx = \int^\infty_0 {x \over \sigma} \exp(-x/\sigma) dx = \sigma
>    $$
>    Thus $\mathbb{E}[\hat{\sigma}] = \sigma$.
>    $$
>    \text{var}(\hat{\sigma}) = {1\over n^2}\sum^n_{i=1} \text{var}(|X_i|)
>    $$
>    Notice,
>    $$
>    \mathbb{E}|X_i|^2 = \int^\infty_0 {x^2 \over \sigma }\exp(-x/\sigma) = \sigma^2  +\sigma^2 = 2 \sigma^2
>    $$
>    Thus, $\text{var}(\hat{\sigma}) = {\sigma^2 \over n}$​​.
>    $$
>    \text{mse}(\hat{\sigma}) = {\sigma^2 \over n} + 0 = {\sigma^2 \over n}
>    $$
>
> 2. For (a) and (b),
>    $$
>    \text{eff}(\hat{\lambda}_1, \hat{\lambda}_2) = {\text{var}(\hat{\lambda}_1) \over \text{var}(\hat{\lambda}_2)} = {{\lambda / n}\over \lambda} = {1\over n}
>    $$
>    Thus, $\hat{\lambda}_1$​​ is a much efficient estimator than $\hat{\lambda}_2$​​ when $n$ is large.
>
>    For (c) and (d),
>
>    The unbiased estimator based on (c) is
>    $$
>    \hat{\theta} = {n+1 \over n} \max_i X_i
>    $$
>    This estimator has variance
>    $$
>    \text{var}(\hat{\theta}) = \left({n+1 \over n}\right)^2 \text{var}(\hat{\theta}) = \left(n+1 \over n\right)^2 {n(n+1)^2 - n^2(n+2) \over (n+2)(n+1)^2 } \theta^2 = {1\over n(n+2)}\theta^2
>    $$
>    Thus, we have a relative efficiency of
>    $$
>    \text{eff}(\hat{\theta}_1,\hat{\theta}_2) = {\text{var}(\hat{\theta}_1) \over \text{var}(\hat{\theta}_2)} = {\theta^2/(n(n+1)) \over \theta^2/(3n)} = {3\over n+2}
>    $$
>    Thus, $\hat{\theta}_1$​​​​ is a much efficient estimator than $\hat{\theta}_2$​​​​ when $n$​ is large.
>
> 3. a. If $X_1, ..., X_n \stackrel{i.i.d.}{\sim} \mathcal{N}(\mu, \sigma^2)$​, then we know that $\overline{X}_n$​ is an unbiased​ estimator of $\mu$. We have
>    $$
>    \begin{aligned}
>    \text{MSE}(\overline{X_n}) &= \text{Var}(\overline{X}_n) + \text{Bias}(\overline{X_n})^2\\
>    &= {1\over n^2} \sum^n_{i=1}\text{Var}(X_i) + (\mu - \mu)^2\\
>    &= {\sigma^2 \over n}.
>    \end{aligned}
>    $$
>    Instead, consider the estimator $a\overline{X}_n, a \in (0, \infty)$.
>    $$
>    \begin{aligned}
>    \text{MSE}(a\overline{X}_n) &= \text{Var}(a \overline{X}_n) + \text{Bias}(a\overline{X})^2\\
>    &= a^2 {\sigma^2 \over n} +(a \mu - \mu)^2\\
>    &= a^2 ({\sigma^2 \over n} + \mu^2) - 2 a \mu^2 + \mu^2
>    \end{aligned}
>    $$
>    Since it is quadratic, it is minimized at
>    $$
>    \hat{a} = {2 \mu ^2 \over 2({\sigma^2 \over n} + \mu^2)} = {\mu^2 \over {\sigma^2 \over n} + \mu^2} < 1
>    $$
>    Plug it in the MSE equation
>    $$
>    \begin{aligned}
>    MSE(\hat{a} \overline{X}_n) & = \left( {\mu^2 \over {\sigma^2 \over n} + \mu^2}  \right)^2 ({\sigma^2 \over n} + \mu^2) - 2 \mu^2 \left( {\mu^2 \over {\sigma^2 \over n} + \mu^2}  \right) + \mu^2\\
>    &= {\mu^4 \over {\sigma^2 \over n} + \mu^2} - {2\mu^4 \over {\sigma^2 \over n} + \mu^2} + {\mu^2 \left({\sigma^2 \over n} + \mu^2\right) \over {\sigma^2 \over n} + \mu^2} \\
>    &= {\sigma^2 \over n}  \cdot {\mu^2 \over {\sigma^2 \over n} + \mu^2} \\
>    &= \hat{a}\ \ \text{MSE}(\overline{X}_n) \ <\text{MSE}(\overline{X}_n)
>    \end{aligned}
>    $$
>    The reason we call this a **shrinkage estimator** is that $\hat{a} < 1$​​​. So we are shrinking the sample mean a little bit toward $0$​​​​, and adding a little bit of bias. By adding this bias, we're actually decreasing the variance by a lot more than we add in the bias. This leads to a smaller MSE than we originally had with our unbiased estimator.
>
>    b. Now we will consider estimation of the variance for a $\mathcal{N}(0,\sigma^2)$​​ distribution. In this case, the unbiased estimator of the variance is given by
>    $$
>    \hat{\sigma}^2 = {1\over n}\sum^n_{i=1}X_i^2
>    $$
>    Since the distribution has a fixed mean parameter of $0$. We have
>    $$
>    \text{MSE}(\hat{\sigma}^2) = \text{Var}(\hat{\sigma}^2) = {1\over n^2} \sum^n_{i=1} \text{Var}(X_i^2) = {2\sigma^4 \over n}
>    $$
>    Repeating the shrinkage procedure as before, we find
>    $$
>    \begin{aligned}
>    \text{MSE}(\hat{\sigma}^2) &= a^2 {2\sigma^4 \over n} + ((1-a)\sigma^2)^2\\
>    &= \left( {2 \sigma^4 \over n} + \sigma^4\right) a^2 - 2 \sigma^4 a + \sigma^4
>    \end{aligned}
>    $$
>    This is minimized when
>    $$
>    \hat{a} = {\sigma^4 \over 2\sigma^4/n  +\sigma^4} = {1\over 2/n + 1} = {n \over 2+n}
>    $$
>    and it results in an MSE of
>    $$
>    \begin{aligned} 
>    \text{MSE}(\hat{\sigma}^2) &= \left( {2 \sigma^4 \over n} + \sigma^4\right) \left({n \over 2+n}\right)^2 - 2 \sigma^4 \left({n \over 2+n}\right) + \sigma^4\\
>    &= \sigma^4\left(\left({n \over 2 + n} \right) - {2n\over n+2} + 1 \right)\\
>    &= {2\sigma^4 \over 2 + n}
>    \end{aligned}
>    $$



Problem set for Lec8-9.

# 1. Kullback-Leibler divergence

For the following pairs of distribution $(\mathbf{P}, \mathbf{Q})$, compute the Kullback-Leibler divergence KL$(\mathbf{P}, \mathbf{Q})$

1. $\mathbf{P}= \mathcal{N}(a, \sigma ^2), \quad \mathbf{Q}= \mathcal{N}(b, \sigma ^2), \quad a, b \in \mathbb {R},\,  \sigma ^2 > 0.$

2. $\mathbf{P}= \textsf{Ber}(a), \quad \mathbf{Q}= \textsf{Ber}(b), \quad a, b \in (0, 1)$.
3. $\mathbf{P} = \textsf{Unif}([0, \theta _1]), \quad \mathbf{Q} = \textsf{Unif}([0, \theta _2]), \quad 0 < \theta _1 < \theta _2.$
4. $\mathbf{P} = \textsf{Exp}(\lambda ), \quad \mathbf{Q} = \textsf{Exp}(\mu ), \quad \lambda , \mu \in (0, \infty ).$

> **Solution**:
>
> 1. If we write $X \sim \mathbf{P}$, to estimating KL by expectation, we can compute
>    $$
>    \begin{aligned}
>    \text{KL}(\mathbf{P}, \mathbf{Q}) &= \mathbb E_{\mathbf{P}} \left[ \ln \left( \frac{\frac{1}{\sqrt{2 \pi }} \exp \left( - \frac{-(X - a)^2}{2\sigma ^2} \right)}{\frac{1}{\sqrt{2 \pi }} \exp \left( - \frac{(X - b)^2}{2\sigma ^2} \right)} \right) \right]\\
>    &= \mathbb E_{\mathbf{P}} \left[ -\frac{(X - a)^2}{2 \sigma ^2} + \frac{(X-b)^2}{2 \sigma ^2}\right]\\
>    &= \frac{1}{2 \sigma ^2} \mathbb E_{\mathbf{P}} \left[ b^2 -a^2 - 2bX + 2aX \right]\\
>    &= \frac{1}{2 \sigma ^2} \mathbb E_{\mathbf{P}} \left[ - 2bX + 2aX + (2ab - 2a^2) - (2ab - 2a^2) + b^2 - a^2 \right]\\
>    &= \frac{1}{2 \sigma ^2} \mathbb E_{\mathbf{P}} \left[ (- 2bX + 2aX + 2ab - 2a^2 )+ (- 2ab + b^2 + a^2 ) \right]\\
>    &= \frac{1}{2 \sigma ^2} \mathbb E_{\mathbf{P}} \left[ 2 (a - b)(X - a) + (a - b)^2 \right]\\
>    &= \frac{(a-b)^2}{2 \sigma ^2}
>    \end{aligned}
>    $$
>    Because $\,   \mathbb E_{\mathbf{P}}[(X - a)] = 0  \,$.
>
> 2. If we write $X \sim \mathbf{P}, Y \sim \mathbf{Q}$, since
>    $$
>    \text{KL}(\mathbf{P}, \mathbf{Q}) = \sum\limits _{x \in E} p(x) \ln \left( \frac{p(x)}{q(x)} \right)
>    $$
>    We have
>    $$
>    \begin{aligned}
>    \textsf{KL}(\mathbf{P},\, \mathbf{Q}) &= \mathbf{P}(X = 0) \ln \frac{\mathbf{P}(X = 0)}{\mathbf{P}(Y = 0)} + \mathbf{P}(X = 1) \frac{\mathbf{P}(X = 1)}{\mathbf{P}(Y=1)}\\
>    &= \displaystyle  a \ln \frac{a}{b} + (1-a) \ln \frac{1-a}{1-b}.\\
>    \end{aligned}
>    $$
>
> 3. We compute
>    $$
>    \begin{aligned}
>    \text{KL}(\mathbf{P},\mathbf{Q}) &= \mathbb E_{\mathbf{P}}\left[\ln \frac{\frac{1}{\theta _1}}{\frac{1}{\theta _2}}\right]\\
>    &= \ln \left( \frac{\theta _2}{\theta _1} \right).
>    \end{aligned}
>    $$
>    If we try to compute the KL divergence the other way round, we notice that $\mathbf{P}$ is not supported between for $\theta _1<X<\theta _2$, when $\mathbf{P} = 0$ in the denominator. Thus we compute the expectation by integrating explicitly:
>    $$
>    \begin{aligned}
>    \textsf{KL}(\mathbf{Q},\, \mathbf{P}) &= \mathbb E_{\mathbf{Q}}\left[\ln \frac{q}{p}\right]\quad \text {where } p,q ,\text {are the pdfs of }\mathbf{P},\mathbf{Q}\, \text {respectively}\\
>    &= \int _{0}^{\theta _1} \frac{1}{\theta _2}\ln \frac{1/\theta _2}{1/\theta _1} dx+ \int _{\theta _1}^{\theta _2} \frac{1}{\theta _2}\ln \frac{1/\theta _2}{0} dx\\
>    &= + \infty
>    \end{aligned}
>    $$
>    because the second term diverges to $+\infty$.
>
>    In general $\textsf{KL}(\mathbf{P},\mathbf{Q})\neq \textsf{KL}(\mathbf{Q},\mathbf{P})$.
>
> 4. If $X \sim \mathbf{P}$, then
>    $$
>    \begin{aligned}
>    \textsf{KL}(\mathbf{P},\, \mathbf{Q}) &= \mathbb E_ P \left[ \ln \frac{\lambda e^{-\lambda x}}{\mu e^{-\mu x}} \right]\\
>    &= \mathbb E_ P \left[ \ln \frac{\lambda }{\mu } + (\mu - \lambda ) X \right]\\
>    &=  \ln \frac{\lambda }{\mu } + (\mu - \lambda ) \frac{1}{\lambda }\\
>    &=  \ln \frac{\lambda }{\mu } + \frac{\mu }{\lambda } - 1
>    \end{aligned}
>    $$
>    Because $\mathbb{E}_p[X] = {1\over \lambda}$.

# 2. Compute Total Variation Distance

Compute the total variation distance $\textsf{TV}(\mathbf{P},\mathbf{Q})$ between
$$
\mathbf{P}= X \quad \text {and} \quad \mathbf{Q}= X+c, \quad \text {where } X \sim \textsf{Ber}(p), p\in (0,1), \text {and } c\in \mathbb {R}.
$$

1. For $c \in \{-1,0,1\}$.
2. For $c=0$.
3. For $c=1$ or $c=-1$.

> **Solution**:
>
> 1. $\textsf{TV}(\mathbf{P},\mathbf{Q})=1$
>
>    For $c \notin \{-1,0,1\}$, the support of $X$ and $X+c$ are disjoint, hence $\textsf{TV}(X,X+c)=1$
>
> 2. $\textsf{TV}(\mathbf{P},\mathbf{Q})=0$
>
>    For $c=0$, by the definiteness property $\textsf{TV}(X,X)=0$.
>
> 3. $\textsf{TV}(\mathbf{P},\mathbf{Q})={1\over 2}(1 + |1-2p|)$ 
>
>    For $c=1$ (resp. $c = -1$), the support of $X$ and $X+c$ intersect at $X=1$ (resp. at $X=0$). Hence
>    $$
>    \begin{aligned}
>    \textsf{TV}(X,X+c) &= {1\over 2} (|p(0) - q(0)| + |p(1) - q(1)| + |p(2) - q(2)|)\\
>    & = \frac{1}{2} (|(1-p)-0|+|p-(1-p)|+|0 - p|)\\
>    &=  \frac{1}{2} (1+|1-2p|)\qquad \text {where } c=1, \, \text {or }-1.
>    \end{aligned}
>    $$

Compute the total variation distance $\textsf{TV}(\mathbf{P},\mathbf{Q})$ between
$$
\mathbf{P}= \textsf{Ber}(p) \quad \text {and} \quad \mathbf{Q}= \textsf{Ber}(q), \quad \text {where } p, q \in [0,1].
$$
Let $X_1 ,...,X_n$ be $n$ i.i.d. Bernoulli random variables with some parameter $p \in [0,1]$, and $\bar{X}_n$ be their empirical average. Consider the total variation distance $\textsf{TV}(\textsf{Ber}(\bar X_ n), \textsf{Ber}(p))$ between $\textsf{Ber}(\bar X_ n)$ and $\textsf{Ber}(p)$ as a function of the random variable $\bar{X}_n$, and hence a random variable itself. Does $\textsf{TV}(\textsf{Ber}(\bar X_ n), \textsf{Ber}(p))$ necessarily converge in probability to a constant ?

>**Solution:**
>
>The TV is
>$$
>\textsf{TV}(\textsf{Ber}(p), \textsf{Ber}(q)) = \frac{1}{2} \left[ | p - q | + | (1-p) - (1-q) | \right] = |p-q|
>$$
>Intuitively, the two distributions: $\textsf{Ber}(\bar X_ n) $ and $\textsf{Ber}(p) $ should behave similarly since
>$$
>\bar X_ n \xrightarrow [n \to \infty ]{\  i.p.} p.
>$$
>Recall that by definition, the convergence in probability means
>$$
>P( | \bar X_ n - p | > \epsilon ) \xrightarrow [n \to \infty ]{} 0.
>$$
>By the Law of Large Numbers, we can say that the total variation distance will converge in probability to $0$ as goes to infinity.
>$$
>\textsf{TV}(\textsf{Ber}(\bar X_ n), \textsf{Ber}(p)) = | \bar X_ n - p | \xrightarrow [n \to \infty ]{\  p.} 0.
>$$

Compute the total variation distance $\textsf{TV}(\mathbf{P},\mathbf{Q})$ between
$$
P = \textsf{Unif}([0,s]) \quad \text {and} \quad Q = \textsf{Unif}([0,t]), \quad \text {where } 0 < s < t.
$$

> **Solution:**
>
> The densities of the two distributions are
> $$
> f_ s(x) = \frac{1}{s} \mathbf{1}\{ 0 \leq x \leq s\} , \quad f_ t(x) = \frac{1}{t} \mathbf{1}\{ 0 \leq x \leq t\} .
> $$
> We have
> $$
> \begin{aligned}
> \textsf{TV}(\textsf{Unif}([0,s]), \textsf{Unif}([0,t])) &= \frac{1}{2} \int _{\mathbb {R}} |f_ s(x) - f_ t(x)| \,  dx\\
> &= \frac{1}{2} \left[ \int _0^ s \left| \frac{1}{s} - \frac{1}{t} \right| \,  dx + \int _ s^ t \left| \frac{1}{t} \right| \,  dx \right]\\
> &= \frac{1}{2} \left[ \left( 1 - \frac{s}{t} \right) + \left( 1 - \frac{s}{t} \right) \right]\\
> &=  1 - \frac{s}{t}.
> \end{aligned}
> $$
> Hence, $\textsf{TV}(\textsf{Unif}([0,s]), \textsf{Unif}([0,t]))$ is a continuous function in $t$ that decreases to $0$ as $t$ approaches $s$.

Let $X \sim \mathcal{N}(\mu, \sigma^2)$ and $Y \sim \mathsf{Ber}(p)$. Compute the total variation distance between the distributions of sign $(X)$ and $Y-1$: $\textsf{TV}(\text {sign}(X),Y-1)$. Note that sign $(X)$ is a function of the random variable with
$$
\text{sign}(X) = \begin{cases}1 &\text{if } X>0\\0 &\text{if } X=0\\-1 &\text{if } X<0 \end{cases}
$$

> **Solution:**
>
> Observe that $\frac{X-\mu }{\sigma }\sim \mathcal{N}(0,1)$. Hence
> $$
> \text {sign}(X)\, =\, \begin{cases} -1&  \, \text {with probability}\, \, \Phi \left(-\frac{\mu }{\sigma }\right)\\ 1& \, \text {with probability}\, \, 1-\Phi \left(-\frac{\mu }{\sigma }\right)\, =\, \Phi \left(\frac{\mu }{\sigma }\right)\\ \end{cases}
> $$
> Hence, 
> $$
> \begin{aligned}
> 2\textsf{TV}(\text {sign}(X),Y-1) &= |f(1) - p(1)| + |f(0) - p(0)| + |f(-1) - p(-1)|\\
> & = \lvert \Phi \left(\frac{\mu }{\sigma }\right) - 0\rvert +\lvert 0-p \rvert +\lvert \Phi \left(-\frac{\mu }{\sigma }\right)-(1-p)\rvert\\
> &=\Phi \left(\frac{\mu }{\sigma }\right)+p+\lvert 1-p-\Phi \left(-\frac{\mu }{\sigma }\right)\rvert .
> \end{aligned}
> $$

Compute the total variation distance $\textsf{TV}(\mathbf{P},\mathbf{Q}) $ between
$$
\mathbf{P}= \textsf{Ber}(p) \quad \text {and} \quad \mathbf{Q}= \textsf{Poiss}(p), \quad \text {where } p\in (0,1).
$$

> **Solution:**
>
> The PMF $f_X(x)$ for $X \sim \mathsf{Poiss}(p)$ is
> $$
> f_ X(x)=e^{-p}\frac{p^ x}{x!}\qquad \text {for } x=0,1,2\ldots .
> $$
> Hence,
> $$
> \begin{aligned}
> 2\textsf{TV}(\textsf{Ber}(p), \textsf{Poiss}(p)) &= \lvert e^{-p}-(1-p)\rvert +\lvert pe^{-p} -p \rvert + e^{-p}\left(\frac{p^2}{2!}+ \frac{p^3}{3!}+\cdots \right)\\
> &=\left( e^{-p}-(1-p)\right)+\left( p\left(1-e^{-p}\right)\right)+e^{-p}\left(e^{p}-(1+p) \right)\quad \text {since }e^{-p}>(1-p) \, \text {for } p>0\\
> &=  2 \left(p\left(1-e^{-p}\right)\right)\\
> \implies \textsf{TV}(\textsf{Ber}(p), \textsf{Poiss}(p))&=p\left(1-e^{-p}\right).
> \end{aligned}
> $$

# 3. Concave Functions

A symmetric $2 \times 2$ matrix $\mathbf{A}$(i.e. $\mathbf{A}^T=\mathbf{A}$) is negative semi-definite, i.e. $\,  \mathbf{x}^ T \mathbf{A}\mathbf{x}\leq 0 \,$ for all $\,  \mathbf{x}\in \mathbb {R}^2 \,$, if and only if both of the following is true:

* $\text{tr}(\mathbf{A})\leq 0$
* $\text{det}(\mathbf{A}) \geq 0$

This fact can be explained in terms the eigenvalues of $\mathbf{A}$. Let $\lambda_1$ and $\lambda_2$ be the eigenvalues of $\mathbf{A}$, then $\text{tr}(\mathbf{A}) = \lambda_1 + \lambda_2$ while $\text{det}(\mathbf{A}) = \lambda_1 \lambda_2$. The two conditions above ensure that $\lambda_1, \lambda_2 \leq 0$.

Use the fact given above to determine whether the following functions concave, convex, or neither.s

1. $f_1(\theta _1, \theta _2) = -\theta _1^2 + \frac{1}{2}(\theta _1 - \theta _2)^2 - 3 \theta _2^2, \quad (\theta _1, \theta _2) \in \mathbb {R}^2$
2. $f_2(\theta _1, \theta _2) = -\theta _1^4-\theta _2^4 - (\theta _2 - \theta _1)^3, \quad (\theta _1, \theta _2) \in \mathbb {R}^2, \text { with } \theta _1 < \theta _2$

> **Solution**:
>
> 1. Concave.
>
>    If $f$ is a function from $\,   \Omega \subseteq \mathbb {R}^ d \to \mathbb {R} \,$, then it is concave if the Hessian of $f$ is negative semi-definite. In the special case of two dimensions, this can be checked by testing whether both $\text{tr}\nabla^2 f\leq 0$ and $\text{det}\nabla^2 f \geq 0$ are true.s
>    $$
>    \nabla f_1(\theta _1, \theta _2) =\begin{pmatrix}  -\theta _1 - \theta _2\\ -\theta _1 - 5 \theta _2 \end{pmatrix}\\
>     H f_1(\theta _1, \theta _2) =\begin{pmatrix}  -1 &  -1\\ -1 &  -5 \end{pmatrix}.
>    $$
>    Since $\operatorname {tr}\nabla ^2 f_1 = -6 < 0$ and $ \mathrm{det}\nabla ^2 f_1 = 4 > 0$, we have $\nabla ^2 f $ is negative semi-definite for all $\theta$, and in turn, $f_1$ is concave.
>
> 2. Concave.
>    $$
>    \begin{aligned}
>    \nabla f_2(\theta _1, \theta _2) &= \begin{pmatrix}  -4 \theta _1^3 + 3(\theta _2 - \theta _1)^2\\ -4 \theta _2^3 - 3(\theta _2 - \theta _1)^2 \end{pmatrix}\\
>     H f_2(\theta _1, \theta _2) &=\begin{pmatrix}  -12 \theta _1^2 - 6(\theta _2 - \theta _1) &  6(\theta _2 - \theta _1)\\ 6(\theta _2 - \theta _1) &  -12 \theta _2^2 - 6(\theta _2 - \theta _1) \end{pmatrix}.
>    \end{aligned}
>    $$
>    We again check
>    $$
>    \begin{aligned}
>    \operatorname {tr}\nabla ^2 f_2(\theta _1, \theta _2) &= -12 \theta _1^2 - 12 (\theta _2 - \theta _1) - 12 \theta _2^2 < 0, \quad \text {if } \theta _1 < \theta _2\\
>    \mathrm{det}\nabla ^2 f_2(\theta _1, \theta _2) &= (12 \theta _1^2 + 6(\theta _2 - \theta _1))(12 \theta _2^2 + 6(\theta _2 - \theta _1)) - 36 (\theta _2 - \theta _1)^2\\
>    &= 144 \theta _1^2 \theta _2^2 + 72 (\theta _1^2 + \theta _2^2)(\theta _2 - \theta _1) > 0, \quad \text {if } \theta _1 < \theta _2.
>    \end{aligned}
>    $$
>    Combined, $f_2$ is concave on $\{\theta_1 < \theta_2\}$.

# 4. Maximum Likelihood Estimators

Compute the likelihood function and the maximum likelihood estimator for $\theta$ for

1. $f_\theta (x) = \sqrt{\theta }x^{\sqrt{\theta }-1} \mathbf{1}(0 < x < 1), \quad \theta >0$
2. $f_\theta (x)= \theta \tau x^{\tau -1} \exp \{ -\theta x^\tau \}  \mathbf{1}(x\geq 0), \quad \theta >0$

> **Solution:**
>
> 1. The likelihood function is
>    $$
>    L= \theta ^{n/2} \prod _ i{X_ i^{\sqrt{\theta }-1}} \mathbf{1}\{ 0 \leq X_ i \leq 1\} .
>    $$
>    The log-likelihood function is
>    $$
>    l= \frac{n}{2} \ln \theta + (\sqrt{\theta } -1) \sum _{i}{\ln X_ i}.
>    $$
>    Take the derivative with respect to $\theta$ and set it to $0$:
>    $$
>    \frac{\partial l}{\partial \theta }= \frac{n}{2\theta } + \frac{1}{2\theta ^{1/2}} \sum _ i{\ln X_ i}=0.
>    $$
>    Then we get
>    $$
>    \hat{\theta }=\frac{n^2}{(\sum {\ln X_ i})^2}.
>    $$
>
> 2. The likelihood function is
>    $$
>    L= \theta ^ n \tau ^ n \prod _ i{X_ i^{\tau -1}} \exp \{ -\theta \sum _ i{X_ i^\tau }\}  \mathbf{1} \{ X_ i\geq 0\}
>    $$
>    The log-likelihood function is
>    $$
>    l= n \ln \theta + n \ln \tau + (\tau -1) \sum _ i{\ln X_ i} - \theta \sum _ i{X_ i^\tau }.
>    $$
>    Take the derivative with respect to $\theta$ and set it to 0
>    $$
>    \frac{\partial l}{\partial \theta }= \frac{n}{\theta } - \sum _ i{X_ i^\tau }=0,
>    $$
>    We get
>    $$
>    \hat{\theta }=\frac{n}{\sum _ i{X_ i^\tau }}.
>    $$

# 5. Constrained Maximum Likelihood Estimator

Let $X_1 ,... ,X_n$ be $n$ i.i.d. random variables with probability density function
$$
f_\theta (x)= \theta x^{-\theta -1}, \quad \theta >0, \quad x \geq 1.
$$

1. What is the likelihood function for $\theta$?

2. What is the maximum likelihood estimator for $\theta$?

3. Suppose we have two numbers $0< a < b$. We are interested in the value of $\theta$ that maximizes the likelihood in the set $[a,b]$.

   Let $\hat{\theta}$ denote the maximum likelihood estimator you found in part (b) above, and let $\hat{\theta}_{const}$ denote the maximum likelihood estimator within the interval $[a,b]$, where $0 < a < b$. Choose the correct answer:

   a. If $b \leq \hat{\theta}$, then $\hat{\theta}_{const}=a$.

   b. If $b \leq \hat{\theta}$, then $\hat{\theta}_{const}=b$.

   c. If $b \leq \hat{\theta}$, then $\hat{\theta}_{const}=\hat{\theta}$.

   d. If $a < \hat{\theta} < b$, then $\hat{\theta}_{const}=a$.

   e. If $a < \hat{\theta} < b$, then $\hat{\theta}_{const}=b $.

   f. If $a < \hat{\theta} < b$, then $\hat{\theta}_{const}=\hat{\theta}$.

   g. If $a \geq \hat{\theta}$, then $\hat{\theta}_{const}=a$.

   h. If $a \geq \hat{\theta}$, then $\hat{\theta}_{const}=$b.

   i. If $a \geq \hat{\theta}$, then $\hat{\theta}_{const}=\hat{\theta}$.

> **Solution:**
>
> 1. The likelihood is
>    $$
>     L_ n  = \prod _{i=1}^{n}{\theta x_ i^{-\theta -1}} \mathbf{1}\{ {X_ i} \geq 1\}   =\theta ^ n \prod _{i=1}^{n}{x_ i^{-\theta -1}} \mathbf{1}\{ \min _ i{X_ i} \geq 1\}  
>    $$
>    Since we assume our statistical model to be well-specified, $\min _ i{X_ i} \geq 1$ will always be satisfied, and so we can drop the corresponding indicator function. Hence, $L_ n\, =\, \theta ^ n \prod _{i=1}^{n}{x_ i^{-\theta -1}}$ is correct under the well-specified assumption.
>
> 2. Take the derivative of the likelihood with respect to $\theta$.
>    $$
>    \frac{\partial L_ n}{\partial \theta } = \frac{n}{\theta }-\sum _{i=1}^{n} \ln X_ i = 0
>    $$
>    Solving the equation for $\theta$, we get
>    $$
>    \hat{\theta }=\frac{n}{\sum _{i=1}^{n}\ln X_ i}
>    $$
>
> 3. b,f,g.
>
>    Take the second derivative of the likelihood function with respect to $\theta$.
>    $$
>    \frac{\partial ^2 L_ n}{\partial \theta ^2} = -\frac{n}{\theta ^2} <0
>    $$
>    Since the second derivative is strictly less than $0$, the function is strictly concave with respect to $\theta$. Therefore, depending on the value of $\frac{n}{\sum _{i=1}^{n}\ln X_ i}$, which is the maximum, the largest value that likelihood function can take in the set $[a,b]$ changes.



# Lecture 4. Parametric Estimation and Confidence Intervals

There are 6 topics and 5 exercises.

## 1. Statistics, Estimators, Consistency, and Asymptotic Normality

* **Statistic**: Any *measurable* function of the sample, e.g., $\bar{X}_n, \max_iX_i, X_1 + \log(1+|X_n|)$, sample variance ,etc.
  * *Measurable*: if you can compute it exactly once given data, it is measurable. You may have some issues with things that are implicitly defined (like "$\inf$" or "$\sup$").

    E.g. we do not have a closed form solution for    $\inf_{f \in \mathcal{F}}{1 \over n} \sum^n_{i=1}(Y_i - f(X_i))^2$.

* **Estimator of** $\theta$: Any statistic whose expression does not depend on $\theta$. (since we want to be able to compute it as an estimator once we get data).
  * The question we are interested in is how good an estimator is.
  * The goal is to have an estimator that will converge to the true unknown parameter $\theta$.

* An estimator $\hat{\theta}_n$ of $\theta$ is weakly (resp. strongly) **consistent** if

$$
\hat{\theta}_n \xrightarrow[n \rightarrow \infty]{\mathbf{P}(\text{resp. a.s.})} \theta \qquad (\text{w.r.s}. \mathbb{P}_\theta)
$$

* An estimator $\hat{\theta}_n$ of $\theta$ is **asymptotically normal** if 
  $$
  \sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow[n \rightarrow \infty]{ (d)} \mathcal{N}(0, \sigma^2)
  $$

  * The quantity $\sigma^2$ if then called **asymptotic variance **of $\hat{\theta}_n$.
  * The limit of asymptotic variance is 0: $\mathsf{var}(\hat{\theta}_n) \xrightarrow[n \rightarrow \infty]{} 0$. (think about $\bar{X}_n= \sum_i X_i/n$ )

> #### Exercise 22
>
> Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Ber(p)}$. Let $\overline{X}_ n$ be the estimator given by $\frac{1}{n} \sum _{i = 1}^ n X_ i$. What is the smallest constant $c$ such that
> $$
> n^ c \left( \overline{X}_ n - p \right) = n^ c \left( \frac{1}{n} \sum _{i = 1}^ n X_ i - p \right)
> $$
> does NOT converge to $0$ in probability as $n \rightarrow \infty$?
>
> **Answer**: 1/2
>
> **Solution**: 
>
> Let $\sigma = \sqrt{p(1-p)}$ denote the common standard deviation of $X_1, ..., X_n$. By the CLT, 
> $$
> \frac{\sqrt{n}}{\sigma }\left(\overline{X}_ n - p\right) = \frac{\sqrt{n}}{\sigma }\left(\frac{1}{n} \sum _{i = 1}^ n X_ i - p\right) \to N(0,1)
> $$
> where the convergence is in distribution. As a result, we see that for $c < 1/2$,
> $$
> n^ c \left( \overline{X}_ n - p \right) = \frac{\sigma }{n^{1/2 -c}} \frac{\sqrt{n}}{\sigma }\left(\overline{X}_ n - p\right) \approx \frac{\sigma }{n^{1/2 -c}} N(0,1) \to 0
> $$
> in probability as $n \rightarrow \infty$. Hence, $c = 1/2$ is the smallest possible value of $c$ such that
> $$
> n^ c \left( \overline{X}_ n - p \right) = n^ c \left( \frac{1}{n} \sum _{i = 1}^ n X_ i - p \right)
> $$
> does NOT converge to $0$ in probability as $n \rightarrow \infty$.
>
> **Remark**: As defined in the third video in this section, this implies that the estimator $\overline{X}_ n$ is $\sqrt{n}$-**consistent**. This means that the **estimator $\overline{X}_ n$ converges to the true parameter at a relatively fast rate**, so this gives us something stronger than just consistency.

## 2. Bias of Estimators; Jensen's Inequality

* **Bias** of an estimator $\hat{\theta}_n$ of $\theta$:
  $$
  \mathsf{bias}(\hat{\theta}_n) = \mathbb{E}[\hat{\theta}_n] - \theta
  $$
  If $\mathsf{bias}(\hat{\theta}_n)=0$, $\hat{\theta}$ is **unbiased**. 

> #### Exercise 23
>
> Assume that $X_1, ...,X_n \stackrel{iid}{\sim} \mathsf{Ber}(p)$ and consider the following estimators for $p$. Compute the biases.
>
> **Answer**: 
>
> 1) $\hat{p}_n = \bar{X}_n: \mathsf{Bias}(\hat{p}_n) = 0$.
>
> 2) $\hat{p}_n = X_1: \mathsf{Bias}(\hat{p}_n) = 0$.
>
> 3) $\hat{p}_n = {X_1+ X_2 \over 2}: \mathsf{Bias}(\hat{p}_n) = 0$.
>
> 4) $\hat{p}_n = \sqrt{\mathbf{1}(X_1 = 1, X_2 = 1)}: \mathsf{Bias}(\hat{p}_n) = p^2 - p$. 
>
> **Solution**: 
>
> The bias is computed by $\mathsf{Bias}(\hat{p}_n) = \mathbb{E}[\hat{p}_n] - p$.
>
> For (4), note that **the square root of expectation is not necessarily the expectation of square root**, or say, **the function of expectation is not necessary the expectation of the function** (See Jenson's Inequality). Even though the square root is taken, it is still a Bernoulli distribution with probability $p^2$, i.e., 
> $$
> \mathbb{E}[\hat{p}_n] = \mathbb{E}\left[\sqrt{\mathbf{1}(X_1 = 1, X_2 = 1)}\right] = p^2
> $$

> #### Exercise 24
>
> Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathcal{U}([a, a + 1])$ where $a$ is an unknown parameter. Let $\overline{X}_ n = \displaystyle \frac{1}{n} \sum _{i = 1}^ n X_ i$ denote the sample mean. In terms of $a$, what is $\mathbb{E}[\overline{X}_n]$? What is the bias of $\overline{X}_n$ w.r.t $a$?
>
> **Answer**: $\mathbb{E}[\overline{X}_n] = {2a+1 \over 2}$; $\mathsf{bias}(\overline{X}_n) = {1\over 2}$
>
> **Solution**:
>
> By linearity of expectation,
> $$
> \mathbb E[\overline{X}_ n] = \frac{1}{n} \sum _{i = 1}^ n \mathbb E[X_ i] = \mathbb E[X_1] = a + \frac{1}{2}.
> $$
> Or equivalently,
> $$
> \mathbb E[\overline{X}_ n] =\int^{a+1}_a 1 \cdot x \mathsf{d}x = {1\over 2}((a+1)^2 - a^2) = {2a+1 \over 2} = a + {1 \over 2}
> $$
> The bias is 
> $$
> \mathsf{bias}(\overline{X}_n) = \mathbb{E}[\overline{X}_n] - a = {1\over 2}
> $$
> Note that this implies that $\overline{X}_ n - \frac{1}{2}$ is an unbiased estimator.

* **Convex**: A function $g: \R \rightarrow \R$ is convex if for all pairs of real numbers $x_1 < x_2$
  $$
  g(tx_1+(1-t)x_2)\leq tg(x_1)+(1-t)g(x_2)\qquad \text {for all } \, 0\leq t\leq 1.
  $$
  Geometrically, the graph of convex $g$ is shown below 

  ![images_u3s2_convex](../assets/images/images_u3s2_convex.svg)

  Note that for $x_1 = 0, x_2 = 1$, the inequality above can be reinterpreted as follows. Let $X \sim \mathsf{Ber}(1-t)$ for some parameter $0 \leq t \leq 1$, then the left and right hand sides of inequality above can be rewritten respectively as:
  $$
  g\left(t(0)+(1-t)(1)\right) = g\left(1-t\right)\, =\, g\left(\mathbb E[X]\right)\\
  t g\left(0\right)+(1-t)g\left(1\right) = \mathbb E\left[g\left(X\right)\right]
  $$
  and hence the inequality defining convexity of $g$ implies
  $$
  g(\mathbb E[X])\leq \mathbb E[g(X)]\qquad (\text {for any Bernoulli random variable} X.)
  $$

* **Jensen's inequality**: for any random variable $X$, and any convex function $g$,
  $$
  g(\mathbb E[X])\leq \mathbb E[g(X)].
  $$
  Jensen's Inequality is also true for random vectors and convex functions on $\R^n$.

**Comments**: **Unbiasedness** is a desirable property and may be one of the first properties we want to check about the estimator even before consistency, but when we enforce that an estimator is unbiased, we may not have the best possible estimator because there are estimators that are unbiased but have a lot of **variability**.

## 3. Variance of Estimators

The variance of estimator $X$ is 
$$
\mathsf{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
$$

> #### Exercise 23 (continued)
>
> Assume that $X_1, ...,X_n \stackrel{iid}{\sim} \mathsf{Ber}(p)$ and consider the following estimators for $p$. Compute the variances.
>
> **Answer**: 
>
> 1) $\hat{p}_n = \bar{X}_n: \mathsf{Var}(\hat{p}_n) = {p(1-p) \over n}$.
>
> 2) $\hat{p}_n = X_1: \mathsf{Var}(\hat{p}_n) = {p(1-p)}$.
>
> 3) $\hat{p}_n = {X_1+ X_2 \over 2}: \mathsf{Var}(\hat{p}_n) = {p(1-p) \over 2}$.
>
> 4) $\hat{p}_n = \sqrt{\mathbf{1}(X_1 = 1, X_2 = 1)}: \mathsf{Var}(\hat{p}_n) = p^2 (1-p^2)$. 
>
> **Solution**: 
>
> The variance of Bernoulli random variable $X \sim \mathsf{Ber}(p)$ is $\mathsf{Var}[X] = p(1-p)$.
>
> The variance of the sum is the sum of variance ( **Variance is addictive**), so
> $$
> \text {Var}(\overline{X}_ n) = \frac{1}{n^2} \sum _{i = 1}^ n \text {Var}(X_ i) = \frac{1}{n} \text {Var}(X_1)
> $$
> For (1) as an example, the variance is 
> $$
> \mathsf{Var}\left[ {1\over n} \sum_i X_i\right] = {1\over n^2}\mathsf{Var}\left[X_1 + X_2 + ... + X_n \right] = {1\over n^2}\mathsf{Var}[X_1] + \mathsf{Var}[X_2] + ... +\mathsf{Var} [X_n] = {np(1-p) \over n^2} = {p(1-p) \over n}
> $$

> #### Exercise 24 (continued)
>
> Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } \mathcal{U}([a, a + 1])$ where $a$ is an unknown parameter. Let $\overline{X}_ n = \displaystyle \frac{1}{n} \sum _{i = 1}^ n X_ i$ denote the sample mean. In terms of $a$, what is $\mathsf{Var}[\overline{X}_n]$?
>
> **Answer**:  $\mathsf{Var}[\overline{X}_n] = {1 \over 12n}$
>
> **Solution**: 
> $$
> \text {Var}(X_1) = \mathbb E[X_1^2] - (\mathbb E[X_1])^2 = \int _{a}^{a +1} 1 \cdot x^2 \,  dx - \left( a + \frac{1}{2} \right)^2 = a^2 + a + 1/3 - a^2 - a - 1/4 = 1/12.\\
> \text {Var}(\overline{X}_ n) = \frac{1}{n} \text {Var}(X_1) = \frac{1}{12n}.
> $$

## 4. Quadratic Risk of Estimators

We want estimators to have low bias and low variance. Bias and variance are combined through **quadratic risk**. 

The **Risk** (or **quadratic risk**) of an estimator $\hat{\theta}_n\in \R$ is 
$$
R(\hat{\theta}_n) = \mathbb{E}\left[ (\hat{\theta}_n - \theta)^2\right]
$$
Low quadratic risk means that both bias and variance are small. $\rm{Quadratic~risk} =\rm{variance} + \rm{bias}^2$.

**Proof**:
$$
\begin{aligned}
R(\hat{\theta}_n) &= \mathbb{E}\left[ (\hat{\theta}_n - \theta)^2\right]\\
&= \mathbb{E}\left[ (\hat{\theta}_n  - \mathbb{E}[\hat{\theta}_n] + \mathbb{E}[\hat{\theta}_n] - \theta)^2\right]\\
&= \mathbb{E}\left[ (\hat{\theta}_n  - \mathbb{E}[\hat{\theta}_n])^2\right] + \mathbb{E}\left[(\mathbb{E} [\hat{\theta}_n] - \theta)^2\right] + 2 \mathbb{E}\left[(\hat{\theta}_n  - \mathbb{E}[\hat{\theta}_n])(\mathbb{E} [\hat{\theta}_n] - \theta)\right]\\
&= \mathbb{E}\left[ (\hat{\theta}_n  - \mathbb{E}[\hat{\theta}_n])^2\right] + \mathbb{E}\left[(\mathbb{E} [\hat{\theta}_n] - \theta)^2\right] \quad \text{Since }\mathbb{E}\left[(\hat{\theta}_n  - \mathbb{E}[\hat{\theta}_n])\right] = 0\\
&= \mathbb{E}\left[ (\hat{\theta}_n  - \mathbb{E}[\hat{\theta}_n])^2\right] + (\mathbb{E} [\hat{\theta}_n] - \theta)^2
\end{aligned}
$$
The first quantity is $\rm{variance}$, the second quantity is $\rm{Bias}^2$.

> #### Exercise 24 (continued)
>
> What is the quadratic risk of the estimator $\overline{X}_ n - \frac{1}{2}$?
>
> **Answer**: $1 \over 12n$
>
> **Solution**: 
>
> Since estimator $\overline{X}_ n - \frac{1}{2}$ is unbiased, and $\text {quadratic risk} = \text {variance} + \text {bias}^2.$
> $$
> \text {Var}(\overline{X}_ n) = \text {Var}(\overline{X}_ n - \frac{1}{2}) = \frac{1}{12 n}
> $$
> which is the quadratic risk.
>
> **Remark**: When the sample size is larger, the quadratic risk is lower.

## 5. Exercise: Strengths and Weaknesses of Estimators

You observe samples $X_1, \ldots , X_ n \stackrel{iid}{\sim } \text {Ber} (\theta )$ where $\theta \in (0,1)$ is an unknown parameter. Suppose that $n$ is much larger than $1$ so we have access to many samples from the specified distribution. Consider three candidate estimators for $\theta$.

* $\overline{X}_ n = \frac{1}{n} \sum _{i = 1}^ n X_ i$
* $0.5$
* $X_1$

Let's consider potential strengths and weaknesses of these estimators.

In this particular section, “**efficiently computable**" refers to the existence of an explicit formula. More precisely, here we say that an estimator $\hat{\theta}_n$ is efficiently computable if there's a formula that takes as input $X_1,\ldots ,X_ n$ whose output is $\hat{\theta}_n$ (Not all estimators are efficiently computable).

1. Which of the following is a potential <u>disadvantage</u> of using $\hat{θ}_n=.5$ as an estimator for $θ$? (Choose all that apply.)

   a. Unless $\theta = 0.5$, this estimator is biased.

   b. Unless $\theta = 0.5$, this estimator is not consistent.

   c. This estimator does not use any of the samples.

   d. This estimator is efficiently computable.

2. Which of the following is a potential <u>disadvantage</u> of using $\hat{θ}_n=X_1$ as an estimator for $θ$? (Choose all that apply.)

   a. This estimator is unbiased.

   b. This estimator is not consistent.

   c. This estimator uses only information given by only one sample, even though we have access to many samples.

   d. The quadratic risk of this estimator does not tend to $0$ as the sample size $n \rightarrow \infty$.

3. Which of the following are potential <u>advantages</u> of using $\hat{\theta}_n = \overline{X}_n$ as an estimator for $θ$? (Choose all that apply.)

   a. This estimator is unbiased.

   b. This estimator is consistent.

   c. This estimator is efficiently computable.

   d. The quadratic risk of this estimator tends to $0$ as the sample size $n \rightarrow \infty$.

> **Answer**:
>
> 1) abc
>
> - **Biased estimators inherently introduce errors in parameter estimation**. Hence, having non-zero bias is a potential disadvantage of an estimator.
> - If the estimator does not converge to the true parameter as the sample size $n$ grows very large, this is also a potential disadvantage. **Estimators that are not consistent have inherently limited accuracy in approximating the true parameter, regardless of how many samples are taken**. 
> - We haven't used any information given to us so we can't expect to learn anything new about the true parameter with this estimator.
> - The given estimator is indeed efficiently computable, and this is in general an *advantage* of certain estimators. In contrast, there are estimators that often **don't have an explicit formula (for example, estimators for Generalized Linear Models), and often requires approximate computation via a computer program.**
>
> 2) bcd
>
> * Unbiasedness is in general an advantage of an estimator, and the estimator $X_1$ is unbiased because $\mathbb{E}[X_1] = \theta$.
> * Since $X_1$ is Bernoulli, it is either $0$ or $1$, it is not equal to the true parameter $\theta$ which is assumed to lie in $(0,1)$. Hence, $\hat{\theta}_n = X_1$ is not consistent. Inconsistency is in general a disadvantage.
> * **It is generally best to use all information that is given about a distribution for parameter estimation**. One sample (which we know is either $0$ or $1$) does not tell much about the underlying distribution. Hence, it is a potential disadvantage that the samples $X_2, \ldots , X_ n$ were not used to construct the estimator $\hat{\theta }_ n = X_1$.
> * Note that $\text {Var}[X_1] = \theta (1 - \theta )$, and $X$ is unbiased, so the quadratic risk is equal to the variance $\theta(1-\theta)$. This estimator does not tend to $\theta$ as $n \rightarrow \infty$. This is a potential disadvantage, because **if the quadratic risk does not go to $0$, then $\hat{\theta}_n$ does not converge to $\theta$ in $L^2$. Intuitively, an estimator that does not converge in $L^2$ inherently has limited accuracy no matter how many samples are collected.**
>
> 3) abcd
>
> * Unbiased estimators avoid inherent inaccuracies of approximation that result from biasedness.
>
> * Consistent estimators become better and better approximations to the true parameter as the sample size increases.
>
> * **Averages can be computed in linear time from the dat**a, so is a computationally efficient estimator.
>
> * The estimator is unbiased, so its variance is the same as its quadratic risk. By independence and the i.i.d. assumption,
>   $$
>   \text {Var}(\overline{X}_ n) = \frac{1}{n^2} \sum _{i = 1}^ n \text {Var}(X_1) = \frac{\theta (1 - \theta )}{n}
>   $$
>   which tends to $0$ as $n \rightarrow \infty$. Thus, $\hat{\theta }_ n \stackrel{L^2}{\sim } \theta$, which ensures consistency. 
>
> **Remark:** 
>
> Convergence in $L^2$ is, mathematically speaking, a stronger guarantee than convergence in probability. That is, if $\hat{\theta }_ n \stackrel{L^2}{\to } \theta$, then also, $\hat{\theta }_ n \stackrel{\text {prob}}{\to } \theta$. 

## 6. Confidence Intervals (CI)

C.I. is to tell precisely where the true $\theta$ will be.

Let $(E, (\mathbb{P}_\theta)_{\theta \in \Theta})$ be a statistical model based on observations $X_1, ..., X_n$, and assume $\Theta \subseteq \R$. Let $\alpha \in (0,1)$.

* C.I. of level $1-\alpha$ for $\theta$: 

  Any random (depending on $X_1, ..., X_n$) interval $\mathcal{I}$ whose boundaries do not depend on $\theta$ and such that:
  $$
  \mathbb{P}_{\theta}[\mathcal{I} \ni \theta] \geq 1 - \alpha, \quad \forall \theta \in \Theta
  $$

* C.I. of **asymptotic** level $1- \alpha$ for $\theta$: 

  Any random interval $\mathcal{I}$ whose boundaries do not depend on $\theta$ and such that:
  $$
  \lim_{n \rightarrow \infty} \mathbb{P}_\theta[\mathcal{I} \ni \theta] \geq 1- \alpha, \quad \forall \theta \in \Theta
  $$

Note that $\mathcal{I} \ni \theta$ means $\mathcal{I}$ contains $\theta$. This notation emphasizes the randomness of $\mathcal{I}$ but we can equivalently write $\theta \in \mathcal{I}$.

**Example**:

* We observe $R_1, ..., R_n \stackrel{iid}{\sim} \mathsf{Ber}(p)$ for some unknown $p \in (0,1)$.

* Statistical model: $\left(\{0,1\}, (\mathsf{Ber}(p))_{p \in (0,1)}\right)$.

* The estimator for $p$ is $\hat{p} = \bar{R}_n$

* From CLT (CLT gives asymptotic normality):
  $$
  \sqrt{n} {\bar{R}_n - p \over \sqrt{p(1-p)}} \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
  $$
  This means that:

  * $\Phi(x): $ cdf of $\mathcal{N}(0,1); \Phi_n(x): $ cdf of $\sqrt{n} {\bar{R}_n - p \over \sqrt{p(1-p)}}$.

  * Then: $\Phi_n(x) \approx \Phi(x)$ (CLT) when $n$ becomes large. Hence, for all $x > 0$,
    $$
    \begin{aligned}
    &\mathbb{P}\left(\sqrt{n} {\bar{R}_n - p \over \sqrt{p(1-p)}} \leq x\right) \approx \Phi(x)\\
    &\implies \mathbb{P} \left(|\bar{R}_n - p| > {x\sqrt{p(1-p)} \over \sqrt{n}}\right) = 2 \Phi(-x) = 2 \Phi(-{x\sqrt{n} \over \sqrt{p(1-p)}})\\
    &\implies \mathbb{P}\left(|\bar{R}_n - p| \geq x \right) \simeq 2 \left(1 - \Phi\left({x\sqrt{n} \over \sqrt{p(1-p)}}\right)\right)
    \end{aligned}
    $$
    Note that convergence in distribution can be characterized by convergence of the cdf's at each point.

**C.I.**

* Find $x$ such that 
  $$
  \begin{aligned}
  &2\left( 1 - \Phi\left({x\sqrt{n} \over \sqrt{p(1-p)}}\right)\right) = {\alpha}\\
  &\implies \Phi\left({x\sqrt{n} \over \sqrt{p(1-p)}}\right) = 1 - {\alpha \over 2}\\
  &\implies{x\sqrt{n} \over \sqrt{p(1-p)}} = \Phi^{-1}\left(1 - {\alpha \over 2}\right) = q_{\alpha/2}\\
  &\implies x = {q_{\alpha/2 }\sqrt{p(1-p)} \over \sqrt{n}}
  \end{aligned}
  $$
  Hence, 
  $$
  \begin{aligned}
  &\mathbb{P}[|\bar{R}_n - p| \leq x] \simeq 1 - \alpha\\
  &\mathbb{P}[\bar{R}_n - x \leq p \leq \bar{R}_n + x] \simeq 1 - \alpha\\
  \end{aligned}
  $$

* For a fixed $\alpha \in (0,1)$, if $q_{\alpha/2}$ is the $(1-\alpha/2)$-quantile of $\mathcal{N}(0,1)$, then with probability $\simeq 1- \alpha$ (if $n$ is large enough).
  $$
  \bar{R}_n \in \left[ p - {q_{\alpha/2 }\sqrt{p(1-p)} \over \sqrt{n}}, p + {q_{\alpha/2 }\sqrt{p(1-p)} \over \sqrt{n}}\right].
  $$

* It yields
  $$
  \lim_{n\rightarrow \infty} \mathbb{P} 
   \left(\left[ \bar{R}_n - {q_{\alpha/2 }\sqrt{p(1-p)} \over \sqrt{n}}, \bar{R}_n + {q_{\alpha/2 }\sqrt{p(1-p)} \over \sqrt{n}}\right] \ni p \right) = 1 - \alpha.
  $$
  where $\bar{R}_n$ is random, $p, n, q_{\alpha/2}$ are deterministic.

* But this is not a C.I. because it depends on $p$. To fix this, there are $3$ solutions.

### (1) Conservative Bound

Recall that by the CLT, for any $p,(0 < p < 1)$:
$$
\lim _{n\to \infty } \mathbf{P}\left(\left|\sqrt{n}\frac{\overline{R}_ n-p}{{{\sigma _ p}} }\right|< q_{\alpha /2}\right) =\lim _{n\to \infty } \mathbf{P}\left( \overline{R}_ n-q_{\alpha /2}\frac{{{\sigma _ p}} }{\sqrt{n}}\, <\, p\, <\, \overline{R}_ n+q_{\alpha /2}\frac{{{\sigma _ p}} }{\sqrt{n}}\right) {{=}} \, 1-\alpha
$$
where $\sigma _ p=\sqrt{p(1-p)}$.

To construct a confidence interval, we need to replace $σ_p$ above by a number $c$ that does not depend on the unknown parameter $p$, so that it guarantees that all $p$ in $(0,1)$,
$$
\lim _{n\to \infty } \mathbf{P}\left(\left|\sqrt{n}\frac{\overline{R}_ n-p}{{\color{blue}{c}} }\right|< q_{\alpha /2}\right) {\color{blue}{\geq }} \, 1-\alpha ?
$$
The condition $c$ should satisfy is that 
$$
\left(\overline{R}_ n-q_{\alpha /2}\frac{{{c}} }{\sqrt{n}},\, \overline{R}_ n+q_{\alpha /2}\frac{{{c}} }{\sqrt{n}}\right) {{\supseteq }}  \left(\overline{R}_ n-q_{\alpha /2}\frac{{{\sigma _ p}} }{\sqrt{n}},\, \overline{R}_ n+q_{\alpha /2}\frac{{{\sigma _ p}} }{\sqrt{n}}\right) \qquad \text {for all } p
$$
Hence any $c \geq \max_p(\sigma_p^2)$ works. For Bernoulli example above, 
$$
\max _{p}\left(\sigma _ p\right)= \max _{p}\left(\sqrt{p(1-p)}\right) \, =\,  1/2
$$
That is, no matter the (unknown) value of $p$,  $p(1-p) \leq {1\over 4}$. Thus, roughly with probability at least $1-\alpha$
$$
\bar{R}_n \in \left[ p - {q_{\alpha/2}\over 2\sqrt{n}}, p + {q_{\alpha/2}\over 2\sqrt{n}}\right]
$$
We get the asymptotic confidence interval:
$$
\mathcal{I}_{conserv} = \left[ \bar{R}_n - {q_{\alpha/2}\over 2\sqrt{n}}, \bar{R}_n + {q_{\alpha/2}\over 2\sqrt{n}}\right]
$$
Indeed, 
$$
\lim_{n\rightarrow \infty} \mathbb{P}(\mathcal{I}_{conserv} \ni p) \geq 1 - \alpha
$$
This is the widest possible C.I., since when $p = 1/2$ we have the most uncertainty.

### (2) Solving the (quadratic) equation for $p$

We have the system of two inequalities in $p$:
$$
\overline{R}_ n-q_{\alpha /2}\frac{{{\sqrt{p(1-p)}}} }{\sqrt{n}}\, \leq\, p\, \leq\, \overline{R}_ n+q_{\alpha /2}\frac{{{\sqrt{p(1-p)}}} }{\sqrt{n}}
$$
Equivalently,
$$
\left|\sqrt{n}\frac{\overline{R}_ n-p}{\sqrt{p(1-p)}}\right|< q_{\alpha /2}
$$
Each is a quadratic inequality in $p$ of the form
$$
\begin{aligned}
\left|\sqrt{n}\frac{\overline{R}_ n-p}{\sqrt{p(1-p)}}\right|< q_{\alpha /2} &\implies \left(\sqrt{n} \frac{\overline{R}_ n-p}{\sqrt{p(1-p)}}\right)^2< q_{\alpha /2}^2\\
& \implies  \left(\overline{R}_ n-p\right)^2< \frac{p(1-p) q_{\alpha /2}^2}{n}\\
& \implies p^2\left(1+ \frac{q_{\alpha /2}^2}{n}\right)-p\left(2\overline{R}_ n+\frac{q_{\alpha /2}^2}{n}\right)+\left(\overline{R}_ n\right)^2<0
\end{aligned}
$$
We need to find the roots $p_1 < p_2$ of 
$$
p^2\left(1+ \frac{q_{\alpha /2}^2}{n}\right)-p\left(2\overline{R}_ n+\frac{q_{\alpha /2}^2}{n}\right)+\left(\overline{R}_ n\right)^2 = 0
$$
Solving by $p = \frac{-B\pm \sqrt{B^2-4AC}}{2A}$, this leads to a new confidence interval $\mathcal{I}_{solve} = [p_1, p_2]$ such that
$$
\lim_{n \rightarrow \infty} \mathbb{P}(\mathcal{I}_{solve} \ni p) = 1- \alpha
$$
The quadratic function $Ap^2+Bp+C<0$ where $A>0$ is convex, so the parabola opens up, and the region in which the parabola is below the x-axis is the interval between the two roots. Given $0<p_1<p_2<1$, the region is $p_1<p<p_2$.

### (3) Using Slutsky Theorem: Plug-in

Recall by the **LLN** $\hat{p} = \bar{R}_n \xrightarrow[n \rightarrow \infty]{\mathbb{P}, a.s.} p $.

By **Slutsky**, ( if I have a convergence of distribution, multiply by a convergence in probability, or in convergence of distribution to a deterministic number, I can actually multiply the limits. )
$$
\begin{aligned}
&\sqrt{n} {\bar{R}_n - p \over \sqrt{\hat{p}(1-\hat{p})}} = \sqrt{n} {\bar{R}_n - p \over \sqrt{{p}(1-{p})}} {\sqrt{{p}(1-{p})}\over \sqrt{\hat{p}(1-\hat{p})}}\\
&{\sqrt{{p}(1-{p})}\over \sqrt{\hat{p}(1-\hat{p})}} \xrightarrow[n \rightarrow \infty]{} 1 \quad \text{by LLN and continuous mapping theorem (CMT)}\\ 
&\implies 
\sqrt{n} {\bar{R}_n - p \over \sqrt{\hat{p}(1-\hat{p})}} \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,1)
\end{aligned}
$$
This leads to a new confidence interval
$$
\mathcal{I}_{\text{plug-in}} = \left[ \bar{R}_n - {q_{\alpha/2 }\sqrt{\hat{p}(1-\hat{p})} \over \sqrt{n}}, \bar{R}_n + {q_{\alpha/2 }\sqrt{\hat{p}(1-\hat{p})} \over \sqrt{n}}\right]
$$
such that
$$
\lim_{n \rightarrow \infty} \mathbb{P}(\mathcal{I}_{\text{plug-in}} \ni p) = 1-\alpha
$$
This approach is slightly less precise than the second solution.

> #### Exercise 25
>
> Show the convergence of different quantities:
>
> a) $\overline{R}_n$
>
> b) $\sqrt{n} \left(\overline{R}_n - p\right)$
>
> c) $\sqrt{n} {\overline{R}-p \over \sqrt{p(1-p)}}$
>
> d) $ \sqrt{\overline{R}_ n\left(1-\overline{R}_ n\right)}$
>
> e) $\frac{\sqrt{\overline{R}_ n\left(1-\overline{R}_ n\right)}}{\sqrt{p(1-p)}}$
>
> f) $\left(\sqrt{n}\frac{\overline{R}_ n-p}{\sqrt{p(1-p)}}\right) \left(\displaystyle \frac{\sqrt{p(1-p)}}{\sqrt{\overline{R}_ n\left(1-\overline{R}_ n\right)}}\right)$
>
> **Answer**: 
>
> a) $\overline{R}_n \xrightarrow [n\longrightarrow \infty ]{{{(\mathbf{P})}} } {{p}}$, and it approximated by (in distribution) $\mathcal{N}({{p}} ,{{\frac{p(1-p)}{n}}} )$
>
> b) $\sqrt{n}\left(\overline{R}_ n-p\right) \xrightarrow [n\longrightarrow \infty ]{{{(d)}} } \mathcal{N}({{0}} ,{{p(1-p)}} )$
>
> c) $\sqrt{n}\frac{\overline{R}_ n-p}{\sqrt{p(1-p)}} \xrightarrow [n\longrightarrow \infty ]{{{(d)}} } \mathcal{N}({{0}} ,{{1}} )$
>
> d) $ \sqrt{\overline{R}_ n\left(1-\overline{R}_ n\right)} \xrightarrow [n\longrightarrow \infty ]{{{(\mathbf{P})}} } {{\sqrt{p(1-p)}}}$
>
> e) $\frac{\sqrt{\overline{R}_ n\left(1-\overline{R}_ n\right)}}{\sqrt{p(1-p)}}\xrightarrow [n\longrightarrow \infty ]{{{(\mathbf{P})}} } {{1}}$
>
> f) $\left(\sqrt{n}\frac{\overline{R}_ n-p}{\sqrt{p(1-p)}}\right) \left(\displaystyle \frac{\sqrt{p(1-p)}}{\sqrt{\overline{R}_ n\left(1-\overline{R}_ n\right)}}\right) \xrightarrow [n\longrightarrow \infty ]{{{(d)}} } \mathcal{N}({{0}} ,{{1}} )$
>
> **Solution**: 
>
> a) $\overline{R}_ n\xrightarrow [n\longrightarrow \infty ]{{{(\mathbf{P})}} } \mathbb E\left[\overline{R}_ n\right] = {{p}}$ by the (weak) LLN.
>
> $\overline{R}_ n\xrightarrow [n\longrightarrow \infty ]{{{(d)}} } \mathcal{N}\left(\mathbb E\left[\overline{R}_ n\right], \textsf{Var}\left(\overline{R}_ n\right)\right)=\mathcal{N}\left(p,\frac{p(1-p)}{n}\right)$ by the CLT.
>
> b) $\sqrt{n}\left(\overline{R}_ n-p\right)\xrightarrow [n\longrightarrow \infty ]{{{(d)}} } \mathcal{N}\left(\mathbb E\left[\sqrt{n}\left(\overline{R}_ n-p\right)\right], n\textsf{Var}\left(\overline{R}_ n\right)\right)=\mathcal{N}\left(0,p(1-p)\right)$ by the CLT. Note that with an asymptotic variance that does not depend on $n, \sqrt{n}\left( \overline{R}_n - p\right)$ does not converge in probability to a constant.
>
> c) $ \sqrt{n}\frac{\overline{R}_ n-p}{\sqrt{p(1-p)}}\xrightarrow [n\longrightarrow \infty ]{{{(d)}} } \mathcal{N}\left(0,1)\right)$ by the CLT.
>
> d) $ \sqrt{\overline{R}_ n\left(1-\overline{R}_ n\right)} \xrightarrow [n\longrightarrow \infty ]{{{(\mathbf{P})}} } {{\sqrt{p(1-p)}}}$ by the continuous mapping theorem.
>
> e) $\frac{\sqrt{\overline{R}_ n\left(1-\overline{R}_ n\right)}}{\sqrt{p(1-p)}}\xrightarrow [n\longrightarrow \infty ]{{{(\mathbf{P})}} } {{1}}$ since constant multiple of sequences that converge in probability still converge in probability.
>
> f) $\left(\sqrt{n}\frac{\overline{R}_ n-p}{\sqrt{p(1-p)}}\right) \left(\displaystyle \frac{\sqrt{p(1-p)}}{\sqrt{\overline{R}_ n\left(1-\overline{R}_ n\right)}}\right) \xrightarrow [n\longrightarrow \infty ]{{{(d)}} } \mathcal{N}({{0}} ,{{1}} )$ by Slutsky.

> #### Exercise 26: Confidence Intervals of the mean of Gaussian random variables
>
> Let $X_1, ..., X_n$ be i.i.d. Gaussian random variables, with unknown mean $\mu$ and known variance 1.
>
> 1. Find an estimator $\hat{\mu}$ of $\mu$, based on $X_1, ..., X_n$.
> 2. Determine the distribution of $\hat{\mu}$.
> 3. Compute a confidence interval for $\mu$ with level $95\%$ that is centered about $\hat{\mu}$.
> 4. Propose two other confidence intervals for $\mu$ with level $95\%$, which are unbounded either to the left or to the right.
>
> **Answer**: 
>
> 1) $\hat{\mu} = \overline{X}_n = {1 \over n }\sum^n_{i=1}X_i$ (LLN: $\overline{X_n} \xrightarrow[n \rightarrow \infty]{\mathbb{P}} \mathbb{E}[X_1] = \mu$)
>
> 2) Since the sum of Gaussian variables is also Gaussian, $\overline{X}_n$ is Gaussian distributed.
> $$
> \begin{aligned}
> \mathbb{E}[\overline{X}_n] &= \mathbb{E}\left[{1\over n}\sum^n_{i=1}X_i\right] = {1\over n}\sum^n_{i=1}\mathbb{E}[X_i] = {n \over n} \mu = \mu\\
> \mathsf{Var}(\overline{X}_n) &= \mathsf{Var}({1\over n} \sum^n_{i=1}X_i) ={1\over n^2}\sum^n_{i=1}\mathsf{Var}(X_i) = {n \over n^2} = {1 \over n}
> \end{aligned}
> $$
> Thus, the distribution is 
> $$
> \begin{aligned}
> &\overline{X}_n \sim \mathcal{N}(\mu, {1\over n})\\
> \implies& \sqrt{n}(\hat{\mu} - \mu) \sim \mathcal{N}(0,1)
> \end{aligned}
> $$
> where $\sqrt{n}(\hat{\mu} - \mu)$ is called **pivot**.
>
> 3) The **two sided** C.I. $\mathcal{I}$ can be written as 
> $$
> \mathcal{I} = \hat{\mu} + [-s,s] = [\hat{\mu} - s, \hat{\mu} + s]
> $$
> Given level $1-\alpha =0.95$, we have
> $$
> \begin{aligned}
> & 1- \alpha = \mathbb{P}(\mu \in \mathcal{I})\\
> & \Leftrightarrow \mu \in [\hat{\mu} -s, \hat{\mu} + s]\\
> & \Leftrightarrow \hat{\mu} - s \leq \mu \leq \hat{\mu} +s\\
> & \Leftrightarrow -s \leq \mu - \hat{\mu} \leq s\\
> & \Leftrightarrow -s \leq  \hat{\mu}-\mu \leq s\\
> & \Leftrightarrow -s\sqrt{n} \leq  \sqrt{n}(\hat{\mu}-\mu) \leq s\sqrt{n}\\
> \end{aligned}
> $$
> Let $q = s\sqrt{n}, Z = \sqrt{n}(\hat{\mu}-\mu)$, then this can be rewritten as
> $$
> \begin{aligned}
> 1- \alpha &= \mathbb{P}(Z \in [-q,q])\\
> & = \mathbb{P}(Z \leq q) - \mathbb{P}(Z \leq -q)\\
> & = \mathbb{P}(Z \leq q) - (1-\mathbb{P}(Z \leq q))\\
> & = 2 \cdot \mathbb{P}(Z \leq q ) - 1\\
> & = 2 \cdot \Phi(q) - 1\\
> \implies & \Phi(q) = 1 - {\alpha \over 2}\\
> \implies & q = q_{\alpha/2}
> \end{aligned}
> $$
> Therefore, the C.I. is 
> $$
> \mathcal{I} = \hat{\mu} + \left[ - {q_{\alpha/2} \over \sqrt{n}}， {q_{\alpha/2} \over \sqrt{n}} \right] ,\quad  \alpha/2=0.025, q_{\alpha/2} \approx 1.96 
> $$
> The one sided C.I. $\mathcal{J}$ can be written as
> $$
> \mathcal{J} = (-\infty, \hat{\mu} + s]
> $$
> Given level $1 - \alpha = 0.95$, we have
> $$
> \begin{aligned}
> & 1- \alpha = \mathbb{P}(\mu \in \mathcal{J})\\
> & \Leftrightarrow \mu \in (-\infty, \hat{\mu} + s ]\\
> & \Leftrightarrow  \mu \leq \hat{\mu} +s\\
> & \Leftrightarrow -s \leq \mu - \hat{\mu}\\
> & \Leftrightarrow -s \sqrt{n} \leq  (\hat{\mu}-\mu)\sqrt{n} 
> \end{aligned}
> $$
> Let $q = s\sqrt{n}, Z = \sqrt{n}(\hat{\mu}-\mu)$, then this can be rewritten as
> $$
> \begin{aligned}
> 1- \alpha &= \mathbb{P}(Z \geq -q)\\
> & = \mathbb{P}(Z \leq q)\\
> & = \Phi(q)\\
> \implies & q = q_{\alpha}
> \end{aligned}
> $$
> Therefore, the C.I. is 
> $$
> \mathcal{J} = ( -\infty, \hat{\mu} + {q_{\alpha} \over \sqrt{n}}],\quad  \alpha=0.05, q_{\alpha} \approx 1.65
> $$



# Probability and Linear Algebra Review

There are 8 topics.

## 1. Discrete Random Variables

### Normalization constant for the Poisson distribution

The probability mass function (pmf) of a **Poisson distribution** with parameter $λ$ is given by
$$
\text {Poi}(\lambda ) = \frac{e^{-\lambda}\lambda ^ k}{k!}, \quad k=0,1,2,\ldots .
$$
Note that a probability distribution must satisfy
$$
e^{-\lambda} \sum^{\infty}_{k=0} \frac{\lambda ^ k}{k!} = 1
$$
where 
$$
\sum _{k = 0}^{\infty } \frac{\lambda ^ k}{k!} = \exp (\lambda )
$$

### Moments of Bernoulli variables

The $n$th **moment** of a random variable $X$ is defined to be the expectation $E[X^n]$ of the $n$th power of $X$.

> Let $X$ be a Bernoulli random variable with parameter 0.7. Compute the **expectation values** of $X^k$, denoted by $E[X^k]$, for the following three values of $k$: $k=1,4, $ and $3203$.
>
> **Answer**: 
>
> $E[X] = 0.7\\ E[X^4] = 0.7 \\ E[X^{3203}] = 0.7$
>
> **Solution**: 
>
> The expectation of discrete random variable is 
> $$
> \mathbb E[X] = \sum _{j \in \operatorname {range}(X)} j \,  \mathbf{P}(X = j)
> $$
> The higher moments are
> $$
> \mathbb E[X^ n] = \sum _{j \in \operatorname {range}(X)} j^ n \,  \mathbf{P}(X = j)
> $$
> For a Bernoulli random variable with parameter $p = 0.7$
> $$
> \mathbb E[X] = 0 \times (1 - p) + 1 \times p = p =0.7\\
> \mathbb E[X^4] = 0 \times (1 - p) + 1^4 \times p = p =0.7\\
> \mathbb E[X^{3203}] = 0 \times (1 - p) + 1^{3203} \times p = p =0.7
> $$

### Variance of Bernoulli variables

$$
\textsf{Var}[X] = \mathbb E[X^2] - \mathbb E[X]^2 = p - p^2 = p(1-p)
$$

> When $p \in \{0,1\}$, $\textsf{Var}[X]$ is minimized; When $p=1/2$, $Var[X]$ is maximized.

### Sum of Bernoulli variables

Given $n$ i.i.d. realizations $X_1,…,X_n∼Ber(p)$, from probability theory that $∑^n_{i=1}X_i$ follows a **Binomial** distribution with parameter $n$ and $p$.

### Discrete uniform random variables

A **uniform random variable** is a random variable that takes values with equal probability.

## 2. Gaussian Random Variables

### Moments of Gaussian random variables

The **probability density function (pdf)** of a standard normal distribution 
$$
\phi (z)=\frac{1}{\sqrt{2\pi }} e^{-\frac{z^2}{2}}
$$
The **cumulative distribution function (cdf)** of a standard normal distribution
$$
\Phi (x) = \mathbf{P}(Z \leq x), \quad x \in \mathbb {R}
$$
where $Z \sim \mathcal{N}(0,1)$ is a standard normal variable. 

> **Question 1**: Let $X$ be a **Gaussian random variable** with mean $μ$ and variance $σ^2$. Compute the following moments:
>
> **Answer 1** : 
>
> $\mathbb{E}[X^2] = \sigma^2 + \mu^2\\\mathbb{E}[X^3] = 3 \times \sigma^2 \times \mu + \mu^3\\ \mathbb{E}[X^4] = 3 \times \sigma^4 + 6 \times \sigma^2 \times \mu^2 + \mu^4\\ \textsf {Var}[X^2] = 2 \times \sigma^4 + 4 \times \sigma^2 \times \mu^2$
>
> **Solution 1**: 
>
> We can write a general Gaussian variable $\,   X \sim \mathcal{N}(\mu , \sigma ^2)  \,$ as $\,   X = \sigma Z + \mu  \,$, where $\,   Z \sim \mathcal{N}(0,1)  \,$ is a standard normal variable. Hence, the calculation can be made by factoring out the corresponding polynomials and calculating (or looking up the moments of $Z$).
>
> $\mathbb E[Z] = 0\\\mathbb E[Z^2] = 1\\\mathbb E[Z^3] = 0\\ \mathbb E[Z^4] = 3$
>
> Compute $\mathbb{E}[X^3]$ for example, 
> $$
> \begin{aligned}
> \mathbb E[X^3] &= \int _{-\infty }^{\infty } \left(\sigma z+\mu \right)^3 \phi (z) dz\\
> &=\int _{-\infty }^{\infty }\sigma ^3 z^3 \phi (z) dz +3\sigma ^2 \mu z^2 \phi (z) dz +3\sigma \mu ^2 z \phi (z) dz + \mu ^3 \phi (z) dz\\
> &=\sigma ^3 \mathbb E[Z^3]+3\sigma ^2 \mu \mathbb E[Z^2]+3\sigma \mu ^2 \mathbb E[Z]+ \mu ^3\\
> &=3\sigma ^2 \mu +\mu ^3
> \end{aligned}
> $$
> Compute $Var(X^2)$ using formula
> $$
> \,  \textsf{Var}(X^2)=\mathbb E[X^4]-\left(\mathbb E[X^2]\right)^2 \,
> $$
> **Question 2**: Write $P(X>0)$ in terms of the **cumulative distribution function (cdf)** $Φ$ of the standard Gaussian distribution, evaluated at a function of $μ$ and $σ$.
>
> **Answer 2**: $P(X > 0) = 1 - \mathbb{\Phi}(-\frac{\mu}{\sigma})$.
>
> **Solution 2**: 
> $$
> \begin{aligned}
> \mathbf{P}(X > 0) &=\mathbf{P}( \sigma Z + \mu > 0 ) \\ 
> &= \mathbf{P}(\sigma Z > - \mu )\\
> &= \mathbf{P}( Z > - \frac{\mu }{\sigma } ) \\
> &= 1 - \Phi \left( -\frac{\mu }{\sigma } \right)
> \end{aligned}
> $$

### Covariance of Gaussians

A collection of random variables $X_1,…,X_n$ are **independent and identically distributed (i.i.d.)** if all of them follow the same distribution, and each $X_i$ does not contain information about the other realizations.

The **covariance** of two random variables $X$ and $Y$, denoted by $\textsf{Cov}(X,Y)$, is defined as
$$
\textsf{Cov}(X,Y) = \mathbb E\left[\left(X-\mathbb E[X]\right)\left(Y-\mathbb E[Y]\right)\right]
$$

> Compute the following variances and covariances.
>
> **Answer**: 
>
> $\,  \textsf{Var}(X+Y)= \, 2\\ \textsf{Var}(XY) = 1\\ \textsf{Cov}(X,XY) = 0\\ \textsf{Cov}(X, X+Y)  = 1$
>
> **Solution**:
>
> By the definition of a standard Gaussian random variable
> $$
> \mathbb E[X] = \mathbb E[Y] = 0 \quad \mathbb E[X^2] = \mathbb E[Y^2] = 1
> $$
> By the rule of **independence**
> $$
> \begin{aligned}
> &\textsf{Var}(X + Y) =  \textsf{Var}(X) + \textsf{Var}(Y) = 1 + 1 = 2,\\
> &\textsf{Var}(XY) =  \mathbb E[(XY)^2] - \left(\mathbb E[XY]\right)^2=  \mathbb E[X^2] \mathbb E[Y^2] - \mathbb E[X]^2\mathbb E[Y]^2 = 1 \times 1 - 0 = 1,\\
> &\textsf{Cov}(X,XY) = \mathbb E[X(XY)] - \mathbb E[X] \mathbb E[XY] = \mathbb E[X^2] \mathbb E[Y] - \mathbb E[X]^2 \mathbb E[Y] = 1 \times 0 - 0 \times 0 = 0
> \end{aligned}
> $$
> By the rule of **independence** and **linearity of expectation**
> $$
> \begin{aligned}
> \textsf{Cov}(X, X+Y) &=\mathbb E[X(X+Y)] - \mathbb E[X] \mathbb E[X+Y] \\
> &=  \mathbb E[X^2] + \mathbb E[XY] - \mathbb E[X] (\mathbb E[X] + \mathbb E[Y])\\
> &= \mathbb E[X^2] + \mathbb E[X] \mathbb E[Y] - \mathbb E[X]^2 - \mathbb E[X] \mathbb E[Y]\\
> &=1
> \end{aligned}
> $$

### Variance, covariance and independence

For any two random variables:
$$
\textsf{Var}(X+Y)=\textsf{Var}(X)+\textsf{Var}(Y)+2{\rm Cov}(X,Y)
$$
In particular, if $X$ and $Y$ are dependent, then ${\rm Cov}(X,Y)\neq 0$. But the reverse is not necessarily true.  In other word, if the covariance, $\textsf{Cov}(X,Y)$ between two random variables $X,Y$ is $0$, then $X$ and $Y$ are not necessarily independent.

For example, let $X \sim \rm{Unif}[-1, 1]$ and let $Y = X^2$ Then, 
$$
{\rm Cov}(X,Y)=\mathbb E[XY]-\mathbb E[X]\mathbb E[Y]=\mathbb E[X^3]-\mathbb E[X]\mathbb E[X^2]=0,
$$
using the fact that $X$ is **centered and symmetric around 0**, and its **odd moments vanish**. Even though they are uncorrelated, they are (highly) dependent, $Y$ is obtained from $X$.

##  3. Uniform random variables

### Expectation, variance and probabilities

> Let $X$ be a uniform random variable in the interval $[2, 8.5]$. Find the following quantities 
>
> **Answer**:
>
> $\mathbb{E}[X] = 21/4\\ \rm{Var}[X] = 169/48\\ P(X > 4) = 9/13\\ P(\rm{log}(X) \leq 1)$ 
>
> **Solution**:
>
> We can write $X = 6.5 Z +2$, where $Z$ follows a uniform distribution on $[0,1]$. By properties of the uniform distribution, we then conclude
> $$
> \mathbb E[X] = 2 + 6.5 \mathbb E[Z] = 2 + 6.5 \times \frac{1}{2} = \frac{21}{4},\\
> \textsf{Var}[X] = 6.5^2 \times \textsf{Var}[Z] = \frac{169}{4} \times \frac{1}{12} = \frac{169}{48},\\
> \mathbf{P}(X \geq 4) = \mathbf{P}(Z \geq \frac{4}{13}) = 1 - \frac{4}{13} = \frac{9}{13},\\
> \mathbf{P}(\log (X) \leq 1) = \mathbf{P}(X \leq e) = \mathbf{P}(Z \leq \frac{2(\mathbf e- 2)}{13}) \approx 0.110549.
> $$

### Two independent copies

> Let $U, V$ be i.i.d, random variables uniformly distributed in $[0,1]$. Compute the following quantities
>
> **Answer**: 
>
> $\mathbb{E}[|U - V|] = 1/3\\ P(U = V) = 0\\ P(U \leq V) = 1/2$
>
> **Solution**: 
>
> For the first quantity, we write the joint expectation as an **iterated expectation and conditional expectation**,
> $$
> \mathbb E[|U - V|] = \mathbb E[\mathbb E[|U - V| | V]].
> $$
> By independence, we can compute the inner expectation as
> $$
> \begin{aligned}
> \mathbb E[|U - V| | V = v] &= \int _0^1 | u - v | \,  du\\
>  &= \int _0^ v (v-u) \,  du + \int _ v^1 (u-v) \,  du\\
>  &= \left[ vu - \frac{1}{2}u^2 \,  \right]_0^ v + \left[ \frac{1}{2}u^2 - vu \,  \right]_ v^1 \\
>  &=v^2 - v + \frac{1}{2}
>  \end{aligned}
> $$
> so
> $$
> \mathbb E[\, |U - V|\, ] = \mathbb E\left[V^2 - V + \frac{1}{2}\right] = \frac{1}{3} - \frac{1}{2} + \frac{1}{2} = \frac{1}{3}.
> $$
> For the probability $ \mathbf{P}(U = V)$, just write this as double expectation as well as notice that
> $$
> \mathbf{P}(U = V) = \mathbb E[ \mathbb E[ \mathbf{1}(U = V) | V] ] = \mathbb E[0] = 0,
> $$
> because the probability of a uniform random variable being equal to any fixed number between $0$ and $1$ is zero.
>
> For $P(U≤V)$, write it again as a double expectation, 
> $$
> \mathbf{P}(U \leq V) = \mathbb E[ \mathbb E[\mathbf{1}(U \leq V) | V] ] = \mathbb E[ \mathbf{P}(U \leq V) | V ] = \mathbb E[V] = \frac{1}{2}.
> $$
> Alternatively, by symmetry of the two variables, $P(U≤V)=P(V≤U)$ and either one of the two is true.

### Maximum and sum of independent copies

> Let $X,Y$ be independent random variables uniformly distributed in $[0,1]$. In the graph below, sketch
>
> 1. The probability density $f_{X + Y} (z)$ of $X + Y$;
> 2. The probability density $f_{\max(X,Y)}(z)$ of $\max(X,Y)$;
>
> **Answer**: 
>
> $f_{X + Y} (z) = \left\{  \begin{aligned}  0, & \qquad z < 0\\ z, & \qquad 0 < z < 1\\ 2 - z, & \qquad 1 < z < 2\\ 0, & \qquad z > 2. \end{aligned} \right.\\f_{\max(X,Y)}(z) = \left\{  \begin{aligned}  0, \quad & z < 0\\ 2z, \quad & 0 \leq z \leq 1\\ 0, \quad & z > 1. \end{aligned} \right. $
>
> **Solution**: 
>
> The density of a uniform random variable is 
> $$
> f(x) = \mathbf{1}[0,1] = \left\{  \begin{aligned}  1, \quad & \text {if } x \in [0,1]\\ 0, \quad & \text {otherwise} \end{aligned} \right.
> $$
> The density of $X + Y$ is given by the **convolution** of the density of a uniform random variable. Therefore, the density $g$ of $X+ Y$ is
> $$
> \begin{aligned}
> g(z) &= \int _{\mathbb {R}} f(x) f(z - x) \,  dx\\
> &= \int _{\mathbb {R}} \mathbf{1}(x \in [0,1]) \mathbf{1}(z - x \in [0, 1]) \,  dx\\
> &= \int _0^1 \mathbf{1} (z - 1 \leq x \leq z) \,  dx\\
> &= \mathbf{1}(z \leq 2) \int _{\max \{ 0, z-1\} }^{\min \{ 1, z\} } \,  dx\\
> &= \left\{  \begin{aligned}  0, & \qquad z < 0\\ z, & \qquad 0 < z < 1\\ 2 - z, & \qquad 1 < z < 2\\ 0, & \qquad z > 2. \end{aligned} \right.
> \end{aligned}
> $$
> For the density of $\max\{X,Y\}$, first note that it is supported in $[0,1]$. Now, first compute the **cdf** on that interval:
> $$
> \mathbf{P}(\max \{ X,Y\}  \leq y) = \mathbf{P}(X \leq y) \mathbf{P}(Y \leq y) \quad \text {(by independence)} = z^2
> $$
> Hence the density $h$ of $\max\{X,Y\}$ is given by taking the derivative of the cdf and get
> $$
> h(z) = \left\{  \begin{aligned}  0, \quad & z < 0\\ 2z, \quad & 0 \leq z \leq 1\\ 0, \quad & z > 1. \end{aligned} \right.
> $$

### Maximum of uniform random variables

> Let $U_1, ..., U_n$ be i.i.d. random variables uniformly distributed in $[0,1]$ and let $\,  \displaystyle M_ n= \max _{1\leq i\leq n} U_ i \,$. Find the cdf of $M_n$, which we denote by $G(t)$, for $t \in [0,1]$. And compute $ \lim _{n \to \infty } F_ n(t)$ where $F_ n(t)$ denote the cdf of $n(1-M_n)$, for $t > 0$.
>
> **Answer**: 
>
> For $t \in [0,1]$, $G(t) = t^n$;  For $t > 0$, $ \lim _{n \to \infty } F_ n(t) = 1-\exp(-t)$.
>
> **Solution**: 
>
> The cdf of $M_n$ is 
> $$
> \mathbf{P}(M_ n \leq t) = \mathbf{P}\left(\max _{i = 1, \dots , n} U_ i \leq t\right) = \mathbf{P}\left(\cap _{i=1}^ n \{ U_ i \leq t\} \right) \quad \text{(by independence) }= \prod _{i=1}^ n \mathbf{P}(U_ i \leq t) = t^ n,
> $$
> The cdf of $n(1-M_n)$ is 
> $$
> \begin{aligned}
> \mathbf{P}\left(n(1-M_ n) \leq t\right) &= \mathbf{P}\left(1 - M_ n \leq \frac{t}{n}\right) = \mathbf{P}\left(M_ n \geq 1 - \frac{t}{n}\right)\\
> &=1 - \mathbf{P}\left(M_ n < 1 - \frac{t}{n}\right) = 1 - \left( 1 - \frac{t}{n} \right)^ n \xrightarrow {n \to \infty } 1 - \mathbf e^{-t}.
> \end{aligned}
> $$

## 4. Exponential random variables

### Sums and products

Recall that:

If $X \sim \mathrm{Exp}(\lambda )$ with $\lambda>0$, then
$$
\mathbb E[X] = \lambda , \quad \mathbb E[X^2] = \frac{2}{\lambda ^2}, \quad \textsf{Var}(X) = \frac{1}{\lambda ^2}.
$$
If $ Y \sim \mathrm{Poi}(\mu )$, again with $\mu > 0$, then
$$
\mathbb E[Y] = \mu , \quad \mathbb E[Y^2] = \mu + \mu ^2, \quad \textsf{Var}(Y) = \mu .
$$

> Let $X$ be an exponential random variable with parameter $λ>0$ and $Y$ be a Poisson random variable with parameter $μ>0$. Assume that $X$ and $Y$ are independent. Compute the following quantities:
>
> By linearity of expectation,
> $$
> \mathbb E[X^2 + Y^2] = \mathbb E[X^2] + \mathbb E[Y^2] = \frac{2}{\lambda ^2} + \mu + \mu ^2 
> $$
> By multiplicativity of expectation for independent variables,
> $$
> \mathbb E[X^2 Y] =\mathbb E[X^2] \mathbb E[Y] = \frac{2 \mu }{\lambda ^2}
> $$
> By additivity of variance for independent variables and scaling property of variance,
> $$
> \textsf{Var}(2X + 3Y) = \textsf{Var}(2 X) + \textsf{Var}(3 Y) =2^2 \textsf{Var}(X) + 3^2 \textsf{Var}(Y) = \frac{4}{\lambda ^2} + 9 \mu
> $$

### Estimators

Recall that the cdf of exponential distribution with parameter $\lambda$ on $(0, \infty)$ is 
$$
F(x) = 1 - \exp (-\lambda x).
$$

> Let $X_1,…,X_n$ be i.i.d exponential random variables with parameter $λ$ and let $Z_i=1(X_i≤1),i=1,…,n$.  What is the limit in probability, as $n$ goes to infinity, of $\frac{1}{n}\sum _{i=1}^ n Z_ i$.
>
> **Answer**: $\frac{1}{n} \sum _{i=1}^{n} Z_ i \xrightarrow [n \to \infty ]{\mathrm{\mathbf{P}}} \, 1-\exp(-\lambda)$
>
> **Solution**: 
>
> Since $X_i$ are i.i.d. by the Law of Large Numbers 
> $$
> \frac{1}{n} \sum _{i=1}^{n} Z_ i \xrightarrow [n \to \infty ]{\mathrm{\mathbf{P}}} \mathbb E[Z_ i],
> $$
> And the following formula follows the cdf of an Exponential distribution:
> $$
> \mathbb E[Z_ i] = \mathbf{P}(X_ i \leq 1) = 1 - \exp (-\lambda \times 1) = 1 - \exp (-\lambda )
> $$

### Properties of the exponential distribution

> Let $X$ be an exponential random variable with parameter $\lambda = 2$ that models the lifetime (in years) of a lightbulb. Compute the probability that the lightbulb lasts for at least 2 years. Given the lightbulb has lasted 2 years, find the probability that it lasts for $k$ more years for any positive integer $k$.
>
> **Answer**: 
>
> $\mathbf{P}(X \geq 2) = \exp(-4)\\\mathbf{P}(X \geq k + 2 | X \geq 2) = \exp(-2k)$
>
> **Solution**: 
>
> For $\lambda = 2$, using the cdf of exponential distribution
> $$
> \mathbf{P}(X \geq 2) = 1- \mathbf{P}(X \leq 2) = 1- F(X=2) =  1 - (1- \exp(-2 \times 2)) = e^{-4}
> $$
> Note that $ \{ X \geq k+2\}  \subseteq \{ X \geq 2\}$, so
> $$
> \mathbf{P}(X \geq k + 2 | X \geq 2) = \frac{\mathbf{P}(\{ X \geq k + 2\}  \cap \{ X \geq 2\} )}{\mathbf{P}(X \geq 2)} = \frac{\mathbf{P}(X \geq k + 2)}{\mathbf{P}(X \geq 2)} =\frac{e^{-2(k + 2)}}{e^{-4}} = e^{-2k}
> $$

This is an example of the exponential distribution being **memoryless** : The probability of the lightbulb lasting $k$ more years given that it already lasted 2 years is exactly the same as the probability of it lasting $k$ years in the first place.

## 5. Probability tables

### Gaussian probabilities

For $Z \sim \mathcal{N}(0, 1), x > 0$ we have
$$
\mathbf{P}(Z \leq -x) = \mathbf{P}(Z \geq x) = 1 - \mathbf{P}(Z \leq x),
$$
and 
$$
\mathbf{P}(Z \geq x) = 1 - \mathbf{P}(Z \leq x).
$$
To obtain probabilities for any normal distribution with any given $\mu$ and $\sigma$, we can represent $X = \sigma Z +\mu$ where $Z \sim \mathcal{N}(0,1)$.

> Let $X \sim N(1, 2.25)$, compute the probabilities below (also look up the normal probability table)
>
> **Answer**: 
>
> $\mathbf{P}(X > 1) = 0.5\\ \mathbf{P}(|X - 2| \leq 1) = 0.4082 \\ \mathbf{P}(X^2 > 4) = 0.2752427 \\ \mathbf{P}(X^2 - 2X - 1 > 0) = 0.3457786$
>
> **Solution**: 
> $$
> \begin{aligned}
> \mathbf{P}(X > 1) &= \mathbf{P}(1.5Z + 1 > 1) = \mathbf{P}(1.5Z > 0)\\
> &= \mathbf{P}(Z \geq 0) = 1 - \mathbf{P}(Z \leq 0) = 1 - 0.5000 = 0.5000\\
> \mathbf{P}(| X - 2 | \leq 1) &= \mathbf{P}(-1 \leq (X - 2) \leq 1) = \mathbf{P}(-1 \leq (1.5Z + 1 -2) \leq 1)\\
> &=\mathbf{P}(0 \leq 1.5Z \leq 2)\\
> & \simeq\mathbf{P}(0 \leq Z \leq 1.33)\\
> &=\mathbf{P}(Z \leq 1.33) - \mathbf{P}(Z \leq 0) \simeq 0.9082 - 0.5000 = 0.4082\\
> \mathbf{P}(X^2 > 4) &=\mathbf{P}(| X | > 2) = \mathbf{P}(| 1.5Z + 1 | > 2)\\
> &= \mathbf{P}(1.5Z + 1 \leq -2) + \mathbf{P}(1.5 Z + 1 \geq 2)\\
> &=\mathbf{P}(Z \leq -2) + \mathbf{P}\left(Z \geq \frac{2}{3}\right)\\
> &=1 - \mathbf{P}(Z \leq 2) + 1 - \mathbf{P}\left(Z \leq \frac{2}{3}\right)\\
> &\simeq 2 - 0.9772 - 0.7486 = 0.2742\\
> \mathbf{P}( X^2 - 2X - 1 > 0 ) &= \mathbf{P}((X-1)^2 - 2 > 0) = \mathbf{P}(|X-1| > \sqrt{2})\\
> &=\mathbf{P}(|1.5Z| > \sqrt{2})\\
> &=\mathbf{P}(Z > \frac{\sqrt{2}}{1.5}) + \mathbf{P}(Z < -\frac{\sqrt{2}}{1.5})\\
> &=2 - 2\mathbf{P}(Z < \frac{\sqrt{2}}{1.5})\\
> & \simeq 2 - 2(0.8264)\\
> &= 0.3472.
> \end{aligned}
> $$

### Approximation of Binomial variables

> Using the normal probability table, evaluate approximately $P(X>400)$, where $X$ is a binomial random variable with parameters $1000$ and $0.3$.
>
> **Answer**: $\mathbf{P}(X > 400) \simeq 0.0002$
>
> **Solution**: 
>
> A binomial distribution with parameters $(n,p)$ has expectation $np$ and variance $np(1−p)$. Hence, by the Central Limit Theorem, we have
> $$
> \frac{1}{\sqrt{n p (1-p)}} (X - np) \xrightarrow {\text{(D)}} Z \sim \mathcal{N}(0,1).
> $$
> The probability in question can therefore be approximated by
> $$
> \begin{aligned}
> \mathbf{P}(X > 400) &=  \mathbf{P}\left(\frac{1}{\sqrt{1000 \times 0.3 \times 0.7}} (X - 300) > \frac{100}{\sqrt{1000 \times 0.3 \times 0.7}}\right)\\
> &\simeq 1 - \mathbf{P}(Z \leq \frac{100}{\sqrt{1000 \times 0.3 \times 0.7}})\\
> & \simeq 1 - \mathbf{P}(Z \leq 6.90)\\
> &\leq 1 - 0.9998 = 0.0002.
> \end{aligned}
> $$

## 6. Matrices and Vectors

### Matrix Multiplication

The size of the output is the number of rows of the left matrix, and the number of columns of the right matrix. 

### Vector Inner product

The product $\mathbf{u}^ T \mathbf{v}$ evaluates the **inner product** (also called the **dot product** ) of $\mathbf{u}$ and $\mathbf{v}$. The inner product of $\mathbf{u}$ and $\mathbf{v}$ is sometimes written as $⟨\mathbf{u},\mathbf{v}⟩$. The inner product is always a scalar. 

In general, if $\mathbf{u}= (u_1, u_2, \ldots , u_ n)^ T$ and $\mathbf{v}= (v_1, v_2, \ldots , v_ n)^ T$, then $\mathbf{u}^ T \mathbf{v}= \sum _{i=1}^{n} u_ i v_ i$.
$$
\begin{pmatrix}  u_1 &  \cdots &  u_ n \end{pmatrix} \begin{pmatrix}  v_1 \\ \vdots \\ v_ n \end{pmatrix} = (\cdot )
$$

### Vector Outer product

The product $\mathbf{u}\mathbf{v}^T$ evaluates the **outer product** of $\mathbf{u}$ and $\mathbf{v}$, which is a 2×2 matrix in this case. 

In general, if $\mathbf{u}= \begin{pmatrix}  u_1 \\ \vdots \\ u_ m \end{pmatrix}$ and $\mathbf{v}= \begin{pmatrix}  v_1 \\ \vdots \\ v_ n \end{pmatrix}$, $\mathbf{u}\mathbf{v}^ T$ is an $m \times n$ matrix whose $(i,j)$ entry is $(\mathbf{u}\mathbf{v}^ T)_{i,j} = u_ i v_ j$.

## 7. Linear Independence, Subspaces and Dimension

Vectors $\mathbf{v}_1, \ldots , \mathbf{v}_ n$ are said to be **linearly dependent** if there exist scalars $c_1, \ldots , c_ n$ such that 

(1) not all $c_i$'s are zero and 

(2) $\mathbf{c}_1\mathbf{v}_1+⋯+\mathbf{c}_n\mathbf{v}_n=0$.

Otherwise, they are said to be **linearly independent** : the only scalars $c_1, \ldots , c_ n$ that satisfy $c_1 \mathbf{v}_1 + \cdots + c_ n \mathbf{v}_ n = 0$ are $c_1 = \cdots = c_ n = 0$.

The collection of non-zero vectors $\mathbf{v}_1, \ldots , \mathbf{v}_ n \in \mathbb {R}^ m$ determines a **subspace** of $\R^m$, which is the set of all linear combinations $c_1 \mathbf{v}_1 + \cdots + c_ n \mathbf{v}_ n$ over different choices of $c_1,\ldots ,c_ n \in \mathbb {R}$. The **dimension** of this subspace is the size of the **largest possible, linearly independent** sub-collection of the (non-zero) vectors $\mathbf{v}_1, \ldots , \mathbf{v}_ n$.

### The rank of a matrix

In general, the rank of any $m×n$ matrix can be at most $\min(m,n)$, since rank = column rank = row rank. In general, a matrix $\mathbf{A}$ is said to have **full rank** if $\mathrm{rank}(\mathbf{A}) = \min (m,n)$.

> The rank of matrix $\begin{pmatrix}  0 &  0 \\ 0 &  0 \end{pmatrix}$ is 0.
>
> By definition, the rank is equal to the number of **nonzero** linearly independent vectors.
>
> The rank of matrix $\begin{pmatrix}  1 &  1 &  0 \\ 0 &  -3 &  2 \\ 0 &  0 &  1 \end{pmatrix}$ is 3.
>
> All three rows are independent. An easy way to check is to notice that this matrix is **upper triangular** , with nonzero entries along the diagonal.

**Every rank-1 matrix can be written as an outer product. Conversely, every outer product $\mathbf{u} \mathbf{v} ^T$ is a rank-1 matrix.**

In general, the sum of two matrices can have a varying range of ranks, and they can be greater **or** less than the ranks of matrices that are being summed up. On the other hand, it is a general fact that if $\mathbf{A}$ and $\mathbf{B}$ are arbitrary (possibly rectangular) matrices, $\mathrm{rank}(\mathbf{A}\mathbf{B}) \leq \min (\mathrm{rank}(\mathbf{A}), \mathrm{rank}(\mathbf{B}))$. It is possible to use **determinants** to reason about rank. Other times, one may resort to using **Gaussian Elimination** – the rank of any upper triangular matrix is **at least** the number of non-zero entries along the diagonal.

> Given $\mathbf{A}= \begin{pmatrix} -1 &  1 \\ -3 &  3\end{pmatrix}, \mathbf{B}= \begin{pmatrix} 1 &  -1 \\ -1 &  1\end{pmatrix}, \mathbf{C}= \begin{pmatrix}  0 &  0 \\ 0 &  1\end{pmatrix}, \mathbf{D}= \begin{pmatrix} 1 &  1 \\ 1 &  1\end{pmatrix}$, which are rank 1. We can write them as $\mathbf{A}= \mathbf{u}\mathbf{v}^ T, \mathbf{B}= \mathbf{v}\mathbf{v}^ T, \mathbf{C}= \mathbf{w}\mathbf{w}^ T,$ and $\mathbf{D}= \mathbf{x}\mathbf{x}^ T$, where
> $$
> \mathbf{u}= \begin{pmatrix} 1 \\ 3 \end{pmatrix}, \mathbf{v}= \begin{pmatrix} -1 \\ 1 \end{pmatrix}, \mathbf{w}= \begin{pmatrix} 0 \\ 1 \end{pmatrix}, \mathbf{x}= \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
> $$
> which combination of these matrices has rank 2 ? which combination of these matrices has rank 1? 
>
> **Answer**: 
>
> Considering the sums of matrices,
>
> $\mathbf{A}+\mathbf{A}= 2\mathbf{A}$, which has rank 1.
>
> $\mathbf{A}+\mathbf{B}= \mathbf{u}\mathbf{v}^ T + \mathbf{v}\mathbf{v}^ T = (\mathbf{u}+ \mathbf{v})\mathbf{v}^ T$ which has rank 1
>
> $\mathbf{A}+\mathbf{C}= \begin{pmatrix} -1 &  1 \\ -3 &  4\end{pmatrix}$ which has two linearly independent rows, hence its rank is 3.
>
> Considering the products of matrices,
>
> $\mathbf{A}\mathbf{B}= \mathbf{u}\mathbf{v}^ T \mathbf{v}\mathbf{v}^ T = \mathbf{u}\langle \mathbf{v}, \mathbf{v}\rangle \mathbf{v}^ T = \langle \mathbf{v},\mathbf{v}\rangle \mathbf{u}\mathbf{v}^ T$. Note that the inner product $\mathbf{v}^ T \mathbf{v}= \langle \mathbf{v}, \mathbf{v}\rangle$ is a scalar. So this outer product of two vectors has rank 1.
>
> $\mathbf{A}\mathbf{C}= \mathbf{u}\mathbf{v}^ T \mathbf{w}\mathbf{w}^ T = \langle \mathbf{v}, \mathbf{w}\rangle \mathbf{u}\mathbf{w}^ T$ , which has rank 1.
>
> $\mathbf{B}\mathbf{D}= \mathbf{v}\mathbf{v}^ T \mathbf{x}\mathbf{x}^ T = \langle \mathbf{v}, \mathbf{x}\rangle \mathbf{v}\mathbf{x}^ T$ which has rank 1. Notice that $\mathbf{v}$ is orthogonal to $\mathbf{x}$, so $\mathbf{B}\mathbf{D}= 0 \mathbf{v}\mathbf{x}^ T$ is the zero matrix. Its rank is zero.

### Invertibility of a matrix

An $n×n$ matrix $\mathbf{A}$ is invertible if and only if $\mathbf{A}$ has full rank, i.e. $\mathrm{rank}(\mathbf{A}) = n$.

## 8. Eigenvalues, Eigenvectors and Determinants (Optional)

### Eigenvalues and Eigenvectors of a matrix

Solve $\mathbf{A} \mathbf{v} = \lambda \mathbf{v}$, where $\lambda$ is a diagonal matrix. 

> Let $\mathbf{A}= \begin{pmatrix}  3 &  0 \\ \frac{1}{2} &  2 \end{pmatrix}$, $\mathbf{v}= \begin{pmatrix}  2 \\ 1 \end{pmatrix}$, $\mathbf{w}= \begin{pmatrix}  0 \\ 1 \end{pmatrix}$. 
>
> **Answer**: 
>
> $\mathbf{A}\mathbf{v}= \lambda _1 \mathbf{v}$, where $\lambda_1 = 3$.
>
> $\mathbf{A}\mathbf{w}= \lambda _2 \mathbf{w}$, where $\lambda_2 = 2.$
>
> **Solution**:
> $$
> \mathbf{A}\mathbf{v}= \begin{pmatrix}  3 &  0 \\ \frac{1}{2} &  2 \end{pmatrix} \begin{pmatrix}  2 \\ 1 \end{pmatrix} = \begin{pmatrix}  6 \\ 3 \end{pmatrix} \implies \lambda _1 = 3\\
> \mathbf{A}\mathbf{w}= \begin{pmatrix}  3 &  0 \\ \frac{1}{2} &  2 \end{pmatrix} \begin{pmatrix}  0 \\ 1 \end{pmatrix} = \begin{pmatrix}  0 \\ 2 \end{pmatrix} \implies \lambda _2 = 2
> $$

### Geometric Interpretation of Eigenvalues and Eigenvectors

> Let $\mathbf{A}= \begin{pmatrix}  3 &  0 \\ \frac{1}{2} &  2 \end{pmatrix}$, $\mathbf{v}= \begin{pmatrix}  2 \\ 1 \end{pmatrix}$, $\mathbf{w}= \begin{pmatrix}  0 \\ 1 \end{pmatrix}$.  Suppose $\mathbf x= \mathbf{v}+ 2\mathbf{w}= \begin{pmatrix}  2 \\ 3 \end{pmatrix}$. Then $\mathbf{A}\mathbf x= s \mathbf{v}+ t \mathbf{w}$, where:
>
> **Answer**: 
>
> $s = 3$ and $t =4 $,
>
> **Solution**:
>
> We have
> $$
> \begin{aligned}
> \mathbf{A}\mathbf x &= \mathbf{A}(\mathbf{v}+ 2\mathbf{w})\\
> &= \mathbf{A}\mathbf{v}+ 2\mathbf{A}\mathbf{w}\\
> &= (3\mathbf{v}) + 2(2\mathbf{w})\\
> &= 3\mathbf{v}+ 4\mathbf{w}.
> \end{aligned}
> $$
> From this, we get $s = 3, t = 4$.
>
> In particular, $s$ describes the amount that $\mathbf{A}$ stretches $\mathbf{x}$ in the direction of $\mathbf{v}$, and $t \over 2$ (note the “2" in front of $\mathbf{w}$ in $\mathbf{x}$) describes the amount that $\mathbf{A}$ stretches $\mathbf{x}$ in the direction of $\mathbf{w}$. 

### Determinant and Eigenvalues 

The **determinant** of a matrix indicates whether it is singular. For 2×2 matrices, it has the formula
$$
\mathrm{det}\left( \begin{array}{cc} a &  b \\ c &  d \end{array} \right) = ad-bc
$$
For general n×n matrices, the **product of the eigenvalues is always equal to the determinant**.

### Trace and Eigenvalues

The **trace** of a matrix is the sum of the diagonal entries.

For general n×n matrices, the **sum of the eigenvalues is always equal to the trace of the matrix**.

### Nullspace

> If a (nonzero) vector is in the nullspace of a square matrix $\mathbf{A}$, it is an eigenvector of $\mathbf{A}$.
>
> If $0$ is an eigenvalue for a given square matrix $\mathbf{A}$,
>
> * $\mathrm{det}(\mathbf{A})= 0$
> * $\text {NS}(\mathbf{A})\neq \mathbf{0}$
>
> **Solution**: 
>
> If a vector $\mathbf{v}$ is in the nullspace of $\mathbf{A}$, then $\mathbf{Av}=0=(0)\mathbf{v}$. So it is an eigenvector of $\mathbf{A}$ associated to the eigenvalue 0.
>
> If 0 is an eigenvalue for a matrix $\mathbf{A}$, then by definition, there exists a nonzero solution to $\mathbf{Av}=0$; that is, $\rm{NS}(\mathbf{A})≠0$, and this only happens if and only if $\rm{det}(\mathbf{A})=0$.

# Additional Readings

![distributions](../assets/images/distribution-summary.jpeg)

From : https://medium.com/@ciortanmadalina/overview-of-data-distributions-87d95a5cbf0a



# Lecture 5. Confidence Intervals and the Delta Method

There are 5 topics and 6 exercises.

## 1. Confidence Intervals Concept Check

Let $X_1, ... X_n \stackrel{iid}{\sim} P_\theta$, where $\theta$ is an unknown parameter. You construct a C.I. $\mathcal{I}=\left[ L(X_1, ..., X_n), U(X_1, ..., X_n)\right]$ for $\theta$. The C.I. $\mathcal{I}$ is a **random object** (not deterministic). As defined, a C.I. $\mathcal{I} = [L,U]$ for an unknown parameter $\theta$ is a **random interval** such that the expressions for its endpoints $L, U$ do not depend on $\theta$.

$L$ and $U$ are functions of the random sample that do not depend on $\theta$. In practice, one uses data (e.g. realizations $x_1, ..., x_n$ of the i.i.d. observations $X_1, ..., X_n$) to compute the realization $\mathcal{I}_\text{real}$ of the C.I. $\mathcal{I}$:
$$
\mathcal{I}_{\text {real}} := [L(x_1, \ldots , x_ n), U(x_1, \ldots , x_ n)].
$$
Note that it is important to distinguish the **random variable** $\mathcal{I}$ (C.I.) from its **realization** $\mathcal{I}_\text{real}$, which can be formed only after collecting data.

> ####  Exercise 27
>
> Let $I, J$ be some $95\%$ and $98\%$ asymptotic C.I.s (respectively) for $p$. Which one is correct? 
>
> a. We always have $I \subset J$ ; Any realization of $\mathcal{I}$ is a subinterval of any realization of $\mathcal{J}$.
>
> b. We always have $J \subset I$; Any realization of $\mathcal{J}$ is a subinterval of any realization of $\mathcal{I}$.
>
> c. None of the above.
>
> Find a $98\%$ asymptotic C.I. for $p$.
>
> **Answer**: C
>
> **Solution**: 
>
> The $98\%$ C.I. is not necessarily bigger than $95\%$ one. And those two C.I.s can be calculated by two different techniques. Moreover, with the same confidence level, the centered data have the narrowest C.I., any deviation from the center will strictly give wider C.I.
>
> Find a $98\%$ asymptotic C.I. for $p$.
> $$
> 1-\alpha = 0.98 \text{ C.I.}, \quad \overline{R}_n \pm {q_{\alpha/2} \over 2 \sqrt{n}}, \quad q_{1\%}
> $$
> The C.I. becomes larger when the confidence level increases.

> #### Exercise 28
>
> In a new experiment consisting of $150$ couples, $75$ couples are observed to turn their heads to the left and the remaining 75 couples turned their heads to the right when kissing. Let $p$ denote the (unknown) parameter which specifies the probability that a couple turns their head to the right. Which of the following is true?
>
> a. $[0,0.5]$ is a $50\%$ asymptotic C.I. for $p$.
>
> b. $[0.5,0]$ is a $50\%$ asymptotic C.I. for $p$.
>
> **Answer**: ab
>
> **Solution**:  Take option **a** for example:
>
> Let $R_1, R_2, \ldots , R_{150} \stackrel{iid}{\sim } \text {Ber}(p)$ denote the sampled response (without loss of generality, assume that $R_i=1$ encodes that the $i$-th couple turns their heads to the right, and $R_i=0$ encodes that the couple turns their heads to the left.) Let $P=\text{Ber}(p)$ denote the common distribution of $R_1, ...,R_{150}$.
>
> Consider the sample mean $\overline{R}_n$. By CLT,
> $$
> \sqrt{n} \left( \frac{\overline{R}_ n - p}{\sqrt{p(1-p)}} \right) \xrightarrow [(d)]{} N(0,1).
> $$
> Consider the interval $[0,0.5]$. Since $\overline{R}_n=0.5$, this interval is a realization of the (random) C.I. $\mathcal{I}=(0,\overline{R}_n)$. We compute that
> $$
> P( \mathcal{I} \ni p ) = P( p \leq \overline{R}_ n ) = P( \overline{R}_ n - p \geq 0 ) = P\left(\frac{\overline{R}_ n - p}{\sqrt{p(1-p)}} \geq 0 \right).
> $$
> This probability goes to $0.5$ as $n→∞$. Therefore, the realization $[0,0.5]$ of the random interval $[0,R_n]$ is an asymptotic C.I. of level $0.5$.
>
> **Remark**: Difference between **asymptotic and non-asymptotic C.I.**:
>
> *Non-asymptotic* C.I.s are those where the probability is exact; i.e. $P(I∋θ)≥1−α$, rather than $\lim_{n→∞}P(I∋θ)≥1−α$, where C.I.s are called *asymptotic* C.Is. In other words, for *non-asymptotic* C.I.s we're not using any techniques involving convergence of random variables (like Slutsky's theorem or the CLT).

> #### Exercise 29
>
> If $[0.34, 0.57]$ is a realization of a (non-asymptotic, for some fixed $n$) 95% C.I. for an unknown parameter $p$, then which of the following is true? 
>
> The probability that the unknown parameter $p$ is in this interval is 
>
> a. $\geq 0.95$
>
> b. None of the above, because $p$ and $[0.34, 0.57]$ are both deterministic.
>
> **Answer**: b
>
> **Solution**: 
>
> Given some unknown but fixed parameter $\theta \in \mathbb {R}$ for a parametric model and random variables $X_1, \ldots , X_ n$ distributed i.i.d. $P_\theta$ recall that the non-asymptotic $95\%$ C.I. of $p$ is an interval $\mathcal{I} = \mathcal{I}(X_1,\ldots ,X_ n)$ such that $\Pr (\mathcal{I} \ni \theta ) \geq 0.95$. It is important to note that there is **randomness** here, given by the randomness of $\mathcal{I}$. That is,
> $$
> \lim_{n \rightarrow \infty} \mathbb{P}\left( p \in \left[ \overline{R}_n \pm {q_{\alpha/2}\over 2\sqrt{n}}\right]\right) \geq 1-\alpha
> $$
> However, a **realization** of a random variable is *deterministic*. The interval $[0.34, 0.57]$ either contains the parameter $p$, or it doesn't. In other words, the expression $\Pr ([0.34,0.57] \ni p)$ is equal to $1$ or $0$. 

## 2. Modeling Inter-arrival Times of a Subway System

### Statistical Problem

The times (in minutes) between arrivals of the $T$ at Kendall are: $T_1,..., T_n$. Assume that these times are 1) mutually independent, 2) exponentially random variables with common parameter. Based on the observed arrival times, we want to estimate the value of $\lambda$.

* Mutual independence of $T_1,..., T_n$: plausible but not completely justified (often the case with independence).

* $T_1,..., T_n$ are exponential r.v. Exponential distribution is a very common distribution for inter-arrival times, mainly because it has **lack of memory** (*memorylessness*). 
  The memorylessness states that
  $$
  \mathbb{P}\left[ T_1 > t+s | T_1 > t \right] = \mathbb{P}\left[ T_1 > s\right], \quad \forall s,t \geq 0
  $$
  $T_i > 0$ almost surely.

  **Proof**:

  Assume $T \sim \mathsf{Exp}(\lambda)$, we want to know $\mathbb{P}[T>t+s | T>t]$:
  $$
  \begin{aligned} 
  \mathbb{P}[T>t+s | T>t]&= {\mathbb{P}(T > t+s,T>t) \over \mathbb{P}(T>t)}\\
  &={\mathbb{P}(T > t+s) \over \mathbb{P}(T>t)} \\
  &={\exp(-\lambda(t+s) )\over \exp(-\lambda t)}\\
  &=\exp(-\lambda s)\\
  &= \mathbb{P}[T>s]
  \end{aligned}
  $$

* The exponential distribution of $T_1,..., T_n$ have the same parameter: in average all the same inter-arrival time. True only for limited period.

> #### Exercise 30
>
> Let $X\sim \exp(1)$, what is $\mathbf{P}(X > 3)$? What is $\mathbf{P}\left( X > t + 3| X > t \right)$ if $t > 0$?
>
> **Answer**: Both $\exp(-3)$
>
> **Solution**: 
>
> The density of $\exp(1)$ is $\exp(-x)$, thus,
> $$
> \mathbf{P}(X > 3) = \left.\int _{3}^{\infty } \exp(-x) \,  dx = - \exp(-x) \right|_3^\infty = \exp(-3).
> $$
> By memoryless property,
> $$
> \mathbf{P}\left( X > t + 3| X > t \right) = \mathbf{P}(X > 3) = \exp(-3)
> $$

### Estimating the Parameter for an Exponential Model

The pdf of $T_1$ is
$$
f_{\lambda}(t)=\lambda e^{-\lambda t} \quad t >0
$$
The mean of $T_1$ is 
$$
\begin{aligned}
  \mathbb {E}[T_1] &= \int _{0}^\infty t \lambda e^{-\lambda t} \,  dt\\
  &= - t e^{-\lambda t} \bigg|_{0}^\infty + \int _{0}^\infty e^{-\lambda t} \,  dt\\
  &= 0 - \frac{1}{\lambda } e^{-\lambda t} \bigg|_{0}^\infty\\
  &= \frac{1}{\lambda }.
  \end{aligned}
$$
Hence, a natural estimate of ${1\over \lambda}$ is
$$
\bar{T}_n = {1\over n}\sum^n_{i=1}T_i
$$
A natural estimator of $\lambda$ is 
$$
\hat{\lambda} = {1 \over \bar{T}_n}
$$
This is a **consistent** estimator, because of 

* By the **LLN**:
  $$
  \bar{T}_n = {1\over n} \sum_{i=1}^n T_i \xrightarrow[n \rightarrow \infty]{a.s./\mathbf{P}} \mathbb{E}[T_i]={1\over \lambda}
  $$
  Hence,
  $$
  \hat{\lambda} \xrightarrow[n\rightarrow \infty]{a.s./\mathbf{P}} \lambda
  $$

* By the **CLT**,
  $$
  \sqrt{n}\left( \bar{T}_n - {1\over \lambda}\right) \xrightarrow[n\rightarrow \infty]{} \mathcal{N}(0, \lambda^{-2})
  $$
  Note that the **asymptotic variance** of what $\sqrt{n}\left( \bar{T}_n - {1\over \lambda}\right)$ converges to is calculated by
  $$
  \begin{aligned}
  \mathsf{Var}\sqrt{n}\left( \bar{T}_n - {1\over \lambda}\right) &= n\mathsf{Var}\left( \bar{T}_n - {1\over \lambda}\right)\\&= n\mathsf{Var}\left( \bar{T}_n\right)\\&= n\mathsf{Var}\left({1\over n}\sum_{i=1}^n T_i\right) \\&=n {1\over n^2} \left(\mathsf{Var}(T_1) +...+\mathsf{Var}(T_n) \right) \\&=\mathsf{Var}(X_i)\\&=  {1\over \lambda^2}
  \end{aligned}
  $$

* By the **continuous mapping theorem (CMT)**
  $$
  {1\over \overline{T_n}}\xrightarrow [n \to \infty ]{a.s/\mathbf{P}} {1\over \mathbb{E}[T_i]} = \lambda
  $$

> #### Exercise 31 Consistency and Biasedness
>
> Let $X_1, X_2, \ldots , X_ n \stackrel{iid}{\sim } \exp (\lambda )$. Let $\overline{X}_ n := \frac{1}{n} \sum _{i = 1}^ n X_ i$ denote the sample mean of the data set.
>
> 1.  $\overline{X}_n \xrightarrow[x \rightarrow \infty]{a.s./\mathbf{P}} ?$
> 2. ${1\over\overline{X}_n} \xrightarrow[x \rightarrow \infty]{a.s./\mathbf{P}} ?$
> 3. What is the bias of ${1 \over \overline{X}_n}$ as an estimator of $\lambda$?
> 4. Which are the properties of ${1\over \overline{X}_n}$ as an estimator of $\lambda$?
>
> **Solution**:
>
> 1. $\overline{X}_n \xrightarrow[x \rightarrow \infty]{a.s./\mathbf{P}} \mathbb{E}[X_i] \text{ or } \overline{X}_n \xrightarrow[x \rightarrow \infty]{a.s./\mathbf{P}} {1 \over \lambda}$
>
> By the (strong/weak) LLN,
> $$
> \overline{X}_n = \frac{\sum _{i = 1}^ n X_ i}{n}\xrightarrow [n \to \infty ]{a.s/\mathbf{P}}\mathbb E[X_ i]\, =\, \frac{1}{\lambda }.
> $$
>
> 2. ${1\over\overline{X}_n} \xrightarrow[x \rightarrow \infty]{a.s./\mathbf{P}}  \lambda \text{ or } {1\over\overline{X}_n} \xrightarrow[x \rightarrow \infty]{a.s./\mathbf{P}} \frac{1}{\mathbb E[X_ i]}$
>
> By the continuous mapping theorem (CMT),
> $$
> {1\over \overline{T_n}}\xrightarrow [n \to \infty ]{a.s/\mathbf{P}} {1\over \mathbb{E}[T_i]} = \lambda
> $$
> Thus, ${1\over \overline{T_n}}$ is a **consistent** estimator of $\lambda$.
>
> 3. $\mathbb E\left[\frac{1}{\overline{X}_ n}\right]-\lambda \text{ or } \mathbb E\left[\frac{1}{\overline{X}_ n}\right]-\frac{1}{\mathbb E[X_ i]} \text{ or } \mathbb E\left[\frac{1}{\overline{X}_ n}\right]-\frac{1}{\mathbb E[\overline{X}_ n]}$
>
> The bias of ${1 \over \overline{X}_n}$ as an estimator of $ \lambda =\frac{1}{\mathbb E[X_ i]}=\frac{1}{\mathbb E\left[\overline{X}_ n\right]}$ is 
> $$
> \text {Bias}=\mathbb E\left[\frac{1}{\overline{X}_ n}\right]-\frac{1}{\mathbb E\left[\overline{X}_ n\right]}.
> $$
>
> 4. Consistent by not unbiased.
>
> Note that
> $$
>  \mathbb E\left[\frac{1}{\overline{X}_ n}\right]\neq \frac{1}{\mathbb E\left[\overline{X}_ n\right]}\, =\, \lambda .
> $$
> Since the function ${1\over x}$ is convex (by the shape of its graph or by $\left(1\over x\right)'' = {2 \over x^3} > 0$), Jensen's inequality gives $\mathbb E\left[\frac{1}{\overline{X}_ n}\right]> \frac{1}{\mathbb E\left[\overline{X}_ n\right]}$ and hence the bias is greater than zero.

## 3. The One-Dimensional Delta Method

Let $(Z_n)_{n \geq 1}$ sequence of r.v. that satisfies
$$
\sqrt{n}(Z_n - \theta) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,\sigma^2)
$$
for some $\theta \in \R$ and $\sigma^2 > 0$ (the sequence $(Z_n)_{n \geq 1}$ is **asymptotically normal around** $\theta$)

Let $g: \R \rightarrow \R$ be continuously differentiable at the point $\theta$. Then

* $(g(Z_n))_{n \geq 1}$ is also **asymptotically normal around** $g(\theta)$.

* More precisely,
  $$
  \sqrt{n}(g(Z_n) - g(\theta)) \xrightarrow[n\rightarrow \infty]{(d)} \mathcal{N}(0, (g'(\theta))^2 \sigma^2)
  $$

* Proof: Apply **Taylor theorem**
  $$
  \begin{aligned}
  g(Z_n) - g(\theta) &= (Z_n - \theta)g'(\theta) + {(Z_n-\theta)^2\over2 } g''(w), \quad \text{where }w \in [Z_n, \theta]\\
  &\simeq (Z_n - \theta) g'(\theta), \quad \text{since } (Z_n-\theta)^2 \approx {1\over n}\\
  \sqrt{n}(g(Z_n) - g(\theta))  &\simeq \sqrt{n}(Z_n - \theta) g'(\theta) \xrightarrow[n \rightarrow \infty]{} \mathcal{N}(0, g'(\theta)^2 \sigma^2), \quad \text{since }g'(\theta) \text{ is a deterministic number}
  \end{aligned}
  $$

> #### Exercise 32
>
> Let $(Z_ n)_{n \geq 1}$ be a sequence of random variables such that
> $$
> \sqrt{n}(Z_ n - \theta ) \xrightarrow [n \to \infty ]{(d)}Z
> $$
> for some $\theta \in \R$ and some random variable $Z$.
>
> Let $g(x) = 5x$ and define another sequence by $Y_n = g(Z_n)$.
>
> The sequence $\sqrt{n}(Y_ n - g(\theta ))$ converges. 
>
> 1. In terms of $Z$, what random variable $Y$ does it converge to?
>
> $$
> \sqrt{n}(Y_ n - g(\theta )) \xrightarrow [n \to \infty ]{(d)} \quad Y.
> $$
>
> 2. What theorem did we invoke to compute What theorem did we invoke to compute $Y$?
> 3. If $\mathsf{Var}(Z) = \sigma^2$, what is $\mathsf{Var}(Y)$? 
>
> **Answer**: 
>
> 1. $Y = 5Z$
> 2. Slutsky theorem or continuous mapping theorem
> 3. $\mathsf{Var}(Y) = 25 \sigma^2$.
>
> **Solution**: 
>
> 1. $$
>    \sqrt{n}( Y_ n - g(\theta ) ) \, =\,  \sqrt{n}(g(Z_ n) - g(\theta ) ) = \sqrt{n}(5Z_ n - 5\theta )=5 \left(\sqrt{n} (Z_ n-\theta )\right)\xrightarrow [n \to \infty ]{(d)} 5Z
>    $$
>
> 2. By the **continuous mapping theorem** because $\sqrt{n}(Z_ n - \theta ) \xrightarrow [n \to \infty ]{(d)}Z$ is a linear and hence continuous function of $Z_n$,
>    $$
>    5 \left(\sqrt{n} (Z_ n-\theta )\right)\xrightarrow [n \to \infty ]{(d)} 5Z
>    $$
>    Alternatively, since we were given that $\sqrt{n}(Z_ n - \theta ) \xrightarrow [n \to \infty ]{(d)}Z$, and $5$ converges trivially in probability to itself $5 \rightarrow 5$, we can also use **Slutsky theorem** to conclude.
>
> 3. The **asymptotic variance** of $(Y_ n)_{n \geq 1}$ is
>    $$
>    \textsf{Var}(Y)=25\textsf{Var}(Z)=25 \sigma ^2
>    $$

### Applying the Delta Method

We have
$$
\sqrt{n}(T_n - {1\over \lambda}) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,{1\over \lambda^2})
$$
Suppose $g(\theta) = {1\over \theta}$, then $g'(\theta) = -{1\over \theta^2}$. We first calculate $(g'(\theta))^2$:
$$
(g'(\theta))^2 = (g'({1\over \lambda}))^2 = \lambda^4
$$
Therefore, we obtain
$$
\sqrt{n}\left(\hat{\lambda} - \lambda\right) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}(0,\lambda^2)
$$
Note: For the Delta method to apply, $g′$ exists and is continuous at $\mathbb{E}[X]=g^{−1}(θ)$. Since $θ$ and $μ=\mathbb{E}[X]$ are unknown, for the Delta method to apply, we need to make sure **$g$ is continuously differentiable at all possible values of $\mathbb{E}[X]$ given that $θ>0$.**

Hence, for $\alpha \in (0,1)$ and when $n$ is large enough,
$$
|\hat{\lambda}  - \lambda| \leq \lambda \cdot {q_{\alpha/2} \over \sqrt{n}}
$$
with probability approximately $1-\alpha$.

However, $\left[\hat{\lambda} - {q_{\alpha/2}\lambda\over \sqrt{n}}, \hat{\lambda} + {q_{\alpha/2}\lambda\over \sqrt{n}}\right]$ CAN'T be used as an asymptotic C.I. for $\lambda$, since the expression for the left and right endpoint depends on the true parameter $\lambda$. By definition, a C.I. must be computed only using the data and other known quantities, but not the true parameter, which is unknown.

**Three Solutions:**

1. The conservative bound: we have no a priori way to bound $\lambda$, since $\lambda$ is unbounded.

   We try to find the following defined C.I. using "conservative method",
   $$
   \mathcal{I}_{cons} := \left[ \widehat{\lambda }_ n - \max _{\lambda \in (0, \infty )} \frac{q_{\alpha /2} \lambda }{\sqrt{n}} , \widehat{\lambda }_ n + \max _{\lambda \in (0, \infty )} \frac{q_{\alpha /2} \lambda }{\sqrt{n}} \right].
   $$
   Since we observe that
   $$
   \max _{\lambda \in (0, \infty ) } \frac{q_{\alpha /2} \lambda }{\sqrt{n}} = \infty
   $$
   The C.I. is actually 
   $$
   \mathcal{I}_{cons} = (-\infty , \infty ).
   $$
   It is not useful for statistical purposes because such a confidence interval gives no information about the location of the true parameter.

2. We can solve for $\lambda$:
   $$
   \begin{aligned}
   |\hat{\lambda} - \lambda| \leq {q_{\alpha/2}\lambda \over \sqrt{n}} &\iff \lambda\left(1 - {q_{\alpha/2} \over \sqrt{n}}\right)\leq \hat{\lambda} \leq \lambda \left(1 + {q_{\alpha/2} \over \sqrt{n}}\right)\\
   &\iff {\hat{\lambda}\over1+{q_{\alpha/2} \over \sqrt{n}}} \leq \lambda \leq {\hat{\lambda}\over1-{q_{\alpha/2} \over \sqrt{n}}}
   \end{aligned}
   $$
   It yields
   $$
   \mathcal{I}_{\text{solve}} = \left[\hat{\lambda}\left( 1+{q_{\alpha/2} \over \sqrt{n}}\right)^{-1},\hat{\lambda}\left( 1-{q_{\alpha/2} \over \sqrt{n}}\right)^{-1}\right]
   $$

3. Plug-in yields
   $$
   \mathcal{I}_{\text{plug-in}} = \left[\hat{\lambda}\left( 1-{q_{\alpha/2} \over \sqrt{n}}\right),\hat{\lambda}\left( 1+{q_{\alpha/2} \over \sqrt{n}}\right)\right]
   $$

## 4. Delta Method for Non-invertible $g$

Let $(Z_ n)_{n \geq 1}$ denote an asymptotically normal sequence with known asymptotic variance $1$:
$$
\sqrt{n}(Z_ n - \mu ) \xrightarrow [n \to \infty ]{(d)} N(0,1).
$$
Let
$$
\begin{aligned}
g:\mathbb {R} &\to \mathbb {R}\\
x &\mapsto \sqrt{2}\sin (x).
\end{aligned}
$$
Notice that $g$ is no invertible, but the Delta method still applies. 

By applying the **Delta method**, we conclude that

* The sequence $(g(Z_ n))_{n \geq 1}$ is **asymptotically normal**.
  * Since the Delta method states that continuously differentiable function applied to an asymptotically normal sequence of random variables is again asymptotically normal.
  * Note that the Delta method only concerns asymptotic normality, and does not conclude for finite $n$ that the random variable $g(Z_n)$ is normal (or Gaussian).
* The **asymptotic variance** of the sequence $(g(Z_ n))_{n \geq 1}$ is ${2\cos ^2(\mu )}$. 
  * First find $g'(x) = \sqrt{2}\cos (x),$ then by the Delta method, the asymptotic variance of $g(Z_ n)$ is $g'(\mu )^2 \textsf{Var}(Z)= 2\cos ^2(\mu )$.
  * Note that the asymptotic variance of $g(Z_n)$ depends on the unknown parameter $μ$, even though the asymptotic variance of $Z_n$ is known to be $1$.

Let $Z_n = \overline{X}_n$ where $X_1, ..., X_n$ are i.i.d. with mean $\mu$ and known variance $1$. Then the CLT gives
$$
\sqrt{n}(\overline{X}_ n - \mu ) \xrightarrow [n \to \infty ]{(d)} N(0,1).
$$
You estimates $θ=g(μ)$ by the consistent estimator $\hat{\theta} = g(\overline{X}_n)$. Use the “**plug-in**" method to construct a **C.I.** for $θ=g(μ)$ at **asymptotic** level $1−α$. 

From the problem we know that
$$
\sqrt{n}\left(g(\overline{X}_ n)-g(\mu )\right) \xrightarrow[n \rightarrow \infty]{(d)} \mathcal{N}\left(0,\tau ^2\right) \qquad \text {where }\, \tau ^2=\left(g'(\mu )\right)^2 \textsf{Var}(X)= \left(g'(\mu )\right)^2.
$$
This implies 
$$
\frac{\sqrt{n}}{\tau }\left(g(\overline{X}_ n)-g(\mu )\right) \xrightarrow[n \rightarrow \infty]{(d)}  \mathcal{N}(0,1)\qquad \text {where }\, \tau ^2=\left(g'(\mu )\right)^2 .
$$
We follow the usual procedure for C.I.:
$$
\mathbf{P}\left(\frac{\sqrt{n}}{\tau }\left|g(\overline{X}_ n)-g(\mu )\right|<q_{\alpha /2}\right)=1-\alpha .
$$
Manipulate the event with the probability above:
$$
\begin{aligned}
\frac{\sqrt{n}}{\tau }\left|g(\overline{X}_ n)-g(\mu )\right|<q_{\alpha /2} &\Longleftrightarrow -q_{\alpha /2}\frac{\tau }{\sqrt{n}}<g(\overline{X}_ n)-g(\mu )<q_{\alpha /2}\frac{\tau }{\sqrt{n}}\\
&\Longleftrightarrow g(\overline{X}_ n)-q_{\alpha /2}\frac{\tau }{\sqrt{n}}<g(\mu )<g(\overline{X}_ n)+q_{\alpha /2}\frac{\tau }{\sqrt{n}}
\end{aligned}
$$
Therefore, 
$$
g(\mu ) \in \left[ g(\overline{X}_ n)-q_{\alpha /2}\frac{\mid g'(\mu )\mid }{\sqrt{n}}, g(\overline{X}_ n)+q_{\alpha /2}\frac{\mid g'(\mu )\mid }{\sqrt{n}} \right]\qquad \left(\tau =\sqrt{\left(g'(\mu )\right)^2 }=\mid g'(\mu )\mid \right).
$$
Notice that any interval in terms of unknown parameter is not a C.I. To remedy this, we apply the **Plug-in** method: substitute $g'(\mu)$ with $g'(\overline{X}_n)$ via **Slutsky's theorem** and the **Continuous Mapping theorem**.

This gives
$$
g(\mu ) \in \left( g(\overline{X}_ n)-q_{\alpha /2}\frac{\mid g'(\overline{X}_ n)\mid }{\sqrt{n}}, g(\overline{X}_ n)+q_{\alpha /2}\frac{\mid g'(\overline{X}_ n)\mid }{\sqrt{n}} \right).
$$
Which is the second choice. Equivalently, by plugging in $g(x) = \sqrt{2} \sin (x)$ and $g'(x) = \sqrt{2} \cos (x)$, we obtain
$$
g(\mu) \in \left[\sqrt{2}\sin \left(\overline{X}_ n\right)- q_{\alpha /2}\frac{\sqrt{2}\lvert \cos \left(\overline{X}_ n\right)\rvert }{\sqrt{n}},\sqrt{2}\sin \left(\overline{X}_ n\right)+ q_{\alpha /2}\frac{\sqrt{2}\lvert \cos \left(\overline{X}_ n\right)\rvert }{\sqrt{n}}\right]
$$
Notice that the C.I. does not involve $g^{-1}$, the Delta method works even when $g$ is non-invertible.

## 5. Frequentist Interpretation of a Confidence Interval

Take $\mathcal{I}_{\text{plug-in}} = [0.12, 0.20]$ for example, the meaning of " $\mathcal{I}_{\text{plug-in}}$ is a C.I. of asymptotic level $95\%$" is:

If we were to **repeat** the experiment then the parameter would be in the resulting C.I. about $1-\alpha$ of time.

It does NOT mean that
$$
\lim_{n \rightarrow \infty} \mathbb{P}(\lambda \in [0.12, 0.20])\geq 0.95
$$


Problem set for Lec3-4.

# 1. Biased and unbiased estimation for variance of Bernoulli variables

Let $\,  X_1,\ldots ,X_ n \,$ be i.i.d. Bernoulli random variables, with unknown parameter $\,  p\in (0,1) \,$. The aim is to estimate the common variance of the $X_i$. Let $\overline{X}_n$ be the sample average of $X_i$. Recall the $\,  \textsf{Var}(X_ i) \,$ for Bernoulli random variables is 
$$
\,  \textsf{Var}(X_ i) \, = p(1-p)
$$
We are interested in finding an estimator for $\,  \textsf{Var}(X_ i) \,$ and propose to use 
$$
\hat V = \overline{X}_ n (1 - \overline{X}_ n).
$$
$\hat{V}$ is **consistent** because of the Law of Large Numbers (LLN) and Continuous Mapping Theorem (CMT).

Compute the bias of $\hat{V}$ and an unbiased estimator $\hat{V}'$.

> The bias of $\hat{V}$ is $\,   \mathbb E[\hat V] - \textsf{Var}(X_ i) $.
>
> First compute $\mathbb E[\hat V] $:
> $$
> \begin{aligned}
> \mathbb E[\overline{X}_ n(1-\overline{X}_ n)] &= \mathbb E[\overline{X}_ n] - \mathbb E[\overline{X}_ n^2],\\
> \mathbb E[\overline{X}_ n] &= \mathbb E\left[ \frac{1}{n} \sum _{i=1}^{n} X_ i \right] = \frac{1}{n} \sum _{i = 1}^{n} \mathbb E\left[ X_ i \right] = {}p\\
> 
> \mathbb E[\overline{X}_ n^2] &=\textsf{Var}(X_ n) + \mathbb E[X_ n]^2\\
> &= \frac{1}{n^2} \sum _{i = 1}^{n} \textsf{Var}(X_ i) + p^2\\
> &= \frac{1}{n} p(1-p) + p^2\\
> \end{aligned}
> $$
>
> $$
> \implies \mathbb E[\hat V] =p - p^2 - \frac{1}{n}p(1-p) = \frac{n-1}{n}p(1-p)
> $$
>
> and therefore the bias is 
> $$
> \,   \mathbb E[\hat V] - \textsf{Var}(X_ i) =-\frac{1}{n}p(1-p)
> $$
> We can obtain an unbiased estimator if we compensate the multiplicative bias in $\hat{V}$, so
> $$
> \hat V' = \frac{n}{n-1} \overline{X}_ n (1 - \overline{X}_ n).
> $$

# 2. Estimation of an exponential parameter

Let $X_1,\ldots ,X_ n$ be i.i.d. $\mathsf{Exp}(\lambda)$ random variables ,where $\lambda $ is unknown. 

1. What is the **distribution** of $\min _{1\le i \le n} (X_ i)$? Write down the pdf $f_{\min}(x)$.
2. Use Q1 to give an **unbiased** estimator $\hat{θ}$ for $1/λ$.
3. What is the **variance** and **quadratic risk** of unbiased estimator $\hat{\theta}$ in the previous part?
4. Compute $\mathbf{P}\left( \frac{1}{\lambda } \geq \frac{ n \min _ i X_ i}{\ln (5)} \right)$. This computation allows us to compute a confidence interval. The interpretation is as follows: Let $\alpha$ be a value such that $1-\alpha = \mathbf{P}\left( \frac{1}{\lambda } \leq \frac{ n \min _ i (X_ i)}{\ln (5)} \right).$ Based on this setup, what is the corresponding, non-asymptotic, one-sided confidence interval at level $1-\alpha$ for $1/\lambda$ ?

> 1) The cdf of $X_i$ is 
> $$
> F_ X(x)=\int _{0}^ x \lambda e^{-\lambda t} dt= 1-e^{-\lambda x}
> $$
> The cdf of $\min_i(X_i)$:
> $$
> \begin{aligned}
> \mathbf{P}\left(\min _ i (X_ i)\leq t\right)\, &=\, 1-\mathbf{P}\left(\min _ i (X_ i)\geq t\right)\\
> &=1-\left(\mathbf{P}\left(X_1\geq t\right)\right)\left(\mathbf{P}\left(X_2\geq t\right)\right)\ldots \left(\mathbf{P}\left(X_ n\geq t\right)\right)\\
> &= 1-\left(1-F_ X(t)\right)^ n \\
> &=\, 1-e^{-n\lambda x}.
> \end{aligned}
> $$
> Differentiate w.r.t. $x$ to get the pdf of $\min_i(X_i)$
> $$
> f_{\min}(x)=(n\lambda ) e^{-(n\lambda ) x}
> $$
> That is, $\min_i(X_i)$ follows an exponential distribution with parameter $n\lambda$. 
>
> As a sanity check,
> $$
> \mathbb E\left[ \min _ i (X_ i) \right]\, =\, 1/(n\lambda )<\mathbb E\left[X_ i\right]=1/\lambda \quad \text{for } n > 1
> $$
> 2) Since $\mathbb E\left[ \min _ i (X_ i) \right]\, =\, 1/(n\lambda )$, we have $\mathbb E\left[ n\min _ i (X_ i) \right]\, =\, 1/(\lambda )$. Therefore, $n \min_i(X_i)$ is an unbiased estimator of $1/\lambda$. The unbiased estimator $\hat{\theta} = n \min_i(X_i)$.
>
> 3) $ \textsf{Var}\left(n \min _ i(X_ i)\right) = n^2\textsf{Var}\left(\min _ i(X_ i)\right)\, =\, \frac{n^2}{n^2\lambda ^2}=\frac{1}{\lambda ^2}$
>
> $\text {Quadratic risk}\left(n \min _ i(X_ i)\right) = \left[\text {bias}\left(n \min _ i(X_ i)\right) \right]^2+\textsf{Var}\left(n \min _ i(X_ i)\right)=0+\, \frac{1}{\lambda ^2}\, =\, \frac{1}{\lambda ^2}$
>
> Note that the variance and quadratic risk of this estimator stay constant as $n \rightarrow \infty$
>
> 4) $ \mathbf{P}\left( \frac{1}{\lambda } \geq \frac{ n \min _ i X_ i}{\ln (5)} \right) = 4/5$
> $$
> \frac{1}{\lambda } \geq \frac{ n \min _ i X_ i}{\ln (5)} \Longleftrightarrow\min _ i X_ i\leq \frac{\ln (5)}{n\lambda }
> $$
> Hence,
> $$
> \begin{aligned}
> \mathbf{P}\left( \frac{1}{\lambda } \geq \frac{ n \min _ i X_ i}{\ln (5)} \right) &= \mathbf{P}\left(\min _ i X_ i\leq \frac{\ln (5)}{n\lambda }\right)\\
> &=1-e^{-n\lambda \left(\frac{\ln (5)}{n\lambda }\right)}\, =\, \frac{4}{5} =0.8
> \end{aligned}
> $$
> Note that when the event $\frac{1}{\lambda } \leq \frac{n \min _ i X_ i }{ \ln (5) }$ occurs, ${1\over \lambda}$ lies in the interval $\left[0,\frac{n \min _ i X_ i }{ \ln (5) }\right]$. Thus, the corresponding confidence interval at level $20\%$ is $\left[0,\frac{n \min _ i X_ i }{ \ln (5) }\right]$.

# 3. A confidence interval for Poisson variables

Let $X_1, ..., X_n$ be i.i.d. **Poisson** random variables with parameter $\lambda > 0$ and denote by $\overline{X}_n$ their empirical average,
$$
\overline{X}_ n=\frac{1}{n}\sum _{i=1}^ n X_ i.
$$

1. Find two sequences $(a_n)_{n \geq 1}$ and $(b_n)_{n \geq 1}$ such that $a_n(\overline{X}_n - b_n)$ converges in distribution to a standard Gaussian random variable $Z \sim \mathcal{N}(0,1)$.

2. Express $\,   \mathbf{P}(|Z| \leq t)  \,$ in terms of $\,   \Phi (r) = \mathbf{P}(Z \leq r)  \,$ for $t >0$.

3. Find an interval $\mathcal{I}_\lambda$ that depends on $\lambda$ and that is centered around $\overline{X}_n$ such that 
   $$
   \mathbf{P}[\mathcal{I}_{\lambda }\ni \lambda ]\to .95, \quad n \to \infty .
   $$
   (*Hint*: The $97.5\%$-quantile of the standard Gaussian distribution is $1.96$.)

4. Which of the following is a confidence interval $\mathcal{J}$ that fulfills
   $$
   \mathbf{P}[\mathcal{J}\ni \lambda ]\to .95, \quad n \to \infty .
   $$

> 1) To apply **CLT**, we need to know the mean and variance.
> $$
> \mathbb E[X_ i] = \lambda , \quad \textsf{Var}(X_ i) = \lambda .
> $$
> To make $a_n(\overline{X}_n - b_n)$ converge in distribution to a standard Gaussian random variable, we have
> $$
> \sqrt{\frac{n}{\lambda }} \left(\frac{1}{n} \sum _{i=1}^{n} X_ i - \lambda \right) \xrightarrow {(d)} Z \sim \mathcal{N}(0,1).
> $$
> Therefore,
> $$
> a_ n = \sqrt{\frac{n}{\lambda }}, \quad b_ n = \lambda .
> $$
> 2) We should first observe that by substitution in the **Gaussian integral** and **symmetry**.
> $$
> \begin{aligned}
> \mathbf{P}(Z \geq -t) &=\frac{1}{\sqrt{2 \pi }} \int _{-t}^{\infty } \exp \left(-\frac{x^2}{2}\right) \, dx\\
> &=\frac{1}{\sqrt{2 \pi }} \int _{-\infty }^{t} \exp \left(-\frac{(-x)^2}{2}\right) \, dx\\
> &= \frac{1}{\sqrt{2 \pi }} \int _{-\infty }^{t} \exp \left(-\frac{x^2}{2}\right) \, dx =\mathbf{P}(Z \leq t).
> \end{aligned}
> $$
>  Then, apply this to write 
> $$
> \begin{aligned}
> \mathbf{P}(|Z| \leq t) &=\mathbf{P}(-t \leq Z \leq t)\\
>  &= \mathbf{P}(Z \leq t) - \mathbf{P}(Z \leq -t)\\
>  &=\mathbf{P}(Z \leq t) - (1 - \mathbf{P}(Z \geq -t))\\
>  &=\mathbf{P}(Z \leq t) - 1 + \mathbf{P}(Z \leq t)\\
>  &=2 \Phi (t) - 1.
> \end{aligned}
> $$
> 3) By setting
> $$
> q = \Phi ^{-1}(0.975) = 1.96,
> $$
> we see that
> $$
> \mathbf{P}\left(\sqrt{\frac{n}{\lambda }} (\overline{X}_ n - \lambda ) \in [-q, q] \right) \to \mathbf{P}(Z \in [-q, q]) = 2 \Phi (q) - 1 = 2 \times 0.975 - 1 = 0.95.
> $$
> Hence we have
> $$
> \mathcal{I}_{\lambda } := \left[ \overline{X}_ n - 1.96 \sqrt{\frac{\lambda }{n}}, \overline{X}_ n + 1.96 \sqrt{\frac{\lambda }{n}} \right],
> $$
> where $\mathcal{I}_\lambda$ is centered about $\overline{X}_n$ and $\mathbf{P}( \lambda \in \mathcal{I}_{\lambda } ) \to 0.95$ as desired.
>
> 4)  $\overline{X}_n$ is a **consistent** estimator of $\lambda$ by the **LLN**, so $\,   \sqrt{\frac{n}{\overline{X}_ n}}(\overline{X}_ n - \lambda ) \to Z \sim \mathcal{N}(0,1) \,$ by **Slutsky's** Theorem. Hence, we can obtain an interval that does not depend on $\lambda$ as
> $$
> \,   \mathcal{J}= [\overline{X}_ n - 1.96 \sqrt{\overline{X}_ n/n},\,  \overline{X}_ n + 1.96 \sqrt{\overline{X}_ n/n}]  \,
> $$

# 5. A confidence interval for uniform distributions

Let $\,  X_1,\ldots ,X_ n \,$ be i.i.d. uniform random variables in $[0,\theta]$, for some $\theta >0$. Denote by
$$
M_ n=\max _{i=1,\ldots ,n} X_ i.
$$

1. Compute the following probabilities: 

   $\,   \mathbf{P}(M_ n \geq \theta ) $ 

   For all $0 \leq t\leq \theta: \,   \mathbf{P}(M_ n \leq \theta - t) $

2. Compute the cdf $F_n(t)$ of $n(1-M_n/\theta)$ for fixed $t \in [0,n]$ and any positive integer $n$. Then compute the limit of the cdf. $\lim_{n \rightarrow \infty} F_n(t)$.

3. Find an interval $\mathcal{I}$ of the form $\,  \mathcal I=[M_ n,M_ n+c] \,$, that does not depend on $\theta$ and such that
   $$
   \mathbf{P}[\mathcal I\ni \theta ]\to .95, \text { as } n \to \infty .
   $$
   The strategy now is to use a plug-in estimator for $\theta$ to replace it in the expression for $c$. Q1-2 suggest that we use $c$ of the form $\left(\frac{t}{n}\right) M_ n$, where $t$ ought to equal a certain value in order for $\mathbf{P}[\mathcal I\ni \theta ]\to .95$. What is the appropriate numerical value of $t$? Why can we use a plugin-estimator for the asymptotic confidence interval?

4. Compute the bias of $M_n$ as an estimator of $\theta$: $\mathbb E[M_ n] - \theta$.

> 1) Since all $X_ n \leq \theta$ almost surely, $M_ n \leq \theta $ almost surely, so
> $$
> \,   \mathbf{P}(M_ n \geq \theta )  = 0
> $$
> Let $0 \leq t\leq \theta$. Since having an upper bound on the maximum of $n$ variables $M_n$ is the same as having an upper bound on all of the variables, and the $X_i$ are independent, we can write
> $$
> \begin{aligned}
> \mathbf{P}(M_ n \leq \theta - t) &= \mathbf{P}(X_ i \leq \theta - t \text { for all } i = 1,\dots ,n)\\
> &=\prod^n_{i=1} \mathbf{P}(X_i \leq \theta-t) \quad \text{by independence}\\
> &= \left(\mathbf{P}(X_1 \leq \theta-t)\right)^n \quad \text{all }X_i \text{ have the same distribution}\\
> &=\left( {\theta - t \over \theta} \right)^n \quad \text{cdf of Uniform distribution}\\
> &=\left( {\theta - t \over \theta} \right)^n \xrightarrow[n \rightarrow \infty]{} 0
> \end{aligned}
> $$
> Hence,
> $$
> M_ n \xrightarrow {\mathbf{P}} \theta .
> $$
> 2) Let $t > 0$ and first we can write 
> $$
> n \left( 1 - \frac{M_ n}{\theta } \right) \leq t \iff M_ n \geq \theta - \theta \frac{t}{n}.
> $$
> For $n$ large enough, $t/n \leq 1$. Together with the fact that the cdf of $M_n$ does not have atoms, we can compute the cdf and its limit:
> $$
> \begin{aligned}
> \mathbf{P}\left( n \left( 1 - \frac{M_ n}{\theta } \right) \leq t \right) &=  \mathbf{P}\left( M_ n \geq \theta - \theta \frac{t}{n} \right)\\
> &=1 - \mathbf{P}\left( M_ n \leq \theta - \theta \frac{t}{n} \right)\\
> &=1 - \left( 1 - \frac{t}{n} \right)^ n \quad \text{by Q1}\\
> &\xrightarrow [n \to \infty ]{} 1- \exp(-t).
> \end{aligned}
> $$
> To obtain the limit, we used the limit formula for the **exponential**,
> $$
> \left( 1 + \frac{a}{n} \right)^ n \xrightarrow [n \to \infty ]{} \exp (a), \quad \text {for } a \in \mathbb {R}.
> $$
> Therefore, $n(1-M_ n/\theta )$ converges to an **exponential** random variable with parameter $\lambda = 1$.
> $$
> n(1-M_ n/\theta ) \xrightarrow [n \to \infty ]{\text {(D)}} \mathrm{Exp}(1),
> $$
> 3) Since we want $0.95 = 1 - \exp(-t)$, we have $t = \log(20).$
>
> From Q2 for any $t > 0$ we have that
> $$
> \mathbf{P}\left(\theta \geq M_ n + \theta \frac{t}{n}\right) \xrightarrow [n \to \infty ]{} \exp (-t)
> $$
> From Q1 we know that
> $$
> M_ n \xrightarrow {\mathbf{P}} \theta
> $$
> which is a constant. By **Slutsky's** Theorem, we can substitute $M_n$ for $\theta$ above to obtain
> $$
> \mathbf{P}\left(\theta \geq M_ n + M_ n \frac{t}{n}\right) \xrightarrow [n \to \infty ]{} \exp (-t).
> $$
> Plug in $t = \log(20)$ and we obtain
> $$
> \mathcal{I}= \left[ M_ n, M_ n + M_ n \frac{\log (20)}{n} \right]
> $$
> With this, we obtain
> $$
> \begin{aligned}
> \mathbf{P}(\mathcal{I}\ni \theta ) &= 1 - \underbrace{\mathbf{P}(\theta \leq M_ n)}_{= 0} - \mathbf{P}\left(\theta \geq M_ n + M_ n \frac{\log (20)}{n}\right)\\
> &\to 1 - \exp (-\log (20)) = 0.95.
> \end{aligned}
> $$
> 4) From Q1 we know that for $r \in [0,\theta]$,
> $$
> \mathbf{P}\left( M_ n \leq r \right) = \left( 1 - \frac{\theta - r}{\theta } \right)^ n = \left( \frac{r}{\theta } \right)^ n,
> $$
> and that the support of $M_n$ is $[0,\theta]$. Hence, the density $f_n$ of $M_n$ is 
> $$
> f_ n(r) = \left\{  \begin{aligned}  0, \quad &  r < 0 \text { or } r > \theta \\ \frac{1}{\theta } n \left( \frac{r}{\theta } \right)^{n-1}, \quad &  0 \leq r \leq \theta \\ \end{aligned} \right.
> $$
> Therefore, we can compute its expectation,
> $$
> \begin{aligned}
> \mathbb E[M_ n] &=  \int _{0}^{\theta } \frac{nr}{\theta } \left( \frac{r}{\theta } \right)^{n-1} \,  d r\\
> &= \frac{n}{(n+1)\theta ^ n} \left. r^{n+1} \right|_{0}^{\theta } = \frac{n}{n+1} \theta .
> \end{aligned}
> $$
> That means that the bias of $M_n$ is 
> $$
> \mathbb E[M_ n] - \theta = - \frac{1}{n+1} \theta .
> $$
> If we wanted, we could therefore obtain an unbiased estimator $\,   \tilde M_ n  \,$ by setting
> $$
> \tilde M_ n = \frac{n+1}{n} M_ n.
> $$

# 6. Modes of convergence

For $n \geq 2$, let $X_n$ be a random variable such that ${\mathbf{P}\left(X_ n=\frac{1}{n}\right)=1-\frac{1}{n^2}}$ and ${\mathbf{P}\left(X_ n=n\right)=\frac{1}{n^2}}$. 

1. Does $X_n$ converge in probability? $X_ n \xrightarrow [n\longrightarrow \infty ]{\mathbf{P}} ?$

2. Compute $ \lim _{n\to \infty }\mathbb E\left[X_ n\right]$ and $\lim _{n\to \infty } \textsf{Var}\left(X_ n\right)$.

> 1) $X_ n \xrightarrow [n\longrightarrow \infty ]{\mathbf{P}} 0$ in probability: It is enough to check that for every $\epsilon > 0, \mathbf{P}(|X_ n| \leq \varepsilon ) \to 1$ as $n \rightarrow \infty$, which is true since
> $$
> \begin{aligned}
> \mathbf{P}(|X_ n| \leq \varepsilon ) &= \mathbf{P}(X_ n = 1/n) \quad\text {if } n > \frac{1}{\varepsilon }\\
> &=1 - \frac{1}{n^2} \to 1 \quad \quad \text{as }n \rightarrow \infty
> \end{aligned}
> $$
> 2) The mean and its limit is
> $$
> \mathbb E\left[X_ n\right] =\frac{1}{n} \left( 1 - \frac{1}{n^2} \right) + \frac{n}{n^2} \xrightarrow {n\to \infty } 0.
> $$
> The variance and its limit is 
> $$
> \textsf{Var}\left(X_ n\right)\, =\,  \mathbb E\left[|X_ n|^2\right] = \left(\frac{1}{n}\right)^2 \left( 1 - \frac{1}{n^2} \right) + \frac{n^2}{n^2} \xrightarrow {n\to \infty } 1.
> $$
> **Remark:** Convergence in probability does not necessarily imply convergence in variance.

Let $X_n$ and $Y_n$ be two sequences of random variables. For each of the following statement, say whether it is true or false. When your answer is "false", try to think of a counter example.

1. If $\,  X_ n\xrightarrow [n\to \infty ]{\text{a.s.}} X \,$ and $\,  Y_ n\xrightarrow [n\to \infty ]{\text{a.s.}} Y \,$, then $\,  X_ n+Y_ n\xrightarrow [n\to \infty ]{\text{a.s.}} X+Y \,$
2. If $\,  X_ n\xrightarrow [n\to \infty ]{\mathbf{P}} X \,$ and $\,  Y_ n\xrightarrow [n\to \infty ]{\mathbf{P}} Y \,$, then $\,  X_ n+Y_ n\xrightarrow [n\to \infty ]{\mathbf{P}} X+Y \,$
3. If $\,  X_ n\xrightarrow [n\to \infty ]{(d)} X \,$ and $\,  Y_ n\xrightarrow [n\to \infty ]{(d)} Y \,$, then $\,  X_ n+Y_ n\xrightarrow [n\to \infty ]{(d)} X+Y \,$

> 1) True
>
> 2) True
>
> To show the convergence of $X_n + Y_n$ in probability, let $\epsilon, \delta > 0$. By definition of this mode of convergence, we can choose $n_1$ and $n_2$ such that 
> $$
> \mathbf{P}\left(| X_ n - X | > \frac{\varepsilon }{2} \right) <  \frac{\delta }{2} \quad \text {if } n \geq n_1 \\\mathbf{P}\left(| Y_ n - Y | > \frac{\varepsilon }{2} \right) < \frac{\delta }{2} \quad \text {if } n \geq n_2
> $$
> Hence, by triangle inequality and sub-additivity of $\mathbf{P}$, if $n \geq \max\{n_1, n_2\}$, we have
> $$
> \mathbf{P}\left( |X_ n + Y_ n - (X + Y)| > \varepsilon \right) \leq \mathbf{P}\left( |X_ n - X| > \frac{\varepsilon }{2} \right) + \mathbf{P}\left( |Y_ n - Y| > \frac{\varepsilon }{2} \right) < \frac{\delta }{2} + \frac{\delta }{2} = \delta ,
> $$
> which shows the desired convergence.
>
> 3) False
>
> The intuition is that random variables can be **coupled** in strange ways to make this statement false. 
>
> E.g. let $Z$ and $Z_1, Z_2, ...$ be a sequence of i.i.d. standard Gaussian RVs $\mathcal{N}(0,1)$. Using $(Z_n)$, we now define a pair of sequences $(X_n)$ and $(Y_n)$: let $X_n = Z_n$ and $Y_n = -Z_n$, let $X=Y=Z$. It is clear that $X_n \rightarrow Z$ in probability;and by symmetry of the Gaussian, $Y_n \rightarrow Z$ in probability as well. However, $X_n + Y_n = 0$, so the sequence $(X_n + Y_n)$ converges to the constant $0$ in probability.

# 7. Examples of convergence

**Rescaled Poisson RVs:**

For $m \leq 1$, let $X_n$ be a Poisson random variable with parameter $1/n$. Compute  $\mathbf{P}(X_n = 0)$. What can you conclude?

> The pmf of the Poisson random variable is given by
> $$
> \mathbf{P}(X=x, \lambda) = {\lambda^x \exp(-\lambda) \over x!}
> $$
> we compute
> $$
> \mathbf{P}(X_n = 0) = \left({1 \over n}\right)^0 {1 \over 0!}\exp\left( - {1 \over n}\right) = \exp\left(-{1\over n}\right)
> $$
> As $n \rightarrow \infty$, $\exp\left(-{1\over n}\right)$ tends to $1$, therefore $X_n \rightarrow 0$ in probability. 
>
> Moreover, the same calculation tells us the probability $\mathbf{P}(nX_n = 0)$, therefore we also obtain $nX_n \rightarrow 0$ in probability.
>
> However, $\mathbb{E}\left[(xX_n)^2\right]$ does not go to zero.
> $$
> \mathbb E[(n X_ n)^2] = n^2 \mathbb E[X_ n^2]  = n^2 \left(\mathsf{Var}(X_n) + (\mathbb{E}(X_n))^2\right)\\
> = n^2 \left( \frac{1}{n}+ \frac{1}{n^2} \right) = n +1 \to \infty .
> $$
> We can conclude that $X_n \rightarrow 0$ and $nX_n \rightarrow 0$ in probability, but $\mathbb{E}\left[(xX_n)^2\right]$ does not converge.
>
> **Remark**: $nX_n$ does NOT "**converge in $L^2$- norm**". 
>
> A sequence of random variable $(Y_n)_{n \geq 1}$ converges in $L^2$-norm to a random variable $Y$, denoted by $\, Y_ n\xrightarrow [n\to \infty ]{L^2}Y,\,$ if $\,  \lim _{n\to \infty } \, \mathbb E\left[\mid Y_ n-Y\mid ^2\right]=0.\, \,$ Moreover, if $\, Y_ n\xrightarrow [n\to \infty ]{L^2} Y,\,$ then $\,  \lim _{n\to \infty } \, \mathbb E\left[\mid Y_ n\mid ^2\right]= \mathbb E[Y^2].\, \,$ Hence, in this example, since $\, \mathbb E[(n X_ n)^2]\, \xrightarrow [n\to \infty ]{} \infty ,\,$ $nX_n$ does not converge in $L^2$-norm.

**Limit of rescaled Binomials:**

Let $X_n$ be a binomial random variable with parameters $n$ and $p=\lambda/n$, where $\lambda$ is a fixed positive number. 

Let $k \in \N$ be fixed. As $n \rightarrow \infty$, the pmf $\mathbf{P}(X_n=k)$ converges to a number that only depends on $\lambda$ and $k$. What is the limit: $\,  \lim _{n \to \infty } \mathbf{P}(X_ n = k) \,$ ? 

> This pmf of the Binomial random variable $X_n$ is given by
> $$
> \mathbf{P}(X_ n = k) = {n \choose k} \left( \frac{\lambda }{n} \right)^{k} \left( 1 - \frac{\lambda }{n} \right)^{n-k}, \quad 0 \leq k \leq n, k \in \N
> $$
> Write the binomial coefficient as
> $$
> {n \choose k} = \frac{1}{k!} \frac{n!}{(n-k)!},
> $$
> We have
> $$
> \mathbf{P}(X_ n = k) = \frac{\lambda ^ k}{k!} \underbrace{\left( 1 - \frac{\lambda }{n} \right)^ n}_{=:A_ n} \underbrace{\left( 1 - \frac{\lambda }{n} \right)^{-k}}_{=:B_ n} \underbrace{\frac{n!}{n^ k (n-k)!}}_{=:C_ n}.
> $$
> Term $A_n$ can be handled by the exponential formula
> $$
> \left(1 + \frac{a}{n}\right)^ n \xrightarrow [n \to \infty ]{} \exp (a), \quad \text {for } a \in \mathbb {R}.
> $$
> Hence,
> $$
> A_ n \to \exp (-\lambda ), \quad \text {as } n \to \infty .
> $$
> Since $k$ is fixed and $\lambda/n \rightarrow 0$, we have $B_n \rightarrow 1$. Finally, write
> $$
> \begin{aligned}
> C_ n &= \frac{n!}{n^ k (n-k)!} = 1 \times \left( \frac{n-1}{n} \right) \times \dots \times \left( \frac{n - k + 1}{n} \right)\\
> &= 1 \times \left( 1 - \frac{1}{n} \right) \times \dots \times \left( 1 - \frac{k-1}{n} \right) \to 1, \quad \text {as } n \to \infty .
> \end{aligned}
> $$
> Combined, we get that
> $$
> \mathbf{P}(X_ n = k) \to \frac{\lambda ^ k}{k!} \exp (-\lambda ).
> $$
> Since that entails the convergence of the cumulative mass function,  $\,   \mathbf{P}(X_ n \leq m)  \,$ , for any $m \in \Z$ as well, we have just shown that $X_n$ converges in distribution to a Poisson distribution with parameter $\lambda$



# Recitation 9. (Review) Multivariate Gaussian

The PDF of the **bivariate Gaussian distribution** is
$$
f(x, y, \mu_x, \mu_y, \sigma_x^2, \sigma_y^2, \rho) = {1 \over 2\pi \sigma_x \sigma_y \sqrt{1 - \rho^2}} \exp\left[ -{1\over 2(1-\rho^2)} \left( {(x - \mu_x)^2 \over \sigma_x^2} + {(y - \mu_y)^2 \over \sigma_y^2}- {2\rho(x - \mu_x)(y - \mu_y) \over \sigma_x \sigma_y} \right) \right].
$$
The PDF for the general $d$​​​​​-dimensional **multivariate Gaussian distribution** with mean $\mathbf{\mu}$​​​​​ and covariance matrix $\mathbf{\Sigma}$​​​​​ is given by
$$
f(\mathbf{x};\mu, \mathbf{\Sigma}) = {1\over (2\pi)^{d/2} \det(\Sigma)^{1/2}} \exp \left[ -{1\over 2} (\mathbf{x} - \mu)^T \mathbf{\Sigma}^{-1} (\mathbf{x} - \mu) \right].
$$

1. Show that the marginal distribution of $X$​​​ and $Y$​​​ are $\mathcal{N}(\mu_x, \sigma_x^2)$​​​ and $\mathcal{N}(\mu_y, \sigma_y^2)$​​.
2. What is $\rm{Cov}(X,Y)$?
3. Show that $X$ and $Y$ can equivalently be defined in the following way. Let $Z_1$ and $Z_2$ be independent $\mathcal{N}(0,1)$ random variables. Then $X = \sigma_x Z_1 + \mu_x$ and $Y = \sigma_y[\rho Z_1 + \sqrt{1-\rho^2} Z_2]+ \mu_y$.
4. Show that the multivariate case in (2) reduces to the PDF in (1) when $d=2$.

> **Solution:**
>
> 1. Notice that
>    $$
>    {1\over (1-\rho^2)} \left( {(x- \mu_x)^2\over \sigma_x^2}  + {(y-\mu_y)^2 \over \sigma_y^2}-{ 2 \rho (x - \mu_x)(y - \mu_y) \over \sigma_x \sigma_y}\right) = {(x - a(y))^2 \over (1-\rho^2)\sigma_x^2}  + {(y - \mu_y)^2 \over \sigma_y^2}
>    $$
>    where $a(y) = \mu_x + \rho {\sigma_x \over \sigma_y}(y - \mu_y)$​​. Therefore, the marginal distribution of $y$ is
>    $$
>    \begin{aligned}
>    f(y; \mu_x, \mu_y, \sigma_x^2, \sigma_y^2, \rho)&= \int{1\over 2\pi \sigma_x \sigma_y \sqrt{1-\rho^2}} \exp\left[ - {1\over 2(1-\rho^2)} \left({(x-\mu_x)^2 \over \sigma_x^2 } + {(y-\mu_y)^2 \over \sigma_y^2 }-{2\rho(x-\mu_x)(y-\mu_y) \over \sigma_x \sigma_y } \right)\right] dx\\
>    &={1\over \sqrt{2\pi}\sigma_y} \exp \left[ -{1 \over2 } { (y - \mu_y)^2\over \sigma_y^2} \right] \int {1\over \sqrt{2\pi} \sigma_x \sqrt{1-\rho^2}} \exp \left( - {1\over 2} {(x - a(y))^2 \over (1-\rho^2)\sigma_x^2} \right)dx\\
>    &= {1\over \sqrt{2\pi}\sigma_y} \exp\left[ -{1\over 2} {(y-\mu_y)^2 \over \sigma_y^2} \right].
>    \end{aligned}
>    $$
>    This means $Y \sim \mathcal{N}(\mu_y, \sigma_y^2), \ \ X \sim \mathcal{N}(\mu_x, \sigma_x^2)$​​​​.
>
> 2. We have
>    $$
>    \text{Cov}(X,Y) = \mathbb{E}(XY) - \mathbb{E}(X) \mathbb{E}(Y)
>    $$
>    Since the marginal distributions are $\mathcal{N}(\mu_x, \sigma_x^2)$​​​ and $\mathcal{N}(\mu_y, \sigma_y^2)$​​​, we know that $\mathbb{E}[X] = \mu_x$​​​ and $\mathbb{E}[Y] = \mu_y$​​​. Using the answer to (1), the random variable $X|Y$​​ has PDF
>    $$
>    f(X=x | Y=y) \propto \exp \left[ -{1\over 2} {(x - a(y))^2 \over \sigma_x^2(1-\rho^2)} \right]
>    $$
>    where again $a(y) = \mu_x + \rho {\sigma_x \over \sigma_y}(y - \mu_y)$. Thus, $X|Y \sim \mathcal{N}(a(Y), \sigma_x^2(1-\rho^2))$.
>
>    The law of total expectation yields
>    $$
>    \begin{aligned}
>    \mathbb{E}[XY] &= \mathbb{E}[\mathbb{E}(XY|Y)] \\
>    &= \mathbb{E}\left[ Y \left( \mu_x + \rho {\sigma_x \over \sigma_y}(Y - \mu_y) \right)\right]\\
>    &= \mathbb{E}\left[Y \mu_x + \rho {\sigma_x \over \sigma_y}(Y^2 - Y \mu_y)\right]\\
>    &= \mu_y \mu_x  +\rho {\sigma_x \over \sigma_y} (\sigma_x^2 + \mu_y^2 - \mu_y^2)\\
>    &= \mu_y \mu_x + \rho \sigma_x \sigma_y
>    \end{aligned}
>    $$
>    Therefore, the covariance is
>    $$
>    \text{Cov}(X,Y) = \mathbb{E}(XY) - \mathbb{E}(X) \mathbb{E}(Y) = \mu_y \mu_x + \rho \sigma_x \sigma_y -\mu_y \mu_x = \rho \sigma_x \sigma_y
>    $$
>    The correlation is
>    $$
>    \text{Corr}(X,Y) = \rho
>    $$
>
> 3. Show that marginal distribution of $X$ and $Y$ are $\mathcal{N}(\mu_x, \sigma_x^2)$ and $\mathcal{N}(\mu_y, \sigma_y^2)$​.
>
>    The expectations:
>    $$
>    \begin{aligned}
>    \mathbb{E}(X) &= \mathbb{E}(\sigma_xZ_1 + \mu_X) = \mathbb{E}(\sigma_xZ_1)+ \mathbb{E}(\mu_x) = \mu_x\\
>    \mathbb{E}(Y) &= \mathbb{E}(\sigma_y(\rho Z_1 + \sqrt{1-p^2}Z_2 )+ \mu_y) = \mathbb{E}(\sigma_y \rho Z_1)  +\mathbb{E}(\sigma_y \sqrt{1-\rho^2}Z_2) + \mathbb{E}(\mu_y) = \mu_y
>    \end{aligned}
>    $$
>    The variances:
>    $$
>    \begin{aligned}
>    \text{Var}(X) &= \text{Var}(\sigma_x Z_1 + \mu_x)\\
>    &= \text{Var}(\sigma_x Z_1)\\
>    &= \sigma_x^2 \text{Var}(Z_1)\\
>    &= \sigma_x^2\\
>    \text{Var}(Y) &= \text{Var}(\sigma_y\rho Z_1  +\sigma_y \sqrt{1-\rho^2}Z_2)\\
>    &= \text{Var}(\sigma_y \rho Z_1) + \text{Var}(\sigma_y \sqrt{1-\rho^2} Z_2)\\
>    &= \sigma_y^2 \rho^2 + \sigma_y^2(1-\rho^2)\\
>    &= \sigma_y^2
>    \end{aligned}
>    $$
>    The covariance:
>    $$
>    \begin{aligned}
>    \text{Cov}(X,Y) &= \text{Cov}(\sigma_x Z_1 +\mu_x, \sigma_y (\rho Z_1 + \sqrt{1-\rho^2} Z_2) + \mu_y)\\
>    &= \text{Cov}(\sigma_x Z_1, \sigma_y \rho Z_1 + \sigma_y\sqrt{1-\rho^2} Z_2)\\
>    &=\text{Cov}(\sigma_x Z_1 , \sigma_y \rho Z_1) + \text{Cov}(\sigma_x Z_1 ,\sigma_y\sqrt{1-\rho^2} Z_2)\\
>    &= \sigma_x \sigma_y \rho \text{Cov}(Z_1, Z_1) + \sigma_x \sigma_y \sqrt{1-\rho}\ \text{Cov}(Z_1, Z_2)\\
>    &= \sigma_x \sigma_y \rho \text{Var}(Z_1) + 0\\
>    &= \sigma_x \sigma_y \rho 
>    \end{aligned}
>    $$
>    As we see, this completely specifies the parameters of the bivariate Gaussian distribution, and therefore $(X, Y)$​​ has the distribution specified in (1).
>
> 4. First, recall that for a matrix
>    $$
>    A = \begin{pmatrix}a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}
>    $$
>    we have
>    $$
>    \det(A) = a_{11} a_{22} - a_{21} a_{22}, \quad A^{-1} = {1\over \det(A)}\ \begin{pmatrix}a_{22} & -a_{12} \\ -a_{21} & a_{11} \end{pmatrix}
>    $$
>    When $d=2$​​, let $x = \begin{pmatrix}X \\ Y \end{pmatrix}$​​ , $\mu = \begin{pmatrix}\mu_x \\ \mu_y \end{pmatrix}$​​, 
>
>    Let
>    $$
>    \mathbf{\Sigma} = \begin{pmatrix}\text{Cov}(X,X) & \text{Cov}(X,Y) \\ \text{Cov}(Y,X) & \text{Cov}(Y,Y)\end{pmatrix}= \begin{pmatrix}\sigma_x^2 & \rho\sigma_x \sigma_y \\ \rho\sigma_x \sigma_y & \sigma_y^2\end{pmatrix}
>    $$
>    which is the covariance matrix in the bivariate case. Its inverse is
>    $$
>    \mathbf{\Sigma}^{-1} = {1\over (1-\rho)^2 \sigma_x^2 \sigma_y^2}\ \begin{pmatrix}\sigma_y^2 & -\rho\sigma_x \sigma_y \\ -\rho\sigma_x \sigma_y & \sigma_x^2\end{pmatrix}
>    $$
>    where $(1-\rho)^2 \sigma_x^2 \sigma_y^2 = \det(\mathbf{\Sigma})$​​​​​. 
>
>    Let $\mathbf{X}=(X,Y)^T$ be​ the two-dimensional multivariate Gaussian random vector, and $\mathbf{x} =(x,y)^T$. Then,
>    $$
>    \begin{aligned}
>    (\mathbf{x} - \mu)^T \mathbf{\Sigma}^{-1} (\mathbf{x} - \mu) &= \left( \begin{pmatrix}X \\ Y \end{pmatrix} - \begin{pmatrix}\mu_x \\ \mu_y \end{pmatrix}\right)^T {1\over (1-\rho)^2 \sigma_x^2 \sigma_y^2}\ \begin{pmatrix}\sigma_y^2 & -\rho\sigma_x \sigma_y \\ -\rho\sigma_x \sigma_y & \sigma_x^2\end{pmatrix} \left( \begin{pmatrix}X \\ Y \end{pmatrix} - \begin{pmatrix}\mu_x \\ \mu_y \end{pmatrix}\right)\\
>    &= {1\over (1-\rho)^2 \sigma_x^2 \sigma_y^2} \begin{pmatrix}X - \mu_x \\ Y - \mu_y \end{pmatrix}^T \ \begin{pmatrix}\sigma_y^2 (x - \mu_x) & -\rho\sigma_x \sigma_y (y - \mu_y) \\ -\rho\sigma_x \sigma_y(x - \mu_x) & \sigma_x^2(y - \mu_y)\end{pmatrix}\\
>    &= -{1\over 2(1- \rho^2)} \left[ {(x- \mu_x)^2\over \sigma_x^2}  + {(y-\mu_y)^2 \over \sigma_y^2}-{ 2 \rho (x - \mu_x)(y - \mu_y) \over \sigma_x \sigma_y} \right]
>    \end{aligned}
>    $$



# 1. Introduction and probability

* The central dogma in probability and statistics

![central-dogma](../assets/images/central-dogma.png)

* Modelling assumptions:

  Coming up with a model consists of making assumptions on the observations $R_i, i = 1, . . . ,n$ in order to draw statistical conclusions. Here are the assumptions we make:

  1. Each $R_i$ is a random variable.
  2. Each of the r.v. $R_i$ is ____ distributed with parameter $p$.
  3. $R_1, ..., R_n$ are mutually independent.

* Independent and identically distributed (i.i.d)

  A collection of random variable $X_1, ...,X_n$ are i.i.d if 

  1. each $X_i$ follows a distribution $\mathbf{P}_i$, all those distributions $\mathbf{P}_i$ are the same, and 
  2. $X_i$ area (mutually) independent.




Problem set for Lec10.

# 1. Covariance

Compute the $\mathsf{Cov}(X,Y)$ in the following condition.

1. $X,Y$ have the joint probability density function $\,  f(x, y) = 1 , 0 < x < 1, x < y< x+1$. What is the $\mathsf{Cov}(X,Y)$?
2. $X \sim f(x) = \frac{1}{2b} e^{-|x|/b},\  x \in \mathbb {R},\  b > 0 $ and $Y = \textsf{sign}(X)$.
3. $\,  X \sim \textsf{Unif}(0,1) \,$ And given $Y \sim \textsf{Unif}(x,1) $.

> **Solution:** 
>
> 1. To find $\,  \textsf{Cov}(X, Y) = \mathbb E[XY] - \mathbb E[X]\mathbb E[Y] \,$, we need to find out the expectations of $X,Y,$ and $XY$. 
>
>    From the joint distribution, we can derive the marginal distribution: $\,  f_{X}(x) = \int _{x}^{x+1} 1 \  dy = \left.y\right|_{x}^{x+1} = 1 , x \in (0,1)$, and the conditional distribution $f(y|x) = \frac{f(x, y)}{f(x)} = 1, y \in (x,x+1)$.
>
>    On one hand, we have $\mathbb E[X] = \frac{1}{2}$: since $f_X(x)$ does not depend on $x$, this describes the density of a uniform random variable over $[0,1]$. On the other hand, for the mean of $Y$:
>    $$
>    \begin{aligned}
>    \mathbb E[Y\rvert {X}] &= \int _{x}^{x+1} y\ f(y|x)\  dy\\
>    &= \int _{x}^{x+1} y\  dy\\
>    &= \left.\frac{y^2}{2} \right|_{x}^{x+1}\\
>    &=  \frac{2x+1}{2}
>    \end{aligned}
>    $$
>    According to the **law of iterated expectations**,
>    $$
>    \begin{aligned}
>    \mathbb E[Y] &= \mathbb E[\mathbb E[Y\rvert {X}]]\\
>    &= \mathbb E[\frac{2X+1}{2}]\\
>    &= \int _{0}^1 \frac{2x+1}{2} \  dx\\
>    &= 1\\
>    \mathbb E[XY] &= \int _{0}^{1}x\ \mathbb{E}[X|Y]dx\\
>    &= \int _{0}^{1}x \left[\int _ x^{x+1} y\  dy\right]dx\\
>    &= \int _0^1 x\left.\frac{y^2}{2} \right|_{x}^{x+1} dx\\
>    &= \int _0^1\frac{2x^2+x}{2} \  dx\\
>    &= {7 \over 12}
>    \end{aligned}
>    $$
>    Therefore,
>    $$
>    \textsf{Cov}(X, Y) = \mathbb E[XY] - \mathbb E[X]\mathbb E[Y] = \frac{7}{12} - \frac{1}{2}\times 1 = \frac{1}{12}
>    $$
>
> 2. By symmetry, $ \mathbb E[X] = \int _{-\infty }^{\infty }\frac{x}{2b} e^{-|x|/b}\  dx = 0 $. $\mathbb E[Y] = (-1)\cdot P(X < 0) + 1\cdot P(X>0) = -\frac{1}{2} + \frac{1}{2} = 0 $.
>    $$
>    \begin{aligned}
>     \textsf{Cov}(X, Y) = \mathbb E[XY] &= \int _{-\infty }^{\infty } \frac{x\cdot \textsf{sign}(x)}{2b} e^{-|x|/b}\  dx\\
>     &= \int _0^{\infty } \frac{x}{b} e^{-x/b}\  dx
>    \end{aligned}
>    $$
>    We can think of this as the expectation of an **exponential** random variable $Z$ with parameter ${1\over b}$.
>    $$
>     \int _0^{\infty } \frac{x}{b} e^{-x/b}\  dx = \mathbb E[Z]=b 
>    $$
>    where $Z \sim \textsf{Exp}(\frac{1}{b})$.
>
> 3. To find $\,  \textsf{Cov}(X, Y) = \mathbb E[XY] - \mathbb E[X]\mathbb E[Y] \,$, we need to find out the expectations of $X,Y,$ and $XY$. 
>
>    Since $f(x, y) = f(y\rvert {x})f(x) = \frac{1}{1-x}$, we can obtain the expectation of $XY$:
>    $$
>    \begin{aligned}
>    \mathbb E[XY] &= \int _0^1\int _ x^1 \frac{1}{1-x}\cdot xy \  dydx\\
>    &= \int _0^1\frac{x}{1-x}\cdot \left.\frac{y^2}{2}\right|_{x}^1 \  dx\\
>    & =\int _0^1\frac{x}{1-x}( \frac{1}{2} - \frac{x^2}{2} ) \  dx\\
>    &= \frac{1}{2} \int _0^1(x+ x^2) \  dx\\
>    &= {5\over 12}
>    \end{aligned}
>    $$
>    And we compute other expectations
>    $$
>    \mathbb E[X] = \frac{1}{2}, \quad \mathbb E[Y\rvert {X}] = \frac{X+1}{2} 
>    $$
>    By the **law of iterated expectations**
>    $$
>    \mathbb E[Y] = \mathbb E[\mathbb E[Y\rvert {X}]] = \mathbb E[\frac{X+1}{2}] = \int _{0}^{1} \frac{x+1}{2} \  dx = \frac{3}{4}
>    $$
>    Therefore, we have the covariance
>    $$
>    \textsf{Cov}(X,Y) = \mathbb E[XY] - \mathbb E[X]\mathbb E[Y] = \frac{5}{12} - \frac{1}{2}\times \frac{3}{4} = \frac{1}{24}
>    $$

# 2. A Simple Singular Covariance Matrix

Suppose $\mathbf X$ is a random vector, where $\mathbf{X}= (X^{(1)},\ldots ,X^{(d)})^ T$, with mean $0$ and covariance matrix $\mathbf{v}\mathbf{v}^ T$, for some vector $\mathbf{v}\in \mathbb {R}^ d$.

1. If $d >1$, is the covariance matrix $\mathbf{v}\mathbf{v}^ T$ invertible?

2. Let $\mathbf u$ be a vector in $\R^d$ such that $\mathbf{u}\perp \mathbf{v}$, i.e. $ \mathbf{u}\cdot \mathbf{v}=\mathbf{u}^ T \mathbf{v}=\mathbf{v}^ T \mathbf{u}=0$.

   Find the variance of $\mathbf{u}^ T\mathbf{X}$.

3. Let $\overline{\mathbf{v}} = \frac{\mathbf{v}}{\|  \mathbf{v}\| }$ (i.e. $\overline{\mathbf{v}}$ is the normalized version of $\mathbf{v}$). What is the variance of $ \overline{\mathbf{v}}^ T \mathbf{X}$?

4. Suppose we observe $n$ independent copies of $\mathbf{X}$ and call them $\mathbf{X}_1, ..., \mathbf{X}_ n$. What is the asymptotic distribution of $\overline{\mathbf{X}}_ n=\frac{\sum _{i=1}^ n \mathbf{X}_ i}{n} $?

5. Let $\mathbf{Y}_ i= \overline{\mathbf{v}}(\overline{\mathbf{v}}^ T\mathbf{X}_ i)$, or equivalently $\overline{\mathbf{v}}(\overline{\mathbf{v}}\cdot \mathbf{X}_ i)=(\overline{\mathbf{v}}\cdot \mathbf{X}_ i)\overline{\mathbf{v}}$, where $\overline{\mathbf{v}} = \frac{\mathbf{v}}{\|  \mathbf{v}\| }$ is the same as in part (3). We will compare the asymptotic distribution of $\overline{X}_n$ you obtain in part (4) to the asymptotic distribution of $\overline{Y}_n$ where $\overline{\mathbf{Y}}_ n=\frac{\sum _ i^ n \mathbf{Y}_ i}{n}$. What is the expectation $\mathbb E[\mathbf{Y}_i]$ of $\mathbf{Y}_i$? Find the covariance matrix $\Sigma _{\mathbf{Y}_ I}$ of $\mathbf{Y}_i$ in terms of the vector $\mathbf{v}$.

> **Solution:**
>
> 1. For $d > 1$, the matrix $\mathbf{v}\mathbf{v}^ T$ where $\mathbf{v}$ is a vector in $\R^d$ is not invertible.
>
>    Take an example in $2$ dimensions
>    $$
>    \mathbf{v}=\begin{pmatrix} 1\\ 0\end{pmatrix} \implies \mathbf{v}\mathbf{v}^ T\, =\, \begin{pmatrix} 1& 0\\ 0& 0\end{pmatrix}
>    $$
>    The matrix $\begin{pmatrix} 1& 0\\ 0& 0\end{pmatrix}$ is not invertible. One way to see this is that its determinant is $1(0)-(0)(0)= 0$. Another way to see it is that for any $2\times 2$ matrix $\begin{pmatrix}  a& b\\ c& d\end{pmatrix}$:
>    $$
>    \begin{pmatrix} 1& 0\\ 0& 0\end{pmatrix}\begin{pmatrix}  a& b\\ c& d\end{pmatrix} = \begin{pmatrix} a& b\\ 0& 0\end{pmatrix}\neq \begin{pmatrix} 1& 0\\ 0& 1\end{pmatrix}.
>    $$
>    In fact, the above argument works in general after a change of variables. Given $\mathbf{v}\in \mathbb {R}^ d$, change coordinates of $\R^d$ so that the first axis points in the direction of $\mathbf{v}$ (and so that $\mathbf{v}$ has unit length). In this new coordinate system, $\mathbf{v}$ can be rewritten as $\begin{pmatrix} 1\\ 0\\ \vdots \\ 0\end{pmatrix},$ and the matrix
>    $$
>    \begin{pmatrix} 1\\ 0\\ \vdots \\ 0\end{pmatrix}\begin{pmatrix} 1& 0& \ldots & 0\end{pmatrix} = \begin{pmatrix} 1& 0& \ldots & 0\\ 0& & & \\ \vdots & & \ddots & \vdots \\ 0& & \ldots & 0\end{pmatrix}
>    $$
>    is not invertible because no $d \times d$ matrix when multiplied by it will give the identity matrix.
>
> 2. Given two vectors, $\mathbf{u, X} \in \R^d$, the inner product $\mathbf{u^T X}$ is a scalar, and its variance is also a scalar. Using the covariance matrix formula, we get
>    $$
>    \begin{aligned}
>    \textsf{Var}(\mathbf{u}^ T \mathbf{X}) \, =\, \textsf{Cov}(\mathbf{u}^ T \mathbf{X}) &=\mathbf{u}^ T \textsf{Cov}(\mathbf{X}) \mathbf{u}\\
>    &=  \mathbf{u}^ T(\mathbf{v}\mathbf{v}^ T)\mathbf{u}\\
>    &=(\mathbf{u}^ T\mathbf{v})(\mathbf{v}^ T\mathbf{u})\, \\
>    &=\, 0.
>    \end{aligned}
>    $$
>
> 3. Similarly,
>    $$
>    \begin{aligned}
>    \textsf{Var}(\overline{\mathbf{v}}^ T \mathbf{X})\, =\,  \textsf{Cov}(\overline{\mathbf{v}}^ T \mathbf{X}) &= \overline{\mathbf{v}}^ T \textsf{Cov}(X) \overline{\mathbf{v}}\\
>    &=  \left(\frac{\mathbf{v}}{\left\|  \mathbf{v} \right\| }\right)^ T(\mathbf{v}\mathbf{v}^ T)\left(\frac{\mathbf{v}}{\left\|  \mathbf{v} \right\| }\right)\\
>    &= \frac{(\mathbf{v}^ T\mathbf{v})(\mathbf{v}^ T\mathbf{v})}{\left\|  \mathbf{v} \right\| ^2}\,\\
>    &=\, \left\|  \mathbf{v} \right\| ^2.
>    \end{aligned}
>    $$
>
> 4. By multivariate CLT,
>    $$
>    \sqrt{n}(\overline{\mathbf{X}}_ n - \mathbf{0}) \xrightarrow [n\rightarrow \infty ]{(d)} \mathcal{N}(\mathbf{0}, \mathbf{v}\mathbf{v}^ T)
>    $$
>    However, $\mathbf{v}\mathbf{v}^ T$ is not invertible, so the pdf of $ \mathcal{N}(\mathbf{0}, \mathbf{v}\mathbf{v}^ T)$ is not given by the usual formula that involves the inverse of the determinant of the covariance matrix of the multivariate Gaussian variable.
>
> 5. The expectation $\mathbb E[\mathbf{Y}_ i]$ is
>    $$
>    \mathbb E[\mathbf{Y}_ i] = \mathbb E[\overline{\mathbf{v}}(\overline{\mathbf{v}}^ T\mathbf{X}_ i)] = \overline{\mathbf{v}}\overline{\mathbf{v}}^T \  \mathbb E[\mathbf{Y}_ i]  = 0
>    $$
>    which is a zero vector in $\R^d$.
>
>    Since $ \mathbf{Y}_ i= \overline{\mathbf{v}}(\overline{\mathbf{v}}^ T\mathbf{X}_ i)$, the covariance matrix of $\mathbf{Y}_i$ is
>    $$
>    \begin{aligned}
>    \textsf{Cov}(\mathbf{Y}_ i) &=\overline{\mathbf{v}}\overline{\mathbf{v}}^ T\textsf{Cov}(\mathbf{X}_ i)(\overline{\mathbf{v}}\overline{\mathbf{v}}^ T)^ T\\
>    &=\overline{\mathbf{v}}\overline{\mathbf{v}}^ T\mathbf{v}\mathbf{v}^ T(\overline{\mathbf{v}}\overline{\mathbf{v}}^ T)^ T\\
>    &= \frac{\mathbf{v}(\mathbf{v}^ T\mathbf{v}\mathbf{v}^ T\mathbf{v})\mathbf{v}^ T }{\left\|  \mathbf{v} \right\| ^4}\, \\
>    &=\, \mathbf{v}\mathbf{v}^ T.
>    \end{aligned}
>    $$
>    This implies
>    $$
>    \sqrt{n}(\overline{\mathbf{Y}}_ n - \mathbf{0}) \, =\,  \sqrt{n}(\overline{\mathbf{Y}}_ n ) \xrightarrow [n\rightarrow \infty ]{(d)} \mathcal{N}(\mathbf{0}, \textsf{Cov}(\mathbf{Y}_ i))=\mathcal{N}(\mathbf{0},\mathbf{v}\mathbf{v}^ T).
>    $$
>    Observe that $\mathcal{N}(\mathbf{0},\mathbf{v}\mathbf{v}^ T)$ is also the asymptotic distribution of $\overline{X}_n$. 

# 3. Asymptotic Variance

Let $X_1,\ldots ,X_ n\stackrel{i.i.d.}{\sim }\mathcal{N}(0,\sigma ^2)$, for some $\sigma^2 > 0$. Let
$$
\widehat{\sigma ^2}=\frac{1}{n}\sum _{i=1}^ n X_ i^2, \quad \text {and} \quad \widetilde{\sigma ^2}=\frac{1}{n}\sum _{i=1}^ n(X_ i-\overline{X}_ n)^2.
$$
Argue that both proposed estimators $\widehat{\sigma ^2}$ and $ \widetilde{\sigma ^2}$ below are consistent and asymptotically normal.

Then, give their asymptotic variances $V(\widehat{\sigma ^2})$ and $V(\widetilde{\sigma ^2})$ and decide if one of them is always bigger than the other.

> **Solution:**
>
> Note that 
> $$
> \widehat{\sigma ^2} = \overline{Y}_ n, \quad \text {for } Y_ i = X_ i^2.
> $$
> By the **LLN**,
> $$
> \overline{Y}_ n \xrightarrow [n \to \infty ]{\mathbf{P}} \mathbb E[Y_1] = \sigma ^2.
> $$
> By the **CLT**,
> $$
> \sqrt{n} (\overline{Y}_ n - \sigma ^2) \sim \mathcal{N}(0,\textsf{Var}(Y_1)) = \mathcal{N}(0,2 (\sigma ^2)^2),
> $$
> hence,
> $$
> V(\widehat{\sigma ^2}) = 2(\sigma ^2)^2.
> $$
> For $\widetilde{\sigma ^2}$, first observe that we can write it as
> $$
> \begin{aligned}
> \widetilde{\sigma ^2} &= \frac{1}{n} \sum _{i = 1}^ n (X_ i - \overline{X}_ n)^2\\
> &= \frac{1}{n} \sum _{i = 1}^{n} (X_ i^2 - 2 \overline{X}_ n X_ i + \overline{X}_ n^2)\\
> &= \frac{1}{n}\left( \sum _{i = 1}^{n} X_ i^2 \right) - \overline{X}_ n^2 = \widehat{\sigma ^2} - \overline{X}_ n^2.
> \end{aligned}
> $$
> Again, by the **LLN**,
> $$
> \overline{X}_ n^2 \xrightarrow [n \to \infty ]{\mathbf{P}} \mathbb E[X_1]^2 = 0,
> $$
> So
> $$
> \widetilde{\sigma ^2} = \widehat{\sigma ^2} - \overline{X}_ n^2 \xrightarrow [n \to \infty ]{\mathbf{P}} \sigma ^2.
> $$
> Now, we can consider $\widetilde{\sigma ^2}$ as
> $$
> \widetilde{\sigma ^2} = g \left( \frac{1}{n} \sum _{i=1}^{n} \begin{pmatrix}  X_ i\\ X_ i^2 \end{pmatrix} \right),
> $$
> where
> $$
> g(x, y) = y - x^2.
> $$
> By the above, we have a **multidimensional CLT** for the first and second moments of a Gaussian together,
> $$
> \sqrt{n} \left[ \begin{pmatrix}  \overline{X}_ n\\ \overline{Y}_ n \end{pmatrix} - \begin{pmatrix}  0\\ \sigma ^2 \end{pmatrix}\right] \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}\left( \begin{pmatrix}  0\\ 0 \end{pmatrix}, \begin{pmatrix}  \sigma ^2 &  0\\ 0 &  2(\sigma ^2)^2 \end{pmatrix}\right),
> $$
> where the $0$ s off the diagonal come from the fact that
> $$
> \mathbb E[X_ i \times X_ i^2] = \mathbb E[X_ i^3] = 0.
> $$
> Now, apply the **multidimensional Delta Method**, computing
> $$
> Dg(x,y) = \begin{pmatrix}  -2x &  1 \end{pmatrix},
> $$
> to obtain
> $$
> \begin{aligned}
> \sqrt{n} (\widetilde{\sigma ^2} - \sigma ^2) \xrightarrow [n \to \infty ]{\mathrm{(D)}} & \mathcal{N}\left( 0, Dg(0, \sigma ^2) \begin{pmatrix}  \sigma ^2 &  0\\ 0 &  2 (\sigma ^2)^2 \end{pmatrix} Dg(0, \sigma ^2)^\top \right)\\
> =& \mathcal{N}\left( 0, \begin{pmatrix}  0 &  1 \end{pmatrix}\begin{pmatrix}  \sigma ^2 &  0\\ 0 &  2 (\sigma ^2)^2 \end{pmatrix}\begin{pmatrix}  0\\ 1 \end{pmatrix} \right) = \mathcal{N}(0, 2 (\sigma ^2)^2).
> \end{aligned}
> $$
> Combined, we see that both estimators have the same asymptotic variance.

# 4. Maximum Likelihood Estimator for Curved Gaussian

Let $X_1, \dots , X_ n$ be $n$ i.i.d. random variables with distribution $\mathcal{N}(\theta, \theta)$ for some unknown $\theta > 0$.

1. Compute the MLE $\hat{\theta}$ for $\theta$ in terms of the sample averages of the linear and quadratic means, i.e. $\overline{X}_n$ and $\overline{X^2_n}$.

There are two methods to compute the asymptotic variance of $\hat{\theta}$. One is applying the **CLT** and the **1-dimensional Delta method**. The other is using the **Fisher information**.

2. First, compute the limit to which $ \overline{X_ n^2}$ converges in probability, also known as its **P-limit**.
3. What is the asymptotic variance $V(\overline{X_ n^2})$ of $\overline{X_n^2}$, which is equal to $\mathsf{Var}(X_1^2)$?
4. Now, write $\hat{\theta}$ as the function of $\overline{X_n^2}$ you found in (1) as $\hat\theta = g( \overline{X_ n^2} )$ and give its first derivative $g'(x)$.
5. What can you conclude about the asymptotic variance $V(\hat\theta ) $ of $\hat{\theta}$?

> **Solution:**
>
> 1. The log likelihood is
>    $$
>    \begin{aligned}
>    \ell _ n(\theta ) &= \sum _{i=1}^{n} \log \left[ \frac{1}{\sqrt{2 \pi \theta }} \exp \left( -\frac{(X_ i-\theta )^2}{2 \theta } \right) \right]\\
>    &= -\frac{n}{2} \left( \log (2) + \log (\pi ) + \log (\theta ) \right) - \sum _{i=1}^{n} \frac{(X_ i - \theta )^2}{2 \theta }\\
>    &= -\frac{n}{2} \left( \log (2) + \log (\pi ) + \log (\theta ) \right) - \sum _{i=1}^{n} \left[ \frac{1}{2 \theta } X_ i^2 - X_ i + \frac{1}{2} \theta \right].
>    \end{aligned}
>    $$
>    Differentiating yields 
>    $$
>    \frac{d}{d \theta } \ell (\theta ) = - \frac{n}{2 \theta } + \frac{1}{2 \theta ^2} \sum _{i=1}^{n} X_ i^2 - \frac{n}{2},
>    $$
>    which we set to zero to obtain the equation
>    $$
>    \hat\theta ^2 +\hat\theta - \frac{1}{n} \sum _{i=1}^{n} X_ i^2 = 0.
>    $$
>    Employing the quadratic formula and picking the result that gives a positive $\hat{\theta}$ then leads to
>    $$
>    \hat\theta = -\frac{1}{2} + \frac{1}{2} \sqrt{4 \overline{X_ n^2} + 1}.
>    $$
>
> 2. Compute the limit by the LLN,
>    $$
>    \overline{X_ n^2} \xrightarrow [n \to \infty ]{\mathrm{\mathbf{P}}} \mathbb E[X_1^2] = \textsf{Var}(X_1) + \mathbb E[X_1]^2 = \theta + \theta ^2.
>    $$
>    
>
> 3. Compute the asymptotic variance by the CLT,
>    $$
>    \sqrt{n} (\overline{X_ n^2} - (\theta + \theta ^2)) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, \textsf{Var}(X_1^2)),
>    $$
>    and
>    $$
>    \begin{aligned}
>     \mathsf{Var}(\overline{X_n^2}) = \textsf{Var}(X_1^2) &= \mathbb E[X_1^4] - \mathbb E[X_1^2]^2\\
>     &= \mathbb E[(\theta + \sqrt{\theta }Z)^4] - (\theta + \theta ^2)^2\\
>     &= \theta ^4 + 4 \theta ^3 \sqrt{\theta } \underbrace{\mathbb E[Z]}_{= 0} + 6 \theta ^2 \theta \underbrace{\mathbb E[Z^2]}_{= 1} + 4 \theta \sqrt{\theta }^3 \underbrace{\mathbb E[Z^3]}_{= 0} + \theta ^2 \underbrace{\mathbb E[Z^4]}_{= 3} - \theta ^4 - 2 \theta ^3 - \theta ^2\\
>     &= 2 \theta ^2 (2 \theta + 1)
>    \end{aligned}
>    $$
>    where $Z \sim \mathcal{N}(0,1)$ is a standard Normal variable.
>
> 4. From the previous part, we get
>    $$
>    g(x) = \frac{1}{2} \left( \sqrt{4 x + 1} - 1 \right),
>    $$
>    so
>    $$
>    g'(x) = \frac{1}{\sqrt{4 x + 1}}.
>    $$
>
> 5. Finally, by the Delta Method,
>    $$
>    \sqrt{n}(g(\overline{X_ n^2}) - g(\theta + \theta ^2)) \xrightarrow [n \to \infty ]{\mathrm{(D)}} \mathcal{N}(0, 2 \theta ^2 (2 \theta + 1) g'(\theta + \theta ^2)^2) = \mathcal{N}\left(0, \frac{2 \theta ^2}{2 \theta + 1}\right).
>    $$
>    So 
>    $$
>    \mathsf{Var}(\hat{\theta}) =\frac{2 \theta ^2}{2 \theta + 1}
>    $$





 





