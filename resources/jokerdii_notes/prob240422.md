# Lecture 7. Conditioning on a random variable ; Independence of r.v's

* Conditional PMFs

| Two r.v.s                                                    | >Two r.v.s                                                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| $p_{X\|Y}(x\|y) = \frac{p_{X,Y}(x,y)}{p_Y(y)}$               | $p_{X\|Y,Z}(x\|y,z) = \frac{\mathbf{P}_{X,Y,Z}(x, y, z)}{\mathbf{P}_{Y,Z}(y, z)} \\p_{X,Y\|Z}(x,y\|z) = \frac{\mathbf{P}_{X,Y,Z}(x, y, z)}{\mathbf{P}_{Z}(z)} $ |
| $\sum_xp_{X\|Y}(x\|y) = 1$                                   | $\sum_x p_{X\|Y,Z}(x\|y,z) =1 \\ \sum_x \sum_y p_{X,Y\|Z}(x,y\|z)=1$ |
| $p_{X,Y}(x,y) = p_Y(y)p_{X\|Y}(x\|y) \\ p_{X,Y}(x,y) = p_X(x)p_{Y\|X}(y\|x)$ | $p_{X,Y,Z}(x,y,z) = p_X(x)p_{Y\|X}(y\|x)p_{Z\|X,Y}(z\|x,y)$  |

  * Conditional PMFs

      * Conditional expectations

        | Non-conditional                    | Conditional                                     |
        | ---------------------------------- | ----------------------------------------------- |
        | $\rm E[X] = \sum_x x p_X(x)$       | $\rm E[X\|Y=y] = \sum_x x p_{X\|Y}(x\|y)$       |
        | $\rm E[g(X)] = \sum_x g(x) p_X(x)$ | $\rm E[g(X)\|Y=y] = \sum_x g(x) p_{X\|Y}(x\|y)$ |

      * Total probability and expectation theorem

        $p_X(x)=\sum_yp_Y(y)p_{X|Y}(x|y)$

        $\rm E[X] = \sum_y p_Y(y)E[X|Y=y]$

* Independence of r.v.'s

  * Of two events: $\mathbf{P}(A \cap B) = \mathbf{P}(A) \cdot \mathbf{P}(B), \quad \mathbf{P}(A|B) = \mathbf{P}(A)$

  * Of a r.v. and an event: 

    $\mathbf{P}(X=x, A) = \mathbf{P}(X=x) \cdot \mathbf{P}(A), $ for all $x$. 

    $p_{X|A}(x) = p_X(x),$ for all $x$.

    $\mathbf{P}(A|X=x) = \mathbf{P}(A),$ for all $x$.

  * Of two r.v.'s: 

    $\mathbf{P}(X=x, Y=y) = \mathbf{P}(X=x) \cdot \mathbf{P}(Y=y), $ for all $x,y$.

    $p_{X,Y}(x,y) = p_X(x) p_Y(y),$ for all $x,y$.

    $p_{X|Y}(x|y) = p_X(x)$

    $p_{Y|X}(y|x) = p_Y(y)$

  * Of three r.v.'s

    $p_{X,Y,Z}(x,y,z) = p_X(x) p_Y(y) p_Z(z), $ for all $x,y,z$.

* Independence and Expectation

  If $X, Y$ are independent: $\rm E[XY] = E[X] E[Y]$, $g(X)$ and $h(Y)$ are also independent: $\rm E[g(X)h(Y)] = E[g(X)]E[h(Y)]$.

* Independence and Variance

  If $X,Y$ are independent: $\rm{var}(X+Y) = \rm{var}(X) + \rm{var}(Y)$

* The variance of the binomial

* $X \sim \rm Bin(n,p)$ : $\rm{var}(X) = np(1-p)$

* **The hat problem**: 

     $n$ people throw their hats in a box and then pick one at random

     * All permutations equally likely, equivalent to picking one hat at a time: $\frac{1}{n!}$.

     * $X$: number of people who get their own hat, $X = X_1 + X_2 +... + X_n$, where $X_i$ is the indicator function.

       * Find $\rm E[X]$:

         $\rm E[X_i] = \mathbf{P}(X_1 = 1) = {1 \over n}$.

         $\rm E[X] = E[X_1 + X_2 +... + X_n] = E[X_1] + E[X_2] +... + E[X_n] = n \cdot {1 \over n} = 1$.

       * Find $\rm{var}(X)$: Note that $X_i$ are dependent.

         To compute $\rm{var}(X) = E[X^2] - (E[X])^2$, we have to compute the following quantities first:

         $X^2 = \sum_i X_i^2 + \sum_{i,j,L i\neq j} X_i X_j$.

         $\rm E[X_i^2] = E[X_1^2]=E[X_1] = 1/n, \quad \text{since it is an indicator}$.

         For $i \neq j$: $\rm E[X_iX_j] = E[X_1 X_2] = \mathbf{P}(X_1X_2 = 1) = \mathbf{P}(X_1 =1, X_2 = 1) = \mathbf{P}(X_1 = 1) \mathbf{P}(X_2  = 1| X_1 = 1) = {1\over n}{1\over n-1}$.

         Therefore, $\rm{var}(X) = E[X^2] - (E[X])^2 = n \cdot {1 \over n} + (n^2 - n) \cdot { 1\over n} {1 \over n-1 } - 1 = 1 + 1 - 1 = 1$.

There are 2 selected exercises and 5 solved problems.

---

## Exercise 1 The expected value rule with conditioning

The following equations are all true: 

${{\bf E}[g(X,Y)\mid Y=2]=\sum _ x g(x,2) \frac{p_{X,Y}(x,2)}{p_ Y(2)}}$

${{\bf E}[g(X,Y)\mid Y=2]=\sum _ x g(x,2) \frac{p_{X,Y}(x,2)}{p_ Y(2)}}$

$ {{\bf E}[g(X,Y)\mid Y=2]=\sum _ x \sum _ y g(x,y) p_{X,Y\mid Y}(x,y\mid 2)}$

Note that $p_{X,Y\mid Y}(x,y\mid 2)$ will be zero for any $y \neq 2$. And for $y =2$.

$p_{X,Y\mid Y}(x,2\mid 2)=\mathbf{P}(X=x, Y=2\mid Y=2)=\mathbf{P}(X=x\mid Y=2)=p_{X\mid Y}(x\mid 2),$

So that the sixth formula agrees with the fourth one.

## Exercise 2 Independence and variances

The pair of random variables $(X,Y)$ is equally likely to take any of the four pairs of values $(0,1), (1,0), (−1,0), (0,−1)$. Note that $X$ and $Y$ each have zero mean. Find $\textsf{Var}(X+Y)$.

**Answer**: $\textsf{Var}(X+Y)=\textsf{Var}(X)+\textsf{Var}(Y)$

**Solution**: ${\bf E}[X+Y]=0, \textsf{Var}(X)={\bf E}[X^2], \textsf{Var}(Y)={\bf E}[Y^2]$,
$$
\begin{aligned}
\textsf{Var}(X+Y) &= {\bf E}[(X+Y)^2] - ({\bf E}[X+Y])^2\\
&= {\bf E}[(X+Y)^2]\\
&={\bf E}[X^2]+2{\bf E}[XY]+{\bf E}[Y^2]\\
&= {\bf E}[X^2]+{\bf E}[Y^2]\\
&= \textsf{Var}(X)+\textsf{Var}(Y).
\end{aligned}
$$
**Remark**: If $X$ and $Y$ are independent, $\textsf{Var}(X+Y)=\textsf{Var}(X)+\textsf{Var}(Y)$. But the converse is not true. That is, the condition $\textsf{Var}(X+Y)=\textsf{Var}(X)+\textsf{Var}(Y)$ does not imply independence.

## Problem 1 Indicator variables: the number of inversions

There are $n$ persons, numbered $1$ to $n$. Each person $i$ is assigned a seat number $X_i$. The seat numbers are distinct integers in the range $1,…,n$. We assume that the seating is “completely random", that is, the sequence ($X_1,…,X_n$) is a permutation of the numbers $1,…,n$, and all permutations are equally likely.

For any $i$ and $j$, with $ 1 \leq i < j \leq n$, we say that we have an inversion if $X_ i>X_ j$. Let $N$ be the number of inversions. Find $E[N]$.

**Answer**: $E[N] = {1 \over 2}  {n \choose 2} $

**Solution**: 

We use the method of indicator functions. 
$$
Y_{ij} = \begin{cases}1, &X_i > X_j \\ 0, &X_i \leq X_j  \end{cases}
$$
Then we have
$$
N = \sum_{i<j} Y_{ij}
$$
By the linearity of expectation, we have:
$$
\begin{aligned}
E[N] &= \mathbf{E}\left[\sum_{i < j} Y_{ij}\right]\\
&= \sum_{i<j} \mathbf{E}[Y_{ij}]\\
&= \sum_{i<j} \mathbf{P}(X_i > X_j)\\
&= \sum_{i<j} {1 \over 2} \quad \text{Since }\mathbf{P}(X_i > X_j) \text{ and } \mathbf{P}(X_i < X_j) \text{ are the same}\\
&=\sum^{n-1}_{i=1} \sum^n_{j = i+1} {1 \over 2}\\
&= {1 \over 2} {n(n-1) \over n} = {1 \over 2} {n \choose 2}
\end{aligned}
$$
Another way to see this is to observe that there are $n \choose 2$ pairs total, and by symmetry, the expected number of pairs in inverted order will be half of the total number of pairs.

## Problem 2 Indicator variables: the problem of joint lives

Consider $2m$ persons forming $m$ couples who live together at a given time. Suppose that at some later time, the probability of each person being alive is $p$, independently of other persons. At that later time, let $A$ be the number of persons that are alive and let $S$ be the number of couples in which both partners are alive. For any number of total surviving persons $a$, find $\mathbf{E}[S∣A=a]$.

**Answer**: $\frac{a(a-1)}{2(2m-1)}$

**Solution**: 

Let $X_i$ be the random variable taking the value $1$ or $0$ depending on whether the first partner of the $i$th couple has survived or not. Let $Y_i$ be the corresponding random variable for the second partner of the $i$th couple.  So $X_i$ and $Y_i$ are indicator variables:
$$
X_i = \begin{cases}1, &\text{if 1st partner in coulple alive} \\0, &\text{otherwise}  \end{cases}\\
Y_i = \begin{cases}1, &\text{if 2nd partner in coulple alive} \\0, &\text{otherwise}  \end{cases}
$$
Then, $Z_i = X_iY_i$ is also an indicator variable.
$$
Z_i = \begin{cases}1, &\text{if coulple is alive} \\0, &\text{otherwise}  \end{cases}
$$
So we have $S = \sum^m_{i=1} X_iY_i$. By using linearity of expectations and the total expectation theorem,
$$
\begin{aligned}
\mathbf{E}(S|A = a) &= \sum^m_{i=1} \mathbf{E}[X_iX_j|A=a]\\
&= m\mathbf{E}[X_1Y_1 | A=a]\\
&=m\mathbf{E}[Y_1 = 1 | X_1 = 1, A=a] \mathbf{P}(X_1 = 1 | A=a)\\
&=m\mathbf{P}(Y_1 = 1 | X_1 = 1, A=a) \mathbf{P}(X_1 = 1 | A=a)\\
\end{aligned}
$$
Note that given indicator variable $X_iY_i$, $\mathbf{E}[X_1Y_1 | A=a]  = \mathbf{P}(X_1Y_1 = 1 | A=a)$.

We have
$$
\mathbf{P}(X_1 = 1 | A =a ) = {a \over 2m}, \qquad \mathbf{P}(Y_1=1|X_1 = 1, A= a) = {a-1 \over 2m-1}
$$
Hence,
$$
\mathbf{E}[S | A=a] = m {a-1 \over 2m-1} \cdot {a \over 2m} = {a(a-1) \over 2(2m-1)}
$$
Note that $\mathbf{E}[S|A=a]$ does not depend on $p$.

## Problem 3

Let $N$ be a positive integer random variable with PMF of the form
$$
p_ N(n)=\frac{1}{2}\cdot n\cdot 2^{-n},\qquad n=1,2,\ldots .
$$
Once we see the numerical value of $N$, we then draw a random variable $K$ whose (conditional) PMF is uniform on the set $\{1,2,...,2n\}$. 

1. Write down an expression for the joint PMF $p_{N,K}(n,k)$, for $n=1,2,...$ and $k = 1,2,...,2n$.

2. Find the marginal PMF $p_K(k)$ as a function of $k$, for $k=2,4,6,...$ For simplicity, provide the answer **only for the case when** $k$ **is an even number**. 

   *Hint*: $ \sum _{i=0}^{\infty } r^ i = \frac{1}{1-r}$

3. Let $A$ be the event that $K$ is even. Find $\mathbf{P}(A|N=n)$ and $\mathbf{P}(A)$.

**Solution**:

1) $p_{N,K}(n,k)= {1 \over 2^{n+2}}$

Since $K$ is uniform distributed from $1$ to $2n$, we have
$$
p_{K \mid N}(k \mid n) = \frac{1}{2n}, \qquad k = 1,2,\dots , 2n.
$$
By definition, the joint PMF is 
$$
p_{N,K}(n, k) = p_{K \mid N}(k \mid n) p_{N}(n) = \frac{1}{2n} \frac{1}{2} \cdot n \cdot 2^{-n} = (\frac{1}{2})^{n+2}, \qquad n=1,2,\dots , \quad k=1,2,\dots , 2n
$$
2) $p_ K(k)= {1 \over 2^{k/2+1}}$

By definition of $p_{K}(k) = \sum _{n=1}^{\infty } p_{N,K}(n,k)$, and only the terms from $n = k/2$ and above have non-zero probability. Hence,
$$
\begin{aligned}
p_K(k) &= \sum _{n=k/2}^{\infty } p_{N,K}(n,k) = \sum _{n=k/2}^{\infty } (\frac{1}{2})^{n+2}\\
&=\sum _{n=k/2}^{\infty } (\frac{1}{2})^{n+2} = \frac{1}{4} \sum _{n=k/2}^{\infty } (\frac{1}{2})^ n\\
&= \frac{1}{4} \Big[\sum _{n=0}^{\infty } (\frac{1}{2})^ n - \sum _{0}^{k/2-1} (\frac{1}{2})^ n \Big]\\
&= \frac{1}{4} \Big[\frac{1}{1-\frac{1}{2}} - \frac{1-(\frac{1}{2})^{k/2-1+1}}{1-\frac{1}{2}} \Big]\\
&=(\frac{1}{2})^{k/2+1} \qquad \text {for} k = 2, 4, \dots
\end{aligned}
$$

3)  $\mathbf{P}(A|N=n)=\mathbf{P}(A) = 1/2$.

We need to check the **independence** between $A$ and $N$ by checking whether $\mathbf{P}(A|N=n)=\mathbf{P}(A)$. 

Now because $p_{K|N}(k|n)$ is uniform over the $2n$-size set $\{1,2,...,2n\}$ and there are exactly $n$ even numbers in this set, we have that
$$
P(A \mid N=n) = \frac{n}{2n} = \frac{1}{2}, \qquad n \geq 1.
$$
Intuitively, knowledge of $n$ does not affect the beliefs about $A$, and we have **independence**. A full, formal argument goes as follows
$$
P(A)= \sum _{n=1}^{\infty }P(A\mid N=n) P(N=n)= \frac{1}{2}\sum _{n=1}^{\infty }P(N=n)=\frac{1}{2}
$$
where the last step follows because PMFs always sum to 1. So, $P(A\mid N=n)=P(A)$ for all $n$.

Equivalently, $P(A \  {\rm and} \  N=n) = P(A\mid N=n)\cdot P(N=n) = P(A)\cdot P(N=n)$.

## Problem 4 Joint PMF

The joint PMF, $p_{X,Y}(x,y)$ of the random variables $X$ and $Y$ is given by the following table:

![u4-prob1-table](D:/git/fundamentals-of-statistics-notes/docs/assets/images/u4-prob1-table.png)

1. Find the value of the constant $c$.
2. Find $p_X(1)$.
3. Consider the random variable $Z = X^2 Y^3$. Find $\mathbf{E}(Z|Y=-1)$.
4. Conditioned on the event that $Y\neq 0$, are $X$ and $Y$ independent?
5. Find the conditional variance of $Y$ given that $X=0$.

**Solution**: 

1) Since the probability of the entire sample space equals to 1.
$$
\begin{aligned}
1 &= \sum ^1_{x=-2}\displaystyle \sum ^1_{y=-1} p_{X,Y}(x,y)\\
&=2c + 3c + 4c + 2c + c + 2c + 4c + 2c + 8c\\
&=28c
\end{aligned}
$$
Therefore, $c = 1/28$.

2) $p_ X(1) = \displaystyle \sum ^1_{y=-1}p_{X,Y}(1,y) = 4c + 2c + 8c = 14c = \frac{1}{2}.$

3) 
$$
\begin{aligned}
{\bf E}[Z \mid Y=-1] &= {\bf E}[X^2Y^3 \mid Y=-1]\\
&= {\bf E}[X^2(-1)^3 \mid Y=-1]\\
&=-{\bf E}[X^2 \mid Y=-1]
\end{aligned}
$$
In order to calculate this conditional expectation, we need the conditional PMF of $X$ given $Y=-1$.
$$
p_{X\mid Y}(x \mid -1) = \frac{p_{X,Y}(x,-1)}{p_ Y(-1)}=\left\{ \begin{array}{ll} \frac{2c}{7c} = \frac{2}{7},\;  \;  &  \textrm{if } x = -2, \\ \frac{c}{7c} = \frac{1}{7},\;  \;  &  \textrm{if } x = 0, \\ \frac{4c}{7c} = \frac{4}{7},\;  \;  &  \textrm{if } x = 1, \\ 0, &  \text{otherwise.} \end{array} \right.
$$
Therefore,
$$
\begin{aligned}
{\bf E}[Z \mid Y=-1] &= - \sum ^1_{x=-2}x^2p_{X\mid Y}(x \mid -1)\\
&=-\left((-2)^2\cdot \frac{2}{7} + 1^2\cdot \frac{4}{7}\right)\\
&=-{12\over 7}
\end{aligned}
$$
4) Yes. Given $Y \neq 0$,, the conditional distribution $Y$ given $X =x$ is the same for all $x \in \{-2,-1,0,1\}$: $\mathbf{P}(Y=y \mid X=x,Y\neq 0) = \mathbf{P}(Y=y \mid Y\neq 0)$, for all $x \in \{-2,-1,0,1\}$

For example,
$$
\begin{aligned}
\mathbf{P}(Y=1 \mid X=-2, Y\neq 0) &=\mathbf{P}(Y=1 \mid X=0, Y\neq 0)\\
&=\mathbf{P}(Y=1 \mid X=1, Y\neq 0)\\
&=\mathbf{P}(Y=1 \mid Y \neq 0) = \frac{2}{3}.
\end{aligned}
$$
5) We first find the conditional PMF of $Y$ given $X=0$:
$$
p_{Y\mid X}(y\mid 0) = \frac{p_{X,Y}(0,y)}{p_ X(0)} = \begin{cases}  \frac{c}{c+2c}=\frac13, &  \text{if } y=-1,\\ \frac{2c}{c+2c}=\frac23, &  \text{if } y=1,\\ 0, &  \text{otherwise.} \end{cases}
$$
We can then calculate the conditional expectation:
$$
{\bf E}[Y\mid X=0] = \sum _{y=-1}^1 yp_{Y\mid X}(y\mid 0) = (-1)\cdot \frac{1}{3}+(1)\cdot \frac23 = \frac13.
$$
Finally, the conditional variance can be calculated as
$$
\begin{aligned}
\textsf{Var}(Y\mid X=0) &= {\bf E}[(Y-{\bf E}[Y\mid X=0])^2 \mid X=0]\\
&={\bf E}\left[\left(Y-\frac13\right)^2\mid X=0\right]\\
&=\sum _{y=-1}^1 \left(y-\frac13\right)^2p_{Y\mid X}(y\mid 0)\\
&=\left(-1-\frac13\right)^2\cdot \left(\frac13\right) + \left(1-\frac13\right)^2\cdot \left(\frac23\right)\\
&= {8 \over 9}
\end{aligned}
$$

## Problem 5 Indicator Variables

Consider a sequence of $n+1$ independent tosses of a biased coin, at times $k=0,1,2,…,n$. On each toss, the probability of Heads is $p$, and the probability of Tails is $1−p$.

A reward of one unit is given at time $k$, for $k∈\{1,2,…,n\}$, if the toss at time $k$ resulted in Tails and the toss at time $k−1$ resulted in Heads. Otherwise, no reward is given at time $k$.

Let $R$ be the sum of the rewards collected at times $1,2,..., n$.

1. Find $\mathbf{E}[R]$.
2. Find $\mathsf{Var}(R)$, assuming that $p = 3/4, n = 10$.

**Solution**: 

1) First find $\mathbf{E}(I_k)$. Since $I_k$ is a Bernoulli indicator variable and the tosses are independent, we have
$$
{\bf E}[I_ k] = \mathbf{P}(I_ k = 1) = \mathbf{P}(\text {Tails at time k and Heads at time } k-1) = p(1-p).
$$
Given the total reward over all the tosses $R$ is the sum of all the $I_k$'s, for $k=1,2,...,n.$ To find $\mathbf{E}[R]$, by linearity of expectations, we have
$$
{\bf E}[R] = {\bf E}\left[ \sum _{k=1}^ n I_ k \right] = \sum _{k=1}^ n {\bf E}\left[ I_ k \right] = np(1-p).
$$
2) First find $\mathbf{E}[I_k^2]$. Since $I_k$ can only be $0$ or $1$, we have
$$
\mathbf{E}[I_k^2]=\mathbf{E}[I_k]=p(1-p)
$$
Then find $\mathbf{E}[I_kI_{k+1}]$. Since $I_kI_{k+1} = 1$ if $I_k = 1$ and $I_{k+1} = 1$. i.e. if a reward was given at time $k $ and at time $k+1$ which is impossible. Therefore
$$
\mathbf{E}[I_kI_{k+1}] = 0
$$
Next find $\mathbf{E}[I_kI_{k+l}]$ if $k\geq 1, l \geq 2, k+l \geq n$. Since the reward at time $k$ depends only on the tosses at times $k$ and $k−1$, the rewards at times that are at least $2$ periods apart depend on different, non-overlapping pairs of coin tosses, and hence $I_k$ and $I_{k+l}$ are independent for $l≥2$. Therefore, 
$$
\mathbf{E}[I_kI_{k+l}] = {\bf E}[I_ k]{\bf E}[I_{k+\ell }]  = p^2 \cdot (1-p)^2
$$
Now find $\mathbf{E}[R^2]$. We have
$$
{\bf E}[R^2] = {\bf E}\left[\left(\sum _{k=1}^ n I_ k\right)\left(\sum _{m=1}^ n I_ m\right)\right] = {\bf E}\left[\sum _{k=1}^ n\sum _{m=1}^ nI_ kI_ m\right] = \sum _{k=1}^ n \sum _{m=1}^ n {\bf E}[ I_ k I_ m ]
$$
There are $n^2$ terms in this double summation. We can divide them into three groups:

1. There are $n$ terms where $k=m$. From previous calculation, we know $\mathbf{E}[I_kI_m] = \mathbf{E}[I_k^2]=\mathbf{E}[I_k]=p(1-p)$
2. There are $n-1$ terms where $k= m+1$ and another $n-1$ terms where $m = k+1$. From previous calculation, we know that ${\bf E}[I_ kI_ m]=p^2(1-p)^2$.
3. The remaining $n^2 - n - 2(n-1) = n^2 - 3n + 2$ terms are those where $k$ and $m$ differ by at least 2. From previous calculation, we know that ${\bf E}[I_ kI_ m]=p^2(1-p)^2$.

Putting together, we have
$$
{\bf E}[R^2] = n\cdot p(1-p) + 2(n-1)\cdot 0 + (n^2-3n+2)\cdot p^2(1-p)^2.
$$
Therefore, we can find $\mathsf{Var}(R)$ from formula $\textsf{Var}(R) = {\bf E}[R^2] - ({\bf E}[R])^2$,
$$
\begin{aligned}
\textsf{Var}(R) &= {\bf E}[R^2] - ({\bf E}[R])^2\\
&=np(1-p) + (n^2-3n+2)p^2(1-p)^2 - n^2p^2(1-p)^2\\
&= np(1-p) - (3n-2)p^2(1-p)^2.
\end{aligned}
$$
Assuming that $p = 3/4, n = 10$, we obtain
$$
\mathsf{Var}(R) = 57/64 = 0.890625
$$


# Lecture 14. Introduction to Bayesian Inference

* The big picture

  ![U7-lec14-bayes-framework](../assets/images/U7-lec14-bayes-framework.png) 

  * Problem types 
    * Hypothesis testing: unknown takes one of few possible values, aim at small probability of incorrect decision.
    * Estimation: numerical unknown(s), aim at an estimate that is "close" to the true but unknown value.

* The general framework

  * Bayes' rule $\rightarrow$ Posterior

    Unknown $\Theta$: treated as a random variable

    Prior distribution $p_\Theta$ or $f_\Theta$

    Observation $X$: observation model $p_{X|\Theta}$ or $f_{X|\Theta}$.

    Use appropriate version of the Bayes rule to find $p_{X|\Theta}(\cdot | X=x)$ or $f_{X|\Theta}(\cdot |X=x)$.

    ![U7-lec14-bayes-workflow](../assets/images/U7-lec14-bayes-workflow.png)

  * Point estimates (MAP, LMS)

    estimate: $\hat{\theta} = g(x)$;

    estimator: $\hat{\Theta} = g(X)$.

    * Maximum a posteriori probability (MAP):
      $$
      p_{\Theta|X}(\theta^*|x) = \max_\theta p_{\Theta|X}(\theta|x)\\
      f_{\Theta|X}(\theta^*|x) = \max_\theta f_{\Theta|X}(\theta|x)
      $$

    * Least Mean Squares (LMS):

      Conditional expectation: $\mathbf{E}[\Theta|X=x]$.

  * Types of Bayes rule

    | Types                               | Formula                                                      |
    | ----------------------------------- | ------------------------------------------------------------ |
    | Discrete $\Theta$, discrete $X$     | $p_{\Theta \vert X}(\theta \vert x) = {p_{\Theta}(\theta) p_{X\vert \Theta}(x \vert \theta) \over p_X(x)}\\ p_X(x) = \sum_{\theta'}p_{\Theta}(\theta')p_{X \vert \Theta} (x \vert \theta')$ |
    | Discrete $\Theta$, continuous $X$   | $p_{\Theta \vert X}(\theta \vert x) = {p_{\Theta}(\theta) f_{X\vert \Theta}(x \vert \theta) \over f_X(x)}\\ f_X(x) = \sum_{\theta'}p_{\Theta}(\theta')f_{X \vert \Theta} (x \vert \theta')$ |
    | Continuous $\Theta$, continuous $X$ | $f_{\Theta \vert X}(\theta \vert x) = {f_{\Theta}(\theta) f_{X\vert \Theta}(x \vert \theta) \over f_X(x)}\\ f_X(x) = \int f_{\Theta}(\theta')f_{X \vert \Theta} (x \vert \theta') \ d\theta' $ |
    | Continuous $\Theta$, discrete $X$   | $f_{\Theta \vert X}(\theta \vert x) = {f_{\Theta}(\theta) p_{X\vert \Theta}(x \vert \theta) \over p_X(x)}\\ p_X(x) = \int f_{\Theta}(\theta')p_{X \vert \Theta} (x \vert \theta') \ d\theta' $ |

  * Performance measures (prob. of error; mean squared error)

    * Conditional probability of error:  (smallest under the MAP rule)
      $$
      \mathbf{P}(\hat{\theta}\neq \Theta| X=x) 
      $$

    * Overall probability of error:
      $$
      \begin{aligned}
      \mathbf{P}(\hat{\Theta} \neq \Theta) &= \sum_x\mathbf{P}(\widehat{\Theta} \neq \Theta|X=x)p_X(x)\\
      &= \sum_\theta \mathbf{P}(\widehat{\Theta} \neq \Theta| \Theta = \theta) p_\Theta(\theta)\\
      \text{or}\\
      \mathbf{P}(\hat{\Theta} \neq \Theta) &= \int \mathbf{P}(\widehat{\Theta} \neq \Theta|X=x)f_X(x)dx\\
      &= \sum_\theta \mathbf{P}(\widehat{\Theta} \neq \Theta| \Theta = \theta) p_\Theta(\theta)\\
      \end{aligned}
      $$

  * Example: Inferring the unknown bias of a coin and the Beta distribution

    Coin with bias $\Theta$; prior $f_\Theta(\cdot)$; fix $n$; $K=$ Number of heads

    Assume prior $f_\Theta(\cdot)$ is uniform in $[0,1]$. The posterior probability is
    $$
    f_{\Theta\vert k}(\theta \vert k) = { 1 \cdot {n \choose k} \theta^k (1-\theta)^{n-k}\over p_K(k) } = {1\over d(n,k)} \theta^k (1-\theta)^{n-k}, \quad \theta \in [0,1]
    $$
    which follows Beta distribution, with parameter $(k+1, n-k+1)$.

    If prior is Beta
    $$
    f_\Theta(\theta) = {1\over c} \theta^\alpha (1-\theta)^\beta, \quad \alpha, \beta \geq 0
    $$
    The posterior probability is
    $$
    f_{\Theta \vert K}(\theta \vert k) = {{1\over c} \theta^\alpha (1-\theta)^\beta {n \choose k} \theta^k (1-\theta)^{n-k}\over p_K(k)} = d\ \theta ^{\alpha + k} (1-\theta) ^{\beta + n - k}
    $$
    The MAP estimate:
    $$
    \max_\theta \left[k \log \theta + (n-k) \log(1- \theta) \right]\\
    k/\theta - (n-k)/(1-\theta) = 0\\
    \widehat{\theta}_{MAP} = k/n
    $$
    The LMS is
    $$
    \begin{aligned}
    \mathbf{E}[\Theta | K=k] &= \int^1_0 \theta \ f_{\Theta|K}(\theta | k) \ d\ \theta \\
    &= {1\over d(n,k)} \int^1_0 \theta^{k+1} (1-\theta)^{n-k} \ d\ \theta \\
    &= {1\over {k!(n-k)!\over (n+1)! } }\cdot {(k+1)!(n-k)! \over (n+2)! } \\
    &= {k+1 \over n+2} \approx {k \over n} \quad \text{when n is large}
    \end{aligned}
    $$
    Note that for Beta random variable
    $$
    \int^1_0 \theta^\alpha (1-\theta)^\beta \ d\theta = {\alpha! \beta! \over (\alpha + \beta + 1)!}
    $$


There are 1 selected example and 2 solved problems.

---

## Exercise 1 Continuous unknown and observation

Let $\Theta$ and $X$ be jointly continuous nonnegative random variables. A particular value $x$ of $X$ is observed and it turns out that $f_{\Theta |X}(\theta \, |\, x)=2e^{-2\theta }$, for $\theta \geq 0$.

Recall that for an exponential random variable $Y$ with parameter $\lambda$, we have ${\bf E}[Y]=1/\lambda$ and $\textsf{Var}(Y)=1/\lambda ^2$.

1. What is the LMS estimate (conditional expectation) of $\Theta$?
2. What is the conditional mean squared error ${\bf E}\big [(\Theta -\widehat\Theta _{{\rm LMS}})^2\, |\, X=x]$?
3. What is the MAP estimate of $\Theta$?
4. What is the conditional mean squared error ${\bf E}\big [(\Theta -\widehat\Theta _{{\rm MAP}})^2\, |\, X=x]$?

**Solution:**

1. The posterior PDF is exponential with parameter 2. The LMS estimate is the mean of this distribution
   $$
   \mathbf{E}[\lambda|x] = 1/\lambda = 1/2
   $$

2. The conditional variance - the variance of an exponential random variable with parameter 2 is
   $$
   {\bf E}\big [(\Theta -\widehat\Theta _{{\rm LMS}})^2\, |\, X=x] = 1/\lambda^2 = 1/4
   $$

3. The posterior PDF, which is exponential, is largest at zero. The MAP estimate of $\Theta$ is 0.

4. Since $\widehat{\Theta}=0$, the conditional mean squared error is the second moment of the exponential distribution (that is, of the form ${\bf E}[Y^2]$, where $Y$ is exponential with parameter 2). Using the formula 
   $$
   {\bf E}[Y^2]=\textsf{Var}(Y)+\big ({\bf E}[Y]\big )^2
   $$
   we obtain
   $$
   {\bf E}[Y^2]=\frac{1}{4}+\Big(\frac{1}{2}\Big)^2=\frac{1}{2}.
   $$
   Note that the LMS estimator results in a smaller mean squared error.

## Problem 1 Defective Coin

A defective coin minting machine produces coins whose probability of Heads is a random variable $Q$ with PDF
$$
f_{Q}(q) = \begin{cases}5q^4, &  \text{if } q \in [0,1],\\ 0, &  \text{otherwise}. \end{cases}
$$
A coin produced by this machine is tossed repeatedly, with successive tosses assumed to be independent. Let $A$ be the event that the first toss of this coin results in Heads, and let $B$ be the event that the second toss of this coin results in Heads.

1. Find $\mathbf{P}(A)$.
2. Find the conditional PDF of $Q$ given event $A$. 
3. Find the $\mathbf{P}(B|A)$.

**Solution:**

1. To calculate $\mathbf{P}(A)$, we use the continuous version of the total probability theorem
   $$
   \mathbf{P}(A) = \int _0^1 \mathbf{P}(A\mid Q = q)f_ Q(q)\  dq = \int _0^1 q\cdot (5q^4)\  dq = \left[\frac{5}{6}q^6\right]_0^1 = \frac{5}{6}.
   {"mode":"full","isActive":false}
   $$

2. Using Bayes' rule
   $$
   \begin{aligned}
   f_{Q\mid A}(q) &= \frac{\mathbf{P}(A\mid Q = q)f_ Q(q)}{\mathbf{P}(A)}\\
   &=  \begin{cases} \frac{q\cdot (5q^4)}{5/6}, &  \text{if }0\leq q \leq 1,\\ 0, &  \text{otherwise,} \end{cases}\\
   &= \begin{cases} \displaystyle 6q^5, &  \text{if } 0\leq q \leq 1,\\ 0, &  \text{otherwise.} \end{cases} 
   \end{aligned}
   $$

3. Again we use the continuous version of the total probability theorem
   $$
   \begin{aligned}
    \mathbf{P}(B\mid A) & =\int _0^1\mathbf{P}(B\mid A, Q=q)f_{Q\mid A}(q)\  dq\\
    &= \int _0^1\mathbf{P}(B\mid Q=q)f_{Q\mid A}(q)\  dq\\
    &= \int _0^1 q(6q^5)\  dq\\
    &= 6/7.
   \end{aligned}
   $$
   The second equality holds because for a given value $q$ of $Q$, the events $A$ and $B$ are (conditionally) independent.

## Problem 2 Hypothesis test between two coins

Alice has two coins. The probability of Heads for the first coin is $1/4$, and the probability of Heads for the second is $3/4$. Other than this difference, the coins are indistinguishable. Alice chooses one of the coins at random and sends it to Bob. The random selection used by Alice to pick the coin to send to Bob is such that the first coin has a probability $p$ of being selected. Assume that $0 < p < 1$. Bob tries to guess which of the two coins he received by tossing it $3$ times in a row and observing the outcome. Assume that for any particular coin, all tosses of that coin are independent.

1. Given that Bob observed $k$ Heads out of the $3$ tosses (where $k = 0,1,2,3$), what is the conditional probability that he received the first coin?
2. We define an error to have occurred if Bob decides that he received one coin from Alice, but he actually received the other coin. He decides that he received the first coin when the number of Heads, $k$, that he observes on the $3$ tosses satisfies a certain condition. When one of the following conditions is used, Bob will minimize the probability of error. Choose the correct threshold condition.
3. For this part, assume that $p = 3/4$. What is the probability that Bob will guess the coin correctly? 
4. If $p$ is small, then Bob will always decide in favor of the second coin, ignoring the results of the three tosses. The range of such $p$'s is $[0,t)$. Find $t$.

**Solution:**

1. Let $Y$ be the number of Heads Bob observed in the three tosses. Let $C$ denote the coin that Bob received, so that $C=1$ if Bob received the first coin, and $C =2$ if Bob received the second coin. Then $\mathbf{P}(C=1)=p$ and $\mathbf{P}(C=2)=1-p$. Given the value of $C,Y$ is a binomial random variable.

   We can find the conditional probability that Bob received the first coin given that he observed $k$ Heads using Bayes' rule.
   $$
   \begin{aligned}
   \mathbf{P}(C=1\mid Y=k) &= \frac{\mathbf{P}(Y=k\mid C=1)\mathbf{P}(C=1)}{\mathbf{P}(Y=k)}\\
   &= \frac{\mathbf{P}(Y=k\mid C=1)\mathbf{P}(C=1)}{\mathbf{P}(Y=k\mid C=1)\mathbf{P}(C=1)+\mathbf{P}(Y=k\mid C=2)\mathbf{P}(C=2)}\\
   &= \frac{\binom {3}{k}(1/4)^ k(3/4)^{3-k}\cdot p}{\binom {3}{k}(1/4)^ k(3/4)^{3-k}\cdot p+ \binom {3}{k}(1/4)^{3-k}(3/4)^{k}\cdot (1-p)}\\
   &= \frac{3^{3-k}\cdot p}{3^{3-k}\cdot p + 3^ k \cdot (1-p)}.
   \end{aligned}
   $$

2. Given that Bob observes $k$ Heads, he is to decide whether the first or second coin was used. To minimize the probability of error, he should use the MAP rule, which in this case is to decide on the first coin when $\mathbf{P}(C=1|Y=k)\geq \mathbf{P}(C=2|Y=k)$. From symmetry, the second item, namely $\mathbf{P}(C=2|Y=k)$ is equal to $\frac{3^ k\cdot (1-p)}{3^{3-k}\cdot p + 3^ k \cdot (1-p)}$. We then have the following equivalent versions of this decision rule:
   $$
   \begin{aligned}
   \mathbf{P}(C=1|Y=k) &\geq \mathbf{P}(C=2|Y=k)\\
   \frac{3^{3-k}\cdot p}{3^{3-k}\cdot p + 3^ k \cdot (1-p)} & \geq \frac{3^ k\cdot (1-p)}{3^{3-k}\cdot p + 3^ k \cdot (1-p)}\\
   3^{3-k}\cdot p & \geq 3^ k \cdot (1-p)\\
   3^{2k-3} &\leq \frac{p}{1-p}\\
   2k-3 & \leq \log _3 \frac{p}{1-p}\\
   k & \leq \frac{3}{2}+\frac{1}{2}\log _3 \frac{p}{1-p}.
   \end{aligned}
   $$

3. If $p=3/4$ the threshold in the rule above is equal to $2$. Therefore, Bob will decide that he received the first coin when he observes $0,1,$ or $2$ Heads, and will decide that he received the second coin when he observes $3$ Heads.

   We find the probability of a correct decision using the total probability theorem:
   $$
   \begin{aligned}
   \mathbf{P}(\text{Correct}) &= \mathbf{P}(\text{Correct}|C=1)\cdot p + \mathbf{P}(\text{Correct}|C=2)\cdot (1-p)\\
   &= \mathbf{P}(Y<3|C=1)\cdot p + \mathbf{P}(Y=3|C=2)\cdot (1-p)\\
   &= (1-\mathbf{P}(Y=3|C=1))\cdot p + \mathbf{P}(Y=3|C=2)\cdot (1-p)\\
   &=  (1-(1/4)^3)(3/4) + (3/4)^3(1/4)\\
   &=\frac{216}{256}=\frac{27}{32}.
   \end{aligned}
   $$

4. Bob will never decide that he received the first coin if the threshold in the decision rule is negative, i.e., when
   $$
   \begin{aligned}
   \frac{3}{2}+\frac{1}{2}\log _3 \frac{p}{1-p} &< 0\\
   \log _3 \frac{p}{1-p} &< -3\\
   \frac{p}{1-p} &< {1\over 27}\\
   p &< \frac{1}{28}.
   \end{aligned}
   $$
   If $p < 1/28$, the prior probability of receiving the first coin is so low that no amount of evidence from $3$ tosses of the coin will make Bob decide he received the first coin.



# Lecture 11. Derived Distribution

* Given the distribution of $X$, find the distribution of $Y = g(X)$.

  | Discrete                                                     | Continuous                                                   |
  | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | $Y = g(X),\quad p_Y(y) = \mathbf{P}(g(X) = y) = \sum\limits_{x:g(X)=y} p_X(x)$ |                                                              |
  | $Y=aX+b, \quad p_Y(y) = p_X({y-b \over a})$                  | $Y=aX+b, \quad f_Y(y) = {1 \over \vert a \vert}f_X({y-b \over a})$ |

  * a linear function of a normal r.v. is normal

    $X \sim \mathcal{N}(\mu, \sigma^2),\quad f_X(x) = {1\over \sqrt{2\pi} \sigma} e^{-(x-\mu)^2/2\sigma^2}, \quad f_Y(y)= {1\over \vert a \vert} {1 \over \sqrt{2\pi} \sigma} e^{-({y-b \over a} - \mu)^2 /2 \sigma^2} = {1\over \sqrt{2\pi }\sigma \vert a \vert } e^{-(y-b-a\mu)^2/2 \sigma^2 a^2}$

    Therefore, if $X \sim \mathcal{N}(\mu, \sigma^2),$ then $aX+b \sim \mathcal{N}(a\mu + b, a^2 \sigma^2)$.

  * general approach, using CDFs

    1. Find the CDF of $Y$: $F_Y(y) = \mathbf{P}(Y \leq y) = \mathbf{P}(g(x) \leq y)$
    2. Differentiate: $f_Y(y) = {dF_Y\over dy} (y)$

  * general formula when $g$ is monotonic (strictly increasing or decreasing)

    $Y=g(x), \quad f_Y(y) = f_X(h(y))\vert {dh \over dy}(y) \vert$,   where $h(y)$ is the inverse function of $g(x)$.

  * Non-monotonic example $Y = X^2$

    * Discrete case: $p_Y(y) = p_X(\sqrt{y}) + p_X(-\sqrt{y})$

    * Continuous cases:

      $F_Y(y) = \mathbf{P}(Y \leq y) = \mathbf{P}(X^2 \leq y) = \mathbf{P}(\vert x \vert \leq \sqrt{y}) = \mathbf{P}(-\sqrt{y} \leq x \leq \sqrt{y}) = F_X(\sqrt{y}) - F_X(-\sqrt{y})$

      $f_Y(y) = f_X(\sqrt{y}) {1\over 2 \sqrt{y}} - f_X(-\sqrt{y}){-1\over 2 \sqrt{y}}$

* Given the (joint) distribution of $X$ and $Y$, find the distribution of $Z = g(X,Y)$

  Let $Z = Y/X; X,Y$ independent, uniform on $[0,1]$.
  $$
  F_Z(z) = \mathbf{P}(Y/X \leq z) = \begin{cases}0 & z < 0\\ {1\over 2} z, & 0 \leq z \leq1 \\ 1 - {1\over 2z},\quad &z > 1 \end{cases} 
  $$
  ![lec11-multiple-rv](../assets/images/lec11-multiple-rv.png)

There are 3 selected exercises and 4 solved problems.

---

## Exercise 1 PDF of a general function

The random variable $X$ has a PDF of the form
$$
f_ X(x)=\begin{cases}  \displaystyle {\frac{1}{x^2},}&  \text{for }x\geq 1,\\ 0,& \text{otherwise.} \end{cases}
$$
Let $Y = X^2$. For $y \geq 1$, the PDF of $Y$ takes the form $f_Y(y) = {a \over y^b}.$ Find the values of $a$ and $b$.

**Solution**: 

For any $y \geq 1$, we have
$$
F_ Y(y)=\mathbf{P}(Y\leq y)=\mathbf{P}(X^2\leq y)=\mathbf{P}(X\leq \sqrt{y})=F_ X(\sqrt{y}).
$$
By differentiating and using the chain rule, we have
$$
f_ Y(y)=\frac{1}{2\sqrt{y}}f_ X(\sqrt{y})=\frac{1}{2y^{1.5}}.
$$

## Exercise 2 Using the formula for the monotonic case

The random variable $X$ is **exponential** with parameter $\lambda = 1$. The random variable $Y$ is defined by $Y = g(X) = 1/(1+X)$. For $y \in (0,1]$, find the PDF of $Y$: $f_Y(y)$.

**Solution**:

The exponential PDF of $X$ with parameter $\lambda =  1$ is
$$
f_X(x) = e^{-x}
$$
First find the inverse function,
$$
h(y) = {1-y \over y} = {1\over y} - 1
$$
Then find
$$
{dh\over dy}(y) = - {1\over y^2}
$$
Therefore,
$$
f_Y(y) = f_X(h(y)) \vert {dh \over dy} (y) \vert = e^{- (1/y) + 1} \cdot {1\over y^2}
$$

## Exercise 3 Nonmonotonic functions

Suppose that $X$ is a continuous random variable and that $Y = X^4$. Then, for $y \geq 0$, what is the PDF of $y$: $f_Y(y)$?

**Solution**: 

We have, for $y \geq 0$,
$$
F_ Y(y)=\mathbf{P}(Y\leq y)=\mathbf{P}(X^4\leq y)=\mathbf{P}(-y^{1/4}\leq X \leq y^{1/4}) =F_ X(y^{1/4})- F_ X(-y^{1/4}).
$$
By differentiating, and using also the chain rule, we obtain
$$
f_ Y(y)=f_ X(y^{1/4}) \cdot \frac{1}{4}\cdot y^{-3/4} +f_ X(-y^{1/4}) \cdot \frac{1}{4}\cdot y^{-3/4}.
$$

## Problem 1 The PDF of the absolute value of $X$

Let $X$ be a random variable with PDF $f_X$. Find the PDF of the random variable $Y = |X|$.

1. When $f_ X(x) \  = \    \begin{cases} 1/3, &  \text{if }-2 < x \leq 1, \\ 0, &  \text{otherwise}; \end{cases} $
2. For general $f_X(x)$.

**Solution**:

1. $f_X(x)$ for $-1 \leq x \leq 0$ gets added to $f_X(x)$ for $0 \leq x \leq 1$, so
   $$
   f_Y(y) = \begin{cases}2/3, &\text{if }0\leq y\leq 1,\\ 1/3, &\text{if }1 < y \leq 2,\\ 0, &\text{otherwise}. \end{cases}
   $$

2. When $y \geq 0$,
   $$
   \mathbf{P}(Y \leq y) =\mathbf{P}(|X| \leq y)   = \mathbf{P}(-y\leq X \leq y)=\int^y_{-y}f_X(x)dx = F_X(y) - F_X(-y).
   $$
   Taking derivatives of both sides, we have
   $$
   f_Y(y) = f_X(x) + f_X(-y), \quad y \geq 0.
   $$
   Therefore, the PDF of $Y$ is
   $$
   f_Y(y) = \begin{cases} f_X(x) + f_X(-y), \quad & \text{if }y \geq 0\\ 0, \quad & \text{if }y < 0 \end{cases}
   $$


## Problem 2 Derived distribution

Let $X$ have the normal distribution with mean $0$ and variance $1$, i.e.,
$$
f_ X(x) \  = \  \frac{1}{\sqrt{2\pi }} e^{-x^2/2}.
$$
Also, let $Y = g(X)$ where
$$
g(t) \  = \    \begin{cases} -t, &  \text{for } t \leq 0; \\ \sqrt{t}, &  \text{for }t > 0, \end{cases} 
$$
Find the PDF of $Y$.

**Answer**: $f_Y(y) = {1\over \sqrt{2\pi}} \left( 2y e^{-y^4/2} + e^{-y^2/2}\right)$

**Solution**:

Because of the definition of $g$, the random variable $Y$ takes on only non-negative values. Thus $f_Y(y)=0$ for any negative $y$. 

For $y>0$,
$$
\begin{aligned}
F_Y(y) &= \mathbf{P}(Y \leq y)\\
&= \mathbf{P}(X \in [-y, 0]) + \mathbf{P}(X \in (0,y^2])\\
&= \mathbf{P}(-y \leq X \leq y^2)\\
&= F_X(y^2) - F_x(-y).
\end{aligned}
$$
Taking the derivative of $F_Y(y)$ (and using the chain rule),
$$
\begin{aligned}
f_Y(y) & = 2yf_X(y^2) + f_X(-y)\\
&= {1\over \sqrt{2\pi}} \left( 2y e^{-y^4/2} + e^{-y^2/2}\right).
\end{aligned}
$$

## Problem 3 Ambulance travel time

An ambulance travels back and forth, at a constant speed , along a road of length . We may model the location of the ambulance at any moment in time to be uniformly distributed over the interval . Also at any moment in time, an accident (not involving the ambulance itself) occurs at a point uniformly distributed on the road; that is, the accident's distance from one of the fixed ends of the road is also uniformly distributed over the interval . Assume the location of the accident and the location of the ambulance are independent.

Supposing the ambulance is capable of **immediate** U-turns, compute the CDF and PDF of the ambulance's travel time to the location of the accident.

**Solution**:

We want to compute the CDF of the ambulance's travel time $T, \mathbf{P}(T \leq t) = \mathbf{P}(|X - Y| \leq vt)$, where $X$ and $Y$ are the locations of the ambulance and accident (uniform over $[0,l]$). Since $X$ and $Y$ are independent, we know:
$$
f_{X,Y}(x,y) = \begin{cases} {1\over l^2}, \quad &\text{if }0\leq x,y \leq l\\0, \quad &\text{otherwise} \end{cases}
$$

$$
\begin{aligned}
\mathbf{P}(T \leq t) &= \mathbf{P}(|X - Y| \leq vt) = \mathbf{P}(-vt \leq Y-X \leq vt)\\
&= \mathbf{P}(X - vt \leq Y \leq X + vt)
\end{aligned}
$$

We can see that $ \mathbf{P}(X - vt \leq Y \leq X + vt)$ corresponds to the integral of the joint density of $X$ and $Y$ over the shaded region in the figure below.

![u6-lec11-prob3](../assets/images/u6-lec11-prob3.png)

Because the joint density is uniform over the entire region, for $0 \leq t \leq {l \over  v}$,
$$
F_T(t) = (1/l^2) \times {1\over 2}\times (1-vt)^2 \times 2 = {2vt \over l} - {(vt)^2\over l^2}
$$
Therefore, we have
$$
F_T(t) = (1/l^2) \times (\text{Shaded area}) = \begin{cases}0, \quad &\text{if }t < 0\\ {2vt \over l} - {(vt)^2\over l^2}, &\text{if }0 \leq t \leq {l \over v}\\ 1, \quad &\text{if }t \geq {l \over v} \end{cases}
$$
By differentiating the CDF, we find the density of $T$:
$$
f_T(t) = \begin{cases}{2v\over l} - {2v^2 t \over l^2}, &\text{if } 0\leq t\leq {l\over v}\\ 0, \quad &\text{otherwise} \end{cases}
$$

## Problem 4 The PDF of the maximum

Let $X$ and $Y$ be independent random variables, each uniformly distributed on the interval $[0,1s]$.

1. Let $Z = \max\{X,Y\}$. Find the PDF of $Z$ for $0 < z < 1$.
2. Let $Z = \max \{ 2X,Y\}$. Find the PDF of $Z$, for $0 < z < 1$ and for $1 < z < 2$ respectively.

**Solution:**

1. The CDF of a random variable $U$ distributed uniformly on the interval $[0,1]$ is given by
   $$
   F_ U(u) = \begin{cases}  0, &  \text{if } u<0, \\ u, &  \text{if } 0\leq u\leq 1, \\ 1, &  \text{if } u>1.\end{cases}
   $$
   Let $Z = \max\{X,Y\}$. For $Z \in (0,1)$,
   $$
   \begin{aligned}
   {F}_ Z{(z)} &= \mathbf{P}(Z \leq z)\\
   &= \mathbf{P}(X \leq z \text { and } Y \leq z)\\
   &= {{F}}_ X(z) {{F}}_ Y(z)\\
   &= z^2
   \end{aligned}
   $$
   Hence, 
   $$
   f_ Z(z) = 2z
   $$

2. Let $Z = \max \{ 2X, Y\}$, the CDF is
   $$
   {F}_ Z{(z)} = \mathbf{P}(Z \leq z) = \mathbf{P}(2X \leq z \text { and } Y\leq z)={{F}}_{X}(z/2) {F}_ Y(z).
   $$
   Hence, for $0 < z < 1$, the CDF is
   $$
   {{F}}_ Z(z) = (z/2)\cdot z = z^2/2
   $$
   and the corresponding PDF is
   $$
   f_ Z(z) = z
   $$
   For $1 < z <2$, the CDF is
   $$
   {{F}}_ Z(z) = (z/2)\cdot 1 = z/2
   $$
   and the corresponding PDF is
   $$
   f_ Z(z) = 1/2
   $$



# Lecture 1. Probability Models and Axioms

* Sample space

  * Set ($\Omega$) must be
    * Mutually exclusive
    * Collectively exhaustive
    * At the right granularity
  * Event: a subset of the sample space

* Probability laws

  * Axioms	
    * Nonnegativity: $P(A) \geq 0$
    * Normalization: $P(\Omega) = 1$
    * (Finite) additivity: if $A \cap B = \empty$ (disjoint), then $P(A \cup B) = P(A) + P(B)$
  * Properties
    * From nonnegativity: $P(A) \leq 1$
    * From normalization: $P(\empty) = 0$
    * For A, B, C disjoint: $P(A \cup B \cup C) = P(A) + P(B) + P(C)$.
    * For $k$ disjoint events: $P(\{s_1, s_2, ..., s_k\}) = P(\{s_1\}) +... + P(\{s_k\}) = P(s_1) +... + P(s_k) $.
    * If $A \subset B$, then  $P(A) \leq P(B)$. 
    * $P(A \cup B) = P(A) + P(B) - P(A \cap B)$, where $P(A \cap B) \geq 0$.
    * $P(A \cup B) \leq P(A) + P(B)$.
    * $P(A \cup B \cup C) = P(A \cup (A^c \cap B) \cup (A^c \cap B^c \cap C)) =P(A) + P(A^c \cap B) + P(A^c \cap B^c \cap C).$

* Examples

  * Discrete but infinite sample space

    Given $P(n) = \frac{1}{2^n}, n = 1,2,...$, 
    $$
    P(\{2,4,6,...\}) = P(\{2\} \cup \{4\} ...) = P(2) + P(4) + ... = \frac{1}{2^2} + \frac{1}{2^4} + ... = \frac{1}{4}(1 + \frac{1}{4} + \frac{1}{4^2} + ...) = \frac{1}{4} \cdot \frac{1}{1-1/4} = \frac{1}{3}
    $$
    Using the formula:
    $$
    \sum^\infty_{n=1} = \frac{1}{2^n} = \frac{1}{2}\sum^\infty_{n=0} = \frac{1}{2}\cdot \frac{1}{1-1/2} = 1
    $$

* Discussion

  * Countable additivity

    If $A_1, A_2, ...$ is an infinite **sequence** of **disjoint** events, then $P(A_1 \cup A_2 \cup ...) = P(A_1) + P(A_2) + ...$

    This axiom only applies for a countable "sequence" of events. This requires that the infinite sets should be discrete rather than continuous.

    * A case when it does not apply: The sample space is the 2D plane. For any real number $x$, let $A_x$ be the subset of the plane that consists of all points of the vertical line through the point $(x,0)$ i.e. $A_ x=\{ (x,y): y\in \mathrm{Re\, }\}$.
      $$
      P(\bigcup A_x) \neq \sum P(A_x)
      $$

  * Mathematical subtleties

* Interpretations of probabilities

  ![roleOfProb](../assets/images/U1-lec1-role.png)

* Mathematical background

  * Sets: a collection of distinct elements

    * For some sets $S_n, n=1,2,...$,

      $x \in \bigcup_n S_n$ iff $x\in S_n$ for some $n$; $x \in \bigcap_n S_n$ iff $x \in S_n$, for all $n$.

    * Properties: 

      $S \cup T = T \cup S, \quad S \cup (T \cup U) = (S \cup T) \cup U$

      $S \cap (T \cup U) = (S \cap T) \cup (S \cap U), \quad S \cup(T \cap U) = (S \cup T) \cap (S \cap U)$

      $(S^c)^c = S, \quad S \cap S^c = \empty$

      $S \cup \Omega = \Omega, \quad S \cap \Omega = S$

    * De Morgan's Law

      $\Big(\bigcap_{n} S_n\Big)^c = \bigcup_n S_n^c; \quad \Big( \bigcup_n S_n\Big)^c = \bigcap_n S_n^c$

  * Convergence:

    * If $a_i \leq a_{i+1}$, for all $i$, then either

      1. the sequence “converges to $\infty$”

      2. the sequence converges to some real number $a$

    * If $|a_i - a| \leq b_i$ for all $i$ and $b_i \rightarrow 0$, then $a_i \rightarrow a$.

  * Infinite series: 

    $\sum^{\infty}_{i=1} a_i =  \lim\limits_{n \rightarrow \infty} \sum^n_{i=1}a_i$ provided limit exists.

    * If $a_i \geq 0$, limit exists.

    * If terms $a_i$ do not all have the same sign

      * Limit may exist but be different if we sum in a different order

      * Limit exists and independent of order of summation of $\sum^\infty_{i=1}|a_i| < \infty$.

  * Geometric series

    $\sum^\infty_{i=0}\alpha^i = 1 + \alpha + \alpha^2 + ... = \frac{1}{1-\alpha}, \quad |\alpha| < 1$

    Proof: 

    Set $S =1+ \alpha + \alpha^2 + ...$, Compute $(1-\alpha)S = 1 - \alpha^{n+1}$. When $n \rightarrow \infty$, $(1-\alpha)S = 1$.


---

There are 3 selected exercises and 4 solved problems.

## Exercise 1 Continuous probability calculations

Consider a sample space that is the rectangular region $[0,1]×[0,2]$, i.e., the set of all pairs $(x,y)$ that satisfy $0≤x≤1$ and $0≤y≤2$. Consider a “uniform" probability law, under which the probability of an event is half of the area of the event. Find the probability of the following events:

a) The two components $x$ and $y$ have the same values. 

b) The value, $x$, of the first component is larger than or equal to the value, $y$, of the second component.

c) The value of $x^2$ is larger than or equal to the value of $y$.

**Answer**: 

a) 0; b) 1/4; c) 1/6

**Solution**: 

a) This event is a line, and since a line has zero area, the probability is 0.

b) This event is a triangle with vertices at $(0,0), (1,0), (1,1)$. Its area is $1/2$, and therefore the probability is $1/4$.

c) This event corresponds to the region below the curve $y=x^2$, where $x$ ranges from 0 to 1. The area of this region is
$$
\int _0^1 x^2\,  dx= \left.\frac{x^3}{3}\right|_0^1 =\frac{1}{3},
$$
and therefore the corresponding probability is $1/3 * 1/2 = 1/6$.

## Exercise 2 Using countable additivity

Let the sample space be the set of positive integers and suppose that $P(n)=1/2^n$, for $n=1,2,….$ Find the probability of the set $\{3,6,9,…\}$, that is, of the set of of positive integers that are multiples of 3.

Using countable additivity, and with $α=2^{−3}=1/8$, the desired probability is
$$
\frac{1}{2^3}+\frac{1}{2^6}+\frac{1}{2^9}+\cdots =\alpha +\alpha ^2+\alpha ^3+\cdots =\frac{\alpha }{1-\alpha }=\frac{1/8}{1-(1/8)}=\frac{1}{7}.
$$

## Exercise 3 Uniform probabilities on the integers

Let the sample space be the set of all positive integers. Is it possible to have a “uniform" probability law, that is, a probability law that assigns the same probability $c$ to each positive integer?

**Answer**: No

**Solution**: 

Suppose that $c=0$. Then by countable additivity 
$$
1=\mathbf{P}(\Omega )=\mathbf{P}\big (\{ 1\} \cup \{ 2\} \cup \{ 3\} \cdots \big ) =\mathbf{P}(\{ 1\} )+\mathbf{P}(\{ 2\} )+\mathbf{P}(\{ 3\} )+\cdots = 0+0+0+\cdots =0,
$$
which is a contradiction.

Suppose that $c > 0$. Then there exists an integer $k$ that $kc > 1$. By additivity, 
$$
\mathbf{P}\big (\{ 1,2,\ldots ,k\} ) =kc>1,
$$
which contradicts the normalization axiom.

## Problem 1 Uniform probabilities on a square

Romeo and Juliet have a date at a given time, and each will arrive at the meeting place with a delay between 0 and 1 hour, with all pairs of delays being “equally likely," that is, according to a uniform probability law on the unit square. The first to arrive will wait for 15 minutes and will leave if the other has not arrived. What is the probability that they will meet?

**Answer**: 7/16

**Solution**: 

First assume Romeo and Juliet arrive in 15min interval and we can draw the sample space in a discrete way.

![U1-lec1-prob1](../assets/images/U1-lec1-prob1.png)

So that the probability of meeting each other is 
$$
\mathbf{P}(\text{meet}) = 13/25
$$
Now we draw the sample space in a continuous way

![U1-lec1-prob1-2](../assets/images/U1-lec1-prob1-2.png)

So that the probability of meeting each other is 
$$
\mathbf{P}(\text{meet}) = 1 - 1/2 * 3/4 * 3/4 * 2 = 7/16
$$

## Problem 2 Bonferroni's Inequality

(a) Prove that for any two events $A_1$ and $A_2$, we have
$$
\mathbf{P}(A_1 \cap A_2) \geq \mathbf{P}(A_1) + \mathbf{P}(A_2)-1.
$$
(b) Generalize to the case of $n$ events $A_1,A_2,…,A_n$, by showing that
$$
\mathbf{P}(A_1 \cap A_2 \cap \cdots \cap A_ n)\geq \mathbf{P}(A_1)+\mathbf{P}(A_2)+\cdots +\mathbf{P}(A_ n)-(n-1).
$$
Proof: 

With $\mathbf{P}(A_1 \cup A_2) \leq \mathbf{P}(A_1) + \mathbf{P}(A_2) $, we can prove (a) and (b), for example,
$$
\begin{aligned}
&\mathbf{P}((A_1 \cap A_2)^c) = \mathbf{P}(A_1^c \cup A_2^c) \leq \mathbf{P}(A_1^c) + \mathbf{P}(A_2^c)\\
&\implies 1-\mathbf{P}(A_1 \cap A_2) \leq 1-\mathbf{P}(A_1) + 1-\mathbf{P}(A_2)\\
&\implies -\mathbf{P}(A_1 \cap A_2) \leq 1-\mathbf{P}(A_1) -\mathbf{P}(A_2)\\
&\implies \mathbf{P}(A_1 \cap A_2) \geq \mathbf{P}(A_1) +\mathbf{P}(A_2)-1
\end{aligned}
$$

## Problem 3 Parking lot problem

Mary and Tom park their cars in an empty parking lot with $n≥2$ consecutive parking spaces (i.e, $n$ spaces in a row, where only one car fits in each space). Mary and Tom pick parking spaces at random; of course, they must each choose a different space. (All pairs of distinct parking spaces are equally likely.) What is the probability that there is at most one empty parking space between them?

**Answer**: $\mathbf{P}(A) = \frac{4n-6}{n(n-1)}.$

**Solution**: 

The sample space is $Ω=\{(i,j):i≠j,1≤i,j≤n\}$, where outcome $(i,j)$ indicates that Mary and Tom parked in slots $i$ and $j$, respectively. We apply the **discrete uniform probability law** to find the required probability. We are interested in the probability of the event.
$$
A = \{  (i,j) \in \Omega : |i-j| \leq 2 \}
$$
We first find the cardinality of $Ω$. There are $n^2$ pairs $(i,j)$, but since the set $Ω$ excludes outcomes of the form $(i,i)$, the cardinality of $Ω$ is $n^2−n=n(n−1)$.

![U1-lec1-prob3](../assets/images/U1-lec1-prob3.png)

If $n≥3$, event $A$ consists of the four lines indicated in the figure above and contains $2(n−1)+2(n−2)=4n−6$ elements. If $n=2$, event $A$ contains exactly $2$ elements, namely, $(1,2)$ and $(2,1)$, which agrees with the formula $4(2)−6=2$. Therefore,
$$
\mathbf{P}(A) = \frac{4n-6}{n(n-1)}.
$$

## Problem 4 Upper and lower bounds on the probability of intersection

Given two events $A,B$ with $P(A)=3/4$ and $P(B)=1/3$, what is the smallest possible value of $P(A∩B)$? The largest? That is, find $a$ and $b$ such that,
$$
a\leq \mathbf{P}(A\cap B)\leq b,
$$
holds and any value in the closed interval [a,b] is possible.

**Answer**: a = 1/12; b = 1/3

**Solution**: 

From Bonferroni Inequality, we have the lower bound,
$$
\mathbf{P}(A\cap B)\geq \mathbf{P}(A)+\mathbf{P}(B)-1 = \frac{1}{12}.
$$
Since $A\cap B\subset A$ and $A\cap B\subset B$, we have the higher bound,
$$
\mathbf{P}(A\cap B)\leq \mathbf{P}(A) \quad \text {and}\quad \mathbf{P}(A\cap B)\leq \mathbf{P}(B)\\
\mathbf{P}(A\cap B)\leq \frac{1}{3}.
$$

# Lecture 6. Variance; Conditioning on an event; Multiple r.v.'s

* Variance and its properties

  * Definition of variance: $\rm{var}(X) = E[(X-\mu)^2]$

  * Properties

    * $\rm{var}(aX+b) = a^2\rm{var}(X)$
    * $\rm{var}(X) = E[X^2] - (E[X])^2$

  * Variance of the Bernoulli

    * $\rm{var}(X) = \sum_x(x-E[X])^2p_X(x) = (1-p)^2p + (0-p)^2(1-p) = p(1-p)$
    * $\rm{var}(X) = E[X^2] - (E[X])^2 = p - p^2 = p(1-p)$

  * Variance of the uniform PMFs

    $\begin{aligned}&\rm{var}(X) \\&= E[X^2] - (E[X])^2 \\&= \frac{1}{n+1} (0^2 + 1^2 + 2^2 + ... + n^2) - ({n \over 2})^2 \\&= \frac{1}{n+1} ({1 \over 6} n(n+1)(2n+1)) - ({n \over 2})^2 \\&= {1 \over 12} n (n+2) \quad \text{ where }n = b-a \end{aligned}$

* Conditioning a r.v. on an event
  
    * Conditional PMF, mean, variance
      
        | Non-conditional              | Conditional                          |
        | ---------------------------- | ------------------------------------ |
        | $p_X(x) = \mathbf{P}(X=x)$   | $p_{X\|A}(x) = \mathbf{P}(X=x\|A)$   |
        | $p_X(x) = \mathbf{P}(X=x)$   | $p_{X\|A}(x) = \mathbf{P}(X=x\|A)$   |
      | $E[X] = \sum_xx p_X(x)$      | $E[X\|A] = \sum_x x p_{X\|A}(x)$     |
      | $E[g(X)] = \sum_xg(x)p_X(x)$ | $E[g(X)\|A] = \sum_xg(x)p_{X\|A}(x)$ |
      
    * Total expectation theorem
    
    $E[X] = \mathbf{P}(A_1)E[X|A_1] + ... +\mathbf{P}(A_n)E[X|A_n] $
  
* Geometric PMF

  * **Memorylessness**: 

    * number of remaining coin tosses, conditioned on Tails in the first toss, is Geometric, with parameter $p$. 

      $p_X(k) = (1-p)^{k-1}p, \quad k=1,2,3...$

    * Conditioned on $X > 1$, $X - 1$ is geometric with parameter $p$.

      $p_{X-1|X>1}(k) = p_X(k) = p_{X-n|X>n}(k) $

    * Conditioned on $X > n$, $X - n$ is geometric with parameter $p$.

      $p_{X-n|X>n}(k) = p_X(k) $

  * Mean of Geometric PMF

    $E[X] = \sum^\infty_{k=1}k p_X(k) = \sum^{\infty}_{k=1} k(1-p)^{k-1}p = \frac{1}{p}$
    
  * Variance of Geometric PMF

    * Given $X > 1, X-1$ has the same geometric PMF (unconditional PMF of $X$)

    * Given $X>1$, conditional PMF of $X$: same as unconditional PMF of $X+1$

      $E[X|X>1] = 1 + E[X]$

    $\begin{aligned}E[X^2] &= P(X=1)E[X^2|X=1] + P(X >1)E[X^2|X>1]\\ &= p \cdot 1 + (1-p) \cdot (E[X^2] + 2 E[X]+1)\\&={2-p \over  p^2}\end{aligned}$

    $\mathsf{Var}(X) = E[X^2] - (E[X])^2 = {2-p \over p^2} - {1 \over p^2} = {1-p \over p^2}$

* Multiple random variables

  * Joint and marginal PMFs

    | Joint and marginal PMFs                                      | Properties                                                   |
    | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | $p_{X,Y}(x,y) = \mathbf{P}(X=x \text{ and } Y = y)$          | $\sum_x\sum_y p_{X,Y}(x,y) = 1\\p_X(x) = \sum_yp_{X,Y}(x,y)\\p_Y(y) = \sum_xp_{X,Y}(x,y)$ |
    | $p_{X,Y,Z}(x,y,z) = \mathbf{P}(X=x \text{ and } Y = y \text{ and } Z = z)$ | $\sum_x\sum_y\sum_z p_{X,Y,Z}(x,y,z) = 1\\p_X(x) = \sum_y\sum_zp_{X,Y,Z}(x,y,z)\\p_{X,Y}(x,y) = \sum_zp_{X,Y,Z}(x,y,z)$ |

  * Expected value rule

    $E[g(X,Y)] = \sum_x \sum_y g(x,y) p_{X,Y}(x,y)$

  * Linearity of expectations

    $E[aX+b] = aE[X] + b\\ E[X+Y] = E[X] + E[Y] \\E[X_1 + ... + X_n] = E[X_1] + ... + E[X_n]$

* The mean of the binomial PMF

  * $X$: binomial with parameters $n,p$
  
    $E[X] = \sum^n_{k=0} k {n \choose k} p_x(k)= \sum^n_{k=0} k {n \choose k}p^k(1-p)^{n-k}$
  
    $E[X] = np$

---

There are 4 selected exercises and 3 solved problems.

## Exercise 1 Total expectation calculation

We have two coins, A and B. For each toss of coin A, we obtain Heads with probability $1/2$; for each toss of coin B, we obtain Heads with probability $1/3$. All tosses of the same coin are independent. We select a coin at random, where the probability of selecting coin A is $1/4$, and then toss it until Heads is obtained for the first time. What's the expected number of tosses until the first Heads?

**Answer**: $11/4$

**Solution**: 

Let $T$ be the number of tosses until the first Heads. Once a coin is selected, the conditional distribution of $T$ is **geometric**, with a mean of $1/p$, where $p$ is the probability of Heads for the selected coin. Let $C_A$ and $C_B$ denote the events that coin A or B, respectively, is selected.
$$
{\bf E}[T]=\mathbf{P}(C_ A) {\bf E}[T\mid C_ A]+\mathbf{P}(C_ B){\bf E}[T\mid C_ B]=\frac{1}{4}\cdot 2 +\frac{3}{4}\cdot 3=\frac{11}{4}.
$$

## Exercise 2 Memorylessness of the geometric

Let $X$ be a geometric random variable, and assume that $\rm{Var}(X)=5$. What is the conditional variance $\textsf{Var}(X-4\mid X>4)$.

**Answer**: $\textsf{Var}(X-4\mid X>4) = 5$

**Solution**: The conditional distribution of $X−4$ given $X>4$ is the same geometric PMF that describes the distribution of $X$. Hence, $\textsf{Var}(X-4\mid X>4)=\textsf{Var}(X)=5$.

## Exercise 3 Joint PMF calculation

The random variable $V$ takes values in the set $\{0,1\}$ and the random variable $W$ takes values in the set $\{0,1,2\}$. Their joint PMF is of the form.
$$
p_{V,W}(v,w)=c\cdot (v+w),
$$
where $c$ is some constant, for $v$ and $w$ in their respective ranges, and is zero everywhere else.

Find the value $c$ and $p_V(1)$.

**Answer**: $c = 1/9; p_V(1)  =2/3$

**Solution**: 

The sum of the entries of the PMF is
$$
p_{V,W}(v,w) = c\cdot (0+0)+c\cdot (0+1)+c\cdot (0+2)+c\cdot (1+0)+\ldots = 9c
$$
Since this sum must be equal to 1, we have $c = 1/9$.
$$
p_ V(1)=\sum _{w=0}^2 p_{V,W}(1,w)= p_{V,W}(1,0)+p_{V,W}(1,1)+p_{V,W}(1,2) =\frac{1}{9}(1+2+3)=\frac{6}{9}.
$$

## Exercise 4 Expected value rule

Let $X$ and $Y$ be discrete random variables. For each one of the formulas below, state whether it is true or false.

a) ${\bf E}[X^2]=\sum _ x x^2 p_ X(x)$

b) ${\bf E}[X^2]=\sum _ x\sum _ y x^2 p_{X,Y}(x,y)$

Since ${\bf E}[g(X,Y)]=\sum _ x \sum _ y g(x,y) p_{X,Y}(x,y),$ for the function $g(x,y)=x^2$.

c) ${\bf E}[X^2]=\sum _ z z p_{X^2}(z)$

Since ${\bf E}[Z]=\sum _ z z p_ Z(z)$ where $Z$ is the random variable $X^2$

## Problem 1 Coupon collector problem

A particular professor is known for his arbitrary grading policies. Each paper receives a grade from the set $\{A,A−,B+,B,B−,C+\}$, with equal probability, independently of other papers. How many papers do you expect to hand in before you receive each possible grade at least once?

**Answer**: 

Let $K = 6, Y_i = $ the number of papers till $i$'th new grade. $X_i = Y_{i+1} - Y_i$. so $Y_6 = \sum^5_{i=0}X_i$.
$$
X_i = \mathsf{Geo}(\frac{6-i}{6})\\
\mathbb{E}(X_i) = \frac{6-i}{6}\\
\mathbb{E}(\sum^5_{i=0} X_i) = \sum^5_{i=0} \mathbb{E}[X_i] = 6 \cdot \sum^5_{i=1} \frac{1}{6-i} = 6 \cdot \sum^6_{i=1} \frac{1}{i} = 14.7\\
\mathbb{E}[Y_k] = k \sum^{k-1}_{i=1}\frac{1}{i} \approx k \ln k
$$
$\mathbb{E}[Y_k] \approx k \ln k$ is known as the scaling law for the coupon collector's problem that it takes about $k \ln k$ trials until we collect all $k$ coupons.

## Problem 2 Conditioning example 

Suppose that $X$ and $Y$ are independent, identically distributed, geometric random variables with parameter $p$. Show that
$$
\mathbf{P}(X=i \mid X+Y=n)=\frac{1}{n-1}, \qquad \text{for }i=1,2,\ldots ,n-1.
$$
**Answer**:

Consider repeatedly and independently tossing a coin with probability of heads $p$. We can interpret $\mathbf{P}(X=i | X + Y =n)$ as the probability that we obtained heads for the first time on the $i$th toss given that we obtained heads for the second time on the $n$th toss. (Specifically, the condition part states that it take $n$ trials to get the first two heads. The main part means that it takes $i$ trials to get the first head.)

We can argue that given that the second heads occurred on the $n$th toss, the first heads is equally likely to have come up at any toss between $1$ and $n-1$.
$$
\begin{aligned}
\mathbf{P}(X =i | X + Y = n) &= \frac{\mathbf{P}(X = i, X+Y = n)}{\mathbf{P}(X + Y = n)} \\
&= \frac{\mathbf{P}(X = i, Y =n-i)}{\mathbf{P}(X + Y = n)} \\
&= \frac{\mathbf{P}(X = i) \mathbf{P}(Y = n-i)}{\mathbf{P}(X + Y = n)}\\
\end{aligned}
$$
where the last step follows from the assumption that $X$ and $Y$ are independent. Also,
$$
\mathbf{P}(X = i) = p (1-p)^{i-1}, \quad \text{for } i\geq 1,
$$
and
$$
\mathbf{P}(Y = n-i) = p(1-p)^{n-i-1},\quad \text{for }n-i \geq 1.
$$
It follows that
$$
\mathbf{P}(X=i) \mathbf{P}(Y = n-i) = \begin{cases}p^2(1-p)^{n-2}, & \text{if }i = 1, ..., n-1\\0, & \text{otherwise} \end{cases}
$$

$$
\begin{aligned}
\mathbf{P}(X + Y =n ) &= \sum^{n-1}_{k=1} \mathbf{P}(X =k) \mathbf{P}(X+Y =n | X=k)\\
&= \sum^{n-1}_{k=1} \mathbf{P}(X =k) \mathbf{P}(X+Y = n)\\
&= \sum^{n-1}_{k=1} \mathbf{P}(X =k) \mathbf{P}(Y = n-k)\\
&= \sum^{n-1}_{k=1}(1-p)^{k-1} p (1-p)^{n-k-1}p\\
&= \sum^{n-1}_{k=1} (1-p)^{n-2} p^2
\end{aligned}
$$

To put everything together
$$
\begin{aligned}
\mathbf{P}(X =i | X + Y = n)&= \frac{\mathbf{P}(X = i) \mathbf{P}(Y = n-i)}{\mathbf{P}(X + Y = n)}\\
&=\frac{(1-p)^{n-2}p^2}{(n-1)(1-p)^{n-2}p^2}\\
&=\frac{1}{n-1}
\end{aligned}
$$
Note that for $i \in \{1, ..., n-1\}$, this expression does not depend on $i$. Additionally, $\mathbf{P}(X + Y =n )$ does not depend on $i$ either. Therefore, for any $i \in \{1, ..., n-1\}, \mathbf{P}(X = i| X + Y =n)$ has the same value. Hence 
$$
\mathbf{P}(X=i| X+Y=n)= \frac{1}{n-1}, \quad i=1,...,n-1
$$

## Problem 3

Compute $\mathbf{E}(X)$ for the following random variable $X$:
$$
 X=\, \text {Number of tosses until all 10 numbers are seen (including the last toss) by tossing a fair 10-sided die}.
$$
To answer this, we will use induction and follow the steps below:

Let $\mathbf{E}(i)$ be the expected number of additional tosses until all $10$ numbers are seen (including the last toss) **given** $i$ **distinct numbers have already been seen**.

1. Find $\mathbf{E}(10)$

2. Write down a relation between $\mathbf{E}(i)$ and $\mathbf{E}(i+1)$, for $i = 0,1,...,9$, as 
   $$
   {\bf E}(i)={\bf E}(i+1)+ f(i)
   $$

3. Find $\mathbf{E}(X)$

**Answer**: 

1) $\mathbf{E}(10)=0$

2) The induction step is as follows, for $i=1,2,...,9$:
$$
\begin{aligned}
{\bf E}(i) &=({\bf E}(i)+1)\times \frac{i}{10} + ({\bf E}(i+1)+1) \times \left( 1-\frac{i}{10} \right)\\
\Longleftrightarrow {\bf E}(i) &= {\bf E}(i+1) + {10 \over 10-i}
\end{aligned}
$$
3) Using $\mathbf{E}(10)=0$, we have
$$
\frac{10}{10}+\frac{10}{9}+\ldots +\frac{10}{2}+\frac{10}{1}+0\approx 29.28968.
$$
**Explanation of Q2:**

The solution is making use of *conditional expectations*. For convenience, let $Y_i$ denote the **number of tosses required to get all 10 numbers given that we have already seen $i$ distinct numbers**, $i=0,1,...,9$. So, we are asked to find $\mathbf{E}[Y_i]=\mathbf{E}(i).$

Now, let $A_{i+1}$ denote **the event of getting a distinct number on the next toss, given that we have already seen $i$ distinct numbers**. Then the probability of this event is $\mathbf{P}(A_{i+1})=\frac{10-i}{10}=1-\frac{i}{10}$, and the probability of its complement (NOT seeing a distinct number on the next toss, given that we have already seen $i$ distinct numbers), is $\mathbf{P}(A_{i+1}^\mathsf{c})=\frac{i}{10}$.

Recall that by the conditional expectation rule, 
$$
\begin{aligned}\mathbf{E}[Y_i]&=\mathbf{P}(A_{i+1})\mathbf{E}[Y_i|A_{i+1}]+\mathbf{P}(A_{i+1}^\mathsf{c})\mathbf{E}[Y_i|A_{i+1}^\mathsf{c}]\end{aligned}.
$$
Substituting the probabilities above, we have 
$$
\begin{aligned}\mathbf{E}[Y_i]&=\left(1-\frac{i}{10}\right)\mathbf{E}[Y_i|A_{i+1}]+\left(\frac{i}{10}\right)\mathbf{E}[Y_i|A_{i+1}^\mathsf{c}].\end{aligned}
$$
Notice that if $A_{i+1}$ happens, we change to the state of having seen $(i+1)$ distinct numbers. Hence, **the expected number of tosses until we see all $10$ numbers conditioned on $A_{i+1}$ and having seen $i$ distinct numbers before that**, which is given by $\mathbf{E}[Y_i|A_{i+1}]$ can be written as $\mathbf{E}[Y_i|A_{i+1}] = 1 + \mathbf{E}[Y_{i+1}]$, where $1$ indicates the toss that led to even $A_{i+1}$. Similarly, if we do not see a distinct number on the next toss when we had seen $i$ distinct numbers already, i.e., if $A^c_{i+1}$ happens, our count of distinct numbers seen so far stays at $i$ itself, and hence $\begin{aligned}\mathbf{E}[Y_i|A_{i+1}^\mathsf{c}]&=1+\mathbf{E}[Y_i].\end{aligned}$

Substituting the values of the two expectations into our conditional expectation formula, we get 
$$
\begin{aligned}\mathbf{E}[Y_i]&=\left(1-\frac{i}{10}\right)\left(1+\mathbf{E}[Y_{i+1}]\right)+\left(\frac{i}{10}\right)\left(1+\mathbf{E}[Y_i]\right),\end{aligned}
$$
which is the recursion given in the answer.


# Lecture 15. Linear models with normal noise

* Recognizing normal PDFs

  Recall the normal random variable and its PDF:
  $$
  X \sim \mathcal{N}(\mu, \sigma^2) \qquad f_X(x) = {1\over \sigma \sqrt{2\pi} } \exp\left( {-(x - \mu)^2 \over 2\sigma^2}\right)
  $$
  For $f_X(x) = c \cdot \exp \left( \alpha x^2 + \beta x + \gamma\right),\ a>0$, we recognize it a normal PDF, with mean $\mu = -\beta/2\alpha$ and variance $\sigma^2 = 1/2\alpha$ by 
  $$
  \alpha x^2 + \beta x + \gamma = \alpha(x^2 + {\beta \over \alpha} x + {\gamma \over \alpha}) = \alpha ((x+ {\beta \over 4\alpha^2})^2 - {\beta^2 \over 4 \alpha^2} + {\gamma\over \alpha})
  $$
  Thus, the PDF can be written as
  $$
  f_X(x) = c\cdot \exp \left(-\alpha (x + {\beta \over 2 \alpha})^2 \right) \exp \left( -\alpha\left( -{\beta \over 4\alpha^2 }+ {\gamma \over \alpha} \right)\right)
  $$
  To find the mean of the PDF $f_X(x) = c \cdot \exp \left( \alpha x^2 + \beta x + \gamma\right)$, we know the mean happens to the peak of the distribution, so we can maximize the PDF, which equals to minimizing the quadratic function $\alpha x^2 + \beta x + \gamma$.

  To minimize $\alpha x^2 + \beta x + \gamma$, we take the derivative with respect to $x$ and set it to zero.
  $$
  {\partial \over\partial x }\alpha x^2 + \beta x + \gamma = 2\alpha x + \beta  = 0\\
  x = - {\beta \over 2\alpha}
  $$

* Estimating a normal random variable in the presence of additive normal noise

  $X = \Theta + W, \quad \Theta, W: \mathcal{N}(0,1), \quad $ independent
  $$
  f_{X|\Theta}(x|\theta): \mathcal{N}(\theta, 1)\\
  f_{\Theta|X}(\theta |x) = {1 \over f_X(x) }c \ \exp\left( -{1\over 2} \theta^2 \right) \ c \ \exp\left( -{1\over 2} (x- \theta)^2 \right) = c(x) \exp\left( - \text{quadratic}(\theta) \right)
  $$
  To find the conditional expectation, we maximize the distribution, which equals to minimizing the quadratic function with $x$ fixed.
  $$
  \min_\Theta \left[ {1\over 2} \theta^2 + {1\over 2} (x-\theta)^2 \right]\\
  \theta + (\theta -x ) =0\\
  \hat{\theta}_{MAP}=\hat{\theta}_{LMS} = \mathbf{E}[\Theta | X=x] = {x \over 2}\\
  \hat{\theta}_{MAP}=\hat{\theta}_{LMS} = \mathbf{E}[\Theta | X] = {X \over 2}
  $$
  Properties: 

  * Posterior is normal
  * LMS and MAP estimators coincide
  * These estimators are linear, of the form $\widehat{\Theta} = aX+b$

* The case of multiple observations

  $X_1= \Theta + W_1, ..., X_n = \Theta + W_n$. $\Theta \sim \mathcal{N}(x_0, \sigma^2_0), W_i \sim \mathcal{N}(0, \sigma_i^2)$. $\Theta, W_1, ..., W_n$ independent.

  Given $\Theta = \theta: \quad x_i = \theta + W_i \sim \mathcal{N}(\theta, \sigma_i^2)$. 
  $$
  f_{X_i | \Theta}(x_i | \theta) = c_i \exp\left( {-(x_i - \theta)^2\over 2\sigma_i^2}  \right)
  $$
  Given $\Theta = \theta: \quad W_i$ independent $\implies X_i$ independent
  $$
  \begin{aligned}
  f_{\Theta |X} (\theta|x) &= {1\over f_X(x)} \  c_0  \ \exp\left( -{(\theta -x_0)^2\over 2\sigma_0^2} \right) \prod\limits^n_{i=1} \ c_i \ \exp\left(-{(x_i-\theta)^2\over 2 \sigma_i^2}\right)\\
  &= c \ \exp (- \text{quad}(\theta))\\
  \text{where    quad}(\theta) &= {(\theta-x_0)^2\over 2\sigma_0^2} + {(\theta-x_1)^2\over 2\sigma_1^2} + ... + {(\theta-x_n)^2\over 2\sigma_n^2}
  \end{aligned}
  $$
  To find the conditional expectation,
  $$
  \begin{aligned}
  &{\partial \over \partial \theta} \text{quad}(\theta) = 0\\
  \implies& \sum^n_{i=0} {(\theta - x_i)\over \sigma_i^2} = 0\\
  \implies& \theta \sum^n_{i=0} {1 \over \sigma_i^2} = \sum^n_{i=0} { x_i \over \sigma_i^2}\\
  \implies& \hat{\theta}_{MAP}= \hat{\theta}_{LMS} = \mathbf{E}[\Theta|X=x] = {\sum\limits^n_{i=0}{x_i\over\sigma^2_i } \over \sum\limits^n_{i=0} {1\over \sigma_i^2} }
  \end{aligned}
  $$
  Properties

  * Posterior is normal
  * LMS and MAP estimates coincide
  * These estimates are linear, of the form $\hat{\theta} = a_0 + a_1 x_1 + ... + a_n x_n$.

  Interpretations

  * Estimates $\hat{\theta}$ weighted average of $x_0$ (prior mean) and $x_i$ (observations)
  * Weights determined by variances

* Mean squared error

  $\mathbf{E}\left[(\Theta - \widehat{\Theta})^2|X=x \right] = \mathbf{E}\left[(\Theta - \widehat{\theta})^2|X=x \right] = \mathsf{Var}(\Theta|X=x) = 1/\sum\limits^n_{i=0}{1\over\sigma_i^2 }\\\mathbf{E}\left[(\Theta - \widehat{\Theta})^2\right] = \int \mathbf{E}\left[(\Theta - \widehat{\Theta})^2|X=x \right]f_X(x)\ dx$

  Thus
  $$
  \mathbf{E}\left[(\Theta - \widehat{\Theta})^2|X=x \right]  = \mathbf{E}\left[(\Theta - \widehat{\Theta})^2\right]=1/\sum\limits^n_{i=0}{1\over\sigma_i^2 }
  $$

  * Example 1

    Recall $f_X(x) = c \cdot \exp \left( \alpha x^2 + \beta x + \gamma\right),\ a>0$, is a normal PDF, with mean $\mu = -\beta/2\alpha$ and variance $\sigma^2 = 1/2\alpha$. Consider $f_{\Theta |X} (\theta|x) = c \ \exp (- \text{quad}(\theta))$ , where $\text{quad}(\theta) = {(\theta-x_0)^2\over 2\sigma_0^2} + {(\theta-x_1)^2\over 2\sigma_1^2} + ... + {(\theta-x_n)^2\over 2\sigma_n^2}$. We have
    $$
    \alpha =  {1 \over 2\sigma_0^2} + {1\over 2\sigma_1^2} + ... + {1\over 2\sigma_n^2}
    $$
    Properties:

    * Some $\sigma_i^2$ small $\implies $ MSE small
    * All $\sigma_i^2$ large $\implies $ MSE large

  * Example 2

    $\sigma^2_0=\sigma^2_1=...=\sigma^2_n=\sigma^2$

    $\mathbf{E}\left[(\Theta - \widehat{\Theta})^2|X=x \right] = \mathbf{E}\left[(\Theta - \widehat{\theta})^2|X=x \right] = \mathsf{Var}(\Theta|X=x) = 1/\sum\limits^n_{i=0}{1\over\sigma_i^2 } = {1\over (n+1){1\over \sigma^2}} = {\sigma^2 \over n+1}$

    This implies that as $n$ increases we improve the performance.

* The case of multiple parameters: trajectory estimation

  * Random variables $\Theta_0, \Theta_1, \Theta_2$
  
    Independent; priors $f_{\Theta_j}$
  
  * Measurements at times $t_1, ..., t_n$
  
    The observation model is $Y_i = X_i + W_i$, where $X_i = \Theta_0 + \Theta_1t_i + \Theta_2 t_i^2$.
  
    The noise model: $f_{W_i}$; Independent $W_i$; independent from $\Theta_j$
  
  * Assume $\Theta_j \sim \mathcal{N}(0,\sigma^2),W_i \sim \mathcal{N}(0,\sigma^2)$; independent
  
    Given $\Theta = \theta = (\theta_0, \theta_1, \theta_2), X_i$ is $\mathcal{N}(\theta_0 + \theta_1 t_i + \theta_2 t_i^2, \sigma^2)$
  
    The conditional PDF of $Y$ given $\Theta$ is
    $$
    f_{Y_i|\Theta}(y|\theta) = c \cdot \exp \left({-(x_i - \theta_0 - \theta_1 t_i - \theta_2t_i^2)^2\over 2\sigma^2}\right)\\
    f_{Y|\Theta}(y|\theta) = c \cdot \prod_{i=1}^n \exp \left({-(x_i - \theta_0 - \theta_1 t_i - \theta_2t_i^2)^2\over 2\sigma^2}\right)
    $$
    The joint PDF of $\Theta$ and $Y$ is
    $$
    f_{\Theta,Y}(\theta, y) = \prod^2_{j=0} f_{\Theta_j}(\theta_j) \cdot \prod^n_{i=1}f_{Y_i|\Theta}(y|\theta)
    $$
    The posterior PDF is
    $$
    \begin{aligned}
    f_{\Theta|Y}(\theta|y) &= {1\over f_Y(x)} f_{\Theta,Y}(\theta,y)\\
    &= {1\over f_Y(x)} \prod^2_{j=0}f_{\theta_j}(\theta_j) \prod^n_{i=1} f_{Y_i|\Theta} (y_i|\theta)\\
    &= c(y) \cdot \exp \left( -{1\over 2} \left( {\theta_1^2 \over \sigma_0^2} + {\theta_2^2 \over \sigma_1^2} + {\theta_2^2 \over \sigma_2^2} \right) - {1\over 2\sigma^2} \sum^n_{i=1}(y_i - \theta_0 - \theta_1 t_i - \theta_2 t_i^2)^2 \right)
    \end{aligned}
    $$
    where $c(y)$ is a normalizing constant.
  
    Note that the numerical value of $f_Y(y)$ is not required, if we are interested in just the MAP estimator, or the general shape of the posterior distribution.
  
  * MAP estimate: maximize over $(\theta_0, \theta_1, \theta_2)$: (minimize quadratic function)
    $$
    {\partial \over \partial \theta_j} (\text{quad}(\theta)) = 0\\
    \text{where   quad}(\theta) = \theta_0^2/\sigma_0^2 + \theta_1^2/\sigma_1^2+ \theta_2^2/\sigma_2^2 + {1\over \sigma^2} \sum^n_{i=1}(y_i - \theta_0 - \theta_1 t_i - \theta_2 t_i^2)^2
    $$
  
* Linear normal models

  * $\Theta_j$ and $X_i$ are linear functions of independent normal random variables

  * $f_{\Theta|X} (\theta|x) = c(x) \ \exp (-\text{quad}(\theta_1, ..., \theta_m))$

  * MAP estimate: maximize over $(\theta_1, ..., \theta_m)$ (minimize quadratic function)

    $\widehat{\Theta}_{MAP,j}:$ Linear function of $X = (X_1, ..,X_n)$.

  * Facts

    * $\widehat{\Theta}_{MAP,j} = \mathbf{E}[\Theta_j|X]$
    * marginal posterior PDF of $\Theta_j: f_{\Theta_j|X}(\theta_j|x)$, is normal
    * MAP estimate based on the joint posterior PDF: same as MAP estimate based on the marginal posterior PDF
    * $\mathbf{E}\left[ (\Theta_{i, MAP} - \Theta_i)^2|X=x \right]$: same for all $x$.

There are 4 selected exercises and 2 solved problems.

---

## Exercise 1 Multiple observations, more general model

Suppose that $X_1=\Theta +W_1$ and $X_2=2\Theta +W_2$, where $\Theta ,W_1,W_2$ are independent **standard normal** random variables. If the values that we observe happen to be $X_1 = -1$ and $X_2 = 1$, then what is the MAP estimate of $\Theta$ ?

**Solution:**

The numerator term of the posterior is equal to a constant times
$$
e^{-\theta ^2/2} e^{-(x_1-\theta )^2/2}e^{-(x_2-2\theta )^2/2}.
$$
To find the MAP estimate, we set $x_1$ and $x_2$ to the given values, and set the derivative of the exponent (with respect to $\theta$) to zero. We obtain
$$
\theta +(\theta +1) +2(2\theta -1)=0,
$$
which yields $6 \theta - 1 = 0$ or $\theta =1/6$.

## Exercise 2 The mean-squared error

Recall the mean squared error is
$$
\frac{1}{\displaystyle {\sum _{i=0}^{n} \frac{1}{\sigma _ i^2}}}
$$
Considering two scenarios: 

In the first scenario, $\Theta \sim N(0,1)$ and we observe $X = \Theta + W$, where $W\sim N(0,1)$ is independent of $\Theta$.

In the second scenario, the prior information on $\Theta$ is extremely inaccurate: $\Theta \sim N(0,\sigma _0^2)$, where $\sigma_0^2$ is so large that it can be treated as infinite. But in this second scenario we obtain two observations of the form $X_i = \Theta + W_i$, where the $W_i$ are standard normals, independent of each other and of $\Theta$.

What is the MSE in both scenarios?

**Solution:**

They are the same in both scenarios.

For the second scenario, we set $\sigma _0^2=\infty$. In the first scenario, we obtain
$$
\frac{1}{\frac{1}{1}+\frac{1}{1}}=\frac{1}{2},
$$
and in the second scenario, we obtain the same mean squared error:
$$
\frac{1}{\frac{1}{\infty } + \frac{1}{1}+\frac{1}{1}}=\frac{1}{2}.
$$
**Remark:** the prior information on $\Theta$ in the first scenario is, in a loose sense, exactly as informative as having no useful prior information but one more observation, as in the second scenario.

## Exercise 3 The effect of a stronger signal

For the model $X=\Theta +W$, and under the usual independence and normality assumptions for $\Theta$ and $W$, the mean squared error of the LMS estimator is
$$
\frac{1}{(1/\sigma _0^2)+(1/\sigma _1^2)},
$$
where $\sigma_0^2$ and $\sigma_1^2$ are the variances of $\Theta$ and $W$, respectively.

Suppose now that we change the observation model to $Y = 3\Theta + W$. In some sense the "signal" $\Theta$ has a stronger presence, relative to the noise term $W$, and we should expect to obtain a smaller mean squared error. Suppose $\sigma _0^2=\sigma _1^2=1$. The mean squared error of the original model $X=\Theta +W$ is then $1/2$. In constrast, what is the mean squared error of the new model $Y=3\Theta +W$ ?

**Solution:**

Since $Y'$ is just $Y$ scaled by a factor of $1/3$, $Y'$ carries the same information as $Y$, so that ${\bf E}[\Theta \mid Y]={\bf E}[\Theta \mid Y']$. Thus, the alternative observation model $Y'=\Theta +(W/3)$ will lead to the same estimates and will have the same mean squared error as the unscaled model $Y=3\Theta +W$. In the equivalent $Y'$ model, we have a noise variance of $1/9$ and therefore the mean squared error is
$$
\frac{1}{\frac{1}{1}+\frac{1}{1/9}}=\frac{1}{10}.
$$

## Exercise 4 Multiple observations and unknowns

Let $\Theta_1, \Theta_2, W_1$ and $W_2$ be independent standard normal random variables. We obtain two observations,
$$
X_1=\Theta _1+W_1,\qquad X_2=\Theta _1+\Theta _2+W_2.
$$
Find the MAP estimate $\hat\theta =(\hat\theta _1,\hat\theta _2)$ of $(\Theta _1,\Theta _2)$ if we observe that $X_1=1,X_2=3$. 

**Solution:**

We focus on the exponential term in the numerator of the expression given by Bayes' rule. The prior contributes a term of the form
$$
e^{-\frac{1}{2}(\theta _1^2+\theta _2^2)}.
$$
Conditioned on $(\Theta _1,\Theta _2)=(\theta _1,\theta _2)$, the measurements are independent. In the conditional universe, $X_1$ is normal with mean $\theta_1,X_2$ is normal with mean $\theta_1 + \theta_2$, and both variances are $1$. Thus, the term $f_{X_1,X_2|\Theta _1,\Theta _2}$ makes a contribution of the form
$$
e^{-\frac{1}{2}(x_1-\theta _1)^2}\cdot e^{-\frac{1}{2}(x_2-\theta _1-\theta _2)^2}.
$$
We substitute $x_1 = 1$ and $x_2=3$, and in order to find the MAP estimate, we minimize the expression
$$
\frac{1}{2}\left(\theta _1^2+\theta _2^2+(\theta _1-1)^2+(\theta _1+\theta _2-3)^2\right).
$$
Setting the derivatives (withe respect to $\theta_1$ and $\theta_2$) to zero, we obtain:
$$
\hat\theta _1+(\hat\theta _1-1)+(\hat\theta _1+\hat\theta _2-3)=0, \qquad \hat\theta _2+(\hat\theta _1+\hat\theta _2-3)=0,
$$
or
$$
3\hat\theta _1+\hat\theta _2=4,\qquad \hat\theta _1+2\hat\theta _2=3.
$$
Either by inspection, or by substitution, we obtain the solution $\hat\theta _1=1, \hat\theta _2=1$.

## Problem 1 Trajectory estimation

The vertical coordinate ("height") of an object in free fall is described by an equation of the form
$$
x(t) = \theta _0 + \theta _1 t + \theta _2 t^2,
$$
where $\theta_0, \theta_1,$ and $\theta_2$ are some parameters and $t$ stands for time. At certain times $t_1, ..., t_n$, we make noisy observations $Y_1, ..., Y_n$, respectively, of the height of the object. Based on these observations, we would like to estimate the object's vertical trajectory.

We consider the special case where there is only one unknown parameter. We assume that $\theta_0$ (the height of the object at time zero) is a known constant. We also assume that $\theta_2$ (which is related to the acceleration of the object) is known. We view $\theta_1$ as the realized value of a continuous random variable $\Theta_1$. The observed height at time $t_i$ is $Y_ i = \theta _0 + \Theta _1 t_ i +\theta _2 t_ i^2 + W_ i,\,  i = 1, \ldots , n$, where $W_i$ models the observation noise. We assume that $\Theta _1\sim N(0,1)$, $W_1,\dots ,W_ n \sim N(0,\sigma ^2)$, and that all these random variables are independent.

In this case, finding the MAP estimate of $\Theta_1$ involves the minimization of
$$
\theta _1^2+ \frac{1}{\sigma ^2}\sum _{i=1}^ n(y_ i-\theta _0-\theta _1 t_ i -\theta _2 t_ i^2)^2
$$
with respect to $\theta_1$.

1. Carry out this minimization and find the formula for the MAP estimate, $\hat{\theta}_1$.
2. The formula for $\hat{\theta}_1$ can be used to define the MAP estimator $\hat{\Theta}_1$ (a random variable), as a function of $t_1, ..., t_n$ and the random variables $Y_1, .., Y_n$. Does the MAP estimator $\hat{\Theta}_1$ has a normal distribution?
3. Let $\sigma=1$ and consider the special case of only two observations ($n=2$). Write down a formula for the mean squared error $\mathbb {E}[(\hat{\Theta }_1-\Theta _1)^2]$, as a function of $t_1$ and $t_2$.
4. Consider the "experimental design" problem of choosing when to make measurements. Under the assumptions of the previous part, and under the constraints $0\leq t_1,t_2 \leq 10$, find the values of $t_1$ and $t_2$ that minimize the mean squared error associated with the MAP estimator.

**Solution:**

1. Setting the partial derivative with respect to $\theta_1$ equal to zero, we obtain
   $$
   \theta _1 -\frac{1}{\sigma ^2} \sum _{i=1}^ n t_ i (y_ i-\theta _0-\theta _1 t_ i -\theta _2 t_ i^2)=0,
   $$
   yielding the MAP estimate
   $$
   \hat\theta _1= \frac{\sum _{i=1}^ n t_ i (y_ i-\theta _0-\theta _2 t_ i^2)}{\sigma ^2 + \sum _{i=1}^ n t_ i^2}.
   $$

2. We have
   $$
   \hat\Theta _1= \frac{\sum _{i=1}^ n t_ i (Y_ i-\theta _0-\theta _2 t_ i^2)}{\sigma ^2 + \sum _{i=1}^ n t_ i^2}.
   $$
   Recall that the observation model is $Y_ i = \theta _0 + \Theta _1t_ i + \theta _2t_ i^2 + W_ i$, and so we can rewrite the estimator as
   $$
   \hat\Theta _1 = \frac{\sum _{i=1}^ n t_ i(\Theta _1t_ i + W_ i)}{\sigma ^2 + \sum _{i=1}^ n t_ i^2} = \frac{\Theta _1\sum _{i=1}^ n t_ i^2 + \sum _{i=1}^ n t_ i W_ i}{\sigma ^2 + \sum _{i=1}^ n t_ i^2}.
   $$
   We see that $\hat{\Theta}_1$ is a linear function of $\Theta_1$ and $W_1, ..., W_n$, which are all normal and independent. Since a linear function of independent normal random variables is normal, it follow that $\hat{\Theta}_1$ is normal.

3. For the special case $\sigma=1$ and $n=2$, that the estimation error is
   $$
   \tilde\Theta _1 \triangleq \hat\Theta _1 - \Theta _1 = \frac{t_1W_1 + t_2W_2- \Theta _1 }{1 + t_1^2 + t_2^2}.
   $$
   Since $\Theta _1,W_1,W_2$ are all zero-mean, independent normal random variables, $\tilde{\Theta}_1$ is also a zero-mean normal random variable. Hence, the mean squared error is
   $$
   \begin{aligned}
   \mathbb {E}[(\hat\Theta _1-\Theta _1)^2] = \mathbb {E}[\tilde\Theta _1^2] = {\rm var}(\tilde\Theta _1)
   &=\frac{{\rm var}(\Theta _1)+t_1^2{\rm var}(W_1)+t_2^2{\rm var}(W_2)}{\left(1+t_1^2+t_2^2\right)^2}\\
   &= \frac{1+t_1^2+t_2^2}{\left(1+t_1^2+t_2^2\right)^2}\\
   &= \frac{1}{1+t_1^2+t_2^2}.
   \end{aligned}
   $$

4. In order to minimize the mean squared error found in the previous part, we should choose the observation times to be as large as possible. Under the constraints $0\leq t_1,t_2\leq 10$, we should choose $t_1 = t_2 = 10$. The intuition is that (since and are known constants), we are effectively making observations of the form
   $$
   Z_ i = \Theta _1 t_ i + W_ i.
   $$
   Or equivalently, we are making observations of the form
   $$
   Z_ i' = \frac{Z_ i}{t_ i}=\Theta _1 + \frac{W_ i}{t_ i}.
   $$
   When the noise term is smallest, more precisely, when its variance is smallest, these observations become more informative. This corresponds to choosing as large as possible.

## Problem 2 Hypothesis test between two normals

Conditioned on the result of an unbiased coin flip, the random variables $T_1,T_2,\ldots ,T_ n$ are independent and identically distributed, each drawn from a common normal distribution with mean zero. If the result of the coin flip is Heads, this normal distribution has variance $1$; otherwise, it has variance $4$. Based on the observed values $t_1,t_2,\ldots ,t_ n$, we use the MAP rule to decide whether the normal distribution from which they were drawn has variance $1$ or variance $4s$. In what condition does the MAP rule decide that the underlying normal distribution has variance $1$?

**Solution:**

Let $\Theta=0$ denote that the observations $t_1, t_2, ..., t_n$ were generated from a normal distribution with variance $1$, and let $\Theta=1$ denote that they were generated from a normal distribution with variance $4$. For simplicity, let us use the notation $\mathcal{N}(t_1, ...,t_n;0,\sigma^2)$ to denote the joint PDF of $n$ i.i.d. normal random variables with mean $0$ and variance $\sigma^2$, evaluated at $t_1, ..., t_n$.

Therefore, given the observations $t_1, ..., t_n$, the posterior probability that the samples are generated from a normal distribution with variance $1$ is
$$
\mathbf{P}(\Theta =0 \mid T_1=t_1, \ldots , T_ n=t_ n) = \frac{(1/2)\cdot N(t_1, \ldots , t_ n;0,1)}{(1/2)\cdot N(t_1, \ldots , t_ n;0,1) + (1/2)\cdot N(t_1, \ldots , t_ n;0,4)}.
$$
Similarly, the probability that the samples are generated from a normal distribution with variance $4$ is given by
$$
\mathbf{P}(\Theta =1 \mid T_1=t_1, \ldots , T_ n=t_ n) = \frac{(1/2)\cdot N(t_1, \ldots , t_ n;0,4)}{(1/2)\cdot N(t_1, \ldots , t_ n;0,1) + (1/2)\cdot N(t_1, \ldots , t_ n;0,4)}.
$$
The MAP rule favors $\Theta=0$ if the following inequality holds
$$
\mathbf{P}(\Theta = 0 \mid T_1=t_1, \ldots , T_ n=t_ n) > \mathbf{P}(\Theta = 1\mid T_1=t_1, \ldots , T_ n=t_ n)
$$
We notice that the denominators in the expressions for $\mathbf{P}(\Theta =0\mid \ldots )$ and $\mathbf{P}(\Theta =1\mid \ldots )$ are the same, so it suffices to compare the numerators. Therefore, the MAP rule favors $\Theta=0$ if the following inequality holds
$$
N(t_1, \ldots , t_ n;0,1) > N(t_1, \ldots , t_ n;0,4)\\
\prod _{i=1}^{n} \frac{1}{\sqrt{2 \pi \cdot 1}} e^{-\frac{t_ i^2}{2\cdot 1}} > \prod _{i=1}^{n} \frac{1}{\sqrt{2 \pi \cdot 4}} e^{-\frac{t_ i^2}{2\cdot 4}}.
$$
With a little bit of algebra, we obtain
$$
\left| \frac{3}{8} \sum _{i=1}^{n} t_ i^2 \right|
 < n \cdot \ln(2)
$$


# Lecture 8. Continuous Random Variables and Probability Density Functions

* Probability density functions

  * Definition: A random variable is continuous if it can be described by a PDF.
  * Properties

  | Discrete                                                     | Continuous                                                   |
  | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | $\mathbf{P}(a \leq X \leq b) = \sum_{x: a\leq x \leq b} p_X(x)$ | $\mathbf{P}(a \leq X \leq b) = \int^b_a f_X(x) dx$           |
  | $p_X(x) \geq 0$                                              | $f_X(x) \geq 0$                                              |
  | $\sum_xp_X(x)=1$                                             | $\int^\infty_{-\infty} f_X(x)dx = 1$                         |
  |                                                              | $\mathbf{P}(a \leq X \leq a + \delta) \approx f_X(a) \cdot \delta\\\mathbf{P}(X=a) = 0$ |

  * Examples

* Expectation and its properties

  * The expected value rule

    | Discrete                                | Continuous                                             |
    | --------------------------------------- | ------------------------------------------------------ |
    | $\mathbf{E}[X] = \sum_x x p_X(x)$       | $\mathbf{E}[X] = \int^\infty_{-\infty} x f_X(x) dx$    |
    | $\mathbf{E}[g(X)] = \sum_x g(X) p_X(x)$ | $\mathbf{E}[g(X)] = \int_{-\infty}^\infty g(X) p_X(x)$ |

  * Properties 

    * If $X \geq 0$, then $\mathbf{E}[X] >0$.
    * If $a \leq X \leq b$, then $a \leq \mathbf{E}[X] \leq b$.

    * Linearity: $\mathbf{E}[aX+b] = a\mathbf{E}[X]+b$.

* Variance and its properties

  * Definition: $\mathsf{Var}(X) = \mathbf{E}[(X-\mu)^2]$
  * Standard deviation: $\sigma_{X} = \sqrt{\mathsf{Var}(X)}$
  * Properties: $\mathsf{Var}(aX+b) = a^2 \mathsf{Var}(X)$
  * A useful formula: $\mathsf{Var}(X) = \mathbf{E}[X^2] - (\mathbf{E}[X])^2$

* Uniform random variables

  | Discrete                                    | Continuous                                                   |
  | ------------------------------------------- | ------------------------------------------------------------ |
  | $p_X(x) = {1 \over b-a+1}$                  | $f_X(x) = {1 \over b-a}$                                     |
  | $\mathbf{E}[X] = {a+b \over 2}$             | $\mathbf{E}[X] = {a+b \over 2}\\\mathbf{E}[X^2] = {1\over b-a}\left({b^3\over 3} - {a^3 \over 3}\right)$ |
  | $\mathsf{Var}(X) = {1\over 12}(b-a)(b-a+2)$ | $\mathsf{Var}(X)={1\over 12}(b-a)^2$                         |

* Exponential random variables $\lambda > 0$

  ![U5-lec8-exponential](../assets/images/U5-lec8-exponential.png)

  | Geometric (discrete)          | Exponential (continuous)                                     |
  | ----------------------------- | ------------------------------------------------------------ |
  | $p_X(x) = (1-p)^{k-1}p$       | $f_X(x) = \begin{cases}\lambda e^{-\lambda x}, & x\geq 0\\0, &x < 0 \end{cases}$ |
  | $\mathbf{E}[X] = {1 \over p}$ | $\mathbf{E}[X] = {1 \over \lambda}$                          |
  |                               | $\mathsf{Var}(X)= {1\over \lambda^2} $                       |
  |                               | $\mathbf{P}(X\geq a) = \int^\infty_a\lambda e^{-\lambda x} dx = \lambda \cdot (-{1\over \lambda}) e^{-\lambda x}\vert ^\infty_a = e^{-\lambda a}\\ \mathbf{P}(X\leq a) = 1 - e^{-\lambda a}$ |

* Cumulative distribution functions

  * Definition: $F_X(x) = \mathbf{P}(X\leq x) $

  | Discrete                         | Continuous                          |
  | -------------------------------- | ----------------------------------- |
  | $F_X(x) = \sum_{k \leq x}p_X(k)$ | $F_X(x) = \int^x_{-\infty}f_X(t)dt$ |

  * Properties
    * Non-decreasing: If $y \geq x \implies F_X(y) \geq F_X(x)$.
    * $F_X(x) \xrightarrow[x\rightarrow \infty]{}1$.
    * $F_X(x) \xrightarrow[x\rightarrow -\infty]{}0$.

* Normal (Gaussian) random variables 

  * Standard normal $\mathcal{N}(0,1): f_X(x) = {1\over \sqrt{2 \pi}} \exp(-x^2/2)$.

    $\mathbf{E}[X] = 0,\quad \mathsf{Var}(X)=1$.

  * General normal $\mathcal{N}(\mu, \sigma^2):f_X(x) = {1\over \sigma\sqrt{2\pi}} \exp\left(-(x-\mu)^2\over 2\sigma^2\right)$.

    $\mathbf{E}[X] = \mu,\quad \mathsf{Var}(X)=\sigma^2$.

  * Linearity properties: Let $Y = aX+b, \quad X \sim \mathcal{N}(\mu, \sigma^2)$.

    $\mathbf{E}[X] = a\mu+b,\quad \mathsf{Var}(X)=a^2\sigma^2$

  * Using tables to calculate probabilities since no closed form available for CDF

    Standardizing a random variable $Y = {X-\mu \over \sigma}$.

    $Y \sim \mathcal{N}(0,1), \quad \Phi(y) = F_Y(y) = \mathbf{P}(Y \leq y)$.


There are 0 selected exercise and 2 solved problems.

---

## Problem 1 Mean and variance of the exponential

Let $\lambda$ be a positive number. The continuous random variable $X$ is called exponential with parameter $\lambda$ when its probability density function is
$$
f_{X}(x) =  \begin{cases} \lambda e^{-\lambda x}, &  \text{if } x \geq 0, \\ 0, &  \text{otherwise}.  \end{cases}
$$
(a) Find the cumulative distribution function (CDF) of $X$.

(b) Find the mean of $X$.

(c) Find the variance of $X$.

(d) Suppose $X_1， X_2,$ and $X_3$ are independent exponential random variables, each with parameter $\lambda$ . Find the PDF of $Z = \max\{X_1, X_2, X_3\}$.

(e) Find the PDF of $W =  \min\{X_1, X_2\}$.

**Solution**:

(a)

For $x  >0$,
$$
F_X(x) = \int^x_{-\infty}f_X(x)dt = \int^x_{0}\lambda e^{-\lambda t} dt = \left[ -e^{-\lambda t}\right]^x_0 = 1 - e^{-\lambda x}
$$
For $x < 0$, we have $F_X(x) = \int^x_{-\infty}f_X(t)dt = 0$. 

Thus we conclude
$$
F_X(x) =  \begin{cases}0, & \text{if }x < 0 \\ 1-e^{-\lambda x} &\text{if }x \geq 0 \end{cases}
$$
(b)

The key step in the following computation uses integration by parts, whereby
$$
\int^{\infty}_0 u dv = \left. uv \right\vert ^\infty_0 - \int^\infty_0 vdu
$$
is applied with $u = x$ and $v = -e^{-\lambda x}$:
$$
\mathbf{E}[x] = \int^\infty_{-\infty} x f_X(x)dx = \int^\infty_0x \lambda e^{-\lambda x}dx = \left[-x e^{-\lambda x}\right]^\infty_0 + \int^\infty_0 e^{-\lambda x} dx = {1\over \lambda}
$$
(c)

Integrating by parts with $u = x^2$ and $v = -e^{-\lambda x}$ in the second line below gives
$$
\begin{aligned}
\mathbf{E}[X^2] &= \int^\infty_{-\infty} x^2 f_X(x) dx = \int^\infty_0 x^2 \lambda e^{-\lambda x}dx\\
&= [-x^2 e^{-\lambda x}]^\infty_0 + 2 \int^\infty_0 x e^{-\lambda x} dx = {2\over \lambda} \mathbf{E}[X] = {2\over \lambda^2} 
\end{aligned}
$$
Combining with the previous computation, we obtain
$$
\mathsf{Var}(X) = \mathbf{E}[X^2] - (\mathbf{E}[X])^2 = {2\over \lambda^2} - \left( 1 \over \lambda \right)^2 = {1\over \lambda^2}
$$
(d) 

The maximum of a set is upper bounded by $z$ when each element of the set if upper bounded by $z$. Thus for any positive $z$,
$$
\begin{aligned}
\mathbf{P}(Z \leq z) &= \mathbf{P}(\max\{ X_1, X_2, X_3\} \leq z)\\
&= \mathbf{P}(X_1 \leq z, X_2 \leq z, X_3 \leq z)\\
&=  \mathbf{P}(X_1 \leq z)\mathbf{P}( X_2 \leq z) \mathbf{P}(X_3 \leq z)\\
&=(1-e^{-\lambda z})^3
\end{aligned}
$$
where the third equality uses the independence of $X_1, X_2,$ and $X_3$. Thus,
$$
F_Z(z) = \begin{cases}0, &\text{if }z <0\\ (1-e^{-\lambda z})^3, & \text{if } z \geq 0\end{cases}
$$
Differentiating the CDF gives the desired PDF:
$$
f_Z(z) = \begin{cases}0, &\text{if }z \leq 0\\ 3\lambda e^{-\lambda z}(1 - e^{-\lambda z})^2, &\text{if }z \geq 0 \end{cases}
$$
(e)

The minimum of a set is lower bounded by $w$ when each element of the set is lower bounded by $w$. Thus for any positive $w$,
$$
\begin{aligned}
\mathbf{P}(W \geq w) &= \mathbf{P}(\min\{ X_1, X_2\} \geq w) \\
&= \mathbf{P}(X_1 \geq w, X_2 \geq w) \\
&= \mathbf{P}(X_1 \geq w)\mathbf{P}( X_2 \geq w)\\
&= (e^{-\lambda w})^2\\
&= e^{-2 \lambda w}
\end{aligned}
$$
Where the third equality uses the independence of $X_1$ and $X_2$, Thus,
$$
F_W(w) = \begin{cases}0, &\text{if } w < 0  \\ 1 - e^{-2\lambda w},&\text{if } w \geq 0\end{cases}
$$
We can recognize this as the CDF of an exponential random variable with parameter $2\lambda$. The PDF is
$$
f_W(w) = \begin{cases}0, &\text{if } w < 0 \\ 2 \lambda e^{-2\lambda w}, &\text{if }w \geq 0\end{cases}
$$

## Problem 2 Buffon's needle

A surface is ruled with parallel lines, which are at distance $d$ from each other. Suppose that we throw a needle of length $l$ on the surface at random. What is the probability that the needle will intersect one of the lines?

**Solution**: 

Suppose $X$ represents the distance between the middle of the needle and the line, $\Theta$ represents the angle between the needle and the intersected line. They are independent.

Since $X \sim \mathsf{Unif}(0, {d\over 2})$, the PDF of $X$ is
$$
f_X(x) = {2\over d}, \quad 0 \leq x \leq {d\over 2}
$$
Since $\Theta \sim \mathsf{Unif}(0, {\pi \over 2})$, the PDF of $\Theta$ is
$$
f_\Theta(\theta) = {2\over \pi}, \quad 0 \leq \theta \leq {\pi \over 2}
$$
The needle intersects the line if 
$$
X \leq {l \over 2} \sin \theta
$$
To obtain the probability of $X \leq {l \over 2} \sin \theta$, i.e. the CDF of $X$, we first calculate the joint probability of $X$ and $\Theta$,
$$
f_{X,\Theta}(x,\theta) = {4 \over \pi d}, \quad 0 \leq x\leq d/2, \quad 0\leq \theta \leq \pi/2
$$
Therefore, we get the probability of intersection.
$$
\begin{aligned}
\mathbb{P}(X \leq {l \over 2}\sin\theta)&=\int^{n/2}_0\int^{{l\over 2} \sin \theta}_0 {4\over \pi d} ~dx d\theta\\
&= \int^{n/2}_0 {4 \over \pi d} \cdot {l \over 2} \sin \theta~ d\theta\\
&={2l \over \pi d} (-\cos\theta)|^{n/2}_0\\
&= {2l \over \pi d}
\end{aligned}
$$


# Lecture 9. Conditioning on an event; Multiple r.v.'s

* Conditioning a r.v. on an event

  * Conditional PDF

    | Discrete                                                     | Continuous                                                   |
    | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | $p_X(x) = \mathbf{P}(X=x)$                                   | $f_X(x) \cdot \delta \approx \mathbf{P}(x \leq X \leq x+ \delta)$ |
    | $p_{X\vert A}(x) = \mathbf{P}(X=x\vert A)$                   | $f_{X\vert A}(x) \cdot \delta \approx \mathbf{P}(x \leq X \leq x+ \delta \vert A)$ |
    | $\mathbf{P}(X\in B) = \sum_{x \in B}p_X(x)$                  | $\mathbf{P}(X\in B) = \int_{B}f_X(x)dx$                      |
    | $\mathbf{P}(X\in B \vert A) = \sum_{x \in B}p_{X\vert A}(x)$ | $\mathbf{P}(X\in B\vert A) = \int_{B}f_{X\vert A}(x)dx$      |
    | $\sum_xp_{X\vert A}(x) = 1$                                  | $\int f_{X \vert A}(x)dx = 1$                                |
    |                                                              | $\mathbf{P}(x \leq X \leq x+\delta\vert X \in A) \approx f_{X \vert X \in A}(x) \cdot \delta\\={\mathbf{P}(x \leq X \leq x+\delta , x \in A)\over \mathbf{P}(A)}={\mathbf{P}(x \leq X \leq x+\delta)\over \mathbf{P}(A)} \approx {f_x(x) \over \mathbf{P}(A)}$ |
    |                                                              | $f_{X\vert X\in A(x)}  = \begin{cases} 0, & \text{if } x \notin A\\ {f_x(x) \over \mathbf{P}(A)}, & \text{if } x \in A \end{cases}$ |

  * Conditional expectation and the expected value rule

    | Discrete                                               | Continuous                                             |
    | ------------------------------------------------------ | ------------------------------------------------------ |
    | $\mathbf{E}[X] = \sum_x xp_X(x)$                       | $\mathbf{E}[X] = \int xf_X(x) dx$                      |
    | $\mathbf{E}[X\vert A] = \sum_x xp_{X\vert A}(x)$       | $\mathbf{E}[X\vert A] = \int xf_{X\vert A}(x) dx$      |
    | $\mathbf{E}[g(X)] = \sum_x g(x)p_X(x)$                 | $\mathbf{E}[g(X)] = \int g(x)f_X(x)dx$                 |
    | $\mathbf{E}[g(X)\vert A] = \sum_x g(x)p_{X\vert A}(x)$ | $\mathbf{E}[g(X)\vert A] = \int g(x)f_{X\vert A}(x)dx$ |

  * Exponential PDF: Memorylessness - probabilistically identical.

    * Bulb lifetime $T$: $\mathsf{Exp}(\lambda)$. So $\mathbf{P}(T > x)= e^{-\lambda x},$ for $x \geq 0$.

      We are told that $T > t$, and $X$ is a random variable of remaining lifetime, $T - t$.

      Then we have $\mathbf{P}(X > x \vert T > t) = e^{-\lambda x},$ for $x \geq 0$.

    * Equivalently,
      $$
      \mathbf{P}(t \leq T \leq t + \delta \vert T > t) = \mathbf{P}(0 \leq T \leq \delta) = \lambda \delta
      $$

    * Proof:
      $$
      \begin{aligned}
      \mathbf{P}(X > x \vert T > t) &= {\mathbf{P} (T-t > x, T > t)\over \mathbf{P}(T > t)} \\
      &= {\mathbf{P} (T > t+x, T > t)\over \mathbf{P}(T > t)} \\
      &= {\mathbf{P} (T > t+x) \over \mathbf{P}(T > t)} \\
      &= {e^{-\lambda(t+x)}\over e^{-\lambda t} }\\
      &= e^{-\lambda x}
      \end{aligned}
      $$

  * Total probability and expectation theorems

    | Discrete                                                     | Continuous                                                   |
    | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | $\mathbf{P}(B) = \mathbf{P}(A_1)\mathbf{P}(B \vert A_1) + ... + \mathbf{P}(A_n)\mathbf{P}(B \vert A_n)\\ p_X(x) = \mathbf{P}(A_1)p_{X \vert A_1}(x) + ... + \mathbf{P}(A_n)p_{X \vert A_n}(x) $ | $f_X(x) = \mathbf{P}(A_1)f_{X \vert A_1}(x) + ... +  \mathbf{P}(A_n)f_{X \vert A_n}(x) $ |
    |                                                              | $\mathbf{E}[X] = \mathbf{P}(A_1)\mathbf{E}[X \vert A_1] + ... + \mathbf{P}(A_n)\mathbf{E}[X \vert A_n]$ |

    **Example**: Bill goes to the supermarket shortly, with probability $1/3$, at a time uniformly distributed between $0$ and $2$ hours now; or with probability $2/3$, later in the day at a time uniformly distributed between 6 and 8 hours from now.

    * Given $\mathbf{P}(A_1) = 1/3,\quad f_{X\vert A_1} \sim \mathsf{Unif}[0,2], \quad \mathbf{P}(A_2) = 2/3,\quad f_{X\vert A_2} \sim \mathsf{Unif}[6,8]$

    * $f_X(x) = \mathbf{P}(A_1)f_{X \vert A_1}(x) +  \mathbf{P}(A_2)f_{X \vert A_2}(x)  = 1/3 \times 1/2 + 2/3 \times 1/2 = 1/2$. So the PDF looks like this

      ![U5-lec9-total-prob-ex](../assets/images/U5-lec9-total-prob-ex.png)

    * $\mathbf{E}[X] = \mathbf{P}(A_1)\mathbf{E}[X \vert A_1] + \mathbf{P}(A_2)\mathbf{E}[X \vert A_2] = 1/3 \cdot 1 + 2/3 \cdot 7$.

  * Mixed distributions

* Jointly continuous r.v.'s and joint PDFs

  * Definition: two random variables are jointly continuous if they can be described by a joint PDF $f_{X,Y}(x,y)$.

    | Discrete                                                     | Continuous                                                   |
    | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | $p_{X,Y}(x,y) = \mathbf{P}(X=x, Y=y) \geq 0$                 | $f_{X,Y}(x,y) \geq 0$                                        |
    | $\mathbf{P}((X,Y) \in B) = \sum\limits_{(x,y)}\sum\limits_{\in B} p_{X,Y}(x,y)$ | $\mathbf{P}((X,Y) \in B) = \int\limits_{(x,y)}\int\limits_{\in B} f_{X,Y}(x,y)dxdy$ |
    | $\sum\limits_x\sum\limits_y p_{X,Y}(x,y)=1$                  | $\int^\infty_{-\infty}\int^\infty_{-\infty} f_{X,Y}(x,y)dxdy = 1$ |

  * From the joints to the marginals

    | Discrete                                                     | Continuous                                |
    | ------------------------------------------------------------ | ----------------------------------------- |
    | $p_{X}(x) = \sum\limits_y p_{X,Y}(x,y)$                      | $f_{X}(x) = \int f_{X,Y}(x,y)dy$          |
    | $p_{Y}(y) = \sum\limits_x p_{X,Y}(x,y)$                      | $f_{Y}(y) = \int f_{X,Y}(x,y)dx$          |
    | $\sum\limits_x\sum\limits_y\sum\limits_z p_{X,Y,Z}(x,y,z) = 1$ | $\int\int\int f_{X,Y,Z}(x,y,z)dxdydz = 1$ |
    | $p_X(x) = \sum\limits_y\sum\limits_z p_{X,Y,Z}(x,y,z)$       | $f_X(x) = \int\int f_{Y,Z}(y,z)dydz$      |
    | $p_{X,Y}(x,y) = \sum\limits_z p_{X,Y,Z}(x,y,z)$              | $f_{X,Y}(x,y) = \int f_{X,Y,Z}(x,y,z)dz$  |

  * Uniform joint PDF example

  * The expected value rule

    | Discrete                                                     | Continuous                                              |
    | ------------------------------------------------------------ | ------------------------------------------------------- |
    | $\mathbf{E}[g(X,Y)] = \sum\limits_x \sum\limits_yg(x,y)p_{X,Y}(x,y)$ | $\mathbf{E}[g(X,Y)] = \int \int g(x,y)f_{X,Y}(x,y)dxdy$ |

  * The linearity of expectations

    $\mathbf{E}[aX+b] = a\mathbf{E}[X] + b$

    $\mathbf{E}[X+Y] = \mathbf{E}[X] + \mathbf{E}[Y]$

    $\mathbf{E}[X_1+...+X_n] = \mathbf{E}[X_1] + ...+\mathbf{E}[X_n]$

  * The joint CDF

    $F_{X,Y}(x,y) = \mathbf{P}(X \leq x, Y \leq y) = \int^y_{-\infty} \left[\int^x_{-\infty} f_{X,Y}(s,t)ds\right] dt$

    $f_{X,Y}(x,y) = {\partial^2F_{X,Y} \over \partial x\partial y}(x,y)$

There are 3 selected exercises and 3 solved problems.

---

## Exercise 1 A conditional PDF

Suppose that $X$ has a PDF of the form
$$
f_ X(x)=\begin{cases} 1/x^2,&  \text{if } x\geq 1,\\ 0,& \text{otherwise.}\end{cases}
$$
For any $x > 2$, what is the conditional PDF of $X$, given the event $X > 2$ ?

**Answer**: $2/x^2$

**Solution**: 

The conditional PDF will be a scaled version of the unconditional, of the form $\frac{f_ X(x)}{\mathbf{P}(X>2)}.$ Since
$$
\mathbf{P}(X>2)=\int _2^{\infty }\frac{1}{x^2}\,  dx =-\frac{1}{x}\Big|_2^{\infty }=1/2,
$$
Hence, 
$$
f_X(x \vert X>2) = \frac{f_ X(x)}{\mathbf{P}(X>2)} = 2/x^2
$$

## Exercise 2 Memorylessness of the exponential

Let $X$ be an exponential random variable with  parameter $\lambda$

1. What is the probability that $X > 5$
2. What is the probability that $X > 5$ given that $X > 2$?
3. Given that $X > 2$ and for a small $\delta > 0$, what is the probability that $4\leq X \leq 4 + 2 \delta$?

**Answer**:

1. $e^{-5\lambda }$.
2. $e^{-3\lambda }$.
3. $2\lambda \delta e^{-2\lambda }$.

**Solution**: 

1. $\mathbf{P}(X>5)=e^{-5\lambda }$ according to the formula $\mathbf{P}(X>a)=e^{-\lambda a}$.

2. According to memorylessness property, given that $X>2$, the remaining time $X−2$ is again exponential with the same parameter, we have
   $$
   \mathbf{P}(X>5\, |\, X>2)=\mathbf{P}(X-2>3\, |\, X>2)=\mathbf{P}(X>3)=e^{-3\lambda }
   $$

3. By memorylessness, this is the same as the unconditional probability that an exponential takes values in the interval $[2,2+2\delta ]$, which is approximately the length $2\delta$ of the small interval times the **density (pdf)** evaluated at $2$, yielding $2\lambda \delta e^{-2\lambda }$.
   $$
   \mathbf{P}(4\leq X \leq 4 + 2 \delta \vert X > 2) = \mathbf{P}(2\leq X \leq 2 + 2 \delta)  = f_X(2) \cdot 2\delta=2\lambda \delta e^{-2\lambda }
   $$

## Exercise 3 From joint PDFs to probabilities

1. The probability of the event that $0\leq Y \leq X\leq 1$ is of the form $ \int _ a^ b \left(\int _ c^ d f_{X,Y}(x,y)\, dx\right)\, dy.$ Find the values of $a,b,c,d$.
2. The probability of the event that $0\leq Y \leq X\leq 1$ is of the form $ \int _ a^ b \left(\int _ c^ d f_{X,Y}(x,y)\, dy\right)\, dx.$ Find the values of $a,b,c,d$.

**Answer**: 

$1. a=0, b=1, c=y, d=1\\2. a=0, b=1, c=0, d=x$

## Problem 1 Mixed distribution example

The taxi stand and the bus stop near Al's home are in the same location. Al goes there at a given time and if a taxi is waiting (this happens with probability $2/3$) he boards it. Otherwise, he waits for a taxi or a bus to come, whichever comes first. The next taxi will arrive in a time that is uniformly distributed between $0$ and $10$ minutes, while the next bus will arrive in exactly $54$ minutes. Find the CDF and the expected value of Al's waiting time.

**Answer**:

![U5-prob1-mixed](../assets/images/U5-prob1-mixed.png)

**Solution**: 
$$
\begin{aligned}
\mathbf{E}[X] &= \mathbb{P}(B_1) \mathbf{E}(X | B_1) + \mathbb{P}(B_2) \mathbf{E}(X | B_2) + \mathbb{P}(B_3) \mathbf{E}(X | B_3)\\
&= {2 \over 3} \cdot 0 + {1\over 3} \cdot {1 \over 2} \cdot {5\over 2} + {1\over 3} \cdot {1\over 2} \cdot 5\\
&= {5 \over 12} + {5 \over 6}\\
&= {15 \over 12}
\end{aligned}
$$
For $0 \leq x < 5$, 
$$
\begin{aligned}
\mathbb{P}(X \leq x) &= \mathbb{P}(B_1)\mathbb{P}(X \leq x| B_1) + \mathbb{P}(B_2)\mathbb{P}(X \leq x | B_2) + \mathbb{P}(B_3) \mathbb{P}(X \leq x | B_3)\\
&= {2 \over 3} \cdot 1 +{1\over 3} \cdot {1\over 2} \cdot {1\over 5} \cdot x + {1\over 3}\cdot {1\over 2} \cdot 0\\
&= {2\over 3} + {1\over 30} x
\end{aligned}
$$
Therefore,
$$
\begin{aligned}
F_X(x) &= \mathbb{P}(X \leq x) \\
&= 0 &\quad \text{ if }x < 0 \\
&= {2\over 3} + {1\over 30} x &\quad \text{ if }0 \leq x < 5 \\
&= 1 &\quad \text{ if }x \geq 5 \\
\end{aligned}
$$

## Problem 2 Circular Uniform PDF

Ben throws a dart at a circular target of radius $r$. We assume that he always hits the target and that all points of impact $(x,y)$ are equally likely. Compute the joint PDF $f_{X,Y}(x,y)$ of the random variables $X$ and $Y$ , and compute the conditional PDF $f_{X|Y}(x|y)$ .

**Solution**: 

The random variable $X$ is x-coordinate, $Y$ is y-coordinate. The joint PDF of $X, Y$ is
$$
f_{X,Y}(x,y) = \begin{cases}{1\over \pi r^2}, & x^2 + y^2 \leq r^2\\ 0, & \text{o.w.} \end{cases}
$$
The marginal PDF of $Y$ is 
$$
\begin{aligned}
f_Y(y) &= \int^\infty_{-\infty} f_{X,Y}(x,y)dx\\ 
&= \int^{\sqrt{r^2 - y^2}}_{-\sqrt{r^2 - y^2}} {1\over \pi r^2} dx\\ 
&= \begin{cases}{2 \sqrt{r^2 - y^2} \over \pi r^2}, & -r \leq y \leq r \\ 0, & \text{o.w.}\end{cases}
\end{aligned}
$$
The conditional PDF is 
$$
\begin{aligned}
f_Y(y) &= \begin{cases}{{1 \over \pi r^2} \over {2\sqrt{r^2 - y^2} \over \pi r^2}}, &x^2 + y^2 \leq r^2 \\ 0, & \text{o.w.} \\ \end{cases}\\
&= \begin{cases}{1  \over {2\sqrt{r^2 - y^2} }}, &-r \leq y \leq r, \quad-\sqrt{r^2 -y^2} \leq x \leq \sqrt{r^2 - y^2} \\ 0, & \text{o.w.} \\ \end{cases}\\

\end{aligned}
$$
Notice that ${1  \over {2\sqrt{r^2 - y^2} }}$ is uniform and does not depend on $x$. It is interesting that the original distribution ${1\over \pi r^2}$ is uniform, the marginal distribution of $Y$ is not uniform, which depends on $y$, but when we calculate the conditional distribution, we got a uniform distribution back again which does not depend on $x$.

## Problem 3 The absent minded professor

An absent-minded professor schedules two student appointments for the same time. The appointment durations are **independent** and **exponentially** distributed with mean $30$ minutes. The first student arrives on time, but the second student arrives 5 minutes late. What is the expected value of the time between the **arrival** of the first student and the **departure** of the second student?

**Solution**:

Recall the exponential distribution: $T \sim \mathsf{Exp}(\lambda)$.

The mean is $\mathbf{E}[T] = {1\over \lambda}$.

The CDF is $\mathbf{P}(T \leq t) = 1 - e^{-\lambda t}, \quad t \geq 0$.

Given $T_1, T_2 \sim \mathsf{Exp}({1\over 30})$ and are independent.

Consider two scenarios:

If $T_1 \leq 5$, 
$$
\begin{aligned}
\mathbf{E}[X|T_1 \leq 5] &= 5+ \mathbf{E}[T_2] = 35\\
\mathbb{P}(T_1 \leq 5) &= 1 - e^{-5/30}
\end{aligned}
$$
If $T_2 > 5$,
$$
\begin{aligned}
\mathbf{E}(X|T_1 > 5) &= 5 + 30 + 30 = 65\\
\mathbb{P}(T_1 > 5) &= e^{-5/30}
\end{aligned}
$$
Applying total expectation theorem,
$$
\begin{aligned}
\mathbf{E}[X] &= \mathbb{P}(T_1 \leq 5)\mathbf{E}[X | T_1 \leq 5] + \mathbb{P}(T_2 > 5)\mathbf{E}[X | T_2 > 5]\\
&= 35 (1-e^{-5/30}) + 65 e^{-5/30}\\
&= 60.394
\end{aligned}
$$










# Lecture 4. Counting

* Discrete uniform law

  Assuming $\Omega$ consists of $n$ equally likely elements, $A$ is a subset of $\Omega$, and $A$ consists of $k$ elements.
  $$
  \mathbf{P}(A) = {k \over n}
  $$

* Basic counting principle

  Assuming $r$ stages and $n_i$ choices at stage $i$. The number of choices is $n_1 \cdot n_2 ... n_r$.

* Applications

  * Permutations: Number of ways of ordering $n$ elements:

    $n \cdot (n-1) \cdot (n-2) ... 1 = n!$

  * Number of subsets of $\{1,...,n\}$:

    $\sum^n_{k=0} = {n \choose 0} + {n \choose 1} + ... +{n \choose n}=2^n$

  * Combinations:

    $ {n \choose k}$: number of $k$ element subsets of a given $n$ element set.

    ${n \choose k} = \frac{n!}{k!(n-k)!}= n(n-1)(n-2)...(n-k+1) \quad n = 0,1,2,...$

    ${n \choose n} = \frac{n!}{n! 0!} =1, \quad 0! = 1$

    ${n \choose 0} = \frac{n!}{0!n!} = 1$

  * Binomial probabilities:

    In the case of coin tossing,

    $\mathbf{P}(k \text{ head}) = {n \choose k}p^k (1-p)^{n-k}$, where ${n \choose k}$ is binomial coefficient.

  * Partitions:

    $n \geq 1$ distinct items; $r \geq 1$ persons; give $n_i$ items to person $i$. $n_1 + n_2 + ... + n_r = n$.

    Number of partition = $\frac{n!}{n_1!n_2! ...n_r!}$ (multinomial coefficient)
    
  * Multinomial probabilities
  
    Find $\mathbf{P}(n_1 \text{balls of color 1}, n_2 \text{balls of color 2, ..., } n_r \text{ balls of color r})$.
    $$
    \mathbf{P}(\text{get type} (n_1, n_2, ..., n_r)) = \frac{n!}{n_1!n_2!...n_r!} p_1^{n_1}p_2^{n_2}...p_r^{n_r}
    $$


* Example of card deck: 52-card deck, dealt ( fairly ) to 4 players. Find $\mathbf{P}$(each player gets and ace)

  * Number of outcomes: $\frac{52!}{13!13!13!13!}$

  * Outcome with one ace for each person: 

    * distribute the aces: $4\cdot3\cdot2\cdot1$
    * distribute the remaining 48 cards: $\frac{48!}{12!12!12!12!}$

  * Answer: 
    $$
    \frac{4\cdot3\cdot2\cdot\frac{48!}{12!12!12!12!}}{\frac{52!}{13!13!13!13!}}
    $$

  * Another way of thinking: 
    $$
    \frac{39}{51} \cdot \frac{26}{50} \cdot \frac{13}{49} = 0.105
    $$


---

There are 3 selected exercises and 3 solved problems.

## Exercise 1 Use counting to calculate probabilities

Given the set of letters {A, B, C, D, E}. How many five-letter strings can be made if we require that each letter appears exactly once and the letters A and B are next to each other, as either “AB" or “BA"? What is the probability of that? 

**Answer**: 48; 0.4

**Solution**: 

We first choose whether the order will be “AB" or “BA" (2 choices). We then choose the position of the first letter in “AB" or “BA". There are 4 choices, namely positions 1, 2, 3, or 4. We are left with three positions in which the letters C, D, and E can be placed, in any order. The number of ways that this can be done is the number of permutations of these three letters, namely, $3!=3⋅2⋅1=6$. Thus, the answer to this problem is $2⋅4⋅6=48$.

The sample space has $5!=120$ elements. Thus, the desired probability is $48/120=2/5=0.4$.

## Exercise 2 Counting committees

We start with a pool of $n$ people. A chaired committee consists of $k≥1$ members, out of whom one member is designated as the chairperson. The expression $k {n\choose k}$ can be interpreted as the number of possible chaired committees with $k$ members. This is because we have ${n \choose k}$ choices for the $k$ members, and once the members are chosen, there are then $k$ choices for the chairperson. Thus,
$$
c=\sum _{k=1}^ n k {n \choose k}
$$
is the total number of possible chaired committees of any size.

**Answer**: $n 2^{n-1}$

**Solution**: 

We first choose the chairperson, for which there are $n$ choices, and then choose an arbitrary subset of the remaining $n−1$ people, who will be the remaining committee members. For example, this arbitrary subset could be the empty set, which would mean that the committee is of size 1: only the chairperson. There are $2^{n−1}$ possible subsets of a set with $n−1$ elements, and so there are $2^{n−1}$ ways of choosing the remaining committee members. Thus, an alternative expression for the number of possible chaired committees of any size is $n2^{n−1}$.

## Exercise 3 Coin tossing

Find the probability that the $6$th toss out of a total of $10$ tosses is Heads, given that there are exactly $2$ Heads out of the $10$ tosses. As in the preceding segment, continue to assume that all coin tosses are independent and that each coin toss has the same fixed probability of Heads.

**Answer**:  $1/5$

**Solution**: 
$$
\mathbf{P} = \frac{\text{ the 6th toss out of a total of 10 tosses is Heads}}{\text{there are 2 Heads out of the 10 tosses}} = \frac{9}{{10 \choose 2}} = 0.2
$$

## Problem 1 The birthday problem

Consider $n$ people who are attending a party. We assume that every person has an equal probability of being born on any day during the year, independently of everyone else, and ignore the additional complication presented by leap years (i.e., nobody is born on February 29). What is the probability that each person has a distinct birthday?

**Answer**: 
$$
\mathbf{P}(\text{no two birthdays coincide}) = \frac{365 \cdot 364 ... (365 - n + 1)}{365^n}
$$

## Problem 2 Rooks on a chessboard

Eight rooks are placed in distinct squares of an $8×8$ chessboard, with all possible placements being equally likely. Find the probability that all the rooks are safe from one another, i.e., that there is no row or column with more than one rook.

**Answer**: 
$$
\frac{64 \cdot 49 \cdot 36 \cdot 25 \cdot 16 \cdot 9 \cdot 4}{64!/56!}
$$
**Solution**: 

The numerator: divide the chessboard into 8 rows, each with 8 cells. Every time we assign one rook to one cell on the chessboard and do not consider the row with that cell any more until completion.

The denominator: $64 \cdot 63 \cdot 62 ... 56$, the number of different chessboard with 8 rooks.

## Problem 3 Hypergeometric probabilities

An urn contains $n$ balls, out of which exactly $m$ are red. We select $k$ of the balls at random, without replacement (i.e., selected balls are not put back into the urn before the next selection). What is the probability that $i$ of the selected balls are red?

**Answer**: 
$$
\frac{{m \choose i} {{n-m}\choose {k-i}}}{{n \choose k}}
$$
**Solution**: 

The denominator: ${n \choose k}$ choose $k$ balls from $n$ balls.

The numerator: ${m \choose i }$ choose $i$ red balls from $m$ red balls, ${{n-m}\choose {k-i}}$ other chosen balls are not red balls chosen from all non-red balls.



# Lecture 2. Conditioning and Bayes' rule

* Conditional probability

  * $\mathbf{P}(A|B) = \frac{\mathbf{P}(A \cap B)}{\mathbf{P}(B)} $, defined only when $ \mathbf{P}(B) > 0$.

  * Properties: 

    $\mathbf{P}(A|B) \geq 0 $, assuming $ \mathbf{P}(B) > 0.$

    If $A \cap C = \empty$, then $\mathbf{P}(A \cup C | B) = \mathbf{P}(A | B) + \mathbf{P}(C | B)$.

* Three important tools

  * Multiplication rule

    $\mathbf{P}(A \cap B) = \mathbf{P}(B) \mathbf{P}(A|B) = \mathbf{P}(A)\mathbf{P}(B|A).$

    $\mathbf{P}(A^c \cap B \cap C^c) = \mathbf{P}(A^c) \cdot \mathbf{P}(B|A^c) \cdot \mathbf{P}(C^c|A^c \cap B).$

    $\mathbf{P}(A_1 \cap A_2 \cap ... \cap A_n) = \mathbf{P}(A_1) \prod^n_{i=2} \mathbf{P}(A_i | A_1 \cap ... \cap A_{i-1}).$

  * Total probability theorem

    $\mathbf{P}(B) = \sum_i \mathbf{P}(A_i)\mathbf{P}(B|A_i)$, which is a weighted average of $\mathbf{P}(B|A_i)$. Note that $\sum_i \mathbf{P}(A_i) = 1$.

  * Bayes' rule ($\rightarrow$ inference)

    $\mathbf{P}(A_i | B) = \frac{\mathbf{P}(A_i \cap B)}{\mathbf{P}(B)} = \frac{\mathbf{P}(A_i) \mathbf{P}(B | A_i)}{\sum_j \mathbf{P}(A_j) \mathbf{P}(B|A_j)}.$

    Inference: 

    * initial beliefs $\mathbf{P}(A_i)$ on possible causes of an observed event $B$.

    * model of the world under each $A_i: \mathbf{P}(B | A_i)$.

      $A_i \xrightarrow{\text{model } \mathbf{P}(B|A_i)} B$

    * draw conclusions about causes

      $B \xrightarrow{\text{inference } \mathbf{P}(A_i | B)} A_i$

---

There are 1 selected exercise and 3 solved problems.

## Exercise 1 Total probability theorem

We have an infinite collection of biased coins, indexed by the positive integers. Coin $i$ has probability $2^{−i}$ of being selected. A flip of coin $i$ results in Heads with probability $3^{−i}$. We select a coin and flip it. What is the probability that the result is Heads?

**Solution**: 

Using the geometric sum formula $\sum _{i=1}^{\infty }\alpha ^ i =\frac{\alpha }{1-\alpha }$ when $|\alpha| < 1$. By the total probability theorem, for the case of infinitely many scenarios,
$$
\mathbf{P}(\text{Heads})=\sum _{i=1}^{\infty } \mathbf{P}(A_ i)\mathbf{P}(\text{Heads}\mid A_ i)=\sum _{i=1}^{\infty } 2^{-i}3^{-i} =\sum _{i=1}^{\infty } (1/6)^ i =\frac{1/6}{1-(1/6)}=\frac{1}{5}.
$$

## Problem 1 A chess tournament problem

**A chess tournament problem.** This year's Belmont chess champion is to be selected by the following procedure. Bo and Ci, the leading challengers, first play a two-game match. If one of them wins both games, he gets to play a two-game **second round** with Al, the current champion. Al retains his championship unless a second round is required and the challenger beats Al in both games. If Al wins the initial game of the second round, no more games are played.

Furthermore, we know the following:
∙ The probability that Bo will beat Ci in any particular game is 0.6.
∙ The probability that Al will beat Bo in any particular game is 0.5.
∙ The probability that Al will beat Ci in any particular game is 0.7.

Assume no tie games are possible and all games are independent.

\1. Determine the a priori probabilities that
(a) the second round will be required.
(b) Bo will win the first round.
(c) Al will retain his championship this year.

\2. Given that the second round is required, determine the conditional probabilities that
(a) Bo is the surviving challenger.
(b) Al retains his championship.

\3. Given that the second round was required and that it comprised only one game, what is the conditional probability that it was Bo who won the first round?

**Solution**: 

\1. 

a) $\mathbf{P}(\text{2nd Rnd Req}) = (0.6)^2 + (0.4)^2 = 0.52$

b) $\mathbf{P}(\text{Bo Wins 1st Rnd}) = (0.6)^2 = 0.36$

c) $\mathbf{P}(\text{Al Champ})\\ = 1 - \mathbf{P}(\text{Bo Champ}) - \mathbf{P}(\text{Ci Champ})\\ = 1 - (0.6)^2 \cdot (0.5)^2 - (0.4)^2 \cdot (0.3)^2\\ = 0.8956$

\2.

a) $\mathbf{P}(\text{Bo Challenger | 2nd Rnd Req}) = \frac{(0.6)^2}{0.52} = \frac{0.36}{0.52} = 0.6923$

b) $\mathbf{P}(\text{Al Champ | 2nd Rnd Req}) \\=\mathbf{P}(\text{Al Champ | Bo CHallenger, 2nd Rnd Req}) \cdot \mathbf{P}(\text{Bo Challenger| 2nd Rnd Req})\\ + \mathbf{P}(\text{Al Champ | Ci Challenger, 2nd Rnd Req}) \cdot \mathbf{P}(\text{Ci CHallenger| 2nd Rnd Req})\\=(1-(0.5)^2) \cdot 0.6923 + (1-(0.3)^2) \cdot 0.3077\\ = 0.7992$.

c) $\mathbf{P}(\text{Bo Challenger| \{(2nd Rnd Req)} \cap\text{(One Game)\}})\\ = \frac{(0.6)^2 \cdot (0.5)}{(0.6)^2 \cdot 0.5 + (0.4)^2 \cdot (0.7)}\\ = \frac{(0.6)^2 (0.5)}{0.2920}\\ = 0.6164$

## Problem 2 A coin tossing puzzle

A coin is tossed twice. Alice claims that the event of getting two Heads is at least as likely if we know that the first toss is Heads than if we know that at least one of the tosses is Heads. Is she right? Does it make a difference if the coin is fair or unfair? 

**Solution**: 

Let $A$ be the event that the first toss is a head and let $B$ be the event that the second toss is a head. We must compare the conditional probabilities $\mathbf{P}(A ∩ B | A)$ and $\mathbf{P}(A ∩ B | A ∪ B)$.

Since $\mathbf{P}(A \cup B) \geq \mathbf{P}(A)$, the conditional probability $\mathbf{P}(A ∩ B | A)$ is as least as large, regardless of whether the coin is fair or not.

## Problem 3 Oscar's lost dog in the forest

Oscar has lost his dog in either forest A (with probability $0.4$) or in forest B (with probability $0.6$).

If the dog is in forest A and Oscar spends a day searching for it in forest A, the conditional probability that he will find the dog that day is $0.25$. Similarly, if the dog is in forest B and Oscar spends a day looking for it there, he will find the dog that day with probability $0.15$.

The dog cannot go from one forest to the other. Oscar can search only in the daytime, and he can travel from one forest to the other only overnight.

The dog is alive during day $0$, when Oscar loses it, and during day $1$, when Oscar starts searching. It is alive during day $2$ with probability $2/3$. In general, for $n≥1$, if the dog is alive during day $n−1$, then the probability it is alive during day $n$ is $2/(n+1)$. The dog can only die overnight. Oscar stops searching as soon as he finds his dog, either alive or dead.

a) In which forest should Oscar look on the first day of the search to maximize the probability he finds his dog that day?

b) Oscar looked in forest A on the first day but didn't find his dog. What is the probability that the dog is in forest A?

c) Oscar flips a fair coin to determine where to look on the first day and finds the dog on the first day. What is the probability that he looked in forest A?

d) Oscar decides to look in forest A for the first two days. What is the probability that he finds his dog alive for the first time on the second day?

e) Oscar decides to look in forest A for the first two days. Given that he did not find his dog on the first day, find the probability that he does not find his dog dead on the second day.

f) Oscar finally finds his dog on the fourth day of the search. He looked in forest A for the first 3 days and in forest B on the fourth day. Given this information, what is the probability that he found his dog alive?

**Answer**: 

a) Forest A; b) $1/3$; c) $10/19$; d) $0.05$; e) $35/36$; f) $2/15$;

**Solution**: 
$$
\begin{aligned}
S_A &=  \text{event that Oscar searches for his dog in forest A}\\
S_B &= \text{event that Oscar searches for his dog in forest B}\\
A &=  \text{event that his dog is lost in forest A}\\
B &=  \text{event that his dog is lost in forest B}\\
F_i &= \text{event that Oscar finds his dog on day }i\\
L_i &= \text{event that his dog is alive on day }i
\end{aligned}
$$
a) Oscar has two choices represented by the following tree diagrams:

![oscar-dog1](../assets/images/oscar-dog1.jpg)

Comparing
$$
\mathbf{P}(F_1 \cap S_ A) = (0.4)(0.25) = 0.1\\
\mathbf{P}(F_1 \cap S_ B) = (0.6)(0.15) = 0.09\\
$$
Therefore, he should choose to search in forest A.

b) The desired probability is
$$
\mathbf{P}(A\mid S_ A \cap F^ c_1) = \frac{\mathbf{P}(A \cap S_ A \cap F^ c_1)}{\mathbf{P}(S_ A \cap F^ c_1)} = \frac{(0.4)(0.75)}{(0.4)(0.75) + (0.6)(1)} = \frac{1}{3}.
$$
c) we get the following diagram

![oscar-dog2](../assets/images/oscar-dog2.jpg)

The desired probability is
$$
\mathbf{P}(S_ A \mid F_1) = \frac{\mathbf{P}(S_ A \cap F_1)}{\mathbf{P}(F_1)} = \frac{(0.5)(0.4)(0.25)}{(0.5)(0.4)(0.25) + (0.5)(0.6)(0.15)} = \frac{10}{19}.
$$
d) The following diagram illustrates the sequence of possible events

![oscar-dog3](../assets/images/oscar-dog3.png)

The desired probability is 
$$
\mathbf{P}(A \cap F_1^ c \cap L_2 \cap F_2 \mid S_ A) = (0.4)(0.75)(2/3)(0.25) = 0.05.
$$
e) Using the diagram in d), the desired probability is 
$$
\begin{aligned}
&\mathbf{P}(\text{Oscar does not find dead dog on day 2} \mid F^ c_1 \cap S_ A)\\
&= 1 - \mathbf{P}(\text{Oscar does find dead dog on day 2} \mid F^ c_1 \cap S_ A)\\
&=1 - \frac{\mathbf{P}(S_ A \cap A \cap F_1^ c \cap L_2^ c \cap F_2)}{\mathbf{P}(F_1^ c \cap S_ A)}\\
&=1 - \frac{(0.4)(0.75)(1/3)(0.25)}{(0.4)(0.75)+(0.6)(1.0)}\\
&= \frac{35}{36}.
\end{aligned}
$$
f) We know that Oscar found his dog and we know it took 4 days. It doesn't matter, then, where he searched. We just want the probability the dog survived to day 4. This probability is 
$$
\mathbf{P}(L_4) = \left(\frac{2}{2+1}\right)\left(\frac{2}{3+1}\right)\left(\frac{2}{4+1}\right) = \frac{2}{15}.
$$

# Lecture 10. Conditioning on a random variable; Independence; Bayes' rule

* Conditioning $X$ on $Y$

  * Definition: $f_{X|Y}(x|y) = {f_{X,Y}(x,y)\over f_Y(y)}$ if $f_Y(y) > 0$.

  * Multiplication rule: $f_{X,Y}(x,y) = f_Y(y) \cdot f_{X|Y}(x|y) = f_X(x) \cdot  f_{Y|X}(y|x)$.

  * Total probability theorem and Total expectation theorem

    | Discrete                                                   | Continuous                                                   |
    | ---------------------------------------------------------- | ------------------------------------------------------------ |
    | $p_X(x) = \sum_y p_Y(y) p_{X\vert Y}(x\vert y)$            | $f_X(x) =\int^\infty_{-\infty}f_Y(y)f_{X\vert Y}(x\vert y)dy$ |
    | $\mathbf{E}[X\vert Y=y] = \sum_x x p_{X\vert Y}(x\vert y)$ | $\mathbf{E}[X\vert Y=y] = \int^\infty_{-\infty} xf_{X\vert Y}(x\vert y) dx$ |
    | $\mathbf{E}[X] = \sum_y p_Y(y) \mathbf{E}[X\vert Y=y]$     | $\mathbf{E}[X] = \int^\infty_{-\infty}f_Y(y) \mathbf{E}[X\vert Y=y] dy$ |

* Independence

  * $f_{X,Y}(x,y) = f_X(x)f_Y(y), \quad \text{for all }x \text{ and } y$.

  * $f_{X,Y}(x,y) = f_{X|Y}(x|y)f_Y(y), \quad \text{for all }x \text{ and } y$.

  * $f_{X|Y}(x|y) =f_X(x), \quad \text{for all }x \text{ and } y$.

  * If $X,Y$ are independent: 

    $\mathbf{E}[XY] = \mathbf{E}[X]\mathbf{E}[Y]$

    $\mathsf{Var}(X + Y)=\mathsf{Var}(X) + \mathsf{Var}(Y)$

    $\mathbf{E}[g(X)h(Y)] = \mathbf{E}[g(X)]\cdot \mathbf{E}[h(Y)]$

  * Independent standard normals

    $f_{X,Y}(x,y) = f_X(x)f_Y(y) \\= {1\over \sqrt{2\pi}}\exp \{-{x^2 \over 2}\} \cdot {1\over \sqrt{2\pi} }\exp \{-{y^2 \over 2}\}\\={1\over 2n} \exp \{-{1\over 2}(x^2 + y^2)\}$

  * Independent normals

    $f_{X,Y}(x,y) = f_X(x)f_Y(y)\\={1\over 2\pi \sigma_x \sigma_y}\exp\{-{(x-\mu_x)^2\over 2\sigma_x^2}-{(y-\mu_y)^2\over s\sigma_y^2}\}$

* A comprehensive example

  Break a stick of length $l$ twice: first break at $X$, $X \sim \mathsf{Unif}[0,l]$; second break at $Y$, $Y \sim \mathsf{Unif}[0,X]$.

  * Given PDFs $f_X(x) = 1/l,\quad f_Y(y) = 1/x, \quad 0\leq y \leq x \leq l$.
  * The joint PDF is $f_{X,Y}(x,y) =f_X(x)f_{Y|X}(y|x) = {1\over lx}$.
  * The marginal PDF is $f_Y(y) = \int f_{X,Y}(x,y)dx = \int^l_y {1\over lx} dx = {1\over l} \log({l\over y })$.
  * The expectation of $Y$ is $\mathbf{E}[Y] = \int^l_0 y {1\over e}\log ({l\over y})dy.$
  * Or using total expectation theorem: $\mathbf{E}[Y] = \int^l_0 {1\over e} \mathbf{E}[Y|X=x]dx = \int^l_0 {1\over e} {x \over 2} dx = {1\over 2}\mathbf{E}[X] = {1\over 2} \cdot {l \over 2} = {l \over 4}$.

* Four variants of the Bayes rule

  | Type                           | Formula                                                      |
  | ------------------------------ | ------------------------------------------------------------ |
  | $X$ discrete, $Y$ discrete     | $p_{X\vert Y}(x \vert y) = {p_X(x) p_{Y \vert X}(y \vert x) \over p_Y(y)} \\ p_Y(y) = \sum_{x'}p_X(x') p_{Y\vert X}(y \vert x')\\p_{Y\vert X}(y \vert x) = { p_Y(y) p_{X\vert Y} (x \vert y)\over p_X(x)}\\ p_X(x) = \sum_{y'} f_Y(y') p_{X \vert Y} (x \vert y')$ |
  | $X$ discrete, $Y$ continuous   | $p_{X\vert Y}(x \vert y) = {p_X(x) f_{Y \vert X}(y \vert x) \over f_Y(y) }\\ f_Y(y) = \sum_{x'}p_X(x') f_{Y\vert X}(y \vert x')\\f_{Y\vert X}(y \vert x) = { f_Y(y) p_{X\vert Y} (x \vert y)\over p_X(x)}\\ p_X(x) = \int f_Y(y') p_{X \vert Y} (x \vert y') dy'$ |
  | $X$ continuous, $Y$ discrete   | $f_{X\vert Y}(x \vert y) = {f_X(x) p_{Y \vert X}(y \vert x) \over p_Y(y) }\\ p_Y(y) = \int f_X(x') p_{Y\vert X}(y \vert x')dx'\\p_{Y\vert X}(y \vert x) = { p_Y(y) f_{X\vert Y} (x \vert y)\over f_X(x)}\\ f_X(x) = \sum_{y'} p_Y(y') f_{X \vert Y} (x \vert y')$ |
  | $X$ continuous, $Y$ continuous | $f_{X\vert Y}(x \vert y) = {f_X(x) f_{Y \vert X}(y \vert x) \over f_Y(y) }\\ f_Y(y) = \int f_X(x') p_{Y\vert X}(y \vert x')dx'\\f_{Y\vert X}(y \vert x) = { f_Y(y) f_{X\vert Y} (x \vert y)\over f_X(x)}\\ f_X(x) = \int f_Y(y') f_{X \vert Y} (x \vert y') dy'$ |

There are 3 selected exercises and 3 solved problems.

---

## Exercise 1 Conditional PDFs

The random variables $X$ and $Y$ are jointly continuous, with a joint PDF of the form
$$
f_{X,Y}(x,y)=\begin{cases}  cxy,&  \mbox{if } 0\leq x\leq y\leq 1,\\ 0,&  \mbox{otherwise,}\end{cases}
$$
where $c$ is a normalizing constant.

For $x\in [0,0.5]$, the conditional PDF $f_{X|Y}(x\, |\, 0.5)$ is of the form $ax^b$. Find $a$ and $b$.

**Answer**: $a = 8, b = 1.$

**Solution**: 

By Bayes' Theorem,
$$
f_{X|Y}(x\, |\, 0.5)=\frac{f_{X,Y}(x,0.5)}{f_ Y(0.5)}.
$$
Having fixed $y = 0.5$, the conditional PDF is to be viewed as a function of $x$. For those values of $x$ that are possible (i.e. $x \in [0,0.5]$), the conditional PDF will be proportional to the joint PDF, hence of the form $ax$, for some constant $a$. This implies that $b =1$. To find the normalizing constant, we use normalization equation.
$$
1=\int _0^{0.5} f_{X|Y}(x\, |\, 0.5)\,  dx=\int _0^{0.5} ax\,  dx = a \cdot \frac{x^2}{2}\Big|_0^{0.5} =\frac{a}{8},
$$
which yields $a = 8$.

## Exercise 2 Expected value rule and total expectation theorem

Let $X, Y$, and $Z$ be jointly continuous random variables. Assume that all conditional PDFs and expectations are well defined. E.g., when conditioning on $X=x$, assume that is such that $f_X(x) > 0$. For each one of the following formulas, state whether it is true for all choices of the function $g$ or false (i.e., not true for all choices of $g$).

1) $\mathbf{E}(g(Y)| X=x) = \int g(y) f_{Y|X}(y|x) dy$

True. 

2) $\mathbf{E}(g(y)| X=x) = \int g(y) f_{Y|X}(y|x) dy$

False. $g(y)$ is a number (not a random variable). The LHS is a function of $y$, whereas on the RHS $y$ is a dummy variable that gets integrated away.

3) ${\bf E}\big [g(Y)\big ] =\displaystyle {\int } {\bf E}\big [g(Y)\, |\, Z=z\big ]\,  f_ Z(z)\,  dz$

True.

4) ${\bf E}\big [g(Y)\, |\, X=x, Z=z\big ]=\displaystyle {\int } g(y) f_{Y|X,Z}(y\, |\, x,z)\,  dy$

True.

5) ${\bf E}\big [g(Y)\, |\, X=x\big ] =\displaystyle {\int } {\bf E}\big [g(Y)\, |\, X=x,Z=z\big ]\,  f_{Z|X}(z\, |\, x)\,  dz$

True.

6) ${\bf E}\big [g(X,Y)\, |\, Y=y\big ]= {\bf E}\big [g(X,y)\, |\, Y=y\big ]$

True.

7) ${\bf E}\big [g(X,Y)\, |\, Y=y\big ]= {\bf E}\big [g(X,y)\big ]$

False. Given that $Y=y$, we need to somehow take into account the conditional distribution of $X$, whereas the RHS is determined by the unconditional PDF of $X$.

8) ${\bf E}\big [g(X,Z)\, |\, Y=y\big ]= \displaystyle {\int } g(x,z) f_{X,Z|Y}(x,z\, |\, y)\,  dy$

False. The LHS is a function of $y$, whereas the RHS (after $y$ is integrated out) is a function of $x$ and $z$. The correct form (expected value rule, in a conditional model) is
$$
{\bf E}\big [g(X,Z)\, |\, Y=y\big ]= \int \int g(x,z) f_{X,Z|Y}(x,z\, |\, y)\,  dx\,  dz.
$$


## Exercise 3 Independence and CDFs

1. Suppose that $X$ and $Y$ are independent. Is it true that their joint CDF satisfies $F_{X,Y}(x,y)=F_ X(x)F_ Y(y),$ for all $x$ and $y$.
2. Suppose that $F_{X,Y}(x,y)=F_ X(x)F_ Y(y),$ for all $x$ and $y$. Is it true that $X$ and $Y$ are independent? (Hint: $f_{X,Y}(x,y)=(\partial ^2/\partial x\partial y)F_{X,Y}(x,y)$).

1) True.
$$
\begin{aligned}
F_{X,Y}(x,y) &= \mathbf{P}(X\leq x, Y\leq y)\\
&= \int _{-\infty }^ y \int _{-\infty }^ x f_{X,Y}(x,y)\,  dx\,  dy\\
&=\int _{-\infty }^ x f_ X(x)\,  dx \int _{-\infty }^ y f_ Y(y)\, dy\\
&=F_ X(x)F_ Y(y).
\end{aligned}
$$
2) True.
$$
\begin{aligned}
f_{X,Y}(x,y) &= \frac{\partial ^2}{\partial x\partial y}F_{X,Y}(x,y)\\
&= \frac{\partial ^2}{\partial x\partial y}F_{X}(x) F_ Y(y)\\
&= \frac{\partial }{\partial x} F_ X(x) \frac{\partial }{\partial y} F_ Y(y)\\
&= f_ X(x) f_ Y(y)
\end{aligned}
$$
and therefore we have independence.

## Problem 1 The Bayes rule with continuous random variables

Let $X$ and $Y$ be independent continuous random variables with PDFs $f_X$ and $f_Y$ , respectively, and let $Z = X + Y$.

Show that $f_{Z|X}(z|x) = f_Y(z - x)$ . *Hint:* Write an expression for the conditional CDF of given , and differentiate.

**Solution**:

We first find CDF,
$$
\begin{aligned}
\mathbb{P}(Z \leq z| X=x) &= \mathbb{P}(X+Y \leq z | X=x)\\ 
&= \mathbb{P}(X + Y\leq z | X=x)\\
&= \mathbb{P}(X + Y\leq z)\\
&= \mathbb{P}(Y \leq z - x)\\
&= F_Y(z-x)\\
\end{aligned}
$$
Then differentiate it, we get
$$
f_{Z|X}(z|x) = f_Y(z - x)
$$
**Remark**: Combining it with Bayes' Theorem, we can easily solve many problems.
$$
f_{X|Z}(x|z) = {f_X(x) f_{Z|X}(z|x)\over f_Z(x)}\\
f_{Z|X}(z|x) = f_Y(z - x)
$$


## Problem 2 Sophia's vacation

Sophia is vacationing in Monte Carlo. On any given night, she takes $X$ dollars to the casino and returns with $Y$ dollars. The random variable $X$ has the PDF shown in the figure. Conditional on $X = x$, the continuous random variable $Y$ is uniformly distributed between zero and $3x$.

![lec10-prob2](../assets/images/lec10-prob2.jpg)

1. Determine the joint PDF $f_{X,Y}(x,y).$

   a. If $0 < x < 40$ and $0 < y < 3x$

   b. If $y < 0$ or $y > 3x$

2. On any particular night, Sophia makes a profit $Z = Y-X$ dollars. Find the probability that Sophia makes a positive profit, that is, find $\mathbf{P}(Z > 0)$.

3. Find the PDF of $Z$: $f_Z(z)$. Express your answers in terms of $z$.

   a. If $-40 < z < 0$

   b. If $0 < z < 80$

   c. If $z < -40$ or $z > 80$

4. What is $\mathbf{E}[Z]$?

**Solution**: 

1) First reveal $f_X(x)$ by observing the figure
$$
1 = \int^\infty_{-\infty} f_X(x) dx = \int^{40}_0 ax dx = 800a
$$
Hence,
$$
f_X(x) = {x \over 800}
$$
Given
$$
f_{Y|X} (y | x) =  {1\over 3x} \quad \text{for }0 < y < 3x
$$
By using $f_{Y|X}(y\mid x)=\frac{1}{3x}$. We obtain the following expression for the joint density
$$
f_{X,Y}(x,y)=\left\{ \begin{array}{ll}\displaystyle \frac{1}{2400},&  \text { if } 0<x<40\text { and }0<y<3x\\ 0,& \text {otherwise.}\end{array}\right.
$$
2) The first approach is to consider the region where Sophia makes positive profit. Notice that, this region consists of pairs $(x,y)$ , where $y > x$. Intersecting this region with the region where the joint density is non-negative, we need to consider
$$
\{(x,y): 0 < x < 40, x < y < 3x\}
$$
Thus, the joint CDF is
$$
\mathbf{P}(Y > X)=\int _0^{40} \int _ x^{3x}f_{X,Y}(x,y)\; dy\; dx=\int _0^{40}\int _ x^{3x}\frac{1}{2400}\; dy\; dx=\int _0^{40} \frac{x}{1200}=\frac{2}{3}.
$$
We could have also arrived at this answer by realizing that for each possible value of $X$, there is a probability that $Y>X$, and therefore by the total probability theorem,
$$
\begin{aligned}
\mathbf{P}(Y>X) &=\int _0^{40}\mathbf{P}(Y>X\mid X=x)f_ X(x)\; dx\\
&=\int _0^{40}\frac{2}{3}f_ X(x)\; dx\\
&= {2\over 3}
\end{aligned}
$$
where the last equality follows because a PDF always integrates to $1$, over the region where it is nonzero.

3) Given $X = x$, $Y$ is uniformly distributed on $[0,3x]$, hence $Z = Y-x$ is uniform over $[-x, 2x]$. Thus,
$$
f_{Z|X}(z\mid x)=\frac{1}{3x}, \quad \text { for }-x\leq z\leq 2x.
$$
Therefore,
$$
f_{X,Z}(x,z)=f_ X(x)f_{Z|X}(z\mid x)=\frac{x}{800}\frac{1}{3x}=\frac{1}{2400},\text { for }0<x<40\text { and }-x\leq z \leq 2x.
$$
Now, we will integrate over $x$ to compute the marginal density $f_Z(z)$. Note that, $x \geq -z$ and $x \geq {z \over 2}$ must be satisfied at the same time (in order for $f_{X,Z}$ to be non-zero).

If $-40 < z < 0$, the range of integration is $-z < x < 40$. Hence,
$$
f_{Z}(z)=\int _{-z}^{40}\frac{1}{2400}\; dx=\frac{40+z}{2400}.
$$
If $0 < z < 80$, the range of integration is $z/2 \leq x \leq 40$. Hence,
$$
f_ Z(z)=\int _{z/2}^{40}\frac{1}{2400}\; dx=\frac{80-z}{4800}.
$$
Therefore, the pdf of $Z$ is
$$
f_ Z(z)=\left\{ \begin{array}{ll}\displaystyle \frac{40+z}{2400},& -40<z<0\\ \displaystyle \frac{80-z}{4800},& 0<z<80\\ 0, & \text {otherwise.}\end{array}\right.
$$
4) First, calculate $\mathbf{E}[Y|X=x]$, for any $x \in [0,40]$:
$$
\begin{aligned}
\mathbf{E}[Y|X=x] &= \int^{3x}_0 y f_{Y|X}(y|x) dy\\
&= \int^{3x}_0 {y \over 3x} dy\\
&= \left[ {1\over 6} \cdot {y^2 \over x} \right]^{3x}_0\\
&= {1\over 6x} (9x^2 - 0)\\
&= {3\over 2} x
\end{aligned}
$$
Thus, using the total expectation theorem,
$$
\begin{aligned}
{\bf E}[Y] &=\int _0^{40}{\bf E}[Y|X=x]f_ X(x)\; dx\\
&=\frac{3}{2}\int _0^{40}xf_ X(x)\; dx\\
&=\frac{3}{2}{\bf E}[X].
\end{aligned}
$$
Since, $Z = Y - X$, we have, using linearity of expectation, $\mathbf{E}[Z] = \mathbf{E}[Y] - \mathbf{E}[X] = {1\over 2} \mathbf{E}[X].$

Now, 
$$
{\bf E}[X]=\int _0^{40}xf_ X(x)\; dx=\int _0^{40}\frac{x^2}{800}\; dx=\frac{80}{3}.
$$
Hence, ${\bf E}[Z]=40/3$.

## Problem 3 A joint PDF on a triangular region

This figure below describes the joint PDF of the random variables $X$ and $Y$. These random variables take values in $[0,2]$and $[0,1]$, respectively. At $x=1$, the value of the joint PDF is $1/2$.

![lec10-prob3-1](../assets/images/lec10-prob3.png)

1. Are $X$ and $Y$ independent?

2. Find $f_X(x)$. Express your answers in terms of $x$.

   a. If $0 < x < 1$,

   b. If $1 < x < 2$,

   c. If $x < 0$ or $x \geq 2$,

3. Find $f_{Y|X}(y | 0.5)$

   a. If $0 < y < 1/2$,

   b. If $y < 0$ or $y > 1/2$,

4. Find $f_{X\mid Y}(x\mid 0.5)$

   a. If $1/2<x<1$,

   b. If $1 < x < 3/2$,

   c. If $x < 1/2$ or $x > 3/2$,

5. Let $R = XY$ and let $A$ be the event that $\{ X < 0.5\}$. Find $\mathbf{E}[R|A]$

**Solution**: 

1) 

Since $X$ gives information about $Y$, that is, given $X < 0.5$, we can infer that $Y < 0.5$. In other words, 
$$
f_{Y|X}(y| 0.5) \neq f_Y(y)
$$
 Therefore, they are not independent.

2) 

Using the formula $f_X(x)=\int f_{X,Y}(x,y) dy$, we have
$$
\begin{aligned}
f_ X(x)&=  \begin{cases} \int _0^ x\frac12\,  dy, &  \text{if } 0 < x \leq 1, \\ \int _0^{2-x}\frac32\,  dy ,&  \text{if } 1 < x < 2, \\ 0,&  \text{otherwise}, \end{cases}\\
&=   \begin{cases} x/2, &  \text{if } 0 < x \leq 1, \\ -3x/2+3,&  \text{if } 1 < x < 2, \\ 0,&  \text{otherwise.}\end{cases}
\end{aligned}
$$
A plot of the PDF is shown below:

![lec10-prob3-2](../assets/images/lec10-prob3-2.png)

3) 

Given that $X = 0.5$,  $f_{X,Y}(x,y)  =1/2$ is fixed, thus, $Y$ is uniformly distributed.
$$
f_{Y|X}(y|x) = { f_{X,Y}(x,y) \over f_X(x) } = {1/2 \over 1/4} = 2
$$
Thus, $Y$ is uniform distributed with PDF of a constant $2$ between $0$ and $1/2$.

Therefore, the conditional PDF is
$$
f_{Y|X}(y \mid 0.5)= \begin{cases} 2, &  \text{if } 0 \leq y \leq 1/2, \\ 0, &  \text{otherwise.} \end{cases}
$$
A plot of the conditional PDF is shown below:

![lec10-prob3-3](../assets/images/lec10-prob3-3.png)

4)

The PDF of $Y$ is
$$
\begin{aligned}
f_ Y(y)&= \int_Xf_{X,Y}(x,y)dx=\int_y^1\frac{1}{2}dx+\int_1^{2-y}\frac{3}{2}dx=\frac{1-y}{2}+\frac{3(1-y)}{2}=2(1-y)
\end{aligned}
$$
Given that $Y = 0.5$, the conditional distribution of $X$ is piecewise constant
$$
f_{X|Y}(x|0.5) = { f_{X,Y}(x,0.5) \over f_Y(0.5) } = \begin{cases}{1/2}&\text{if } 1/2 < x \leq 1\\{{3/2}} &\text{if } 1 < x < 3/2\\ 0 &\text{o.w.} \end{cases}
$$
A plot of the conditional PDF is shown below:

![lec10-prob3-4](../assets/images/lec10-prob3-4.png)

5)

Under event $A$, the pair $(X,Y)$ takes values in a triangular region with sides of length $1/2$ , and area $1/8$. The conditional point PDF is uniform, so that $f_{X,Y|A}(x,y)=8$ on that set. The conditional expectation is
$$
\begin{aligned}
\mathbf{E}[R|A] &=\mathbf{E}[XY|A]\\
&=\int\int xy f_{X,Y|A}(x,y) dxdy\\
&=\int^{0.5}_0\int^{0.5}_y 8xy dxdy\\
&= 1/16
\end{aligned}
$$


# Lecture 3. Independence

* Independence of two events 

  * Definition of independence: $\mathbf{P}(A \cap B ) = \mathbf{P}(A) \cdot \mathbf{P}(B)$. 
    * If $\mathbf{P}(B|A) = \mathbf{P}(B)$, occurrence of $A$ provides no new information about $B$.
  * If $A$ and $B$ are independence, then $A$ and $B^c$ are independent.

* Conditional independence

  * Given $C$, the conditional independence is defined as independence under the probability law $\mathbf{P}(\cdot | C)$.
    $$
    \mathbf{P}(A \cap B | C) = \mathbf{P}(A | C) \mathbf{P}(B | C)
    $$

  * Independence does not imply conditional independence.

    In the case below, $A$ and $B$ have no intersection in the condition of $C$. If $A$ happens, $B$ won't happen in the condition of $C$. This means that $A$ and $B$ are not independent.

    ![condIndependent](../assets/images/condIndependent.png)

* Independence of a collection of events

  * Event $A_1, A_2, ..., A_n$ are independent if $\mathbf{P}(A_i \cap A_j \cap ... \cap A_m) = \mathbf{P}(A_i) \mathbf{P}(A_j) ... \mathbf{P}(A_m)$, for any distinct indices $i,j,...,m$.

* Pairwise independence

  * Independent events must be pairwise independent, but the reverse may not be true.

  * E.g. two independent fair coin tosses. $C$: the two tosses had the same result. $H_1$: first toss is $H$, $H_2$: second toss is $H$, $\mathbf{P}(H_1) = \mathbf{P}(H_2) = 1/2, \mathbf{P}(C) = 1/2$. 

    * $\mathbf{P}(H_1 \cap H_2) = 1/4 =  \mathbf{P}(H_1)\mathbf{P}(H_2)$
    * $\mathbf{P}(H_1 \cap C) = 1/4 = \mathbf{P}(H_1)\mathbf{P}(C)$
    * But, $\mathbf{P}(C| H_1 \cap H_2) = 1 \neq \mathbf{P}(C) = 1/2$

    So $H_1, H_2,C$ are pairwise independent, but not independent.

* Reliability

  * $p_i$: probability that unit $i$ is "up" ; $u_i$: $i$th unit up, $u_i$ are independent; $F_i$: $i$th unit down, $F_i$ are independent.

    ![reliability1](../assets/images/reliability1.png)
    $$
    \begin{aligned}
    \mathbf{P}(\text{system up}) &= \mathbf{P}(u_1 \cap u_2 \cap u_3)\\
    &= \mathbf{P}(u_1) \cap \mathbf{P}(u_2) \cap \mathbf{P}(u_3)\\
    &= p_1 p_2 p_3
    \end{aligned}
    $$
    ![reliability2](../assets/images/reliability2.png)
    $$
    \begin{aligned}
    \mathbf{P}(\text{system up}) &= \mathbf{P}(u_1 \cup u_2 \cup u_3) \\
    &= 1-\mathbf{P}(F_1 \cap F_2 \cap F_3)\\
    &= 1 - \mathbf{P}(F_1) \mathbf{P}(F_2) \mathbf{P}(F_3)\\
    &= 1-(1-p_1)(1-p_2)(1-p_3)
    \end{aligned}
    $$
  
  * In general, if a *serial* sub-system contains $m$ components with success probabilities $p_1, p_2...p_m$, then the probability of success of the entire sub-system is given by
    $$
    \mathbf{P}(\text{whole system secceeds}) = p_1p_2...p_m
    $$
    If a parallel sub-system contains $m$ components with success probabilities $p_1, p_2...p_m$, then the probability of success of the entire sub-system is given by
    $$
    \mathbf{P}(\text{whole system succeeds}) = 1 - (1-p_1)(1-p_2)...(1-p_m)
    $$

---

There are 3 selected exercises and 1 solved problem.

## Exercise 1 Independence of two events

Let $A$ be an event, a subset of the sample space $\Omega$. Are $A$ and $\Omega$ independent?

Yes, they are. Because $\mathbf{P}(A\cap \Omega )=\mathbf{P}(A)=\mathbf{P}(A)\cdot 1 =\mathbf{P}(A)\cdot \mathbf{P}(\Omega )$. Intuitively $\mathbf{P}(A)$ represents our beliefs about the likelihood that $A$ will occur. If we are told that $Ω$ occurred, this does not give us any new information; we already knew that $Ω$ is certain to occur. For this reason, $\mathbf{P}(A\mid \Omega )=\mathbf{P}(A)$.

When is an event $A$ independent of itself ?

If and only if $\mathbf{P}(A)$ is either 0 or 1. Since $\mathbf{P}(A\cap A) = \mathbf{P}(A) =\mathbf{P}(A)\cdot \mathbf{P}(A).$

## Exercise 2 Conditional independence

Suppose that $A$ and $B$ are conditionally independent given $C$. Suppose that $\mathbf{P}(C) > 0$ and $\mathbf{P}(C^c) > 0$.

1) Are $A$ and $B^c$ guaranteed to be conditionally independent given $C$? Yes

2) Are $A$ and $B$ guaranteed to be conditionally independent given $C^c$? No

Given $A$ and $B$ are conditionally independent given $C$, $A$ and $B$ must have intersection in $C$.  

For 1) we have seen that in any probability model, independence of $A$ and $B$ implies independence of $A$ and $B^c$. The conditional model (given $C$) is just another probability model, so this property remains true.

For 2) the counter example is that: events $A$ and $B$ have nonempty intersection inside $C$, and are conditionally independent, but have empty intersection inside $C^c$, which would make them dependent (given $C^c$).

## Exercise 3 Reliability

Suppose that each unit of a system is up with probability $2/3$ and down with probability $1/3$. Different units are independent. For each one of the systems shown below, calculate the probability that the whole system is up (that is, that there exists a path from the left end to the right end, consisting entirely of units that are up). What is the probability that the following system is up?

1) $16/27$

![ex_reliability1](../assets/images/ex_reliability1.jpg)

The probability of the parallel units fail = $(1/3) \cdot (1/3) = 1/9$.

The probability of the parallel connection is up = $1 - 1/9 = 8/9$.

The probability of the whole system is up = $(2/3) \cdot (8/9) = 16/27.$ 

2) $22/27$

![ex_reliability2](../assets/images/ex_reliability2.jpg)

The whole system is up only when the parallel both units are up.

The probability of the upper units up = $(2/3) \cdot (2/3) = 4/9$.

The probability of the upper units fail = $1- (4/9) = 5/9$.

The probability of the bottom units fail = $(1/3)$.

The probability of the system fail = $(5/9) \cdot (1/3) = 5/27$.

The probability of the whole system up = $1- (5/27) = 22/27$.

## Problem 1 Network reliability

An electrical system consists of identical components, each of which is operational with probability $p$, independent of other components. The components are connected in three subsystems, as shown in the figure. The system is operational if there is a path that starts at point $A$, ends at point $B$, and consists of operational components. What is the probability of this happening?

![reliability-prob3](../assets/images/reliability-prob3.jpg)

$\mathbf{P}(A \rightarrow B) = \mathbf{P}(A\rightarrow C)\mathbf{P}(C \rightarrow E) \mathbf{P}(E \rightarrow B)$ (since they are in series)

$\mathbf{P}(A \rightarrow C) = p$

$\mathbf{P}(C \rightarrow E) = 1 - (1-p)(1-\mathbf{P}(C \rightarrow D) \mathbf{P}(D \rightarrow E))$

$\mathbf{P}(E \rightarrow B) = 1-(1-p)^2$

The probability $\mathbf{P}(C \rightarrow D), \mathbf{P}(D \rightarrow E)$ can be similarly computed as

$\mathbf{P}(C \rightarrow D) = 1-(1-p)^3$

$\mathbf{P}(D \rightarrow E) = p$

The probability of success of the entire system can be obtained by substituting the subsystem success probabilities:

$\mathbf{P}(A \rightarrow B)\\ = \mathbf{P}(A\rightarrow C)\mathbf{P}(C \rightarrow E) \mathbf{P}(E \rightarrow B)\\ = p\{1-(1-p)[1-p[1-(1-p)^3]]\}[1-(1-p)^2]$



# Lecture 12. Sums of independent r.v.'s; Covariance and correlation

* The PMF/PDF of $X + Y$ ($X$ and $Y$ independent)

  | Discrete                                | Continuous                                                   |
  | --------------------------------------- | ------------------------------------------------------------ |
  | $p_Z(z) = \sum\limits_x p_X(x)p_Y(z-x)$ | $f_Z(z) = \int^\infty_{-\infty} f_X(x) f_Y(z-x)dx$           |
  |                                         | $f_{Z|X}(z|x) = f_Y(z-x);\\ f_{x + b}(x) = f_X(x-b);\\f_{X,Z}(x,z) = f_X(x)f_Y(z-x)$ |

  * the sum of independent normals

    Given $X \sim \mathcal{N}(\mu_x \sigma_x^2), Y \sim \mathcal{N(\mu_y, \sigma_y^2)}$ are independent, $Z = X + Y$.

    $f_X(x)= {1\over \sqrt{2 \pi} \sigma_x} e^{-(x - \mu_x)^2/2 \sigma_x^2}; \quad f_Y(y)= {1\over \sqrt{2 \pi} \sigma_y} e^{-(y - \mu_y)^2/2 \sigma_y^2}$
    $$
    \begin{aligned}
    f_Z(z) &= \int^\infty_{-\infty} f_X(x) f_Y(z-x) dx\\
    &=\int^\infty_{-\infty}{1\over \sqrt{2\pi} \sigma_x}\exp\left(  - {(x-\mu_x)^2\over 2\sigma_x^2}\right) {1\over \sqrt{2\pi} \sigma_y} \exp \left( - {(z-x-\mu_y)^2\over 2 \sigma^2_y } \right) dx \\
    &= {1\over \sqrt{2\pi} (\sigma_x^2 + \sigma_y^2) } \exp \left( - {(z-\mu_x - \mu_y)^2 \over2(\sigma_x^2 + \sigma_y^2) }\right) \quad  \sim \mathcal{N}(\mu_x + \mu_y, \sigma_x^2 + \sigma^2_y)
    \end{aligned}
    $$

* A linear function of two independent continuous random variables
  $$
  f_{aX}(y) = {1\over |a|}f_X({y \over a})
  $$
  Example: What is the PDF of $2X-Y$? (For independent $X,Y$)
  $$
  \begin{aligned}
  f_{2X-Y}(z) &= \int^\infty_{-\infty} f_{2X}(x) f_{-Y}(z-x)dx\\
  \text{Each part:} &\\
  f_{2X}(x) &= {1\over 2}f_X({x\over 2})\\
  f_{-Y}(y) &= f_Y(-y)\\  
  f_{-Y}(z-x) &= f_Y(x-z)\\
  \text{Therefore:} \\
  \implies f_{2X-Y}(z) &= \int^\infty_{-\infty} {1\over 2} f_X(x/2) f_Y(x-z)dx
  \end{aligned}
  $$
  
* Covariance

  * definition

    $\mathsf{cov}(X,Y) = \mathbf{E}\left[(X-\mathbf{E}[X]) \cdot (Y - \mathbf{E}(Y))\right]$

  * independence

    $\mathsf{cov}(X,Y) = \mathbf{E}\left[(X-\mathbf{E}[X]) \cdot (Y - \mathbf{E}(Y))\right] = \mathbf{E}[(X-\mathbf{E}(X))] \cdot \mathbf{E}[(Y-\mathbf{E}(Y))]$

    Independence $\implies \mathsf{cov}(X,Y) = 0$, converse is not true.

  * mathematical properties

    $\mathsf{cov}(X,X) = \mathbf{E}[(X - \mathbf{E}[X])^2] = \mathsf{Var}(X) = \mathbf{E}[X^2] - (\mathbf{E}[X])^2$

    $\mathsf{cov}(aX + b, Y) = a \cdot \mathsf{cov}(X,Y)$
  
    $\mathsf{cov}(X,Y+Z) = \mathsf{cov}(X,Y) + \mathsf{cov}(X,Z)$
  
    $\mathsf{cov}(X, Y) = \mathbf{E}[XY] - \mathbf{E}[X]\mathbf{E}[Y]$
  
  * variance of a sum of random variables
  
    $\mathsf{Var}(X_1 + X_2) = \mathsf{Var}(X_1) + \mathsf{Var}(X_2) + 2 \mathsf{cov}(X_1, X_2)$
  
    $\mathsf{Var}(X_1 + ... + X_n) = \sum\limits^n_{i=1}\mathsf{Var}(X_i) + \sum\limits_{\{(i,j): i \neq j\}} \mathsf{cov}(X_i ,X_j)$
  
* Correlation

  * definition: correlation is the dimensionless version of covariance; measure of the degree of "association" between $X$ and $Y$.

  * $\rho(X,Y) = \mathbf{E}\left[{(X-\mathbf{E}(X)) \over \sigma_X} \cdot {(Y-\mathbf{E}(Y)) \over \sigma_Y}\right] = {\mathsf{cov}(X,Y) \over\sigma_X \sigma_Y }, \quad -1 \leq \rho \leq 1$

  * Independent $\implies \rho = 0,$ "uncorrelated" (converse is not true).

  * $\vert \rho \vert = 1 \iff (X - \mathbf{E}[X]) = c(Y - \mathbf{E}[Y]) $ (linearly related)

  * $\mathsf{cov}(aX + b, Y) = a \cdot \mathsf{cov}(X,Y) \implies \rho(aX + b,Y) = {a \cdot \mathsf{cov}(X,Y) \over \vert a \vert \sigma_x \sigma_y} = \mathsf{sign}(a) \cdot \rho(X,Y)$

  * Example:

    A real-estate investment company invests \$ 10M in each of $10$ states. At each state $i$, the return on its investment is a random variable $X_i$, with mean $1$ and standard deviation $1.3$ (in millions). Find the variance of the sum of variables.

    $\mathsf{Var}(X_1 + ... + X_{10}) = \sum\limits^{10}_{i=1}\mathsf{Var}(X_i) + \sum\limits_{\{(i,j): i \neq j\}} \mathsf{cov}(X_i ,X_j)$

    If the $X_i$ are uncorrelated, then:

    $\mathsf{Var}(X_1 + ... + X_{10}) = 10 \cdot (1.3)^2 = 16.9$

    $\sigma(X_1 + ... + X_{10}) = 4.1$

    If for $i \neq j$, suppose $\rho(X_i, X_j) = 0.9$

    $\mathsf{Var}(X_1 + ... + X_{10}) = 10 \cdot (1.3)^2  +90 \cdot 1.52 = 154$

    $\sigma(X_1 + ... + X_{10}) = 12.4$.

There are 5 selected exercises and 4 solved problems.

---

## Exercise 1 Discrete convolution

The random variables $X$ and $Y$ are independent and have the PMFs shown in this diagram.

![U6-lec12-ex1](../assets/images/U6-lec12-ex1.png)

What is the probability that $X + Y = 6$?

**Solution**:

We flip the PMF of $Y$ to obtain a PMF on the set $\{-4,-3,-2\}$ . We shift it to the right by $6$ and place it underneath the PMF of $X$. By multiplying the probabilities that are on top of each other in the resulting diagram, we obtain
$$
p_{X+Y}(6)=\frac{1}{6}\cdot \frac{3}{6}+\frac{3}{6}\cdot \frac{2}{6}=\frac{9}{36}=1/4.
$$

## Exercise 2 Continuous convolution

When calculating the convolution of two PDFs, one must be careful to use the appropriate limits of integration. Suppose that $X$ and $Y$ are nonnegative random variables. In particular, $f_X(x)$ is equal to some positive function $$h_X(x)$$ for $x \geq 0$ and is zero for $x < 0$. Similarly, $f_Y(y)$ is equal to some positive function $h_Y(y)$ for $y \geq 0$ , and is zero for $y <0$ . Then, the convolution integral $\int^\infty_{-\infty} f_X(x) f_Y(z-x)$ is of the form
$$
\int _ a^ b h_ X(x) h_ Y(z-x)\, dx,
$$
for suitable choices of $a$ and $b$ determined by $z$. Fix some $z \geq 0$. Find $a$ and $b$.

**Answer**: $a = 0, b = z$. 

**Solution**:

The integrand is equal to $h_X(x)h_Y(z-x)$ only for those choices of $x$ for which the arguments of the functions $h_X$ and $h_Y$ are nonnegative; that is, when $x \geq 0$ and $z -x \geq 0$, which yields $0 \leq x \leq z$. Thus, we should only integrate from $0$ to $z$.

## Exercise 3 Covariance calculation

Suppose that $X,Y,$ and $Z$ are independent random variables with unit variance. Furthermore, $\mathbf{E}[X] = 0$ and $\mathbf{E}(Y) = \mathbf{E}(Z)=  2$. Then, what is $\mathsf{cov}(XY, XZ)$?

**Solution**: 

Because of independence and the zero-mean assumption, it follows that $\mathbf{E}[XY] = \mathbf{E}[X] \cdot \mathbf{E}[Y] = 0$ and similarly $\mathbf{E}[XZ] =0$. Thus,
$$
\textsf{Cov}(XY,XZ)={\bf E}[XYXZ]={\bf E}[X^2 YZ]={\bf E}[X^2]\cdot {\bf E}[Y] \cdot {\bf E}[Z]= \textsf{Var}(X)\cdot {\bf E}[Y] \cdot {\bf E}[Z]=4.
$$

## Exercise 4 Covariance properties

1. Find the value of $a$ in the relation $\mathsf{Cov}(2X,-3Y+2) =a \cdot \mathsf{Cov}(X,Y)$

2. Suppose that $X,Y$, and $Z$ are independent, with a common variance of $5$. Then, find $\mathsf{Cov}(2X+Y,3X - 4Z)$

**Solution**:

1)  We have $\textsf{Cov}(aX+b,Y)=a \cdot \textsf{Cov}(X,Y)$. By symmetry, we also have $\textsf{Cov}(X,aY+b)=a \cdot \textsf{Cov}(X,Y)$. By using this relations,
$$
\textsf{Cov}(2X,-3Y+2)=2\cdot \textsf{Cov}(X,-3Y+2)=2\cdot (-3)\cdot \textsf{Cov}(X,Y)=-6\, \textsf{Cov}(X,Y).
$$
2) Using linearity,
$$
\begin{aligned}
 \textsf{Cov}(2X+Y,3X-4Z)&=\textsf{Cov}(2X+Y,3X)+\textsf{Cov}(2X+Y,-4Z)\\
 &=\textsf{Cov}(2X,3X)+\textsf{Cov}(Y,3X)+\textsf{Cov}(2X,-4Z)+\textsf{Cov}(Y,-4Z)\\
 &=6\, \textsf{Var}(X)+0+0+0=30,
\end{aligned}
$$
where the zeros are obtained because independent random variables have zero covariance.

## Exercise 5 Correlation properties

Let $Z,V,W$ be independent random variables with mean $0$ and variance $1$, and let $X= Z + V$ and $Y = Z +W$. We have found that $\rho(X,Y) = 1/2$.

1. $\rho (X,-Y)=\,?\\\rho (-X,-Y)=\,?$

2. Suppose that $X$ and $Y$ are measured in dollars. Let $X' $ and $Y'$ be the same random variables, but measured in cents, so that $X' = 100X$ and $Y' = 100Y$. Then,

   $\rho (X',Y')=\,?$

3. Suppose now thta $\tilde{X} = 3Z + 3V + 3$ and $\tilde{Y} = -2Z -2W$. Then

   $\rho (\tilde{X},\tilde{Y})=\,?$

4. Suppose now that the variance of $Z$ is replaced by a very large number. Then

   $\rho (X,Y)\,$ is close to $?$

5. Alternatively, suppose that the variance of $Z$ is close to zero. Then,

   $\rho (X,Y)\,$ is close to $?$

**Answer**:

1) $\rho (X,-Y)=\,-1/2\\\rho (-X,-Y)=\,1/2$

2) $\rho (X',Y')=\,1/2$

3) $\rho (\tilde{X},\tilde{Y})=\,-1/2$

4) $\rho (X,Y)\,$ is close to $1$

5) $\rho (X,Y)\,$ is close to $0$

**Solution**:

We saw that a linear transformation $x\mapsto ax+b$ of a random variable does not change the value of the correlation coefficient, except for a possible sign change if the coefficient $a$ is negative. 

For the last two parts, if $Z$ has a very large variance, then teh terms $V$ and $W$ become insignificant, and $\rho(X,Y) \approx \rho(Z,Z) = 1$. And if $Z$ has very small variance, then $X$ and $Y$ are approximately independent, so that $\rho(-X,-Y) \approx 0$.

## Problem 1 The difference of two independent exponential random variables

Romeo and Juliet have a date at a given time, and each, independently, will be late by an amount of time that is exponentially distributed with parameter $\lambda$. What is the PDF of the difference between their times of arrival?

**Solution**:

Let $X,Y$ be exponential random variables with parameter $\lambda$. We are interested in the PDF of difference $Z$ between $X$ and $Y$: $Z = X - Y$, denoted as $f_Z(z)$.

Recall that if $W = X + Y$, we have
$$
f_W(w)=\int^\infty_{-\infty} f_X(x)f_Y(w-x)dx
$$
We can rewrite the $Z$ as $Z = X + (-Y)$, so the PDF is
$$
f_Z(z)=\int^\infty_{-\infty} f_X(x)f_{-Y}(z-x)dx
$$
Since
$$
f_{-Y}(z -x) = f_Y(x-z)
$$
We have
$$
f_Z(z)=\int^\infty_{-\infty} f_X(x)f_{Y}(x-z)dx
$$
Recall the PDF of an exponential random variable $X$ is
$$
f_X(x) = \begin{cases}0, &x<0\\ \lambda e^{-\lambda x}, &x \geq0 \end{cases}
$$
When $z < 0$,
$$
\begin{aligned}
f_Z(z) &=\int^\infty_{0} f_X(x)f_{Y}(x-z)dx\\
&=\int^\infty_{0} \lambda e^{-\lambda x} \lambda e^{-\lambda(x-z)}dx\\
&=\lambda e^{\lambda z} \int^\infty_0 \lambda e^{-2\lambda x}dx\\
&= \lambda e^{\lambda z}\left(\right. -{1\over 2} e^{2 \lambda x} \left|^\infty_0 \right)\\
&= {\lambda \over 2}e^{\lambda z}
\end{aligned}
$$
When $z\geq 0$, since $Z = X - Y$ and $-Z = Y-X$, and $X,Y $ are i.i.d., $Z$ and $-Z$ have the same distribution $Z \stackrel{d}{=}-Z$. This means the distribution of $Z$ must be symmetric around $0$. Thus, 
$$
f_Z(z)= f_Z(-z) ={\lambda \over 2}e^{-\lambda z}
$$
Therefore,
$$
f_Z(z) = \begin{cases}{\lambda \over 2}e^{\lambda z}, &z < 0 \\{\lambda \over 2}e^{-\lambda z}, &z \geq 0 \end{cases}
$$

## Problem 2 The sum of discrete and continuous r.v.'s

Let $X$ be a discrete random variable with PMF $p_X$ and let $Y$ be a continuous random variable, independent from $X$, with PDF $f_Y$. Derive a formula for the PDF of the random variable $X+Y$.

**Solution**:

Let $Z = X+Y$. Using the 2 step CDF method,
$$
\begin{aligned}
F_Z(z) &= \mathbf{P}(Z \leq z)\\
&= \mathbf{P}(X + Y \leq z)
\end{aligned}
$$
Using the Total Probability Theorem, we have
$$
\begin{aligned}
F_Z(z) &= \sum_x p_X(x)\mathbf{P}(x + Y \leq z)\\
&= \sum_x p_X(x) \mathbf{P}(Y \leq z -x)\\
&= \sum_x p_X(x) \mathbf{P}(Y \leq z -x)\\
&= \sum_x p_X(x) F_Y(z-x)
\end{aligned}
$$
Differentiating both sides with respect to $z$, we obtain
$$
f_Z(z) = {d\over dz} F_Z(z) = \sum_x p_X(x) f_Y(z-x)
$$

## Problem 3 Convolution calculations

1. Let the discrete random variable $X$ be uniform on $\{0,1,2\}$ and let the discrete random variable $Y$ be uniform on ${3,4}$. Assume that $X$ and $Y$ are independent. Find the PMF of $X+Y$ using **convolution**. The form of the PMF is
   $$
   p_{X+Y}(z) = \left\{  \begin{array}{ll} a, & z = 3, \\ b, & z = 4, \\ c, & z = 5, \\ d, & z = 6, \\ 0, &  \text{otherwise.} \end{array} \right.
   $$

2. Let the random variable $X$ be uniform on $[0,2]$ and the random variable $Y$ be uniform on $[3,4]$ . (Note that in this case, $X$ and $Y$ are continuous random variables.) Assume that $X$ and $Y$ are independent. Let $Z = X+Y$. Find the PDF of $Z$ using convolution. Find $a,b,c,d,e$ in the graph of the PDF

   ![images_chap4_convolution1](../assets/images/images_chap4_convolution1.jpg)

**Solution:**

1. $Z = X+Y \in \{3,4,5,6\}$, by computing the probability of each $Z$, we have the PMF
   $$
   p_{X+Y}(z) = \left\{  \begin{array}{ll} 1/6, &  z\in \{ 3,6\}  \\ 1/3, &  z\in \{ 4,5\} \\ 0, &  \text{otherwise.} \end{array} \right.
   $$

2. The answer is easiest to find graphically, by sliding a rectangle of width 1 along a rectangle of width 2, and is:
   $$
   f_{X+Y}(z) = \left\{  \begin{array}{lcl} \frac{z-3}{2}, & &  3 \leq z < 4, \\ \frac{1}{2}, & &  4 \leq z < 5, \\ \frac{6-z}{2}, & &  5 \leq z \leq 6, \\ 0, & &  \text{otherwise.} \end{array}\right.
   $$
    A more formal approach involves the convolution formula, but requires careful thought in order to identify the appropriate limits of integration. In particular, if $3 \leq z \leq 6 $, we have
   $$
   \begin{aligned}
   f_{X+Y}(z) &= \int _{-\infty }^{\infty } f_ X(x) f_ Y(z-x) \,  dx\\
   &= \int _{\max (0,z-4)}^{\min (2,z-3)} \frac{1}{2} \,  dx\\
   &=  (\min (2,z-3) - \max (0,z-4))/2
   \end{aligned}
   $$
   which actually agrees with the answer obtained through the graphical method.

## Problem 4 Covariance of the multinomial

Consider $n$ independent rolls of a $k$-sided fair die with $k \geq 2$: the sides of the die are labelled $1,2,...,k$ and each side has probability $1/k$. Let the random variable $X_i$ denote the number of rolls that result in side $i$. Thus, the random vector $(X_1, ...,X_k)$ has a multinomial distribution.

1. Are $X_1$ and $X_2$ correlated?

2. Find the covariance, $\mathsf{Cov}(X_1, X_2)$, of $X_1$ and $X_2$. 

3. Suppose now that the die is biased, with a probability $p_i \neq 0$ that the result of any given die roll is $i$, for $i=1,2,...,k.$ We still consider $n$ independent rolls of this biased die and define $X_i$ to be the number of rolls that result in side $i$.

   Find $\mathsf{Cov}(X_1, X_2)$ for this cases of a biased die.

**Solution:**

1. Negatively correlated. There is a fixed number $n$ of rolls of the die. A large number of rolls that result in $1$ leaves fewer remaining rolls that could result in $2$.

2. Let $A_t$ (resp. $B_t$) be a Bernoulli random variable that is equal to $1$ if and only if the $t$th roll resulted in a $1$ (resp. 2). Note that $X_1=\sum _{t=1}^ n A_ t$ and $X_2=\sum _{t=1}^ n B_ t$, and so
   $$
   {\bf E}[X_1]={\bf E}[X_2] = {\bf E}\left[\sum _{t=1}^ n A_ t\right] = n{\bf E}[A_1] = \frac{n}{k}.
   $$
   Since a single roll of the die cannot result in both a $1$ and a $2$, at least one of $A_t$ and $B_t$ must equal $0$. Thus, $\mathbf{E}[A_tB_t]=0$. Furthermore, since different rolls are independent, $A_t$ and $B_s$ are independent when $t \neq s$. Therefore,
   $$
   {\bf E}[A_ t B_ s] = {\bf E}[A_ t]{\bf E}[B_ s] = \frac{1}{k} \cdot \frac{1}{k} = \frac{1}{k^2} \qquad \text{for} \quad t\neq s,
   $$
   and so
   $$
   \begin{aligned}
   \displaystyle  {\bf E}[X_1 X_2] &={\bf E}\left[(A_1+\cdots +A_ n)(B_1+\cdots +B_ n)\right]\\
   &= {\bf E}\left[\sum _{t=s} A_ tB_ t + \sum _{t\neq s}A_ tB_ s\right]\\
   &= n\cdot 0 + n(n-1)\cdot {\bf E}[A_1B_2]\\
   &= n(n-1)\cdot \frac{1}{k^2}.
   \end{aligned}
   $$
   Thus,
   $$
   \begin{aligned}
    {\rm cov}(X_1,X_2) &= {\bf E}[X_1 X_2]-{\bf E}[X_1]{\bf E}[X_2]\\
    &= n(n-1)\cdot \frac{1}{k^2} - \frac{n}{k}\cdot \frac{n}{k}\\
    &=  -\frac{n}{k^2}.
   \end{aligned}
   $$
   The covariance of $X_1$ and $X_2$ is negative as expected.

3. We follow the same reasoning as in (2). Let $A_t$ (resp. $B_t$) be a Bernoulli random variable that is equal to $1$ if an only if the $t$th roll resulted in a $1$ (resp. $2$). As in (2), a single roll of the die result in both a $1$ and a $2$, so $\mathbf{E}[A_tB_t]=0$. Different rolls of the die are independent, and so ${\bf E}[A_ t B_ s] = {\bf E}[A_ t]{\bf E}[B_ s] = p_1\cdot p_2$, for $t \neq s $. Thus,
   $$
   \begin{aligned}
   {\bf E}[X_1 X_2] &= {\bf E}\left[(A_1+\cdots +A_ n)(B_1+\cdots +B_ n)\right]\\
   &= {\bf E}\left[\sum _{t=s} A_ tB_ t + \sum _{t\neq s}A_ tB_ s\right]\\
   &= n\cdot 0 + n(n-1)\cdot {\bf E}[A_1B_2]\\
   &= n(n-1)p_1 p_2.
   \end{aligned}
   $$
   Note that $X_1=\sum _{t=1}^ n A_ t$ and $X_2=\sum _{t=1}^ n B_ t$, and so ${\bf E}[X_1]={\bf E}\left[\sum _{t=1}^ n A_ t\right] = n{\bf E}[A_1] = np_1$. Similarly, ${\bf E}[X_2]=np_2$.

   Therefore,
   $$
   \begin{aligned}
   {\rm cov}(X_1,X_2) &= {\bf E}[X_1 X_2]-{\bf E}[X_1]E[X_2]\\
   &= n(n-1)p_1p_2 - (np_1)(np_2)\\
   &= -np_1p_2.
   \end{aligned}
   $$
   The covariance of $X_1$ and $X_2$ is again negative.



# Lecture 5. Probability mass functions and expectations

* Random variables: 

  * A random variable $X$ associates a value $x$ to every possible outcome. Mathematically, it is a function from the sample space $\Omega$ to the real numbers.
  * Discrete: take values in finite or countable set.

* Probability mass function (PMF):

  * PMF is the probability distribution of $X$.
  * $p_X(x) = \mathbf{P}(X = x) = \mathbf{P}(\{w \in \Omega~s.t. X(w) = x\})$.
  * Properties: $p_X(x) \geq 0, \quad \sum_x p_X(x) = 1$.

* Random variable examples:

  * Bernoulli with parameter $p \in [0,1]$

    $x = \begin{cases} 1, &\quad w.p.\quad p \\0 &\quad w.p.\quad 1-p \end{cases}$

  * Uniform with parameters integer $a, b$

    $p_X(x_i)  =\frac{1}{b-a+1}$

  * Binomial with parameters $n, p$

    $p_X(k) = {n \choose k} p^k (1-p)^{n-k}, \quad \text{for }k = 0,1,...,n$

  * Geometric with parameter $p$

    $p_X(k) = \mathbf{P}(X = k) = (1-p)^{k-1}p, \quad k=1,2,3$

* Expectation (mean) and its properties:

  * The expected value rule

    $\mathbb{E}[X] = \sum_xx p_X(x)$.

    Caution: If we have an infinite sum, it needs to be well-defined. We assume $\sum_x |x|p_X(x) < \infty$.

  * Expectation of Bernoulli r.v.: $\mathbb{E}[X] = p$

  * Expectation of Uniform r.v.: $\mathbb{E}[X] = 0 \cdot \frac{1}{n+1}+ 1\cdot \frac{1}{n+1}+ ... + n\cdot \frac{1}{n+1}= \frac{1}{n+1}(0+1+...+n) = \frac{1}{n+1} \cdot \frac{n(n+1)}{2} = \frac{n}{2} \quad \text{where }n = b-a$

  * Let $X$ be a r.v. and let $Y = g(X)$. 

    $\mathbb{E}[g(X)] = E[Y] = \sum_y y p_Y(y) = \sum_x g(x) p_X(x)$.

    e.g. If $g(x)= x^2, \mathbb{E}[X^2] = \sum_x x^2 p_X(x)$

    Caution: $\mathbb{E}[g(X)] \neq g(\mathbb{E}[X]), \mathbb{E}[X^2] \neq (\mathbb{E}[X])^2$.

  * Linearity of expectation

    $\mathbb{E}[aX + b] = a\mathbb{E}[X] + b$
  
* The inclusion-exclusion formula

  * Let $A_1,A_2,\ldots ,A_ n$ be events.
    $$
    \mathbf{P}\left(\bigcup _{k=1}^ nA_ k\right) = \sum _{i}\mathbf{P}(A_ i)-{\sum _{i_1<i_2} \mathbf{P}(A_{i_1}\cap A_{i_2})}+{\sum _{i_1<i_2<i_3}\mathbf{P}(A_{i_1}\cap A_{i_2}\cap A_{i_3})} -\cdots +(-1)^{n-1}\mathbf{P}\left(\bigcap _{k=1}^ nA_ k\right).
    $$
    Here a sum such as $\displaystyle \sum _{i_1<i_2<i_3}$ stands for a triple sum, over all triples $(i_1, i_2, i_3)$ that satisfy $i_1 < i_2 < i_3$.

  * Example:

    $P(A_1 \cup A_2) = P(A_1) + P(A_2) - P(A_1 \cap A_2)$

    $P(A_1 \cup A_2\cup A_3) = P(A_1) + P(A_2)+ P(A_3) - (P(A_1 \cap A_2)+P(A_1 \cap A_3)+P(A_2 \cap A_3))\\ + P(A_1 \cap A_2 \cap A_3)$

There is 1 selected exercise and 3 solved problems.

---

## Exercise 1 The binomial PMF

You roll a fair six-sided die (all 6 of the possible results of a die roll are equally likely) 5 times, independently. Let X be the number of times that the roll results in 2 or 3. Find the numerical values of the following.

**Answer**: 

a) $p_ X(2.5)=\, 0$

b) $p_ X(1)=\,80/243$

**Solution**: 

a) A value of $2.5$ is not possible for $X$ since the number of rolls must be an integer.

b) For each die roll, there is a probability $2/6=1/3$ of obtaining a $2$ or a $3$. Hence, the random variable $X$ is binomial with parameters $n=5$ and $p=1/3$, so that $p_X(1)={5 \choose 1}⋅(1/3)⋅(2/3)^4≈0.32922$.

## Problem 1 Sampling people on buses

Four buses carrying $148$ job-seeking MIT students arrive at a job convention. The buses carry $40, 33, 25,$ and $50$ students, respectively. One of the students is randomly selected. Let $X$ denote the number of students that were on the bus carrying this randomly selected student. One of the $4$ bus drivers is also randomly selected. Let $Y$ denote the number of students on his bus.

1. Which of $\mathbb{E}[X]$ or $\mathbb{E}[Y]$ do you think is larger? Give your reasoning in words.
2. Compute $\mathbb{E}[X]$ and $\mathbb{E}[Y]$.

**Answer**:

1. We expect $\mathbb{E}[X]$ to be higher than $\mathbb{E}[Y]$ since if we choose the student, we are more likely to pick a bus with more students.

2. The PMF of $X$ is 
   $$
   p_X(x) = \begin{cases}40/148, &x=40 \\33/148, &x=33 \\ 25/148, &x=25\\ 50/148, &x=50\\ 0, &\text{otherwise} \end{cases}
   $$
   and $\mathbb{E}[X] = 40 \times {40 \over 138} + 33 \times {33 \over 148} + 25 \times { 25 \over 148} + 50 \times { 50 \over 148} = 39.28$

   The PMF of $Y$ is 
   $$
   p_Y(y) = \begin{cases}1/4 ,&y \in \{40,33,25,50\},\\0, & \text{otherwise} \end{cases}
   $$
   and $\mathbb{E}(Y) = 40 \times {1 \over 4} + 33 \times {1 \over 4} + 25 \times {1 \over 4} + 50 \times {1 \over 4} = 37$.

## Problem 2.1 From tail probabilities to expectations

Let $X$ be a random variable that takes nonnegative integer values. Show that 
$$
{\bf E}[X]=\sum _{k=1}^{\infty }\mathbf{P}(X\geq k).
$$
**Answer**: 

Recall the expectation formula:
$$
\mathbf{E}[X] = \sum_{k}k p_X(k)
$$
Note that 
$$
\mathbf{P}(X \geq k) = \sum^{\infty}_{i = k}p_X(i)
$$
and proceed as follows
$$
\sum^{\infty}_{k=1} \mathbf{P}(X \geq k) = \sum^{\infty}_{k=1}\sum^{\infty}_{i=k} p_X(i) = \sum^{\infty}_{i=1}\sum^i_{k=1} p_X (i) = \sum^{\infty}_{i=1} i p_X(i) = \mathbf{E}[X]
$$
This can be understood from the diagram below

![lec5_prob2](../assets/images/lec5_prob2.png)

## Problem 2.2 From tail probabilities to expectations

Find the expectation of a random variable $Y$ whose PMF is defined as follows:
$$
p_ Y(y)= \frac{1}{b-a+1}, \qquad y=a,a+1,\ldots ,b
$$
where $a$ and $b$ are nonnegative integers with $b > a$. Note that for $y = a,a+1,...,b$, $p_Y(y)$ does not depend explicitly on $y$ since it is a uniform PMF.

**Answer**: 

First compute
$$
\mathbf{P}(Y \geq k) = \begin{cases} 1, & k \leq a \\ \frac{b-k+1}{b-a+1}, & a+1 \leq k \leq b \\ 0, & k \geq b+1 \end{cases}
$$
Hence,
$$
\begin{aligned}
{\bf E}[Y]=\sum _{k=1}^{\infty }\mathbf{P}(Y\geq k) &= \sum^a_{k=1} a + \sum^b_{k = a+1} \frac{b-k+1}{b-a+1}\\
&=a + \frac{1}{b-a+1}\sum^{b-a}_{k=1}k \\
&= a+ \frac{1}{b-a+1} \frac{(b-a+1)(b-a)}{2}\\
&=a + {b-a \over 2} \\
&= {b+a \over 2}
\end{aligned}
$$

## Problem 3 True or False

Let $X $ and $Y$ be two binomial random variables

a. If $X$ and $Y$ are independent, then $X+Y$ is also a binomial random variable. [FALSE: parameters may be different.]

b. If $X$ and $Y$ have the same parameters, $n$ and $p$, then $X+Y$ is a binomial random variable. [FALSE: they may be dependent.]

c. If $X$ and $Y$ have the same parameters, $p$, and are independent, then $X+Y$ is a binomial random variable. [TRUE]



# Lecture 16. Least mean squares (LMS) estimation

* Minimize mean squared error (MSE), $\mathbb{E}\left[(\Theta - \hat{\theta})^2 \right]: \quad \hat{\theta} = \mathbb{E}[\Theta]$.
  * Optimal mean squared error: $\mathbb{E}[(\Theta - \mathbb{E}[\Theta])^2] = \mathsf{Var}(\Theta)$.
  
* Minimize (conditional) mean squared error, $\mathbb{E}\left[(\Theta - \hat{\theta})^2 | X = x\right]: \quad \hat{\theta} = \mathbb{E}[\Theta | X=x]$.

* LMS estimation of $\Theta$ based on $X$​.

  * Unknown $\Theta$​​; prior $p_{\Theta}(\theta)$​.

    * Interested in a point estimate $\hat{\theta}$​.

  * Observation $X$; model $p_{X|\Theta}(x|\theta)$.

    * LMS estimate: $\hat{\theta} = \mathbb{E}[\Theta|X=x]$​.
    * LMS estimator: $\hat{\Theta} = \mathbb{E}[\Theta | X]$​.

  * $\mathbb{E}[\Theta]$​ minimizes $\mathbb{E}[({\Theta} - \hat{\theta})^2]$:

    $\mathbb{E}[(\Theta - \mathbb{E}[\Theta])^2] \leq \mathbb{E}[(\Theta - c)^2]$​, for all $c$.

  * $\mathbb{E}[\Theta| X=x]$​​​ minimizes $\mathbb{E}[({\Theta} - \hat{\theta})^2| X=x]$:

    $\mathbb{E}[(\Theta - \mathbb{E}[\Theta|X=x])^2|X=x] \leq \mathbb{E}[(\Theta - g(x))^2|X=x]$​​​, for all $x$​​​.

    $\mathbb{E}[(\Theta - \mathbb{E}[\Theta|X])^2|X] \leq \mathbb{E}[(\Theta - g(X))^2|X]$​​​

    $\mathbb{E}[(\Theta - \mathbb{E}[\Theta|X])^2] \leq \mathbb{E}[(\Theta - g(X))^2]$​​

    $\widehat{\Theta}_{LMS} = \mathbb{E}[\Theta|X]$​​ minimizes $\mathbb{E}[(\Theta - g(X))^2]$​​, over all estimators $\widehat{\Theta} = g(X)$​.

  * Expected performance

    * MSE = $\mathbb{E}[(\Theta - \mathbb{E}[\Theta | X=x])^2|X=x]= \text{var}(\Theta|X=x)$​​.
    * MSE = $\mathbb{E}[(\Theta - \mathbb{E}[\Theta|X])^2] = \mathbb{E}[\text{var}(\Theta | X)]$​​.

* Some challenges in LMS estimation

  * Full correct model, $f_{X|\Theta}(x | \theta)$​, may not be available.
  * Can be hard to compute / implement / analyze.

* Properties of the estimation error in LMS estimation

  * Estimator: $\widehat{\Theta} = \mathbb{E}[\Theta | X]$​; Error: $\tilde{\Theta} = \widehat{\Theta}-\Theta$​​.
  * $\mathbb{E}[\tilde{\Theta}|X=x] =\mathbb{E}[\widehat{\Theta} - \Theta|X=x] = \hat{\Theta} - \mathbb{E}[\Theta|X=x] = 0$​​.
  * $\text{cov}(\tilde{\Theta}, \widehat{\Theta})=\mathbb{E}[\tilde{\Theta}\widehat{\Theta}] - \mathbb{E}[\tilde{\Theta}]\mathbb{E}[\hat{\Theta}] = 0$​.
  * $\text{var}(\Theta)=\text{var}(\widehat{\Theta}) + \text{var}(\tilde{\Theta})$​.

There are 2 selected exercises and 3 solved problems.

---

## Exercise 1 LMS estimation error

Let $\Theta$​​ be the bias of a coin, i.e., the probability of Heads at each toss. We assume that $\Theta$​​ is uniformly distributed on $[0,1]$​​. Let $K$​​ be the number of Heads in $9$​​ independent tosses. We have seen that the LMS estimate of $K$​​ is $\mathbb{E}[K|\Theta = \theta] = n\theta$​​.

1. Find the **conditional mean squared error** ${\bf E}\big [\big (K-{\bf E}[K\mid \Theta =\theta ]\big )^2\mid \Theta =\theta \big ]$.
2. Find the **overall mean squared error** of this estimation procedure.

**Solution:**

1. **This is the variance of the conditional distribution of $K$​.** Since the conditional distribution is binomial with parameters $n=9$​ and $\theta = 1/3$​, the conditional variance is $9(1/3)(2/3)=2$​.

2. **This is the averages of the conditional variance, averaged over all possible values of the observation $\Theta$​​,** which has a uniform distribution.
   $$
   \begin{aligned}
   \int _0^1 f_{\Theta }(\theta ) \textsf{Var}(K\mid \Theta =\theta )\,  d\theta &=\int _0^1 9\theta (1-\theta )\,  d\theta\\
   &= \left(9\frac{1}{2}\theta ^2-9\frac{\theta ^3}{3}\right)\Big|_0^1\\
   &= 4.5-3\\
   &= 1.5
   \end{aligned}
   $$

## Exercise 2 Mean squared error

We assume that the random variables $\Theta$ and $X$ are described by the joint PDF which is uniform on the triangular set defined by the constraints $0 \leq x \leq 1, 0 \leq \theta \leq x$.

1. Find an expression for the conditional mean squared error of the LMS estimator given that $X=x$, valid for $x \in [0,1]$. 
2. Find the (unconditional) mean squared error of the LMS estimator.

**Solution:**

1. Since the conditional PDF of $\Theta$ is uniform on the range $[0,x]$. Hence, the conditional variance is $x^2/12$.

2. This is given by the integral of the conditional variance, weighted by the PDF of $X$. 

   The PDF of $X$ is found using the formula for going from the joint to the marginal, and is $f_X(x)=2x$, for $x \in [0,1]$. (We know the area of the triangle is $1/2$ and it's uniformly distribution, so the PDF if $f_X(x) = 2x.$)

   Thus, the mean squared error is
   $$
   \int _0^1 \frac{x^2}{12} \cdot 2x\,  dx =\frac{1}{6}\int _0^1 x^3\,  dx =\frac{1}{24}.
   $$

## Problem 1 Determining the type of a lightbulb

The lifetime of a type-A bulb is exponentially distributed with parameter $\lambda$. The lifetime of a type-B bulb is exponentially distributed with parameter $\mu$, where $\mu > \lambda > 0$. You have a box full of lightbulbs of the same type, and you would like to know whether they are of type A or B. Assume an a priori probability of $1/4$ That the box contains type-B lightbulbs.

1. Assume that $\mu \geq 3\lambda$. You observe the value $t_1$ of the lifetime, $T_1$, of a lightbulb. A MAP decision rule decides that the lightbulb is of type A if and only if $t_1 \geq \alpha$. Find $\alpha$.
2. Assume again that $\mu \geq 3\lambda$. What is the probability of error of the MAP decision rule?
3. Assume that $\lambda =3$ and $\mu = 4$. Find the LMS estimate of $T_2$, the lifetime of another lightbulb from the same box, based on observing $T_1=2$. Assume that conditioned on the bulb type, bulb lifetimes are independent. 

**Solution:**

1. Let $A$ and $B$ be the events that the box contains lightbulbs of type A and type B, respectively. A MAP rule decides in favor of type A if and only if
   $$
   \mathbf{P}(A\mid T_1=t_1) \geq \mathbf{P}(B\mid T_1=t_1)\\
   \frac{f_{T_1\mid A}(t_1)\mathbf{P}(A)}{f_{T_1}(t_1)} \geq \displaystyle \frac{f_{T_1\mid B}(t_1)\mathbf{P}(B)}{f_{T_1}(t_1)}
   $$
   Equivalently, we decide that the bulb is of type A if and only if 
   $$
   \begin{aligned}
   f_{T_1\mid A}(t_1)\mathbf{P}(A) &\geq f_{T_1\mid B}(t_1)\mathbf{P}(B)\\
   \lambda e^{-\lambda t_1}\frac{3}{4} &\geq \mu e^{-\mu t_1}\frac{1}{4}\\
   \frac{\lambda }{\mu }e^{(\mu -\lambda )t_1} &\geq {1\over 3}\\
   (\mu -\lambda )t_1 &\geq \ln \left(\frac{\mu }{3\lambda }\right).
   \end{aligned}
   $$
   Thus, since $\mu - \lambda > 0$, a MAP rule decides in favor of type A if and only if $t_1 \geq \ln \left(\frac{\mu }{3\lambda }\right)\cdot \frac{1}{\mu -\lambda }$. Hence, we deduce that
   $$
   \alpha = \frac{1}{\mu -\lambda }\ln \left(\frac{\mu }{3\lambda }\right).
   $$

2. Let events $A$ and $B$ be defined as in part (1). Let $\hat{A}$ be the event that the MAP rule decides in favor of type A, and let $\hat{B}$ be the event that the MAP rule decides in favor of type B. An error occurs whenever the decision is different from the actual type of the bulb. Thus,
   $$
   \begin{aligned}
   \mathbf{P}(\text{error}) &= \mathbf{P}(\hat{A}\cap B)+\mathbf{P}(A\cap \hat{B})\\
   &= \mathbf{P}(\hat{A}\mid B)\cdot \mathbf{P}(B) + \mathbf{P}(\hat{B}\mid A)\cdot \mathbf{P}(A)\\
   &= \mathbf{P}(T_1\geq \alpha \mid B)\cdot \frac{1}{4}+\mathbf{P}(T_1<\alpha \mid A)\cdot \frac{3}{4}\\
   &= e^{-\mu \alpha }\cdot \frac{1}{4}+(1-e^{-\lambda \alpha })\cdot \frac{3}{4}.
   \end{aligned}
   $$

3. The LMS estimate of $T_2$ based on observing $T_1= t_1$ is
   $$
   \begin{aligned}
   {\bf E}[T_2\mid T_1=t_1] &= {\bf E}[T_2\mid T_1=t_1,A]\cdot \mathbf{P}(A\mid T_1=t_1) + {\bf E}[T_2\mid T_1=t_1,B]\cdot \mathbf{P}(B\mid T_1=t_1)\\
   &= {\bf E}[T_2\mid A]\cdot \mathbf{P}(A\mid T_1=t_1) + {\bf E}[T_2\mid B]\cdot \mathbf{P}(B\mid T_1=t_1)\\
   &= \frac{1}{\lambda }\cdot \left(\frac{f_{T_1\mid A}(t_1)\cdot \mathbf{P}(A)}{f_{T_1}(t_1)}\right) + \frac{1}{\mu }\cdot \left(\frac{f_{T_1\mid B}(t_1)\cdot \mathbf{P}(B)}{f_{T_1}(t_1)}\right)\\
   &= \frac{\frac{1}{\lambda }\frac{3}{4}\lambda e^{-\lambda t_1}+\frac{1}{\mu }\frac{1}{4}\mu e^{-\mu t_1}}{\frac{3}{4}\lambda e^{-\lambda t_1}+\frac{1}{4}\mu e^{-\mu t_1}}.
   \end{aligned}
   $$
   Inserting the values $\lambda=3, \mu = 4$, and $t_1=2$, we obtain ${\bf E}[T_2\mid T_1=2]=0.328$.

## Problem 2 Estimating the parameter of a geometric r.v.

We have $k$ coins. The probability of Heads is the same for each coin and is the realized value $q$ of a random variable $Q$ that is uniformly distributed on $[0,1]$. We assume that conditioned on $Q=q$, all coin tosses are independent. Let $T_i$ be the number of tosses of the $i^{th}$ coin until that coin results in Heads for the first time, for $o=1,2,...,k$. ($T_i$ includes the toss that results in the first Heads.)

Hint: For any non-negative integers $k$ and $m$,
$$
\int _0^1 q^ k(1-q)^ m dq = \frac{k! m!}{(k+m+1)!}.
$$

1. Find the PMF of $T_1$, denoted by $p_{T_1}(t)$, for $t=1,2,...$.
2. Find the least mean squares (LMS) estimate of $Q$ based on the observed value, $t$, of $T_1$, denoted by ${\bf E}[Q \mid T_1 = t]$.
3. We flip each of the $k$ coins until they result in Heads for the first time. Compute the maximum a posteriori (MAP) estimate $\hat{q}$ of $Q$ given the number of tosses needed, $T_1=t_1, \ldots , T_ k=t_ k$, for each coin. Choose the correct expression for $\hat{q}$.

**Solution:**

1. The conditional probability of $T_1$ given $q$ is 
   $$
   p_{T_1 \mid Q}(t\mid q)=(1-q)^{t-1}q
   $$
   Using the total probability theorem, we have
   $$
   p_{T_1}(t)=\int ^1_0 p_{T_1\mid Q}(t\mid q)f_ Q(q)\, dq=\int ^1_0 (1-q)^{t-1} q\, dq=\frac{1}{(t+1)t}, \,  \text{ for }t=1,2,\ldots.
   $$

2. The LMS estimate is
   $$
   \begin{aligned}
   {\bf E}[Q\mid T_1=t] &=  \int ^1_0 f_{Q\mid T_1}(q\mid t)q\, dq\\
   &= \int ^1_0 \frac{p_{T_1\mid Q}(t\mid q)f_ Q(q)}{p_{T_1}(t)}q\, dq\\
   &= \int ^1_0 t(t+1)q(1-q)^{t-1}q\, dq\\
   &= \int ^1_0 t(t+1)q^2(1-q)^{t-1}dq\\
   &= t(t+1)\frac{2(t-1)!}{(t+2)!}\\
   &= {2\over t+2}
   \end{aligned}
   $$

3. We compute the posterior distribution of $Q$ given that $T_1=t_1, \ldots , T_ k=t_ k$.
   $$
   \begin{aligned}
   f_{Q|T_1, \ldots , T_ k}(q\mid t_1, \ldots , t_ k) &= \frac{f_ Q(q)\prod ^ k_{i=1} p_{T_ i|Q}(t_ i\mid q)}{\int ^1_0 f_ Q(q)\prod ^ k_{i=1} p_{T_ i|Q}(t_ i\mid q)dq}\\
   & =\frac{q^ k(1-q)^{\sum ^ k_{i=1} t_ i -k}}{c}
   \end{aligned}
   $$
   where $c$ is a normalizing constant that does not depend on $q$.

   To maximize the above expression, we set its derivative with respect to $q$ to zero and obtain
   $$
   kq^{k-1}(1-q)^{\sum ^ k_{i=1} t_ i -k}-\left(\sum ^ k_{i=1} t_ i -k\right)q^{k}(1-q)^{\sum ^ k_{i=1} t_ i -k-1}=0,\\
   \implies k(1-q)-\left(\sum ^ k_{i=1} t_ i -k\right)q=0,
   $$
   which yields the MAP estimate
   $$
   \hat q=\frac{k}{\sum ^ k_{i=1}t_ i}.
   $$

## Problem 3 Inference example

Continuous random variables $X$ and $Y$ have a joint PDF given by
$$
f_{X,Y}(x,y) = \begin{cases} 2/3, &  \text{if } (x,y) \text{ belongs to the closed shaded region,} \\ 0, &  \text{otherwise.} \end{cases}
$$
![images_8_3_lms_2_02](/Users/dizhen/Git/Notebooks/probability-and-statistics-notes/docs/assets/images/images_8_3_lms_2_02.png)

We want to estimate $Y$ based on $X$.

1. Find the LMS estimator $g(X)$ of $Y$.
2. Calculate the conditional mean squared error ${\bf E}\left[\left(Y-g(X)\right)^2\mid X=x\right]$.
3. Calculate the mean squared error ${\bf E}\left[\left(Y-g(X)\right)^2\right]$. Is it the same as ${\bf E}\left[\textsf{Var}(Y\mid X)\right]$?
4. Find $L(X)$, the linear LMS estimator of $Y$ based on $X$.
5. How do you expect the mean squared error of $L(X)$ to compare to that of $g(X)$?
6. What problem do you expect to encounter, if any, if you try to find the MAP estimator for $Y$ based on observations of $X$.

**Solution:**

1. We can observe the expectation of $Y$ given $X$ from the graph above, so the LMS estimator is
   $$
   g(X) = \mathbf{E}[Y|X] = \begin{cases}{1\over 2}X, & 0\leq X \leq 1 \\ X - {1\over 2}, & 1 \leq X \leq 2\\ \text{undefined}, & \text{otherwise} \end{cases}
   $$

2. With a fixed $x$ we can observe the conditional PDF of $Y$ from the graph above. If $x\in[0,1]$, the conditional PDF of $Y$ is uniform over the interval $[0,x]$, and
   $$
   {\bf E}\left[\left(Y-g(X)\right)^2\mid X=x\right] = {x^2 \over 12}.
   $$
   Similarly, if $x\in [1,2]$, the conditional PDF of $Y$ is uniform over the interval $[1-x,x]$, and
   $$
   {\bf E}\left[\left(Y-g(X)\right)^2\mid X=x\right] = {1 \over 12}.
   $$

3. The expectations ${\bf E}\left[\left(Y-g(X)\right)^2\right]$ and ${\bf E}\left[\textsf{Var}(Y\mid X)\right]$ are equal because by the Law of Iterated Expectations,
   $$
   {\bf E}\left[\left(Y-g(X)\right)^2\right] = {\bf E}\left[{\bf E}\left[ \left(Y-g(X)\right)^2|X\right]\right] = {\bf E}\left[\textsf{Var}(Y\mid X)\right]
   $$
   Recall from (2) that
   $$
   \textsf{Var}(Y|X=x) = \begin{cases}{x^2 \over 12} & 0 \leq x \leq 1 \\ {1\over 12 }& 1 \leq x \leq 2 \end{cases}
   $$
   It follows that
   $$
   \mathbf{E}[\textsf{Var}(Y|X)] = \int_x \textsf{Var}(Y|X=x) f_X(x) dx = \int^1_0 {x^2 \over 12} \cdot {2\over 3}x \ dx+ \int^2_1 {1\over 12}\cdot {2\over 3} dx ={5\over 72}
   $$
   Note that
   $$
   f_X(x) = \begin{cases}2x/3, & 0\leq x < 1\\ 2/3, & 1\leq x \leq 2 \end{cases}
   $$

4. The linear LMS estimator is
   $$
   L(X)  =\mathbf{E}[Y] + {\text{cov}(X,Y) \over \text{var}(X)} [X-\mathbf{E}[X]].
   $$
   In order to calculate $\text{var}(X)$ we first calculate $\mathbf{E}[X^2]$ and $\mathbf{E}[X]^2$.
   $$
   \begin{aligned}
   \mathbf{E}[X^2] &= \int^1_0 x^3 {2\over 3} dx + \int^2_1 x^2 {2\over 3} dx\\
   &= {31 \over 18}\\
   \mathbf{E}[X] &= \int^1_0 x^2 {2\over 3}dx + \int^2_1 x{2\over 3} dx\\
   &= {11\over 9}
   \end{aligned}
   $$
   Thus, $\textsf{Var}(X) = \mathbf{E}[X^2] - \mathbf{E}[X]^2 = {37\over 162}$. Also,
   $$
   \mathbf{E}[Y] = \int^1_0 \int^x_0 {2\over 3}\ y\ dy\ dx + \int^2_1 \int^x_{x-1} {2\over 3}y \ dy \ dx = {1\over 9} + {2\over 3} = {7 \over 9}
   $$
   To determine $\textsf{Cov}(X,Y)$ we need to evaluate $\mathbf{E}[XY]$:
   $$
   \begin{aligned}
   \mathbf{E}[XY] &= \int_x \int_y xyf_{X,Y}(x,y) \ dy \ dx\\
   &= \int^1_0 \int^x_0 yx {2\over 3}\ dy \ dx+ \int^2_1\int^x_{x-1} yx {2\over 3}\ dy\ dx\\
   & ={41 \over 36}
   \end{aligned}
   $$
   and so $\textsf{Cov}(X,Y) = \mathbf{E}[XY] - \mathbf{E}[X]\mathbf{E}[Y] = {61 \over 324}$. Therefore,
   $$
   L(X) = {7 \over 9} + {61 \over 74}(X- {11 \over 9})
   $$

5. The LMS estimator is the one that minimizes mean squared error (among all estimators of $Y$ based on $X$). The linear LMS estimator, therefore, cannot perform better than the LMS estimator, i.e., we expect $\mathbf{E}[(Y-L(X))^2] \geq \mathbf{E}[(Y-g(X))^2]$. In fact,
   $$
   \begin{aligned}
   \mathbf{E}[(Y-L(X))^2] &= \sigma_Y^2(1 - \rho)\\
   &= \sigma_Y^2\left(1 - {\textsf{Cov}(X,Y)^2 \over \sigma_X^2 \sigma_Y^2}\right)\\
   &= {37\over 162} \left(1 - \left({61\over 74}\right)^2 \right)\\
   &= 0.073\\
   & \geq {5 \over 72}
   \end{aligned}
   $$

6. For a single observation $x$ of $X$, the MAP estimate is not unique since all possible values of $Y$ for this $x$ are equally likely. Therefore, the MAP estimator does not give meaningful results.

   


# Lecture 17. Linear least mean squares (LLMS) estimation

* LLMS

  * Minimize $\mathbb{E}[(\widehat{\Theta} - \Theta)^2]$​​.

  * Estimators $\widehat{\Theta}=g(X) \rightarrow \widehat{\Theta}_{LMS} = \mathbb{E}[\Theta |X]$​​.

  * Consider estimators of $\Theta$​​ of the form $\widehat{\Theta} =  aX+b$​​​.

  * Minimize $\mathbb{E}[(\Theta-aX-b)^2]$​​, w.r.t. $a,b$​​​.

  * If $\mathbb{E}[\Theta|X]$​ is linear in $X$​, then $\widehat{\Theta}_{LMS} = \widehat{\Theta}_{LLMS}$​.

* Solution to LLMS: 

  * $\widehat{\Theta}_L = \mathbb{E}[\Theta] + {\text{Cov}(\Theta, X) \over \text{var}(X) } (X - \mathbb{E}[X]) = \mathbb{E}[\Theta] + \rho {\sigma_{\Theta} \over \sigma_X}(X-\mathbb{E}[X])$.

    where $a = {\text{Cov}(\Theta, X) \over \text{var}(X) } =\rho {\sigma_{\Theta} \over \sigma_X}$​​, and $\rho={\text{Cov}(\Theta,X)\over \sigma_\Theta \sigma_X }$​​​.

  * $\mathbb{E}[(\widehat{\Theta}_L - \Theta)^2]=(1-p^2)\text{var}(\Theta)$​.​

* LLMS with multiple observations

  * Unknown $\Theta$; observations $X=(X_1, ..., X_n)$

  * Consider estimators of the form: $\widehat{\Theta} = a_1X_1 + ... + a_nX_n+b$

  * Find best choices of $a_1, ..., a_n,b$

    Minimize: $\mathbb{E}[(a_1X_1 + ... + a_nX_n+b-\Theta)^2]$​​​

  * If $\mathbb{E}[\Theta|X]$ is linear in $X$, then $\widehat{\Theta}_{LMS} = \widehat{\Theta}_{LLMS}$.

  * Solve linear system in $b$ and the $a_i$.

  * Only means, variances, covariances matter.

  * If multiple unknown $\Theta_j$, apply to each one, separately.

* The simplest LLMS example with multiple observations

  * Given $X_1 =\Theta  +W_1, ..., X_n =\Theta  +W_n$​​​​, $\Theta \sim x_0, \sigma_0^2,\ W_i \sim 0, \sigma_i^2$​​. 

  * Suppose $\Theta, W_1, ..., W_n$ are independent normal
    $$
    \hat{\theta}_{LMS} = \mathbb{E}[\Theta| X=x] = {\sum\limits^n_{i=0} {x_i\over \sigma_i^2} \over \sum\limits^n_{i=0} {1 \over \sigma_i^2}}\\
    \hat{\theta}_{LMS} = \mathbb{E}[\Theta| X]  = {{x_0\over \sigma_0^2} + \sum\limits^n_{i=1}{X_i \over \sigma_i^2} \over \sum\limits^n_{i=0} {1\over \sigma_i^2}} = \widehat{\Theta}_{LLMS}
    $$

* The representation of the data matters in LLMS

  Estimation based on $X$ vs. $X^3$.

  * LMS: $\mathbb{E}[\Theta |X]$ is the same as $\mathbb{E}[\Theta | X^3]$
  * LLMS is different: estimator $\widehat{\Theta} = aX + b$​ vs. $\widehat{\Theta}=aX^3 + b$​. (Since $\text{cov}(\Theta, X^3), \text{var}(X^3)$​​).
  * Can also consider:
    * $\widehat{\Theta} = a_1 X + a_2 X^2 + a_3X^3 + b$
    * $\widehat{\Theta} = a_1 X + a_2 e^X + a_3 \log X + b$​​

* Comparing LMS and LLMS:

  * It's true that LMS estimator is guaranteed to take values only in the interval $[0,1]$​. But LLMS estimator is NOT guaranteed to take values only in the interval $[0,1]$​.

    The conditional expectation ${\bf E}[\Theta \mid X=x]$ is a weighted average of the values of $\Theta$, weighted according to the posterior PDF. A weighted average of values in $[0,1]$ must lie in $[0,1]$.

    On the other hand, there is no such guarantee for the LLMS estimator. Consider the example where $X=\Theta + W$, where $W$ can take any real value. Then, the term $aX$ can take any real value, and can therefore fall outside the range $[0,1]$.

  * The MAP estimator for the problem of estimating the bias of a coin is $X/n$​​, which is different from the LLMS estimator $(X+1)/(n+2)$​​. LLMS has a smaller MSE.

    The LLMS estimator coincides with the LMS estimator and therefore achieves the smallest possible mean squared error.
    
  * The relationship is linear in $X$ when the RVs are normal and have the same mean. The LMS and LLMS estimators would be the same no matter what distribution we have, as long as the RVs have the same mean.
  
* (Supplement) The Beta formula: For any nonnegative integers $\alpha$ and $\beta$, we have
  $$
  \int _0^1 \theta ^\alpha (1-\theta )^{\beta }\,  d\theta =\frac{\alpha !\,  \beta !}{(\alpha +\beta +1)!},
  $$

There are 2 selected exercises and 2 solved problems.

---

## Exercise 1 LLMS drill

Suppose that $\Theta$​​​​​ and $W$​​​​​ are independent, both with variance $1$​​​​​, and that $X=\Theta + W$​​​​​. Furthermore, $\mathbb{E}[\Theta] =1$​​​​​ and $\mathbb{E}[W]=2$​​​​​. What is the LLMS estimator $\widehat{\Theta}$​​​​​?

Hint: Recall the formula $\textsf{Cov}(X+Y,Z)=\textsf{Cov}(X,Z)+\textsf{Cov}(Y,Z)$​.

**Solution:**

We have ${\bf E}[X] = {\bf E}[\Theta ] + {\bf E}[W] = 3$ and $\textsf{Var}(X)=\textsf{Var}(\Theta )+\textsf{Var}(W)=2$. Also,
$$
\textsf{Cov}(X,\Theta )=\textsf{Cov}(\Theta ,\Theta )+\textsf{Cov}(\Theta ,W)=\textsf{Var}(\Theta )+0=1.
$$
Therefore, the LLMS estimator is
$$
\widehat\Theta = 1+\frac{1}{2}(X-3)=\frac{1}{2}X-\frac{1}{2}.
$$

## Exercise 2 LLMS with multiple observations

Suppose that $\Theta, X_1, $ and $X_2$ have zero means. Furthermore,
$$
\textsf{Var}(X_1)=\textsf{Var}(X_2)=\textsf{Var}(\Theta )=4,
$$
and
$$
\textsf{Cov}(\Theta ,X_1)=\textsf{Cov}(\Theta ,X_2)=\textsf{Cov}(X_1,X_2)=1.
$$
The LLMS estimator of $\Theta$ based on $X_1$ and $X_2$ is of the form $\widehat\Theta = a_1X_1 + a_2X_2 + b$. Find the coefficients $a_1, a_2$, and $b$.

**Solution:**

By the same argument as in the case of a single observation, we will have $b={\bf E}[\Theta -a_1X_1-a_2X_2]=0$. Using the variance and covariance information we are given, the expression we want to minimize is
$$
{\bf E}\left[(a_1X_1+a_2X_2-\Theta )^2\right]=4a_1^2+4a_2^2+4+2a_1a_2 -2a_1-2a_2.
$$
Because of symmetry, we see that the optimal solution will satisfy $a_1 = a_2 = a$​​, so the expression is of the form $8a^2+4+2a^2-4a$​​. By setting the derivative to zero, we find that $20a=4$, or $a=1/5$.

## Problem 1 LLMS estimation

Let $N$ be a random variable with mean ${\bf E}[N]=m$, and $\textsf{Var}(N)=v$; let $A_1,A_2,\dots$ be a sequence of i.i.d. random variables, all independent of $N$, with mean $1$ and variance $1$; let $B_1, B_2, ...$ be another sequence of i.i.d. random variables, all independent of $N$ and of $A_1, A_2, ...$, also with mean $1$ and variance $1$. Let $A=\sum _{i=1}^ N A_i $ and $B=\sum _{i=1}^ N B_ i$.

1. Find the expectations ${\bf E}[AB]$ and ${\bf E}[NA]$ using the law of iterated expectations.
2. Let $\hat{N}=c_1 A + c_2$ be the LLMS estimator of $N$ given $A$. Find $c_1$ and $c_2$ in terms of $m$ and $v$.

**Solution:**

1. Compute ${\bf E}[AB]$:
   $$
   \begin{aligned}
   {\bf E}[AB] &= {\bf E}[(A_1+\dots +A_ N)(B_1+\dots +B_ N)]\\
   &= {\bf E}[{\bf E}[(A_1+\dots +A_ N)(B_1+\dots +B_ N)\mid N]]\\
   &= {\bf E}[{\bf E}[(A_1+\dots +A_ N)\mid N]{\bf E}[(B_1+\dots +B_ N)\mid N]]\\
   &= {\bf E}[N{\bf E}[A_1]N{\bf E}[B_1]]\\
   &= {\bf E}[N^2]\\
   &= \textsf{Var}(N)+({\bf E}[N])^2\\
   &= m^2 + v
   \end{aligned}
   $$
   Similarly,
   $$
   \begin{aligned}
   {\bf E}[NA] &= {\bf E}[{\bf E}[N(A_1+\dots +A_ N)\mid N]]\\
   &= {\bf E}[N{\bf E}[A_1+\dots +A_ N\mid N]]\\
   &= {\bf E}[N(N{\bf E}[A_1])]\\
   &= {\bf E}[N^2]\\
   &= m^2 + v
   \end{aligned}
   $$

2. $A$ is the sum of a random number, $N$, of iid random variables $A_1, ..., A_N$. Therefore,
   $$
   {\bf E}[A]={\bf E}[{\bf E}[A\mid N]] = {\bf E}[{\bf E}[A_1]N]=m,
   $$
   and
   $$
   \textsf{Var}(A)=\textsf{Var}(A_ i){\bf E}[N] + ({\bf E}[A_ i])^2 \textsf{Var}(N) = m + v.
   $$
   Similarly, ${\bf E}[B]=m$, and $\textsf{Var}(B)=m + v$. Furthermore,
   $$
   \begin{aligned}
   \text{cov}(N,A) &= {\bf E}[NA]-{\bf E}[N]{\bf E}[A]\\
   &= (m^2 + v) -m^2\\
   &= v.
   \end{aligned}
   $$
   Finally, 
   $$
   \begin{aligned}
   \hat{N} &= {\bf E}[N]+\frac{{\rm cov}(N,A)}{\textsf{Var}(A)}(A-{\bf E}[A])\\
   &=  m + \frac{v}{m+v}(A-m)\\
   &= \frac{m^2}{m+v}+\frac{v}{m+v}A.
   \end{aligned}
   $$

## Problem 2 Estimating the parameter of a uniform r.v.

The random variable $X$ is uniformly distributed over the interval $[\theta, 2\theta]$. The parameter $\theta$ is unknown and is modeled as the value of a continuous random variable $\Theta$, uniformly distributed between zero and one.

1. Given an observation $x$ of $X$, find the posterior distribution of $\Theta$. Express your answers below in terms of $\theta$  and $x$. 
2. Find the MAP estimate of $\Theta$ based on the observation $X=x$ and assuming that $0 \leq x \leq 1$. 

3. Find the LMS estimate of $\Theta$ based on the observation $X=x$ and assuming that $0 \leq x \leq 1$.
4. Find the linear LMS estimate $\hat{\theta }_{\text {LLMS}}$ of $\Theta$ based on the observation $X=x$. 

**Solution:**

1. The prior PDF of $\Theta$ is
   $$
   f_{\Theta }(\theta ) = \begin{cases}  1, &  \text{if } 0\leq \theta \leq 1, \\ 0, &  \text{otherwise,} \end{cases}
   $$
   and the conditional PDF of the observation $X$ is
   $$
   f_{X\mid \Theta }(x\mid \theta ) = \begin{cases}  1/\theta , &  \text{if } \theta \leq x\leq 2\theta , \\ 0, &  \text{otherwise.} \end{cases}
   $$
   Using Bayes' rule, we find that for any $x \in [0,1]$ and for $\theta \in [x/2,x]$, the posterior PDF is
   $$
   \begin{aligned}
   f_{\Theta \mid X}(\theta \mid x) & = \frac{f_{\Theta }(\theta )f_{X\mid \Theta }(x\mid \theta )}{\displaystyle \int _{x/2}^{x}f_{\Theta }(\tilde{\theta })f_{X\mid \Theta }(x\mid \tilde{\theta }) d \tilde{\theta }}\\
   &= \frac{1/\theta }{\displaystyle \int _{x/2}^{x}\frac{1}{\tilde{\theta }}d \tilde{\theta }}\\
   &= \frac{1}{\theta \cdot (\ln (x) - \ln (x/2))}\\
   &= \frac{1}{\theta \cdot \ln (2)}.
   \end{aligned}
   $$

2. For $x\in [0,1]$ and $x/2 \leq \theta \leq x$, the posterior PDF is
   $$
   f_{\Theta \mid X}(\theta \mid x) = \frac{1}{\theta \cdot \ln (2)},
   $$
   which is decreasing in $\theta$ over the range $[x/2, x]$ of possible values of $\Theta$. Thus, the MAP estimate for this case is equal to $x/2$.

3. The LMS estimate is the conditional expectation estimate. For $x \in [0,1]$,
   $$
   {\bf E}[\Theta \mid X = x] = \int _{x/2}^{x}\theta \frac{1}{\theta \cdot \ln (2)}d\theta = \frac{x}{2\cdot \ln (2)}.
   $$

4. The LLMS estimate is of the form
   $$
   \hat{\theta }_{LLMS}(x) = {\bf E}[\Theta ] + \frac{{\rm cov}(\Theta , X)}{\textsf{Var}(X)} (x - {\bf E}[X]).
   $$
   Here, 
   $$
   \begin{aligned}
   {\bf E}[\Theta ] &= 1/2,\\
   {\bf E}[\Theta ] &= {\bf E}[{\bf E}[X\mid \Theta ]]\\
   &= {\bf E}\left[\frac{3}{2} \Theta \right]\\
   &= {\bf E}\left[\int_\Theta^{2\Theta}xf_{X|\Theta}(X|\Theta) dx \right]\\
   &= {\bf E}\left[\int_\Theta^{2\Theta}{x\over \Theta} dx \right]\\
   &= {\bf E}\left  .\left [{x^2\over 2\Theta} \right|_\Theta^{2\Theta} \right]\\
   &= {3\over 4}\\
   {\bf E}[X^2] & = {\bf E}[{\bf E}[X^2\mid \Theta ]]\\
   &= {\bf E}\left[\int_\Theta^{2\Theta}x^2f_{X|\Theta}(X|\Theta) dx \right]\\
   &= {\bf E}\left[\int_\Theta^{2\Theta}{x^2\over \Theta} dx \right]\\
   &= {\bf E}\left  .\left [{x^3\over 3\Theta} \right|_\Theta^{2\Theta} \right]\\
   & = {\bf E}\left[\frac{7}{3} \Theta ^2\right]\\
   &= {7 \over 9}\\
   \end{aligned}
   $$
   Hence,
   $$
   \begin{aligned}
   \textsf{Var}(X) & = {\bf E}[X^2] - ({\bf E}[X])^2\\
   & = \frac{31}{144},\\
   {\bf E}[\Theta X] & = {\bf E}[{\bf E}[X \Theta \mid \Theta ]]\\
   & = {\bf E}\left[\frac{3}{2} \Theta ^2\right]\\
   &= {1\over 2},\\
   {\rm cov}(\Theta , X) & = {\bf E}[\Theta X] - {\bf E}[\Theta ] {\bf E}[X]\\
   & = \frac{1}{2} - \frac{1}{2} \cdot \frac{3}{4}\\
   &= {1\over 8}
   \end{aligned}
   $$
   Finally, we have
   $$
   \begin{aligned}
   \hat{\Theta }_{LLMS} & = {\bf E}[\Theta ] + \frac{{\rm cov}(\Theta , X)}{\textsf{Var}(X)} (x - {\bf E}[X])\\ 
   &= \frac{1}{2} + \frac{1/8}{31/144} \left(x-\frac{3}{4} \right)\\
   &= \frac{2}{31} + \frac{18}{31}x.
   \end{aligned}
   $$
   


# Lecture 13. Conditional expectation and variance revisited; Application: Sum of a random number of independent r.v.'s

* A more abstract version of the conditional expectation

  * view it as a random variable

    $g(Y) = \mathbf{E}[X|Y]$

    $g(Y):$ is the r.v. that takes the value $\mathbf{E}[X | Y=y]$, if $Y$ happens to take the value $y$.

    | Discrete                                                  | Continuous                                         |
    | --------------------------------------------------------- | -------------------------------------------------- |
    | $g(y) = \mathbf{E}[X|Y=y] = \sum\limits_x x p_{X|Y}(x|y)$ | $g(y) = \mathbf{E}[X|Y=y] = \int x p_{X|Y}(x|y)dx$ |

    Remark: 

    * it is a function of $Y$
    * it is a random variable
    * it has a distribution, mean, variance, etc.

  * the law of iterated expectations

    $g(y) = \mathbf{E}[X | Y =y ], \quad g(Y) = \mathbf{E}[X | Y ]$

    $\mathbf{E}\left[\mathbf{E}[X|Y]\right] = \mathbf{E}[X]$

  * Conditional expectation properties

    * ${\bf E}\big [g(Y)X \, |\,  Y\big ] =g(Y) {\bf E}[X\, |\,  Y]$.

    * If $h$ is an invertible function, then $\mathbf{E}[X|Y] = \mathbf{E}[X|h(Y)]$.

  * Stick example: stick of length $l$ break at uniformly chosen point $Y$, break what is left at uniformly chosen point $X$.

    $\mathbf{E}[X|Y=y] = y/2, \quad \mathbf{E}[X|Y]= y/2$.

    $\mathbf{E}[X] = \mathbf{E}[\mathbf{E}[X|Y]] = \mathbf{E}[Y/2] = {1\over 2} \mathbf{E}[Y] =  {1\over 2} \cdot {l \over 2} = {l \over 4}$

* A more abstract version of the conditional variance

  * view it as a random variable

    $\mathsf{Var}(X) = \mathbf{E}\left[(X -\mathbf{E}[X])^2\right]$

    $\mathsf{Var}(X|Y = y) = \mathbf{E}\left[(X -\mathbf{E}[X|Y=y])^2|Y=y \right]$

    $\mathsf{Var}(X|Y)$ is the r.v. that takes the value $\mathsf{Var}(X|Y=y)$, when $Y=y$.

  * the law of total variance

    $\mathsf{Var}(X) = \mathbf{E}\left[\mathsf{Var}(X|Y)\right] + \mathsf{Var}\left(\mathbf{E}[X|Y]\right)$

    $\mathsf{Var}(X) = $ (average variability **within** sections) + (variability between sections)

* Sum of a random number of independent r.v.'s

  Let $Y = X_1 + ... + X_N$

  * mean

    $\mathbf{E}[Y | N= n] = \mathbf{E}[X_1+ ... + X_n|N=n] =  \mathbf{E}[X_1+ ... + X_n] = n \cdot \mathbf{E}[X] \implies \mathbf{E}[Y|N] = N \cdot \mathbf{E}[X]$

    $\mathbf{E}[Y] = \mathbf{E}[\mathbf{E}[Y|N]] = \mathbf{E}[N \cdot \mathbf{E}[X]] = \mathbf{E}[N] \cdot \mathbf{E}[X]$

  * variance

    $\mathsf{Var}(\mathbf{E}[Y|N]) = \mathsf{Var}(N \cdot \mathbf{E}[X]) = (\mathbf{E}[X])^2\mathsf{Var}(N)$

    $\mathsf{Var}(Y|N=n) = \mathsf{Var}(X_1 + ... + X_n | N=n) = \mathsf{Var}(X_1 + ... + X_n) = n \cdot \mathsf{Var}(X) \implies \mathsf{Var}(Y|N) = N \cdot \mathsf{Var}(X)$

    $\mathbf{E}[\mathsf{Var}(Y|N)] = \mathbf{E}[N \cdot \mathsf{Var}(X)] = \mathbf{E}[N]\cdot \mathsf{Var}(X) $

    $\mathsf{Var}(Y) = \mathbf{E}\left[\mathsf{Var}(Y|N)\right] + \mathsf{Var}\left(\mathbf{E}[Y|N]\right) = \mathbf{E}[N]\mathsf{Var}(X) + (\mathbf{E}[X])^2\mathsf{Var}(N)$

There are 3 selected exercises and 5 solved problems.

---

## Exercise 1 Iterated expectations

1. The law of iterated expectations tells us that ${\bf E}\big [{\bf E}[X\, |\, Y]\big ]={\bf E}[X]$. Suppose that we want apply this law in a conditional universe, given another random variable $Z$, in order to evaluate ${\bf E}[X \, |\, Z]$. Then which is true

   a. ${\bf E}\big [{\bf E}[X\, |\, Y,Z] \, |\, Z\big ]={\bf E}[X\, |\, Z]$

   b. ${\bf E}\big [{\bf E}[X\, |\, Y] \, |\, Z\big ]={\bf E}[X\, |\, Z]$

   c. ${\bf E}\big [{\bf E}[X\, |\, Y,Z] \big ]={\bf E}[X\, |\, Z]$

2. Determine which of the following statements are true about the quantity ${\bf E}\big [g(X,Y)\, |\, Y,Z \big ]$.

   a. A random variable

   b. A number

   c. A function of $(X,Y)$

   d. A function of $(Y,Z)$

   e. A function of $Z$ only

**Solution**:

1. a.

2. ad.

   A conditional expectation is generally a random variable, a function of the random variables on which we are conditioning, and so a function of $(Y,Z)$ in this case.

## Exercise 2 Conditional expectation and variance

1. The random variable $Q$ is uniform on $[0,1]$. Conditioned on $Q=q$, the random variable $X$ is Bernoulli with parameter $q$. Then what is the conditional expectation $\mathbf{E}[X|Q]$?
2. The random variable $Q$ is uniform on $[0,1]$. Conditioned on $Q=q$, the random variable $X$ is Bernoulli with parameter $q$. Then what is the conditional variance $\textsf{Var}(X\, |\, Q)$?
3. Recall that a uniform random variable on $[0,1]$ has a variance of $1/12$ and satisfies ${\bf E}[Q^2]=1/3$. Then what are $\textsf{Var}\big ({\bf E}[X\, |\, Q]\big )$ and ${\bf E}\big [\textsf{Var}(X\, |\, Q)\big ]$? 

**Solution**: 

1. ${\bf E}[X\, |\, Q]=Q$.

   Since ${\bf E}[X\, |\, Q=q]=q$, for all $q \in [0,1]$, the abstract statement ${\bf E}[X\, |\, Q]=Q$.

2. $\textsf{Var}(X\, |\, Q)=Q(1-Q)$.

   Since $\textsf{Var}(X\, |\, Q=q)=q(1-q)$, for all $q \in [0,1]$.

3. Since ${\bf E}[X\, |\, Q]=Q$, we have $\textsf{Var}\big ({\bf E}[X\, |\, Q]\big )=\textsf{Var}(Q)=1/12$.

   Since $\textsf{Var}(X\, |\, Q)=Q(1-Q)$, we have
   $$
   {\bf E}\big [\textsf{Var}(X\, |\, Q)\big ]={\bf E}\big [Q(1-Q)\big ]={\bf E}[Q]-{\bf E}[Q^2\big ]=\frac{1}{2}-\frac{1}{3}=\frac{1}{6}.
   $$

## Exercise 3 Second generation offspring

Every person has a random number of children, drawn from a common distribution with mean 3 and variance 2. The numbers of children of each person are independent. Let $M$ be the number of grandchildren of a certain person. Then what are ${\bf E}[M]$ and $\textsf{Var}(M)$?

**Answer**:

${\bf E}[M] = 9 ; \quad \textsf{Var}(M)= 24$. 

**Solution**:

Let $N$ be the number of children and let $X_i$ be the number of children of the $I$th child. Then $M = X_1 + ... + X_N$. And ${\bf E}(N) = 3, \quad \mathsf{Var}(N) = 2$ according to the problem statement.

It follows that
$$
{\bf E}[M]={\bf E}[N]\cdot {\bf E}[X]=3\cdot 3=9
$$
Furthermore,
$$
\textsf{Var}(M)={\bf E}[N]\textsf{Var}(X)+\big ({\bf E}[X]\big )^2\textsf{Var}(N)=3\cdot 2+9\cdot 2 =24.
$$

## Problem 1 Using conditional expectation and variance

The random variables $X$ and $Y$ are described by a joint PDF which is constant within the unit area quadrilateral with vertices $(0,0),(0,1),(1,2),$ and $(1,1)$. Use the law of total variance to find the variance of $X + Y$.

**Solution**:

Recall the formula of variance
$$
\mathsf{Var}(X+Y) = \mathsf{Var}(\mathbf{E}[X+Y | X]) + \mathbf{E}[\mathsf{Var}(X+Y | X)]\\
\mathsf{Var}(X+Y) = \mathsf{Var}(\mathbf{E}[X+Y | Y]) + \mathbf{E}[\mathsf{Var}(X+Y | Y)]
$$
We pick the first one (condition on $X$), because when $x$ is fixed, the PDF of $Y$ conditioning on $X$ is uniformly distributed between $[x,x+1]$, as shown in the figure below. 

![u6-lec13-prob1](../assets/images/u6-lec13-prob1.png)

First compute $\mathbf{E}[X+Y | X] $,
$$
\mathbf{E}[X+Y | X] = x +\mathbf{E}[Y | X] = x + {2x+1\over 2} = 2x + {1\over 2}
$$
Recall that the variance of Uniform distribution (uniformly distributed between $a$ and $b$) is $\mathsf{Var}(X) = {(b-a)^2\over 12}$. We then compute $\mathsf{Var}(X+Y | X)$,
$$
\mathsf{Var}(X+Y | X) = \mathsf{Var}(Y|X) = {1\over 12}
$$
Therefore,
$$
\mathsf{Var}(X + Y) = \mathsf{Var}(2x + {1\over 2}) + \mathbb{E}[{1\over 12}] = 4 \mathsf{Var}(x) + {1\over 12}
$$
To figure out $\mathsf{Var}(X)$, we need PDF of $X$ given a joint PDF. Note that if we fix $X$ and integrate over $Y$, we get $1$. Thus, the PDF of $X$ $f_X(x)$ is uniformly distributed between $[0,1]$.

Therefore,
$$
\mathsf{Var}(X + Y)  = 4 \mathsf{Var}(x) + {1\over 12} = 4 \times ({1\over 12}) + {1\over 12} = {5\over 12}
$$

## Problem 2 The variance in the stick-breaking problem

We start with a stick of length $\ell$ . We break it at a point which is chosen randomly and uniformly over its length, and keep the piece that contains the left end of the stick. We then repeat the same process on the piece that we were left with.

1. What is the expected value of the length of the piece that we are left with after breaking twice?
2. What is the variance of the length of the piece that we are left with after breaking twice?

**Solution**:

1. Let $Y$ be the length of the left stick after the first break, and $X$ be the length of the left stick after the second break. We have $Y \sim \mathsf{Unif}[0,\ell]$. We know that $\mathbb{E}[Y] = {l \over 2}, \mathsf{Var}(Y) = {l^2\over 12}$. If we were given $Y=y$, $X \sim \mathsf{Unif}[0,y]$. We then know $\mathbb{E}[X|Y] = {Y\over 2}, \mathsf{Var}(X|Y) = {Y^2\over 2} $.

   Therefore,
   $$
   \mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y]] = \mathbb{E}[{Y\over 2}] = {l\over 4}
   $$

2. Recall that $\mathsf{Var}(X) = \mathbb{E}[\mathsf{Var}(X|Y)] + \mathsf{Var}(\mathbb{E}[X|Y])$

   For the first part,
   $$
   \begin{aligned}
   \mathbb{E}[\mathsf{Var}(X|Y)] &= \mathbb{E}[{Y^2\over12}] = {1\over 12} \mathbb{E}[Y^2]\\
   &= {1\over 12}\left(\mathsf{Var}(Y) + (\mathbb{E}[Y])^2\right)\\
   &= {1\over 12}\left({l^2\over 12} + {l^2 \over 4}\right)\\ 
   &= {l^2 \over 36}
   \end{aligned}
   $$
   For the second part,
   $$
   \mathsf{Var}(\mathbb{E}[X|Y]) = \mathsf{Var}({Y\over 2}) = {1\over 4} \mathsf{Var}(Y) = {l^2\over 48}
   $$
   Combined together, we obtain
   $$
   \mathsf{Var}(X) = {l^2 \over 36} + {l^2 \over 48} = {7 l^2 \over 144 }
   $$

## Problem 3 A coin with random bias

We toss $n$ times a biased coin whose probability of heads, denoted by $q$, is the value of a random variable $Q$ with given mean $\mu$ and positive variance $\sigma^2$. Let $X_i$ be a Bernoulli random variable that models the outcome of the $i$th toss (i.e. $X_i=1$ if the $i$th toss is a head). We assume that $X_1,...,X_n$ are conditionally independent, given $Q=q$. Let $X$ be the number of head obtained in the $n$ tosses.

1. Use the law of iterated expectations to find $\mathbf{E}[X_i]$ and $\mathbf{E}[x]$.
2. Find $\mathsf{Cov}(X_i, X_j)$. Are $X_1, ..., X_n$ independent?
3. Use the law of total variance to find $\mathsf{Var}(X)$. Verify your answer using the covariance results of (1).

**Solution**:

1. By the law of iterated expectations and the fact $\mathbf{E}[X_i|Q] = Q$, we have
   $$
   \mathbf{E}[X_i] = \mathbf{E}[\mathbf{E}[X_i|Q]] = \mathbf{E}[Q]=\mu
   $$
   Since $X=X_1 + ... + X_n$, it follows that
   $$
   \mathbf{E}[X] = \mathbf{E}[X_1] + ... + \mathbf{E}[X_n] = n \mu
   $$

2. We have, for $ i \neq j$, using the conditional independence assumption,
   $$
   \mathbf{E}[X_iX_j|Q] = \mathbf{E}[X_i|Q]\mathbf{E}[X_j|Q] = Q^2
   $$
   and 
   $$
   \mathbf{E}[X_iX_j] = \mathbf{E}[\mathbf{E}[X_iX_j|Q]] = \mathbf{E}[Q^2]
   $$
   Thus, 
   $$
   \mathsf{Cov}(X_i, X_j) = \mathbf{E}[X_iX_j] - \mathbf{E}[X_i]\mathbf{E}[X_j] = \mathbf{E}[Q^2] - \mu^2 = \sigma^2
   $$
   Since $\mathsf{Cov}(X_i, X_j) > 0, X_1, ...,X_n$ are not independent,

   Also, for $i=j$, using the observation that $X_i^2 = X_i$,
   $$
   \begin{aligned}
   \mathsf{Var}(X_i) &= \mathbf{E}[X_i^2] - (\mathbf{E}[X_i])^2\\
   &= \mathbf{E}[X_i] - (\mathbf{E}[X_i])^2\\
   &= \mu -\mu^2
   \end{aligned}
   $$

3. Using the law of total variance, and the conditional independence of $X_1, ...,X_n$, we have
   $$
   \begin{aligned}
   \mathsf{Var}(X) &= \mathbf{E}[\mathsf{Var}(X|Q)] + \mathsf{Var}(\mathbf{E}[X|Q])\\
   &=\mathbf{E}[\mathsf{Var}(X_1 + ... + X_n|Q)] + \mathsf{Var}(\mathbf{E}[X_1 + ... + X_n|Q])\\
   &= \mathbf{E}[nQ(1-Q)]+ \mathsf{Var}(nQ)\\
   &= n \mathbf{E}[Q-Q^2] + n^2 \mathsf{Var}(Q)\\
   &= n(\mu - \mu^2 - \sigma^2) + n^2 \sigma^2\\
   &= n(\mu - \mu^2) + n(n-1)\sigma^2.
   \end{aligned}
   $$
   To verify the result using the covariance formulas of (2), we write
   $$
   \begin{aligned}
   \mathsf{Var}(X) &= \mathsf{Var}(X_1 + ... + X_n)\\
   &= \sum^n_{i=1}\mathsf{Var}(X_i) + \sum_{\{(i,j) | i \neq j\}} \mathsf{Cov}(X_i, X_j)\\
   &= n \mathsf{Var}(X_1) + (n^2-n)\mathbf{Cov}(X_1, X_2)\\
   &= n(\mu - \mu^2) + (n^2-n)\sigma^2s
   \end{aligned}
   $$

## Problem 4 Random number of coin flips

1. You roll a fair six-sided die, and then you flip a fair coin the number of times shown by the die. Assuming that the coin flips are independent, find the expected value and the variance of the number of heads obtained.
2. Repeat part (1) for the case where you roll two dice, instead of one.

**Solution**:

1. Let $X_i$ be independent Bernoulli random variable that are equal to $1$ if the $i$th flip results in heads. Let $N$ be the number of coin flips, and let $H$ be the number of heads. 

   Using the notation, we have $H = X_1 + ... + X_N$. We also know $\mathbf{E}[X_i] = 1/2, \mathsf{Var}(X) = 1/4$ for all $i$ since the coin if fair.

   Since $N$ follows a **discrete uniform distribution** from $1$ to $6$ with $p = 1/6$, we have $\mathbf{E}[N] = 7/2, \mathsf{Var}(N)=35/12$.

   Therefore, the expected number of heads is
   $$
   \mathbf{E}[H] = \mathbf{E}[X_i]\mathbf{E}[N] = {1\over 2}\cdot {7 \over 2} = {7 \over 4}
   $$
   and the variance is
   $$
   \mathsf{Var}(H) = \mathsf{Var}(X_i)\mathbf{E}[N] + (\mathbf{E}(X_i))^2\mathsf{Var}(N) = {1\over 4} \cdot {7\over 2} + {1\over 4} \cdot {35 \over 12} = {77\over 48}
   $$

2. Let $N_1$ denotes the 1st die, $N_2$ denotes the 2nd dies, $H_1$ denotes the number of heads for $N_1$ coin flips, $H_2$ denotes the number of heads for $N_2$ coin flips.

   $N^* = N_1 + N_2 $ represents the total coin flips, $H^* = H_1 + H_2$ represents the total heads.

   Therefore, 
   $$
   \mathsf{Var}(H^*) = 2 \mathsf{Var}(H) = {7 \over 2} \\ 
   \mathbf{E}[H^*] = \mathbf{E}[H_1] + \mathbf{E}[H_2] = 2 \mathbf{E}[X]={77\over 24}
   $$

## Problem 5 Sum of a random number of r.v.'s

A fair coin is flipped independently until the first Heads is observed. Let the random variable $K$ be the number of tosses until the first Heads is observed **plus 1**. For example, if we see TTTHTH, so that the first head is observed after $4$ tosses, then $K=4+1=5$. For $k=1,2,...,K$, let $X_k$ be a continuous random variable that is uniform over the interval $[0,5]$. The $X_k$ are independent of one another and of the coin flips. Let $X=\sum _{k=1}^ K X_ k$. Find the mean and variance of $X$.

**Solution:**

Recall the mean and variance of a geometric random variable with parameter $p $ are $1/p$ and $(1-p)/p^2$.

Since $X_k$ is uniform over $[0,5]$, we have ${\bf E}[X_ k]=5/2$ and $\textsf{Var}(X_ k)=5^2/12=25/12$.

Note that $K-1$ is geometric with parameter $p = 1/2$. So ${\bf E}[K-1]=2$ and ${\rm Var}(K-1)=2$, which implies ${\bf E}[K]=3$ and $\textsf{Var}(K)=2$.

Since $X = \sum ^{K}_{k=1}X_ k$ is the sum of a random number of independent and identically distributed random variables, we have
$$
{\bf E}[X]={\bf E}[X_1]{\bf E}[K] = \frac{5}{2}\cdot 3 = 15/2,
$$
and
$$
\textsf{Var}(X)= \textsf{Var}(X_1){\bf E}[K] + ({\bf E}[X_1])^2 \textsf{Var}(K) = \frac{25}{12}\cdot 3+\frac{25}{4}\cdot 2= 75/4.
$$




