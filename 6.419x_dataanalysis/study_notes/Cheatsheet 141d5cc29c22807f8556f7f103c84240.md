# Cheatsheet

# Module 1: Hypothesis Testing

## Determining a Model

- **Statistical Models** represent the data generation process.
    - **Bernoulli Distribution**: Models binary outcomes (success/failure).
        - PMF: $P(X = x) = p^x (1-p)^{1-x}$
    - **Poisson Distribution**: Models count data (number of events in fixed interval).
        - PMF: $P(X = k) = \frac{e^{-\lambda} \lambda^k}{k!}$

## Null and Alternative Hypotheses

- **Null Hypothesis $H_0$**: Default assumption (e.g., no effect, no difference).
- **Alternative Hypothesis $H_a$**: Contradicts $H_0$ (e.g., there is an effect).
    - **Two-tailed Test**: Tests for any significant difference.
    - **One-tailed Test**: Tests for a difference in a specific direction.

## Test Statistic

- A function of sample data used to decide whether to reject $H_0$.
- Common test statistics:
    - **Z-statistic**: $z = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}}$ (known population variance).
    - **T-statistic**: $t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}$ (unknown population variance).

## P-Value and Significance Level

- **P-Value**: Probability of observing data at least as extreme as current, assuming $H_0$ is true.
- **Significance Level $\alpha$**: Threshold for rejecting $H_0$ (commonly 0.05).
    - **Decision Rule**:
        - If $\text{P-value} \leq \alpha$: Reject $H_0$.
        - If $\text{P-value} > \alpha$: Fail to reject $H_0$.

## Type I and Type II Errors

- **Type I Error $\alpha$:** Rejecting $H_0$ when it's true (false positive).
- **Type II Error $\beta$**: Failing to reject $H_0$ when $H_a$ is true (false negative).

## Power of a Test

- **Power $1 - \beta$**: Probability of correctly rejecting $H_0$ when $H_a$ is true.
    - Higher power means a lower chance of Type II error.

## Likelihood Ratio Test

- Compares likelihoods under $H_0$ and $H_a$.
- **Likelihood Ratio $\Lambda$**: $\Lambda = \frac{L(\theta_0)}{L(\hat{\theta})}$
    - $L(\theta_0)$: Likelihood under $H_0$.
    - $L(\hat{\theta})$: Likelihood under $H_a$ (maximum likelihood estimate).
- **Test Statistic**: $-2 \ln \Lambda \sim \chi^2(k)$
    - $k$: Degrees of freedom (difference in parameters between $H_a$ and $H_0$).

### Example: Binomial Model

- **Data**: $X \sim \text{Binomial}(n, p)$
- $H_0$: $p = p_0$
- $H_a$: $p \neq p_0$
- **Likelihoods**:
    - Under $H_0$: $L(p_0) = \binom{n}{x} p_0^x (1 - p_0)^{n - x}$
    - Under $H_a$: $L(\hat{p}) = \binom{n}{x} \hat{p}^x (1 - \hat{p})^{n - x}$
        - $\hat{p} = \frac{x}{n}$ (MLE of $p$)

## Python Libraries and Template Code

### Template Code

### Z-Test (Known Variance)

```python
def z_test(sample_mean, population_mean, population_std, n, alpha=0.05):
    z_stat = (sample_mean - population_mean) / (population_std / np.sqrt(n))
    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))
    reject_null = p_value < alpha
    return z_stat, p_value, reject_null
```

### T-Test (Unknown Variance)

```python
def t_test(sample_data, population_mean, alpha=0.05):
    n = len(sample_data)
    sample_mean = np.mean(sample_data)
    sample_std = np.std(sample_data, ddof=1)
    t_stat = (sample_mean - population_mean) / (sample_std / np.sqrt(n))
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1))
    reject_null = p_value < alpha
    return t_stat, p_value, reject_null

```

### Likelihood Ratio Test

```python
def likelihood_ratio_test_binomial(x, n, p0, alpha=0.05):
    # Under H0
    L0 = stats.binom.pmf(x, n, p0)
    # Under Ha (MLE of p)
    phat = x / n
    L1 = stats.binom.pmf(x, n, phat)
    # Likelihood Ratio
    LR = L0 / L1
    test_stat = -2 * np.log(LR)
    p_value = 1 - stats.chi2.cdf(test_stat, df=1)
    reject_null = p_value < alpha
    return test_stat, p_value, reject_null
```

### Calculating P-Values and Critical Values

- **P-Value Approach**: Compute p-value and compare with $\alpha$.
- **Critical Value Approach**: Find critical value from distribution and compare test statistic.

### Example: Critical Value for Z-Test

```python
def z_test_critical_value(alpha=0.05, two_tailed=True):
    if two_tailed:
        critical_value = stats.norm.ppf(1 - alpha/2)
    else:
        critical_value = stats.norm.ppf(1 - alpha)
    return critical_value
```

## Additional Notes

- **Assumptions**:
    - Data are independent and identically distributed (i.i.d).
    - For Z-test: Population variance is known.
    - For T-test: Population variance is unknown, and sample size is small $n < 30$.
- **Effect Size**: Measures the magnitude of the difference (e.g., Cohen's d).

# Module 2: Testing the Efficacy of a Sleeping Drug

## Data and Question

- **Data**: Hours of sleep recorded for patients under two conditions:
    - **Drug Group**: Patients receiving the sleeping drug.
    - **Placebo Group**: Patients receiving a placebo.
- **Question**: Does the sleeping drug significantly increase sleep duration compared to the placebo?

## Model and Hypotheses

- **Paired Data**: Same subjects measured under both conditions (before and after).
- **Model**: Let $D_i = X_{i,\text{Drug}} - X_{i,\text{Placebo}}$, where $D_i$ is the difference in sleep hours for subject $i$.
- **Assumptions**:
    - The differences $D_i$ are independent and identically distributed (i.i.d).
    - The differences come from a normally distributed population.

### Hypotheses

- **Null Hypothesis $H_0$**: $\mu_D = 0$: The mean difference in sleep duration is zero (no effect).
- **Alternative Hypothesis $H_a$**: $\mu_D > 0$: The mean difference is greater than zero (drug increases sleep).

## Z-Test and T-Test

### Z-Test

- **When to Use**: Population standard deviation $\sigma$ of the differences is known.
- **Test Statistic**: $z = \frac{\bar{D} - \mu_0}{\sigma / \sqrt{n}}$
    - $\bar{D}$: Sample mean difference.
    - $\mu_0 = 0$: Mean difference under $H_0$.
    - $n$: Number of paired observations.

### T-Test

- **When to Use**: Population standard deviation is unknown; estimate using sample standard deviation.
- **Assumptions**: Differences $D_i$ are approximately normally distributed.
- **Test Statistic**: $t = \frac{\bar{D} - \mu_0}{s_D / \sqrt{n}}$
    - $s_D$: Sample standard deviation of the differences.
- **Degrees of Freedom**: $\text{df} = n - 1$
- **Decision Rule**: Compare $t$ to critical value from the t-distribution with $\text{df}$ degrees of freedom.

### Properties of the T-Distribution

- Symmetric and bell-shaped, like the normal distribution.
- Has heavier tails, which account for the additional uncertainty from estimating $\sigma$.
- As $n$ increases, the t-distribution approaches the standard normal distribution.

## Wilcoxon Signed Rank Test

- **When to Use**: Data are not normally distributed; non-parametric alternative to the paired t-test.
- **Assumptions**:
    - The differences are symmetric about the median.
- **Procedure**:
    1. Calculate differences $D_i$ and their absolute values.
    2. Rank the absolute differences.
    3. Assign signs based on the original differences.
    4. Sum the signed ranks.
- **Test Statistic**: The sum of positive ranks or negative ranks.
- **Decision Rule**: Compare the test statistic to critical values from the Wilcoxon distribution or compute a p-value.

## Confidence Interval

- **Definition**: A range of values that is likely to contain the true population parameter with a specified confidence level (e.g., 95%).
- **Connection to Hypothesis Testing**: If the confidence interval for $\mu_D$ does not contain $\mu_0$, reject $H_0$.
- **Formula for the Confidence Interval of the Mean Difference**: $\bar{D} \pm t_{\alpha/2, n-1} \times \frac{s_D}{\sqrt{n}}$
    - $t_{\alpha/2, n-1}$: Critical t-value for confidence level $1 - \alpha$.

## Python Libraries and Template Code

### Libraries to Import

```python
import numpy as np
from scipy import stats
```

### Template Code

### Paired T-Test

```python
def paired_t_test(data_drug, data_placebo, alpha=0.05):
    differences = np.array(data_drug) - np.array(data_placebo)
    n = len(differences)
    mean_diff = np.mean(differences)
    std_diff = np.std(differences, ddof=1)
    t_stat = mean_diff / (std_diff / np.sqrt(n))
    df = n - 1
    p_value = 1 - stats.t.cdf(t_stat, df=df)
    reject_null = p_value < alpha
    return t_stat, p_value, reject_null
```

### Wilcoxon Signed Rank Test

```python
def wilcoxon_signed_rank_test(data_drug, data_placebo, alpha=0.05):
    differences = np.array(data_drug) - np.array(data_placebo)
    stat, p_value = stats.wilcoxon(differences, alternative='greater')
    reject_null = p_value < alpha
    return stat, p_value, reject_null
```

### Confidence Interval for Mean Difference

```python
def confidence_interval_mean_difference(data_drug, data_placebo, confidence=0.95):
    differences = np.array(data_drug) - np.array(data_placebo)
    n = len(differences)
    mean_diff = np.mean(differences)
    std_err = stats.sem(differences)
    df = n - 1
    t_crit = stats.t.ppf((1 + confidence) / 2, df)
    margin_of_error = t_crit * std_err
    ci_lower = mean_diff - margin_of_error
    ci_upper = mean_diff + margin_of_error
    return (ci_lower, ci_upper)
```

### Example Usage

```python
# Sample data
data_drug = [7.1, 6.9, 8.2, 7.8, 7.5]
data_placebo = [6.5, 6.7, 7.0, 6.8, 7.1]

# Paired T-Test
t_stat, p_value, reject_null = paired_t_test(data_drug, data_placebo)
print(f"T-Statistic: {t_stat}, P-Value: {p_value}, Reject H0: {reject_null}")

# Wilcoxon Signed Rank Test
stat, p_value, reject_null = wilcoxon_signed_rank_test(data_drug, data_placebo)
print(f"Wilcoxon Statistic: {stat}, P-Value: {p_value}, Reject H0: {reject_null}")

# Confidence Interval
ci_lower, ci_upper = confidence_interval_mean_difference(data_drug, data_placebo)
print(f"95% Confidence Interval for Mean Difference: ({ci_lower}, {ci_upper})")
```

## Additional Notes

- **Normality Assumption**:
    - Check the normality of differences using a **Shapiro-Wilk test** or a **Q-Q plot**.
- **Effect Size**: **Cohen's d** for paired samples: $d = \frac{\bar{D}}{s_D}$
- **Power Analysis**: Determine the sample size needed to detect a meaningful effect.
- **One-Tailed vs. Two-Tailed Tests**:
    - **One-Tailed**: Tests for an effect in one direction (used here since $H_a: \mu_D > 0$).
    - **Two-Tailed**: Tests for any difference $\mu_D \neq 0$.

Remember to validate the assumptions before choosing a test. If data are not normally distributed or sample size is small, consider non-parametric tests like the Wilcoxon Signed Rank Test.

# Module 3: Multiple Testing Problem

## Multiple Testing Problem

- **Definition**: The issue that arises when multiple hypothesis tests are conducted simultaneously, increasing the chance of incorrectly rejecting at least one true null hypothesis (Type I error).
- **False Positives in Multiple Testing**:
    - **Family-Wise Error Rate (FWER)**: The probability of making at least one Type I error among all tests.
    - **Significance Level ($\alpha$)**: The probability of rejecting a true null hypothesis in a single test.
    - **Problem**: With multiple tests, the probability of at least one false positive increases.

### Example: The "Wonder-Syrup"

- **Scenario**: Testing a "wonder-syrup" on multiple unrelated health outcomes (e.g., blood pressure, cholesterol, glucose levels, etc.).
- **Assumption**: The syrup has no real effect on any outcome  ($H_0$ is true for all tests).
- **Testing**: Conducting $m$ independent hypothesis tests at significance level $\alpha$.
- **Risk of False Positives**:
    - The probability of obtaining at least one significant result by chance increases with $m$.

## Binomial Distribution and False Positives

- **Modeling False Positives**:
    - When $m$ independent tests are performed, each with a probability $\alpha$ of a Type I error, the number of false positives $X$ follows a **Binomial Distribution**: $X \sim \text{Binomial}(m, \alpha)$
- **Calculating Probabilities**:
    - **Probability of exactly k false positives**: $P(X = k) = \binom{m}{k} \alpha^k (1 - \alpha)^{m - k}$
    - **Probability of at least one false positive: $P(X \geq 1) = 1 - P(X = 0) = 1 - (1 - \alpha)^m$**

### Example Calculation

- **Suppose**:
    - Number of tests ($m$): 20
    - Significance level ($\alpha$): 0.05
- **Probability of at least one false positive**: $P(X \geq 1) = 1 - (1 - 0.05)^{20} \approx 1 - 0.3585 = 0.6415$
- **Interpretation**: There's approximately a 64% chance of observing at least one false positive when conducting 20 independent tests at $\alpha = 0.05$.

## Controlling for Multiple Testing

- **Bonferroni Correction**:
    - Adjust the significance level to control the Family-Wise Error Rate.
    - **Adjusted Significance Level**: $\alpha_{\text{adjusted}} = \frac{\alpha}{m}$
- **Example**:
    - Original $\alpha = 0.05$
    - Number of tests $m = 20$
    - Adjusted $\alpha_{\text{adjusted}} = \frac{0.05}{20} = 0.0025$

## Python Libraries and Template Code

### Libraries to Import

```python
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
```

### Template Code

### Calculating Probability of False Positives

```python
def probability_of_false_positives(m, alpha):
    """
    Calculate the probability of at least one false positive
    when conducting m independent tests at significance level alpha.
    """
    prob = 1 - (1 - alpha)**m
    return prob
```

### Example Usage

```python
# Number of tests
m = 20
# Significance level
alpha = 0.05

prob = probability_of_false_positives(m, alpha)
print(f"Probability of at least one false positive: {prob:.4f}")
```

### Plotting the Probability of False Positives

```python
def plot_false_positive_rate(max_tests, alpha):
    ms = range(1, max_tests + 1)
    probs = [probability_of_false_positives(m, alpha) for m in ms]
    plt.figure(figsize=(10, 6))
    plt.plot(ms, probs, marker='o')
    plt.title(f'Probability of At Least One False Positive (α = {alpha})')
    plt.xlabel('Number of Tests (m)')
    plt.ylabel('Probability')
    plt.grid(True)
    plt.show()
```

### Example Usage

```python
# Plot for up to 50 tests
plot_false_positive_rate(50, 0.05)
```

## Additional Notes

- **Key Takeaways**:
    - The more tests you perform, the higher the chance of obtaining false positives.
    - Adjustments like the Bonferroni correction help control the Family-Wise Error Rate but can be conservative.
- **Other Multiple Testing Corrections**:
    - **Holm-Bonferroni Method**
    - **False Discovery Rate (FDR)** control methods (e.g., Benjamini-Hochberg procedure)
- **Recommendations**:
    - Plan your analysis to minimize the number of tests.
    - Use corrections appropriately when multiple testing cannot be avoided.

Remember to account for multiple testing issues in your analyses to avoid misleading conclusions due to false positives.

# Module 4: Correlation and Regression

## I. Correlation

### Scatter Diagram and Association

- **Scatter Plot**: A graphical representation of the relationship between two quantitative variables.
    - **Purpose**: To visualize patterns, trends, and possible correlations.
    - **Example**: Plotting father heights (x-axis) vs. son heights (y-axis).

### Summarizing the Plot

- **Mean ($\bar{x}, \bar{y}$)**: Average values of $x$ and $y$.
- **Standard Deviation ($s_x, s_y$)**: Measures the dispersion of $x$ and $y$.
- **Covariance**: $\text{Cov}(X, Y) = \frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$
- **Correlation Coefficient (r)**: Standardized measure of linear association.

### Correlation Coefficient

- **Definition**: $r = \frac{\text{Cov}(X, Y)}{s_x s_y}$
- **Properties**:
    - **Range**: $-1 \leq r \leq 1$.
        - $r = 1$: Perfect positive linear relationship.
        - $r = -1$: Perfect negative linear relationship.
        - $r = 0$: No linear relationship.
    - **Symmetric**: $r_{xy} = r_{yx}$.
    - **Unit-Free**: Dimensionless measure.
- **Interpretation**:
    - **Positive r**: As x increases, y tends to increase.
    - **Negative r**: As x increases, y tends to decrease.
    - **Magnitude**: Indicates the strength of the linear relationship.
- **Cautions**:
    - **Non-Linearity**: A high r value does not imply causation or a perfect relationship if the association is non-linear.
    - **Outliers**: Can greatly affect the value of r.
    - **Restriction of Range**: Limited variability can affect r.

## II. Regression Line

### Predicting Son's Height

- **Objective**: Predict the son's height based on the father's height using linear regression.

### Meaning of Regression Line

- **Interpretations**:
    1. **Change in y per unit change in x**: $\beta_1$ represents the average change in y for a one-unit increase in x.
    2. **Conditional Averages**: The line passes through the mean values of y for given x.
    3. **Least Squares Solution**: Minimizes the sum of squared residuals between observed and predicted y values.

### Regression Line Equation

- **Simple Linear Regression Model**: $y = \beta_0 + \beta_1 x + \varepsilon$
    - y: Dependent variable.
    - x: Independent variable.
    - $\beta_0$: Intercept (value of y when $x = 0$).
    - $\beta_1$: Slope (change in y per unit change in x).
    - $\varepsilon$: Error term (assumed to be normally distributed with mean 0).
- **Estimating Parameters**:
    - **Least Squares Estimates**: $\hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} = r \left( \frac{s_y}{s_x} \right)$ ; $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$
- **Prediction**: $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$
- **Residuals**: $e_i = y_i - \hat{y}_i$
    - Difference between observed and predicted values.
- **Difference Between Regression Lines**:
    - **Regression of y on x** is not the same as x on y due to asymmetry in minimizing residuals.

## III. Evaluation

### Model Assessment

- **Assumptions to Check**:
    - Linearity
    - Independence
    - Homoscedasticity (constant variance)
    - Normality of residuals
- **Residual Analysis**:
    - **Residual Plot**: Plot residuals vs. fitted values or independent variable.
        - **Patterns**: Look for randomness (no pattern indicates good fit).
        - **Non-Linearity**: Curved patterns suggest non-linear relationships.
        - **Heteroscedasticity**: Funnel shapes indicate changing variance.
- **Transformations**:
    - **Log Transformation**: For exponential growth patterns.
    - **Square Root Transformation**: For count data.
    - **Polynomial Terms**: Adding $x^2, x^3$ terms to capture curvature.

## IV. Multiple Regression

### Introduction and Model Formulation

- **Purpose**: Predict a dependent variable using multiple independent variables.
- **Model**: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon$
    - p: Number of predictors.
    - $\varepsilon$: Error term.
- **Examples**:
    - **Polynomial Regression**: Include $x^2, x^3$ to model non-linear relationships.
    - **Categorical Variables**: Use dummy variables to represent groups.

### Ordinary Least Squares Estimator (OLS)

- **Objective**: Find $\beta$ that minimizes the sum of squared residuals.
- **Matrix Formulation**:
    - **Design Matrix (X)**: $X = \begin{bmatrix}
    1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
    1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{n,1} & x_{n,2} & \dots & x_{n,p} \\
    \end{bmatrix}$
    - **Response Vector (y)**: $y = \begin{bmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_n \\
    \end{bmatrix}$
    - **Parameter Vector ($\beta$)**: $\beta = \begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    \vdots \\
    \beta_p \\
    \end{bmatrix}$
- **Normal Equations**: $X^\top X \hat{\beta} = X^\top y$
- **OLS Estimator**: $\hat{\beta} = (X^\top X)^{-1} X^\top y$
    - **Invertibility**: $X^\top X$ must be invertible.
    - **Multicollinearity**: High correlation between predictors can make $X^\top X$ nearly singular.
- **Regularization Techniques**:
    - **Ridge Regression (L2 Penalty)**: $\hat{\beta}_{\text{ridge}} = (X^\top X + \lambda I)^{-1} X^\top y$
    - **Lasso Regression (L1 Penalty)**:
        - Minimizes $\sum (y_i - \hat{y}_i)^2 + \lambda \sum |\beta_j|$
    - **Purpose**: Prevent overfitting, handle $p > n$ scenarios.

## V. Variable Selection and Regularization

### Model Selection

- **Challenge**: Identifying which variables significantly contribute to the model.
- **Approaches**:
    - **T-Test for Individual Coefficients**:
        - **Null Hypothesis**: $\beta_j = 0$
        - **Test Statistic**: $t_j = \frac{\hat{\beta}_j}{\text{SE}(\hat{\beta}_j)}$
        - **Decision Rule**: Compare $t_j$ to critical t-value.
    - **Backward Elimination**:
        1. Fit the full model with all predictors.
        2. Remove the predictor with the highest p-value (if above a threshold, e.g., 0.05).
        3. Refit the model and repeat until all predictors are significant.
    - **Forward Selection**:
        - Start with no variables, add the most significant predictor at each step.
    - **Stepwise Selection**:
        - Combination of forward and backward methods.
- **Regularization Methods**:
    - **Purpose**: Penalize complexity, perform variable selection.
    - **Lasso Regression**:
        - Can shrink some coefficients exactly to zero, performing variable selection.
- **Model Selection Criteria**:
    - **Akaike Information Criterion (AIC)**: $\text{AIC} = 2k - 2\ln(L)$
        - k: Number of parameters.
        - L: Likelihood function.
    - **Bayesian Information Criterion (BIC)**: $\text{BIC} = \ln(n)k - 2\ln(L)$
        - Penalizes models with more parameters more heavily than AIC.

## Python Libraries and Template Code

### Libraries to Import

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score
```

### Correlation and Scatter Plot

```python
def plot_scatter_and_compute_correlation(x, y):
    # Scatter plot
    plt.figure(figsize=(8, 6))
    plt.scatter(x, y)
    plt.xlabel('Father Height')
    plt.ylabel('Son Height')
    plt.title('Father vs. Son Heights')
    plt.grid(True)
    plt.show()

    # Compute correlation coefficient
    r, p_value = stats.pearsonr(x, y)
    print(f"Correlation Coefficient (r): {r:.4f}")
    print(f"P-Value: {p_value:.4e}")
```

### Simple Linear Regression

```python
def simple_linear_regression(x, y):
    x = np.array(x).reshape(-1, 1)
    y = np.array(y)
    model = LinearRegression()
    model.fit(x, y)
    y_pred = model.predict(x)

    # Parameters
    beta_0 = model.intercept_
    beta_1 = model.coef_[0]
    print(f"Intercept (β₀): {beta_0:.4f}")
    print(f"Slope (β₁): {beta_1:.4f}")

    # Plot regression line
    plt.figure(figsize=(8, 6))
    plt.scatter(x, y, label='Data')
    plt.plot(x, y_pred, color='red', label='Regression Line')
    plt.xlabel('Independent Variable (x)')
    plt.ylabel('Dependent Variable (y)')
    plt.title('Simple Linear Regression')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Residuals
    residuals = y - y_pred
    plt.figure(figsize=(8, 6))
    plt.scatter(y_pred, residuals)
    plt.axhline(0, color='red', linestyle='--')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.title('Residual Plot')
    plt.grid(True)
    plt.show()

    # Model evaluation
    mse = mean_squared_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    print(f"Mean Squared Error: {mse:.4f}")
    print(f"R² Score: {r2:.4f}")
```

### Multiple Linear Regression

```python
def multiple_linear_regression(X, y):
    model = LinearRegression()
    model.fit(X, y)
    y_pred = model.predict(X)

    # Parameters
    beta_0 = model.intercept_
    beta_coefficients = model.coef_
    print(f"Intercept (β₀): {beta_0:.4f}")
    print(f"Coefficients (β₁ to βₚ): {beta_coefficients}")

    # Model evaluation
    mse = mean_squared_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    print(f"Mean Squared Error: {mse:.4f}")
    print(f"R² Score: {r2:.4f}")

    # Residual analysis
    residuals = y - y_pred
    plt.figure(figsize=(8, 6))
    plt.scatter(y_pred, residuals)
    plt.axhline(0, color='red', linestyle='--')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.title('Residual Plot')
    plt.grid(True)
    plt.show()
```

### Regularization Techniques

### Ridge Regression

```python
def ridge_regression(X, y, alpha=1.0):
    model = Ridge(alpha=alpha)
    model.fit(X, y)
    y_pred = model.predict(X)
    # Rest of the code is similar to multiple_linear_regression
```

### Lasso Regression

```python
def lasso_regression(X, y, alpha=1.0):
    model = Lasso(alpha=alpha)
    model.fit(X, y)
    y_pred = model.predict(X)
    # Rest of the code is similar to multiple_linear_regression
```

### Variable Selection Example

```python
def backward_elimination(X, y, significance_level=0.05):
    import statsmodels.api as sm
    X = sm.add_constant(X)
    model = sm.OLS(y, X).fit()
    while True:
        max_p_value = model.pvalues.max()
        if max_p_value > significance_level:
            excluded_feature = model.pvalues.idxmax()
            X = X.drop(columns=excluded_feature)
            model = sm.OLS(y, X).fit()
        else:
            break
    print(model.summary())
    return model
```

### Model Evaluation Metrics

- **Mean Squared Error (MSE)**: $\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$
- **Coefficient of Determination (R²)**: $R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}$
    - Represents the proportion of variance explained by the model.

## Additional Notes

- **Assumptions of Linear Regression**:
    - **Linearity**: Relationship between predictors and response is linear.
    - **Independence**: Observations are independent.
    - **Homoscedasticity**: Constant variance of residuals.
    - **Normality**: Residuals are normally distributed.
- **Dealing with Multicollinearity**:
    - **Variance Inflation Factor (VIF)**: Quantifies the severity of multicollinearity. $\text{VIF}_j = \frac{1}{1 - R_j^2}$
        - $R_j^2$: Coefficient of determination when predictor j is regressed on other predictors.
    - **Remedies**: Remove or combine correlated variables, use regularization.
- **Standardization of Variables**:
    - **Purpose**: Put variables on the same scale, especially important in regularization.
    - **Method: $x_{\text{standardized}} = \frac{x - \bar{x}}{s_x}$**
- **Interactions and Non-Linear Terms**:
    - **Interactions**: Include terms like $x_1 x_2$ to capture interaction effects.
    - **Non-Linear Terms**: Include $x^2, \log(x)$ to model curvature.
- **Cross-Validation**:
    - **Purpose**: Assess how the results of a statistical analysis will generalize to an independent dataset.
    - **Methods**: k-fold cross-validation, leave-one-out cross-validation.

**Remember**: Always visualize your data and residuals, check assumptions, and consider the context of your analysis when interpreting results.

# Module 5: Convex Functions, Gradient Descent, and Stochastic Gradient Descent

## I. Convex Functions

### Definition and Properties

- **Convex Function**: A function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is **convex** if for all $x, y \in \mathbb{R}^n$ and $\theta \in [0,1]$: $f(\theta x + (1 - \theta)y) \leq \theta f(x) + (1 - \theta) f(y)$
- **Properties**:
    - **Local Minimum is Global Minimum**: Any local minimum of a convex function is also a global minimum.
    - **First-Order Condition**:
        - A differentiable function $f$ is convex if and only if: $f(y) \geq f(x) + \nabla f(x)^\top (y - x), \quad \forall x, y \in \mathbb{R}^n$
    - **Second-Order Condition**:
        - A twice-differentiable function $f$ is convex if and only if its Hessian matrix $\nabla^2 f(x)$ is positive semidefinite for all x.
- **Visualization**:
    - **Bowl-Shaped**: Convex functions curve upwards and have a single global minimum.
- **Example**:
    - **Quadratic Function**: $f(x) = \frac{1}{2} x^\top Q x + b^\top x + c$
        - Convex if Q is positive semidefinite.

## II. Gradient Descent

### Optimization and Local Search

- **Goal**: Minimize a differentiable function $f(w)$.
- **Gradient Descent Algorithm**:
    - **Update Rule: $w^{(k+1)} = w^{(k)} - \alpha^{(k)} \nabla f(w^{(k)})$**
        - $w^{(k)}$: Current parameter vector at iteration k.
        - $\alpha^{(k)}$: Step size (learning rate).
        - $\nabla f(w^{(k)})$: Gradient of f at $w^{(k)}$.
- **Step Size ($\alpha$)**:
    - Determines how far we move along the gradient direction.
    - **Choice of ($\alpha$)**:
        - **Fixed Step Size**: Simple but may not guarantee convergence.
        - **Line Search**: Find the optimal (**$\alpha$**) at each iteration.
        - **Diminishing Step Size**: $\alpha^{(k)}$ decreases over iterations.

### Deriving Gradient Descent

- **Intuition**:
    - **Taylor Expansion**:
        - Approximate $f$ near $w^{(k)}$: $f(w) \approx f(w^{(k)}) + \nabla f(w^{(k)})^\top (w - w^{(k)}) + \frac{1}{2} (w - w^{(k)})^\top \nabla^2 f(w^{(k)}) (w - w^{(k)})$
    - **Quadratic Upper Bound**:
        - For convex functions, we can use a Lipschitz constant L such that: $f(w) \leq f(w^{(k)}) + \nabla f(w^{(k)})^\top (w - w^{(k)}) + \frac{L}{2} \| w - w^{(k)} \|^2$
    - **Minimizing the Upper Bound**:
        - Solve: $\min_w \left[ f(w^{(k)}) + \nabla f(w^{(k)})^\top (w - w^{(k)}) + \frac{L}{2} \| w - w^{(k)} \|^2 \right]$
        - Solution leads to the gradient descent update with $\alpha^{(k)} = \frac{1}{L}$.

### Example: Least Squares Regression

- **Objective Function**: $f(w) = \frac{1}{2n} \sum_{i=1}^n (y_i - x_i^\top w)^2$
- **Gradient**: $\nabla f(w) = -\frac{1}{n} \sum_{i=1}^n x_i (y_i - x_i^\top w) = -\frac{1}{n} X^\top (y - Xw)$
- **Update Rule: $w^{(k+1)} = w^{(k)} - \alpha \nabla f(w^{(k)})$**

### Step Sizes and Progress

- **Role of Step Size ($\alpha$)**:
    - **Too Small**: Slow convergence.
    - **Too Large**:
        - May overshoot the minimum.
        - Can cause divergence.
- **Choosing Step Size**:
    - **Exact Line Search**: Compute the optimal **$\alpha$** at each step.
    - **Backtracking Line Search**: Start with a large **$\alpha$**, reduce it until a sufficient decrease condition is met.
    - **Fixed Step Size**: Requires knowledge of L, the Lipschitz constant of $\nabla f$.
- **Visualization**: Plotting the function and the path taken by gradient descent with different step sizes.

### Convergence Analysis

- **For Convex Functions**:
    - **Sublinear Convergence**: $f(w^{(k)}) - f(\widehat{w}) \leq \frac{L \| w^{(0)} - \widehat{w} \|^2}{2k}$
        - $w^*$: Global minimum.
- **For Strongly Convex Functions**:
    - **Linear Convergence**: $f(w^{(k)}) - f(w^) \leq \left( 1 - \frac{\mu}{L} \right)^k \left( f(w^{(0)}) - f(w^) \right)$
        - $\mu$: Strong convexity parameter.
- **Key Concepts**:
    - **Lipschitz Continuity of Gradient**: $\| \nabla f(w) - \nabla f(v) \| \leq L \| w - v \|$
    - **Strong Convexity**:
        - Function \( f \) is strongly convex if: $f(y) \geq f(x) + \nabla f(x)^\top (y - x) + \frac{\mu}{2} \| y - x \|^2$

## III. Stochastic Gradient Descent (SGD)

### Motivation and Algorithm

- **Problem with Gradient Descent**: Computing the gradient over the entire dataset can be computationally expensive for large n.
- **Stochastic Gradient Descent**:
    - **Idea**: Use a random subset (mini-batch) or a single randomly sampled data point to estimate the gradient.
- **Algorithm**:
    - **Initialize** $w^{(0)}$
    - **For** $k = 0, 1, 2, \dots$:
        - Sample $i_k \in \{1, 2, \dots, n\}$
        - Compute stochastic gradient: $g^{(k)} = \nabla f_{i_k}(w^{(k)})$
            - $f_{i_k}(w)$: Loss function for data point $i_k$.
        - Update: $w^{(k+1)} = w^{(k)} - \alpha^{(k)} g^{(k)}$
- **Trade-offs**:
    - **Pros**:
        - Computational efficiency per iteration.
        - Can handle large datasets.
    - **Cons**:
        - Noisy updates.
        - Requires more iterations to converge.

### Demo and Applications

- **Fitting a Quadratic Function**:
    - Use SGD to minimize $f(w)$ for a quadratic loss function.
- **Behavior**:
    - **Gradient Descent**: Smooth convergence path.
    - **SGD**: Fluctuations due to stochasticity.
- **Applications**:
    - Training machine learning models (e.g., neural networks).
    - Online learning where data comes in streams.

---

## IV. Appendix: Linear Algebra Concepts

### Rank, Eigenvalues, Invertibility

- **Rank of a Matrix**:
    - The maximum number of linearly independent column vectors.
- **Eigenvalues ($\lambda$)**:
    - For matrix A, $Ax = \lambda x$ where $x \neq 0$.
- **Invertibility**:
    - A square matrix A is invertible if it has full rank (rank equals its dimension).
    - **Positive Definite Matrix**:
        - All eigenvalues are positive.
        - Ensures that A is invertible and that quadratic forms $x^\top A x > 0$.

### Relevance in Optimization

- **Hessian Matrix ($\nabla^2 f(w)$)**:
    - Second derivative matrix of f.
    - **Positive Definite Hessian**:
        - Function f is strongly convex.
- **Condition Number**:
    - Ratio of the largest to smallest eigenvalues of $\nabla^2 f(w)$.
    - Affects the convergence rate of gradient descent.

### Additional Notes on "L"

- **Lipschitz Constant (L)**:
    - Upper bound on the rate of change of the gradient.
    - **Relation to Hessian**:
        - For twice-differentiable functions: $\| \nabla^2 f(w) \|_2 \leq L$
        - $\| \cdot \|_2$: Spectral norm (largest eigenvalue).
- **Importance**:
    - Determines the maximum step size for convergence in gradient descent.
    - Used in convergence analysis to bound the error.

---

## Python Libraries and Template Code

### Libraries to Import

```python
import numpy as np
import matplotlib.pyplot as plt
```

### Gradient Descent Implementation

### Example Function: Quadratic Function

```python
# Objective function: f(w) = (1/2) * w^T Q w + b^T w + c
def objective_function(w, Q, b):
    return 0.5 * np.dot(w.T, np.dot(Q, w)) + np.dot(b.T, w)

# Gradient of the objective function
def gradient(w, Q, b):
    return np.dot(Q, w) + b
```

### Gradient Descent Algorithm

```python
def gradient_descent(Q, b, w_init, alpha, num_iterations):
    w = w_init
    history = [w]
    for k in range(num_iterations):
        grad = gradient(w, Q, b)
        w = w - alpha * grad
        history.append(w)
    return w, history
```

### Example Usage

```python
# Define Q, b for the quadratic function
Q = np.array([[3, 0], [0, 2]])  # Positive definite matrix
b = np.array([-5, -4])
w_init = np.array([0.0, 0.0])
alpha = 0.1  # Step size
num_iterations = 50

# Run gradient descent
w_opt, history = gradient_descent(Q, b, w_init, alpha, num_iterations)

# Print optimal w
print("Optimal w:", w_opt)
```

### Stochastic Gradient Descent Implementation

### Stochastic Gradient Function

```python
def stochastic_gradient(w, X, y, i):
    xi = X[i]
    yi = y[i]
    return -xi * (yi - np.dot(xi, w))
```

### SGD Algorithm

```python
def stochastic_gradient_descent(X, y, w_init, alpha, num_epochs):
    n_samples = X.shape[0]
    w = w_init
    history = [w]
    for epoch in range(num_epochs):
        for i in range(n_samples):
            grad = stochastic_gradient(w, X, y, i)
            w = w - alpha * grad
            history.append(w)
    return w, history
```

### Example Usage

```python
# Simulated data
X = np.random.randn(100, 2)
true_w = np.array([2.0, -3.0])
y = X @ true_w + np.random.randn(100) * 0.5  # Add noise

w_init = np.zeros(2)
alpha = 0.01
num_epochs = 10

# Run SGD
w_sgd, history_sgd = stochastic_gradient_descent(X, y, w_init, alpha, num_epochs)

print("SGD Estimated w:", w_sgd)
```

### Visualizing Convergence

```python
def plot_convergence(history):
    history = np.array(history)
    plt.figure(figsize=(8, 6))
    plt.plot(history[:, 0], history[:, 1], 'o-', markersize=3)
    plt.xlabel('w[0]')
    plt.ylabel('w[1]')
    plt.title('Convergence Path')
    plt.grid(True)
    plt.show()
```

## Additional Notes

- **Choosing Step Sizes**:
    - For quadratic functions, optimal $\alpha = \frac{2}{\lambda_{\text{max}} + \lambda_{\text{min}}}$.
    - Adaptive methods like **Adam**, **Adagrad**, **RMSprop** adjust $\alpha$ during training.
- **Batch vs. Stochastic vs. Mini-Batch Gradient Descent**:
    - **Batch Gradient Descent**: Uses all data to compute the gradient.
    - **Stochastic Gradient Descent**: Uses one data point.
    - **Mini-Batch Gradient Descent**: Uses a small subset (batch) of data.
- **Convergence of SGD**:
    - SGD converges in expectation to the global minimum for convex functions.
    - Learning rate $\alpha^{(k)}$ often decreases over time.
- **Common Pitfalls**:
    - Poorly chosen step size can prevent convergence.
    - Non-convex functions may have local minima; gradient descent may not find the global minimum.
- **Practical Tips**:
    - **Normalize Features**: Helps with convergence.
    - **Check Gradient Norm**: Use it as a stopping criterion.
- **Further Reading**:
    - **Convex Optimization** by Stephen Boyd and Lieven Vandenberghe.
    - **Optimization Methods in Machine Learning** courses and lectures.

**Remember**: Understanding the underlying mathematics of optimization algorithms enhances their effective application in data analysis and machine learning tasks.

# Module 6: The t-SNE Loss Function Cheatsheet

## I. Introduction to t-SNE and SNE

### Stochastic Neighbor Embedding (SNE) Framework

- **Purpose**: Dimensionality reduction technique for visualizing high-dimensional data in lower dimensions (typically 2D or 3D).
- **Key Idea**: Preserve the local structure of the data by converting high-dimensional similarities into low-dimensional similarities.

### Directional Similarity in SNE

- **High-Dimensional Similarity ($p_{j|i}$)**:
    - Represents the probability that data point $x_i$ would pick $x_j$ as its neighbor.
    - **Gaussian Kernel**: $p_{j|i} = \frac{\exp(-\| x_i - x_j \|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\| x_i - x_k \|^2 / 2\sigma_i^2)}$
        - $\sigma_i$: Perplexity-based variance for point $x_i$.
- **Perplexity ($\text{Perp}$)**:
    - Controls the effective number of neighbors considered.
    - **Definition**: $\text{Perp}(P_i) = 2^{H(P_i)}$
        - $H(P_i)$: Shannon entropy of the conditional probability distribution $P_i$.
- **Adjusting $\sigma_i$**:
    - Chosen such that the perplexity matches a user-defined value.

### Limitations of SNE

- **Asymmetry**: $p_{j|i} \neq p_{i|j}$
- **Crowding Problem**: Difficulty in modeling high-dimensional data in low-dimensional space.

## II. Symmetric SNE and Undirectional Similarities

### Symmetric SNE (SSNE)

- **Undirectional Similarity ($p_{ij}$)**: $p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$
    - n: Number of data points.
- **Advantages**:
    - Simplifies computations.
    - Makes the gradient of the loss function simpler.

### High-Dimensional Joint Probabilities

- **Symmetric Similarity**: $p_{ij} = \frac{\exp(-\| x_i - x_j \|^2 / 2\sigma^2)}{\sum_{k \neq l} \exp(-\| x_k - x_l \|^2 / 2\sigma^2)}$
    - Uses a common $\sigma$ for all points.

## III. The t-SNE Loss Function and Optimization

### t-Distributed Stochastic Neighbor Embedding (t-SNE)

- **Modification**: Uses a Student's t-distribution with one degree of freedom (Cauchy distribution) in the low-dimensional space.

### Low-Dimensional Similarities ($q_{ij}$)

- **Definition**: $q_{ij} = \frac{\left(1 + \| y_i - y_j \|^2 \right)^{-1}}{\sum_{k \neq l} \left(1 + \| y_k - y_l \|^2 \right)^{-1}}$
    - $y_i$: Low-dimensional representation of $x_i$.
    - Heavy-tailed distribution to alleviate the crowding problem.

### t-SNE Loss Function (Kullback-Leibler Divergence)

- **Objective Function**: $C = \sum_{i \neq j} p_{ij} \log \left( \frac{p_{ij}}{q_{ij}} \right)$
- **Interpretation**:
    - Measures the mismatch between $p_{ij}$ and $q_{ij}$.
    - **Attractive Forces**: Similar points ($p_{ij}$ large) are pulled together.
    - **Repulsive Forces**: Dissimilar points are pushed apart.

### Gradient of the Loss Function

- **Gradient with Respect to $y_i$**: $\frac{\partial C}{\partial y_i} = 4 \sum_{j} (p_{ij} - q_{ij}) (y_i - y_j) \left(1 + \| y_i - y_j \|^2 \right)^{-1}$
- **Optimization**:
    - **Method**: Gradient Descent or variants (e.g., Momentum, Adaptive learning rates).
    - **Initialization**: Random or using PCA for a better starting point.
    - **Learning Rate**: Critical for convergence.

### Perplexity in t-SNE

- **Role**:
    - Determines the balance between local and global aspects of the data.
    - Typical values range from 5 to 50.

### Algorithm Summary

1. **Compute High-Dimensional Similarities ( $p_{ij}$)**: Using perplexity and Gaussian kernels.
2. **Initialize Low-Dimensional Points ($y_i$)**: Randomly or with PCA.
3. **Iteratively Update $y_i$**:
    - Compute $q_{ij}$ using Student's t-distribution.
    - Compute gradients and update $y_i$.
    - Apply momentum and learning rate adjustments.

---

## IV. Python Libraries and Template Code

### Libraries to Import

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
```

### Using Scikit-Learn's t-SNE Implementation

### Basic t-SNE Example

```python
def perform_tsne(X, n_components=2, perplexity=30.0, learning_rate=200.0, n_iter=1000, random_state=None):
    """
    Perform t-SNE dimensionality reduction on dataset X.
    Parameters:
    - X: High-dimensional data (numpy array of shape [n_samples, n_features])
    - n_components: Dimension of the embedded space (usually 2 or 3)
    - perplexity: Related to the number of nearest neighbors (5 to 50)
    - learning_rate: Controls the step size during optimization
    - n_iter: Number of iterations for optimization
    - random_state: Seed for reproducibility
    Returns:
    - X_embedded: Low-dimensional representation (numpy array of shape [n_samples, n_components])
    """
    tsne = TSNE(n_components=n_components, perplexity=perplexity, learning_rate=learning_rate,
                n_iter=n_iter, random_state=random_state)
    X_embedded = tsne.fit_transform(X)
    return X_embedded
```

### Example Usage

```python
# Sample high-dimensional data
from sklearn.datasets import load_digits
digits = load_digits()
X = digits.data
y = digits.target

# Perform t-SNE
X_tsne = perform_tsne(X, n_components=2, perplexity=30, learning_rate=200, n_iter=1000, random_state=42)

# Plot the results
plt.figure(figsize=(8, 6))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='jet', s=15)
plt.colorbar(scatter, label='Digit Label')
plt.title('t-SNE Visualization of Digits Dataset')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.grid(True)
plt.show()
```